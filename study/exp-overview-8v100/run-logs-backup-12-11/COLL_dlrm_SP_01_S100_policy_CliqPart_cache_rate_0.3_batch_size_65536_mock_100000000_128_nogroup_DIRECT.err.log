2022-12-12 07:21:59.316191: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.324726: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.336636: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.342741: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.352697: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.358422: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.374520: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.383730: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.397814: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.399748: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.401404: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.402867: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.404470: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.406066: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.407206: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.408288: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.417499: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.418659: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.420141: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.421592: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.421624: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.423115: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.423215: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.424585: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.424730: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.426013: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.426335: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.427772: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.427911: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.429416: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.429488: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.431223: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.431368: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.433448: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.433820: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.434663: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.435619: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.437170: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.437444: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.437517: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.439552: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.439667: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.439909: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.442091: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.442234: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.442638: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.442951: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.444519: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.444761: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.445400: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.445858: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.446273: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.446765: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.447509: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.448317: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.448872: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.449747: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.450110: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.451069: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.451749: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.452491: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.452861: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.453077: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.453177: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.455294: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.455989: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.456986: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.457274: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.457340: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.457854: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.460888: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.462517: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.462624: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.462663: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.463388: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.465701: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.465895: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.465978: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.466639: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.468534: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.468689: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.468972: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.469645: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.471184: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.471290: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.472406: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.473098: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.473588: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.474466: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.475584: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.475913: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.476751: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.477749: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.477981: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.478946: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.479589: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.479977: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.480250: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.481507: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.482355: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.482530: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.482778: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.484204: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.485116: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.485148: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.485346: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.486990: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.487782: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.487910: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.488060: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.489709: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.490402: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.490602: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.490649: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.492418: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.493187: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.493327: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.493373: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.495751: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.495871: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.495901: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.498025: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.498129: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.499000: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.499600: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 07:21:59.499968: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.500008: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.501192: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.502010: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.502143: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.503416: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.504741: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.504792: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.506008: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.506852: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.506944: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.508295: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.509039: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.509105: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.509120: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.509227: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.510548: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.511217: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.512391: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.512675: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.512876: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.514012: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.514064: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.514697: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.515186: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.515810: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.516138: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.516374: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.517901: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.518002: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.518109: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.518847: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.519600: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.520255: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.520572: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.520866: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.522862: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.523057: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.524387: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.525193: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.525388: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.525782: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.526926: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.527156: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.528043: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 07:21:59.528387: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.529210: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.529404: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.529873: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.531006: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.531289: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.532508: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.533449: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.533548: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.533995: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.535231: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.535490: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.536644: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.537723: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.537758: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.538111: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.538372: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.539812: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.540013: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.541475: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.542631: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.542817: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.543213: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.543529: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.544999: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.545307: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.546893: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.547744: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.548309: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.548619: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.548920: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.550196: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.551935: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.552786: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.553095: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.554067: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.554377: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.554451: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 07:21:59.556848: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.557891: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.559010: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.560007: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.560280: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.561967: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.562986: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.563325: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.564901: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.564974: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.566130: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.566988: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.567521: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.568070: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 07:21:59.578769: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.581314: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.581523: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.582969: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.583973: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.584393: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.584869: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.586351: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.586486: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.587904: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.589318: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.589649: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.590124: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.591826: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.627604: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.629428: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.629820: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.630458: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.632482: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.635014: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.635551: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.636314: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.638615: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.641162: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.641695: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.644788: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.645033: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.646599: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.647141: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.649777: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.650016: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.667466: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.668846: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.671347: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.671614: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.673863: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.674486: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.677021: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.677295: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.678663: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.679585: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.711248: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.711400: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.712874: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.714317: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.717561: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.717665: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.722807: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.722838: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.723800: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 07:21:59.724103: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 07:21:59.729912: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.733507: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 07:21:59.734362: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.734839: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.743470: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.748105: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.748173: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.749934: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.751014: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 07:21:59.760943: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.782803: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.782864: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.784040: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.816397: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:21:59.823386: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:22:01.106372: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:22:01.106991: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:22:01.107552: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:22:01.108202: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 07:22:01.108259: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-12 07:22:01.125209: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:22:01.126258: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:22:01.126775: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:22:01.127381: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:22:01.128467: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:22:01.129152: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-12 07:22:01.150548: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:22:01.151203: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:22:01.151749: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:22:01.152392: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 07:22:01.152447: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-12 07:22:01.169799: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:22:01.170446: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:22:01.170955: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:22:01.171579: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:22:01.172112: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:22:01.172809: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:1/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-12 07:22:01.175065: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 07:22:01.175235: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 07:22:01.200583: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:22:01.201199: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:22:01.201715: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:22:01.202186: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 07:22:01.202236: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-12 07:22:01.203641: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12340
2022-12-12 07:22:01.204386: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:22:01.204991: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:22:01.205535: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:22:01.206012: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 07:22:01.206065: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-12 07:22:01.219875: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:22:01.220502: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:22:01.221013: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:22:01.221582: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:22:01.222101: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:22:01.222741: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:3/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-12 07:22:01.223396: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:22:01.224045: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:22:01.224555: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:22:01.225370: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:22:01.225889: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:22:01.226367: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:5/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-12 07:22:01.250135: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 07:22:01.250325: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 07:22:01.252207: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12341
2022-12-12 07:22:01.264097: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:22:01.264794: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:22:01.265331: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:22:01.266011: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 07:22:01.266065: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-12 07:22:01.269575: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 07:22:01.269729: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 07:22:01.271644: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12343
2022-12-12 07:22:01.271724: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 07:22:01.271902: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 07:22:01.273734: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12345
2022-12-12 07:22:01.277606: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:22:01.277606: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:22:01.278755: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:22:01.278756: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:22:01.279761: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:22:01.279809: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:22:01.280651: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 07:22:01.280700: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-12 07:22:01.280729: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 07:22:01.280774: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-12 07:22:01.284063: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:22:01.284672: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:22:01.285193: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:22:01.285760: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:22:01.286282: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:22:01.286760: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:2/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-12 07:22:01.287540: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:22:01.288136: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:22:01.288659: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:22:01.289135: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 07:22:01.289177: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-12 07:22:01.297406: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:22:01.297597: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:22:01.298404: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:22:01.298571: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:22:01.299299: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:22:01.299507: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:22:01.300286: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:22:01.300464: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:22:01.301164: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:22:01.301367: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:22:01.302029: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:6/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-12 07:22:01.302189: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:7/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-12 07:22:01.306713: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:22:01.307362: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:22:01.307874: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:22:01.308455: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:22:01.308970: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:22:01.309442: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:4/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-12 07:22:01.345773: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 07:22:01.345977: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 07:22:01.346290: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 07:22:01.346429: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 07:22:01.347781: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12347
2022-12-12 07:22:01.348223: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12346
2022-12-12 07:22:01.355170: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 07:22:01.355327: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 07:22:01.357167: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12344
2022-12-12 07:22:01.395370: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 07:22:01.395560: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 07:22:01.398430: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12342
[HCTR][07:22:02.679][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][07:22:02.679][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][07:22:02.679][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][07:22:02.679][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][07:22:02.679][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][07:22:02.679][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][07:22:02.679][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][07:22:02.679][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 1it [00:01,  1.53s/it]warmup run: 1it [00:01,  1.54s/it]warmup run: 1it [00:01,  1.52s/it]warmup run: 1it [00:01,  1.52s/it]warmup run: 1it [00:01,  1.53s/it]warmup run: 1it [00:01,  1.51s/it]warmup run: 1it [00:01,  1.52s/it]warmup run: 1it [00:01,  1.47s/it]warmup run: 99it [00:01, 84.34it/s]warmup run: 91it [00:01, 77.13it/s]warmup run: 96it [00:01, 82.04it/s]warmup run: 97it [00:01, 82.82it/s]warmup run: 99it [00:01, 84.51it/s]warmup run: 98it [00:01, 84.04it/s]warmup run: 97it [00:01, 83.36it/s]warmup run: 100it [00:01, 88.19it/s]warmup run: 197it [00:01, 181.72it/s]warmup run: 182it [00:01, 167.22it/s]warmup run: 192it [00:01, 177.66it/s]warmup run: 197it [00:01, 182.72it/s]warmup run: 197it [00:01, 181.96it/s]warmup run: 196it [00:01, 182.07it/s]warmup run: 194it [00:01, 180.40it/s]warmup run: 201it [00:01, 191.57it/s]warmup run: 297it [00:01, 291.75it/s]warmup run: 274it [00:01, 267.72it/s]warmup run: 289it [00:01, 284.13it/s]warmup run: 297it [00:01, 292.64it/s]warmup run: 292it [00:01, 285.14it/s]warmup run: 295it [00:01, 291.15it/s]warmup run: 291it [00:01, 286.84it/s]warmup run: 303it [00:01, 305.76it/s]warmup run: 396it [00:01, 403.81it/s]warmup run: 365it [00:01, 370.71it/s]warmup run: 386it [00:01, 394.28it/s]warmup run: 397it [00:01, 406.31it/s]warmup run: 390it [00:01, 396.61it/s]warmup run: 394it [00:01, 404.12it/s]warmup run: 388it [00:01, 397.31it/s]warmup run: 404it [00:01, 421.46it/s]warmup run: 494it [00:02, 510.92it/s]warmup run: 456it [00:02, 470.48it/s]warmup run: 484it [00:02, 503.33it/s]warmup run: 497it [00:02, 516.99it/s]warmup run: 488it [00:02, 505.64it/s]warmup run: 493it [00:02, 513.72it/s]warmup run: 486it [00:02, 506.09it/s]warmup run: 505it [00:01, 532.87it/s]warmup run: 593it [00:02, 610.91it/s]warmup run: 548it [00:02, 563.90it/s]warmup run: 583it [00:02, 605.73it/s]warmup run: 598it [00:02, 619.76it/s]warmup run: 589it [00:02, 610.62it/s]warmup run: 594it [00:02, 617.31it/s]warmup run: 585it [00:02, 608.24it/s]warmup run: 608it [00:02, 638.32it/s]warmup run: 693it [00:02, 699.51it/s]warmup run: 682it [00:02, 694.39it/s]warmup run: 643it [00:02, 651.63it/s]warmup run: 699it [00:02, 708.50it/s]warmup run: 688it [00:02, 697.13it/s]warmup run: 694it [00:02, 705.83it/s]warmup run: 685it [00:02, 698.55it/s]warmup run: 709it [00:02, 724.96it/s]warmup run: 793it [00:02, 773.27it/s]warmup run: 782it [00:02, 768.52it/s]warmup run: 740it [00:02, 728.70it/s]warmup run: 786it [00:02, 767.21it/s]warmup run: 801it [00:02, 784.26it/s]warmup run: 796it [00:02, 782.47it/s]warmup run: 784it [00:02, 770.67it/s]warmup run: 812it [00:02, 799.95it/s]warmup run: 893it [00:02, 831.04it/s]warmup run: 881it [00:02, 825.10it/s]warmup run: 837it [00:02, 789.82it/s]warmup run: 903it [00:02, 845.57it/s]warmup run: 885it [00:02, 823.91it/s]warmup run: 897it [00:02, 841.14it/s]warmup run: 883it [00:02, 827.41it/s]warmup run: 912it [00:02, 849.06it/s]warmup run: 994it [00:02, 878.98it/s]warmup run: 933it [00:02, 835.60it/s]warmup run: 980it [00:02, 867.45it/s]warmup run: 1005it [00:02, 890.81it/s]warmup run: 984it [00:02, 866.75it/s]warmup run: 998it [00:02, 885.33it/s]warmup run: 982it [00:02, 870.26it/s]warmup run: 1012it [00:02, 886.45it/s]warmup run: 1096it [00:02, 917.80it/s]warmup run: 1029it [00:02, 868.66it/s]warmup run: 1079it [00:02, 900.76it/s]warmup run: 1108it [00:02, 927.96it/s]warmup run: 1085it [00:02, 905.66it/s]warmup run: 1099it [00:02, 918.84it/s]warmup run: 1081it [00:02, 903.23it/s]warmup run: 1113it [00:02, 918.66it/s]warmup run: 1196it [00:02, 940.23it/s]warmup run: 1124it [00:02, 891.25it/s]warmup run: 1178it [00:02, 925.04it/s]warmup run: 1210it [00:02, 953.21it/s]warmup run: 1185it [00:02, 930.34it/s]warmup run: 1199it [00:02, 941.65it/s]warmup run: 1180it [00:02, 927.25it/s]warmup run: 1213it [00:02, 940.24it/s]warmup run: 1277it [00:02, 943.19it/s]warmup run: 1220it [00:02, 907.51it/s]warmup run: 1296it [00:02, 944.52it/s]warmup run: 1313it [00:02, 973.10it/s]warmup run: 1285it [00:02, 950.11it/s]warmup run: 1301it [00:02, 963.16it/s]warmup run: 1280it [00:02, 945.62it/s]warmup run: 1313it [00:02, 957.06it/s]warmup run: 1377it [00:02, 957.31it/s]warmup run: 1321it [00:02, 936.68it/s]warmup run: 1395it [00:02, 947.86it/s]warmup run: 1416it [00:02, 988.12it/s]warmup run: 1386it [00:02, 966.22it/s]warmup run: 1404it [00:02, 981.56it/s]warmup run: 1379it [00:02, 958.18it/s]warmup run: 1414it [00:02, 970.13it/s]warmup run: 1477it [00:03, 969.16it/s]warmup run: 1423it [00:03, 959.81it/s]warmup run: 1493it [00:03, 952.34it/s]warmup run: 1519it [00:03, 997.90it/s]warmup run: 1488it [00:03, 979.61it/s]warmup run: 1506it [00:03, 990.78it/s]warmup run: 1480it [00:03, 973.05it/s]warmup run: 1515it [00:02, 980.55it/s]warmup run: 1577it [00:03, 977.54it/s]warmup run: 1524it [00:03, 974.48it/s]warmup run: 1591it [00:03, 947.25it/s]warmup run: 1622it [00:03, 1004.80it/s]warmup run: 1590it [00:03, 989.23it/s]warmup run: 1582it [00:03, 986.30it/s]warmup run: 1608it [00:03, 991.71it/s]warmup run: 1616it [00:03, 988.24it/s]warmup run: 1677it [00:03, 983.51it/s]warmup run: 1624it [00:03, 981.31it/s]warmup run: 1688it [00:03, 949.99it/s]warmup run: 1691it [00:03, 995.01it/s]warmup run: 1725it [00:03, 1010.01it/s]warmup run: 1684it [00:03, 994.48it/s]warmup run: 1709it [00:03, 992.86it/s]warmup run: 1717it [00:03, 992.74it/s]warmup run: 1725it [00:03, 989.06it/s]warmup run: 1778it [00:03, 988.39it/s]warmup run: 1785it [00:03, 954.89it/s]warmup run: 1792it [00:03, 998.68it/s]warmup run: 1827it [00:03, 1011.07it/s]warmup run: 1786it [00:03, 999.76it/s]warmup run: 1810it [00:03, 994.26it/s]warmup run: 1818it [00:03, 993.81it/s]warmup run: 1827it [00:03, 995.81it/s]warmup run: 1879it [00:03, 991.88it/s]warmup run: 1882it [00:03, 956.02it/s]warmup run: 1894it [00:03, 1002.24it/s]warmup run: 1929it [00:03, 1009.60it/s]warmup run: 1887it [00:03, 998.55it/s]warmup run: 1911it [00:03, 995.86it/s]warmup run: 1919it [00:03, 991.50it/s]warmup run: 1928it [00:03, 998.41it/s]warmup run: 1979it [00:03, 992.12it/s]warmup run: 1979it [00:03, 955.54it/s]warmup run: 1996it [00:03, 1005.65it/s]warmup run: 2036it [00:03, 1027.34it/s]warmup run: 1988it [00:03, 1000.11it/s]warmup run: 2012it [00:03, 999.05it/s]warmup run: 2024it [00:03, 1007.78it/s]warmup run: 2034it [00:03, 1015.12it/s]warmup run: 2093it [00:03, 1036.06it/s]warmup run: 2092it [00:03, 1005.34it/s]warmup run: 2114it [00:03, 1057.06it/s]warmup run: 2157it [00:03, 1080.95it/s]warmup run: 2108it [00:03, 1058.03it/s]warmup run: 2133it [00:03, 1060.01it/s]warmup run: 2145it [00:03, 1066.89it/s]warmup run: 2153it [00:03, 1065.74it/s]warmup run: 2213it [00:03, 1082.47it/s]warmup run: 2211it [00:03, 1057.71it/s]warmup run: 2235it [00:03, 1100.08it/s]warmup run: 2278it [00:03, 1118.16it/s]warmup run: 2230it [00:03, 1106.15it/s]warmup run: 2254it [00:03, 1103.86it/s]warmup run: 2267it [00:03, 1109.77it/s]warmup run: 2272it [00:03, 1101.44it/s]warmup run: 2333it [00:03, 1115.23it/s]warmup run: 2330it [00:03, 1094.86it/s]warmup run: 2356it [00:03, 1130.79it/s]warmup run: 2399it [00:03, 1143.91it/s]warmup run: 2352it [00:03, 1137.72it/s]warmup run: 2375it [00:03, 1135.44it/s]warmup run: 2389it [00:03, 1141.81it/s]warmup run: 2391it [00:03, 1126.02it/s]warmup run: 2453it [00:03, 1138.31it/s]warmup run: 2440it [00:03, 1092.03it/s]warmup run: 2520it [00:03, 1163.28it/s]warmup run: 2477it [00:03, 1152.33it/s]warmup run: 2473it [00:03, 1159.05it/s]warmup run: 2497it [00:03, 1158.55it/s]warmup run: 2511it [00:03, 1164.22it/s]warmup run: 2511it [00:04, 1147.23it/s]warmup run: 2573it [00:04, 1155.43it/s]warmup run: 2559it [00:04, 1119.58it/s]warmup run: 2597it [00:04, 1166.01it/s]warmup run: 2640it [00:04, 1173.11it/s]warmup run: 2595it [00:04, 1174.93it/s]warmup run: 2618it [00:04, 1173.76it/s]warmup run: 2632it [00:03, 1175.58it/s]warmup run: 2630it [00:04, 1159.06it/s]warmup run: 2692it [00:04, 1164.17it/s]warmup run: 2676it [00:04, 1134.55it/s]warmup run: 2717it [00:04, 1175.66it/s]warmup run: 2761it [00:04, 1183.61it/s]warmup run: 2716it [00:04, 1182.65it/s]warmup run: 2738it [00:04, 1179.39it/s]warmup run: 2754it [00:04, 1188.21it/s]warmup run: 2750it [00:04, 1171.00it/s]warmup run: 2812it [00:04, 1172.50it/s]warmup run: 2795it [00:04, 1148.74it/s]warmup run: 2882it [00:04, 1190.60it/s]warmup run: 2837it [00:04, 1180.22it/s]warmup run: 2838it [00:04, 1191.02it/s]warmup run: 2859it [00:04, 1187.87it/s]warmup run: 2876it [00:04, 1196.47it/s]warmup run: 2870it [00:04, 1178.35it/s]warmup run: 2933it [00:04, 1183.16it/s]warmup run: 2913it [00:04, 1157.50it/s]warmup run: 2957it [00:04, 1185.94it/s]warmup run: 3000it [00:04, 690.65it/s] warmup run: 2959it [00:04, 1193.85it/s]warmup run: 2980it [00:04, 1192.28it/s]warmup run: 3000it [00:04, 681.70it/s] warmup run: 3000it [00:04, 684.56it/s] warmup run: 3000it [00:04, 688.36it/s] warmup run: 2997it [00:04, 1197.77it/s]warmup run: 3000it [00:04, 675.80it/s] warmup run: 3000it [00:04, 697.78it/s] warmup run: 2989it [00:04, 1179.49it/s]warmup run: 3000it [00:04, 685.88it/s] warmup run: 3000it [00:04, 671.47it/s] 


warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]




warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   5%|▌         | 160/3000 [00:00<00:01, 1598.54it/s]warmup should be done:   6%|▌         | 165/3000 [00:00<00:01, 1646.45it/s]warmup should be done:   5%|▌         | 162/3000 [00:00<00:01, 1614.82it/s]warmup should be done:   6%|▌         | 169/3000 [00:00<00:01, 1683.35it/s]warmup should be done:   5%|▌         | 163/3000 [00:00<00:01, 1622.37it/s]warmup should be done:   6%|▌         | 165/3000 [00:00<00:01, 1640.32it/s]warmup should be done:   5%|▌         | 162/3000 [00:00<00:01, 1611.17it/s]warmup should be done:   6%|▌         | 165/3000 [00:00<00:01, 1640.63it/s]warmup should be done:  11%|█▏        | 338/3000 [00:00<00:01, 1687.03it/s]warmup should be done:  11%|█         | 322/3000 [00:00<00:01, 1608.77it/s]warmup should be done:  11%|█         | 330/3000 [00:00<00:01, 1645.35it/s]warmup should be done:  11%|█         | 330/3000 [00:00<00:01, 1644.77it/s]warmup should be done:  11%|█         | 325/3000 [00:00<00:01, 1621.55it/s]warmup should be done:  11%|█         | 324/3000 [00:00<00:01, 1611.84it/s]warmup should be done:  11%|█         | 331/3000 [00:00<00:01, 1648.88it/s]warmup should be done:  11%|█         | 328/3000 [00:00<00:01, 1633.97it/s]warmup should be done:  16%|█▌        | 483/3000 [00:00<00:01, 1605.81it/s]warmup should be done:  17%|█▋        | 497/3000 [00:00<00:01, 1653.25it/s]warmup should be done:  16%|█▋        | 495/3000 [00:00<00:01, 1643.15it/s]warmup should be done:  16%|█▋        | 492/3000 [00:00<00:01, 1634.11it/s]warmup should be done:  16%|█▋        | 495/3000 [00:00<00:01, 1641.71it/s]warmup should be done:  17%|█▋        | 507/3000 [00:00<00:01, 1679.93it/s]warmup should be done:  16%|█▌        | 486/3000 [00:00<00:01, 1610.36it/s]warmup should be done:  16%|█▋        | 488/3000 [00:00<00:01, 1610.12it/s]warmup should be done:  22%|██▏       | 664/3000 [00:00<00:01, 1658.48it/s]warmup should be done:  21%|██▏       | 644/3000 [00:00<00:01, 1604.57it/s]warmup should be done:  22%|██▏       | 657/3000 [00:00<00:01, 1639.57it/s]warmup should be done:  22%|██▎       | 675/3000 [00:00<00:01, 1678.70it/s]warmup should be done:  22%|██▏       | 660/3000 [00:00<00:01, 1641.60it/s]warmup should be done:  22%|██▏       | 660/3000 [00:00<00:01, 1637.43it/s]warmup should be done:  22%|██▏       | 648/3000 [00:00<00:01, 1607.94it/s]warmup should be done:  22%|██▏       | 650/3000 [00:00<00:01, 1608.77it/s]warmup should be done:  27%|██▋       | 822/3000 [00:00<00:01, 1642.66it/s]warmup should be done:  28%|██▊       | 830/3000 [00:00<00:01, 1656.70it/s]warmup should be done:  27%|██▋       | 805/3000 [00:00<00:01, 1603.28it/s]warmup should be done:  28%|██▊       | 843/3000 [00:00<00:01, 1677.02it/s]warmup should be done:  28%|██▊       | 825/3000 [00:00<00:01, 1638.78it/s]warmup should be done:  27%|██▋       | 810/3000 [00:00<00:01, 1609.04it/s]warmup should be done:  28%|██▊       | 825/3000 [00:00<00:01, 1635.27it/s]warmup should be done:  27%|██▋       | 811/3000 [00:00<00:01, 1607.15it/s]warmup should be done:  33%|███▎      | 987/3000 [00:00<00:01, 1644.01it/s]warmup should be done:  33%|███▎      | 996/3000 [00:00<00:01, 1653.01it/s]warmup should be done:  34%|███▎      | 1011/3000 [00:00<00:01, 1671.88it/s]warmup should be done:  32%|███▏      | 966/3000 [00:00<00:01, 1597.05it/s]warmup should be done:  33%|███▎      | 989/3000 [00:00<00:01, 1634.61it/s]warmup should be done:  32%|███▏      | 971/3000 [00:00<00:01, 1604.34it/s]warmup should be done:  32%|███▏      | 972/3000 [00:00<00:01, 1600.94it/s]warmup should be done:  33%|███▎      | 989/3000 [00:00<00:01, 1617.65it/s]warmup should be done:  38%|███▊      | 1152/3000 [00:00<00:01, 1642.04it/s]warmup should be done:  39%|███▊      | 1162/3000 [00:00<00:01, 1654.18it/s]warmup should be done:  39%|███▉      | 1179/3000 [00:00<00:01, 1670.57it/s]warmup should be done:  38%|███▊      | 1133/3000 [00:00<00:01, 1609.02it/s]warmup should be done:  38%|███▊      | 1126/3000 [00:00<00:01, 1593.22it/s]warmup should be done:  38%|███▊      | 1153/3000 [00:00<00:01, 1632.63it/s]warmup should be done:  38%|███▊      | 1133/3000 [00:00<00:01, 1599.58it/s]warmup should be done:  38%|███▊      | 1151/3000 [00:00<00:01, 1608.32it/s]warmup should be done:  44%|████▍     | 1317/3000 [00:00<00:01, 1642.14it/s]warmup should be done:  44%|████▍     | 1328/3000 [00:00<00:01, 1653.04it/s]warmup should be done:  43%|████▎     | 1296/3000 [00:00<00:01, 1615.37it/s]warmup should be done:  45%|████▍     | 1347/3000 [00:00<00:00, 1670.08it/s]warmup should be done:  43%|████▎     | 1286/3000 [00:00<00:01, 1590.27it/s]warmup should be done:  44%|████▍     | 1317/3000 [00:00<00:01, 1627.36it/s]warmup should be done:  43%|████▎     | 1293/3000 [00:00<00:01, 1596.58it/s]warmup should be done:  44%|████▎     | 1312/3000 [00:00<00:01, 1598.48it/s]warmup should be done:  49%|████▉     | 1482/3000 [00:00<00:00, 1642.83it/s]warmup should be done:  50%|████▉     | 1494/3000 [00:00<00:00, 1652.87it/s]warmup should be done:  49%|████▊     | 1458/3000 [00:00<00:00, 1616.18it/s]warmup should be done:  50%|█████     | 1515/3000 [00:00<00:00, 1668.23it/s]warmup should be done:  48%|████▊     | 1446/3000 [00:00<00:00, 1589.69it/s]warmup should be done:  48%|████▊     | 1455/3000 [00:00<00:00, 1602.93it/s]warmup should be done:  49%|████▉     | 1480/3000 [00:00<00:00, 1623.06it/s]warmup should be done:  49%|████▉     | 1472/3000 [00:00<00:00, 1590.70it/s]warmup should be done:  55%|█████▍    | 1647/3000 [00:01<00:00, 1644.61it/s]warmup should be done:  55%|█████▌    | 1660/3000 [00:01<00:00, 1653.92it/s]warmup should be done:  54%|█████▍    | 1622/3000 [00:01<00:00, 1622.37it/s]warmup should be done:  56%|█████▌    | 1682/3000 [00:01<00:00, 1667.75it/s]warmup should be done:  54%|█████▎    | 1606/3000 [00:01<00:00, 1592.13it/s]warmup should be done:  55%|█████▍    | 1644/3000 [00:01<00:00, 1627.68it/s]warmup should be done:  54%|█████▍    | 1618/3000 [00:01<00:00, 1609.36it/s]warmup should be done:  54%|█████▍    | 1632/3000 [00:01<00:00, 1589.05it/s]warmup should be done:  60%|█████▉    | 1785/3000 [00:01<00:00, 1623.33it/s]warmup should be done:  62%|██████▏   | 1849/3000 [00:01<00:00, 1667.31it/s]warmup should be done:  60%|██████    | 1812/3000 [00:01<00:00, 1625.35it/s]warmup should be done:  60%|██████    | 1808/3000 [00:01<00:00, 1628.80it/s]warmup should be done:  59%|█████▉    | 1781/3000 [00:01<00:00, 1613.27it/s]warmup should be done:  61%|██████    | 1826/3000 [00:01<00:00, 1635.99it/s]warmup should be done:  59%|█████▉    | 1766/3000 [00:01<00:00, 1580.74it/s]warmup should be done:  60%|█████▉    | 1791/3000 [00:01<00:00, 1587.86it/s]warmup should be done:  67%|██████▋   | 2016/3000 [00:01<00:00, 1667.39it/s]warmup should be done:  65%|██████▍   | 1948/3000 [00:01<00:00, 1617.04it/s]warmup should be done:  65%|██████▍   | 1944/3000 [00:01<00:00, 1616.75it/s]warmup should be done:  66%|██████▌   | 1973/3000 [00:01<00:00, 1632.25it/s]warmup should be done:  64%|██████▍   | 1925/3000 [00:01<00:00, 1582.95it/s]warmup should be done:  66%|██████▋   | 1990/3000 [00:01<00:00, 1625.44it/s]warmup should be done:  66%|██████▌   | 1975/3000 [00:01<00:00, 1607.96it/s]warmup should be done:  65%|██████▌   | 1953/3000 [00:01<00:00, 1597.31it/s]warmup should be done:  73%|███████▎  | 2183/3000 [00:01<00:00, 1667.05it/s]warmup should be done:  70%|███████   | 2110/3000 [00:01<00:00, 1613.32it/s]warmup should be done:  70%|███████   | 2107/3000 [00:01<00:00, 1620.45it/s]warmup should be done:  71%|███████   | 2137/3000 [00:01<00:00, 1634.55it/s]warmup should be done:  70%|██████▉   | 2085/3000 [00:01<00:00, 1585.69it/s]warmup should be done:  71%|███████   | 2137/3000 [00:01<00:00, 1610.39it/s]warmup should be done:  72%|███████▏  | 2153/3000 [00:01<00:00, 1619.06it/s]warmup should be done:  71%|███████   | 2117/3000 [00:01<00:00, 1608.27it/s]warmup should be done:  78%|███████▊  | 2350/3000 [00:01<00:00, 1664.95it/s]warmup should be done:  77%|███████▋  | 2301/3000 [00:01<00:00, 1634.11it/s]warmup should be done:  76%|███████▌  | 2270/3000 [00:01<00:00, 1619.73it/s]warmup should be done:  76%|███████▌  | 2272/3000 [00:01<00:00, 1608.33it/s]warmup should be done:  75%|███████▍  | 2244/3000 [00:01<00:00, 1585.29it/s]warmup should be done:  77%|███████▋  | 2299/3000 [00:01<00:00, 1612.97it/s]warmup should be done:  77%|███████▋  | 2315/3000 [00:01<00:00, 1612.79it/s]warmup should be done:  76%|███████▌  | 2282/3000 [00:01<00:00, 1618.82it/s]warmup should be done:  84%|████████▍ | 2517/3000 [00:01<00:00, 1666.16it/s]warmup should be done:  82%|████████▏ | 2465/3000 [00:01<00:00, 1633.27it/s]warmup should be done:  81%|████████  | 2433/3000 [00:01<00:00, 1607.08it/s]warmup should be done:  80%|████████  | 2403/3000 [00:01<00:00, 1586.54it/s]warmup should be done:  81%|████████  | 2432/3000 [00:01<00:00, 1605.23it/s]warmup should be done:  82%|████████▏ | 2461/3000 [00:01<00:00, 1612.38it/s]warmup should be done:  83%|████████▎ | 2477/3000 [00:01<00:00, 1610.12it/s]warmup should be done:  82%|████████▏ | 2448/3000 [00:01<00:00, 1629.01it/s]warmup should be done:  89%|████████▉ | 2684/3000 [00:01<00:00, 1664.60it/s]warmup should be done:  88%|████████▊ | 2629/3000 [00:01<00:00, 1630.17it/s]warmup should be done:  86%|████████▋ | 2594/3000 [00:01<00:00, 1600.81it/s]warmup should be done:  85%|████████▌ | 2562/3000 [00:01<00:00, 1581.82it/s]warmup should be done:  87%|████████▋ | 2623/3000 [00:01<00:00, 1613.28it/s]warmup should be done:  86%|████████▋ | 2593/3000 [00:01<00:00, 1601.50it/s]warmup should be done:  88%|████████▊ | 2639/3000 [00:01<00:00, 1608.43it/s]warmup should be done:  87%|████████▋ | 2614/3000 [00:01<00:00, 1636.64it/s]warmup should be done:  95%|█████████▌| 2851/3000 [00:01<00:00, 1660.73it/s]warmup should be done:  93%|█████████▎| 2793/3000 [00:01<00:00, 1621.72it/s]warmup should be done:  93%|█████████▎| 2785/3000 [00:01<00:00, 1612.38it/s]warmup should be done:  92%|█████████▏| 2755/3000 [00:01<00:00, 1591.25it/s]warmup should be done:  91%|█████████ | 2721/3000 [00:01<00:00, 1574.62it/s]warmup should be done:  92%|█████████▏| 2754/3000 [00:01<00:00, 1599.58it/s]warmup should be done:  93%|█████████▎| 2780/3000 [00:01<00:00, 1642.85it/s]warmup should be done:  93%|█████████▎| 2800/3000 [00:01<00:00, 1606.02it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1668.58it/s]warmup should be done:  99%|█████████▊| 2959/3000 [00:01<00:00, 1631.19it/s]warmup should be done:  96%|█████████▌| 2883/3000 [00:01<00:00, 1587.13it/s]warmup should be done:  97%|█████████▋| 2916/3000 [00:01<00:00, 1595.03it/s]warmup should be done:  98%|█████████▊| 2951/3000 [00:01<00:00, 1623.67it/s]warmup should be done:  97%|█████████▋| 2914/3000 [00:01<00:00, 1592.03it/s]warmup should be done:  98%|█████████▊| 2948/3000 [00:01<00:00, 1652.83it/s]warmup should be done:  99%|█████████▉| 2964/3000 [00:01<00:00, 1613.45it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1632.10it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1631.22it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1626.87it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1623.99it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1607.40it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1605.93it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1590.85it/s]







warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   6%|▌         | 167/3000 [00:00<00:01, 1669.13it/s]warmup should be done:   6%|▌         | 165/3000 [00:00<00:01, 1649.27it/s]warmup should be done:   6%|▌         | 168/3000 [00:00<00:01, 1677.19it/s]warmup should be done:   6%|▌         | 170/3000 [00:00<00:01, 1696.53it/s]warmup should be done:   6%|▌         | 165/3000 [00:00<00:01, 1643.20it/s]warmup should be done:   6%|▌         | 166/3000 [00:00<00:01, 1652.72it/s]warmup should be done:   6%|▌         | 171/3000 [00:00<00:01, 1702.67it/s]warmup should be done:   5%|▌         | 164/3000 [00:00<00:01, 1632.96it/s]warmup should be done:  11%|█         | 334/3000 [00:00<00:01, 1664.37it/s]warmup should be done:  11%|█         | 337/3000 [00:00<00:01, 1679.08it/s]warmup should be done:  11%|█         | 333/3000 [00:00<00:01, 1657.78it/s]warmup should be done:  11%|█         | 331/3000 [00:00<00:01, 1647.68it/s]warmup should be done:  11%|█▏        | 343/3000 [00:00<00:01, 1707.01it/s]warmup should be done:  11%|█         | 328/3000 [00:00<00:01, 1619.16it/s]warmup should be done:  11%|█         | 330/3000 [00:00<00:01, 1621.90it/s]warmup should be done:  11%|█▏        | 340/3000 [00:00<00:01, 1662.14it/s]warmup should be done:  17%|█▋        | 496/3000 [00:00<00:01, 1648.14it/s]warmup should be done:  17%|█▋        | 499/3000 [00:00<00:01, 1654.90it/s]warmup should be done:  17%|█▋        | 514/3000 [00:00<00:01, 1703.33it/s]warmup should be done:  17%|█▋        | 501/3000 [00:00<00:01, 1652.28it/s]warmup should be done:  17%|█▋        | 505/3000 [00:00<00:01, 1664.41it/s]warmup should be done:  16%|█▋        | 495/3000 [00:00<00:01, 1632.53it/s]warmup should be done:  17%|█▋        | 511/3000 [00:00<00:01, 1680.31it/s]warmup should be done:  16%|█▋        | 490/3000 [00:00<00:01, 1602.91it/s]warmup should be done:  22%|██▏       | 662/3000 [00:00<00:01, 1652.39it/s]warmup should be done:  22%|██▏       | 667/3000 [00:00<00:01, 1664.20it/s]warmup should be done:  23%|██▎       | 686/3000 [00:00<00:01, 1708.56it/s]warmup should be done:  22%|██▏       | 668/3000 [00:00<00:01, 1655.94it/s]warmup should be done:  22%|██▏       | 673/3000 [00:00<00:01, 1667.90it/s]warmup should be done:  22%|██▏       | 659/3000 [00:00<00:01, 1632.68it/s]warmup should be done:  23%|██▎       | 683/3000 [00:00<00:01, 1693.44it/s]warmup should be done:  22%|██▏       | 652/3000 [00:00<00:01, 1606.82it/s]warmup should be done:  28%|██▊       | 834/3000 [00:00<00:01, 1665.88it/s]warmup should be done:  29%|██▊       | 857/3000 [00:00<00:01, 1707.60it/s]warmup should be done:  28%|██▊       | 838/3000 [00:00<00:01, 1670.87it/s]warmup should be done:  28%|██▊       | 828/3000 [00:00<00:01, 1640.37it/s]warmup should be done:  28%|██▊       | 843/3000 [00:00<00:01, 1676.68it/s]warmup should be done:  28%|██▊       | 854/3000 [00:00<00:01, 1699.23it/s]warmup should be done:  27%|██▋       | 815/3000 [00:00<00:01, 1613.61it/s]warmup should be done:  27%|██▋       | 823/3000 [00:00<00:01, 1621.27it/s]warmup should be done:  33%|███▎      | 1001/3000 [00:00<00:01, 1665.94it/s]warmup should be done:  34%|███▍      | 1028/3000 [00:00<00:01, 1707.10it/s]warmup should be done:  34%|███▎      | 1008/3000 [00:00<00:01, 1677.56it/s]warmup should be done:  33%|███▎      | 995/3000 [00:00<00:01, 1649.30it/s]warmup should be done:  34%|███▎      | 1011/3000 [00:00<00:01, 1673.50it/s]warmup should be done:  34%|███▍      | 1026/3000 [00:00<00:01, 1702.85it/s]warmup should be done:  33%|███▎      | 977/3000 [00:00<00:01, 1613.21it/s]warmup should be done:  33%|███▎      | 991/3000 [00:00<00:01, 1639.19it/s]warmup should be done:  39%|███▉      | 1168/3000 [00:00<00:01, 1664.34it/s]warmup should be done:  40%|███▉      | 1199/3000 [00:00<00:01, 1705.67it/s]warmup should be done:  39%|███▊      | 1162/3000 [00:00<00:01, 1654.55it/s]warmup should be done:  39%|███▉      | 1179/3000 [00:00<00:01, 1684.94it/s]warmup should be done:  40%|███▉      | 1198/3000 [00:00<00:01, 1706.27it/s]warmup should be done:  38%|███▊      | 1140/3000 [00:00<00:01, 1616.79it/s]warmup should be done:  39%|███▊      | 1160/3000 [00:00<00:01, 1652.87it/s]warmup should be done:  39%|███▉      | 1179/3000 [00:00<00:01, 1435.60it/s]warmup should be done:  44%|████▍     | 1335/3000 [00:00<00:00, 1666.04it/s]warmup should be done:  46%|████▌     | 1371/3000 [00:00<00:00, 1707.99it/s]warmup should be done:  44%|████▍     | 1328/3000 [00:00<00:01, 1653.40it/s]warmup should be done:  45%|████▌     | 1350/3000 [00:00<00:00, 1690.89it/s]warmup should be done:  46%|████▌     | 1370/3000 [00:00<00:00, 1710.28it/s]warmup should be done:  43%|████▎     | 1302/3000 [00:00<00:01, 1615.47it/s]warmup should be done:  44%|████▍     | 1327/3000 [00:00<00:01, 1657.62it/s]warmup should be done:  44%|████▍     | 1335/3000 [00:00<00:01, 1468.50it/s]warmup should be done:  51%|█████     | 1520/3000 [00:00<00:00, 1693.02it/s]warmup should be done:  51%|█████▏    | 1542/3000 [00:00<00:00, 1699.54it/s]warmup should be done:  50%|████▉     | 1494/3000 [00:00<00:00, 1652.92it/s]warmup should be done:  51%|█████▏    | 1542/3000 [00:00<00:00, 1711.36it/s]warmup should be done:  49%|████▉     | 1465/3000 [00:00<00:00, 1618.87it/s]warmup should be done:  50%|█████     | 1502/3000 [00:00<00:00, 1641.63it/s]warmup should be done:  50%|████▉     | 1495/3000 [00:00<00:00, 1662.72it/s]warmup should be done:  50%|█████     | 1500/3000 [00:00<00:00, 1519.07it/s]warmup should be done:  55%|█████▌    | 1661/3000 [00:01<00:00, 1657.93it/s]warmup should be done:  57%|█████▋    | 1714/3000 [00:01<00:00, 1711.65it/s]warmup should be done:  56%|█████▋    | 1690/3000 [00:01<00:00, 1682.85it/s]warmup should be done:  55%|█████▌    | 1663/3000 [00:01<00:00, 1667.94it/s]warmup should be done:  57%|█████▋    | 1712/3000 [00:01<00:00, 1685.51it/s]warmup should be done:  54%|█████▍    | 1627/3000 [00:01<00:00, 1611.99it/s]warmup should be done:  56%|█████▌    | 1667/3000 [00:01<00:00, 1625.88it/s]warmup should be done:  56%|█████▌    | 1668/3000 [00:01<00:00, 1564.44it/s]warmup should be done:  61%|██████    | 1830/3000 [00:01<00:00, 1667.28it/s]warmup should be done:  63%|██████▎   | 1887/3000 [00:01<00:00, 1714.58it/s]warmup should be done:  62%|██████▏   | 1861/3000 [00:01<00:00, 1690.51it/s]warmup should be done:  61%|██████    | 1832/3000 [00:01<00:00, 1672.06it/s]warmup should be done:  60%|█████▉    | 1790/3000 [00:01<00:00, 1615.72it/s]warmup should be done:  63%|██████▎   | 1881/3000 [00:01<00:00, 1675.85it/s]warmup should be done:  61%|██████    | 1830/3000 [00:01<00:00, 1614.77it/s]warmup should be done:  61%|██████    | 1837/3000 [00:01<00:00, 1600.52it/s]warmup should be done:  67%|██████▋   | 1999/3000 [00:01<00:00, 1671.48it/s]warmup should be done:  68%|██████▊   | 2032/3000 [00:01<00:00, 1695.00it/s]warmup should be done:  69%|██████▊   | 2060/3000 [00:01<00:00, 1716.76it/s]warmup should be done:  67%|██████▋   | 2000/3000 [00:01<00:00, 1672.08it/s]warmup should be done:  65%|██████▌   | 1952/3000 [00:01<00:00, 1616.42it/s]warmup should be done:  68%|██████▊   | 2049/3000 [00:01<00:00, 1668.68it/s]warmup should be done:  66%|██████▋   | 1992/3000 [00:01<00:00, 1604.60it/s]warmup should be done:  67%|██████▋   | 2006/3000 [00:01<00:00, 1624.59it/s]warmup should be done:  72%|███████▏  | 2168/3000 [00:01<00:00, 1674.58it/s]warmup should be done:  73%|███████▎  | 2202/3000 [00:01<00:00, 1696.09it/s]warmup should be done:  74%|███████▍  | 2232/3000 [00:01<00:00, 1716.82it/s]warmup should be done:  72%|███████▏  | 2168/3000 [00:01<00:00, 1673.36it/s]warmup should be done:  71%|███████   | 2118/3000 [00:01<00:00, 1628.40it/s]warmup should be done:  74%|███████▍  | 2216/3000 [00:01<00:00, 1661.55it/s]warmup should be done:  72%|███████▏  | 2153/3000 [00:01<00:00, 1598.50it/s]warmup should be done:  72%|███████▎  | 2175/3000 [00:01<00:00, 1641.65it/s]warmup should be done:  78%|███████▊  | 2337/3000 [00:01<00:00, 1676.67it/s]warmup should be done:  80%|████████  | 2404/3000 [00:01<00:00, 1714.83it/s]warmup should be done:  78%|███████▊  | 2336/3000 [00:01<00:00, 1673.23it/s]warmup should be done:  76%|███████▌  | 2281/3000 [00:01<00:00, 1625.12it/s]warmup should be done:  79%|███████▉  | 2372/3000 [00:01<00:00, 1680.48it/s]warmup should be done:  80%|███████▉  | 2385/3000 [00:01<00:00, 1668.29it/s]warmup should be done:  77%|███████▋  | 2313/3000 [00:01<00:00, 1597.95it/s]warmup should be done:  78%|███████▊  | 2345/3000 [00:01<00:00, 1656.53it/s]warmup should be done:  84%|████████▎ | 2506/3000 [00:01<00:00, 1678.64it/s]warmup should be done:  86%|████████▌ | 2576/3000 [00:01<00:00, 1715.49it/s]warmup should be done:  84%|████████▎ | 2505/3000 [00:01<00:00, 1675.64it/s]warmup should be done:  82%|████████▏ | 2446/3000 [00:01<00:00, 1631.49it/s]warmup should be done:  85%|████████▍ | 2543/3000 [00:01<00:00, 1687.96it/s]warmup should be done:  85%|████████▌ | 2555/3000 [00:01<00:00, 1675.71it/s]warmup should be done:  82%|████████▏ | 2473/3000 [00:01<00:00, 1595.75it/s]warmup should be done:  84%|████████▍ | 2514/3000 [00:01<00:00, 1666.31it/s]warmup should be done:  89%|████████▉ | 2674/3000 [00:01<00:00, 1677.17it/s]warmup should be done:  92%|█████████▏| 2749/3000 [00:01<00:00, 1716.97it/s]warmup should be done:  89%|████████▉ | 2673/3000 [00:01<00:00, 1675.36it/s]warmup should be done:  87%|████████▋ | 2612/3000 [00:01<00:00, 1637.02it/s]warmup should be done:  90%|█████████ | 2714/3000 [00:01<00:00, 1693.36it/s]warmup should be done:  91%|█████████ | 2724/3000 [00:01<00:00, 1677.56it/s]warmup should be done:  88%|████████▊ | 2634/3000 [00:01<00:00, 1597.72it/s]warmup should be done:  89%|████████▉ | 2682/3000 [00:01<00:00, 1647.38it/s]warmup should be done:  95%|█████████▍| 2842/3000 [00:01<00:00, 1672.81it/s]warmup should be done:  97%|█████████▋| 2921/3000 [00:01<00:00, 1717.66it/s]warmup should be done:  95%|█████████▍| 2841/3000 [00:01<00:00, 1676.09it/s]warmup should be done:  93%|█████████▎| 2778/3000 [00:01<00:00, 1642.95it/s]warmup should be done:  96%|█████████▌| 2884/3000 [00:01<00:00, 1695.28it/s]warmup should be done:  96%|█████████▋| 2892/3000 [00:01<00:00, 1671.24it/s]warmup should be done:  93%|█████████▎| 2801/3000 [00:01<00:00, 1617.88it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1708.96it/s]warmup should be done:  95%|█████████▍| 2848/3000 [00:01<00:00, 1646.54it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1685.75it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1684.46it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1663.28it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1662.44it/s]warmup should be done:  98%|█████████▊| 2945/3000 [00:01<00:00, 1650.82it/s]warmup should be done:  99%|█████████▉| 2969/3000 [00:01<00:00, 1634.32it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1631.57it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1627.09it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1616.67it/s]2022-12-12 07:23:38.654325: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f4b4382c6b0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 07:23:38.654384: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 07:23:39.097839: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f4b43795a30 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 07:23:39.097892: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 07:23:39.495447: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f4b4b833740 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 07:23:39.495520: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 07:23:39.503532: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f4b4aa8f190 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 07:23:39.503590: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 07:23:39.511792: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f2c3002d1c0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 07:23:39.511853: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 07:23:39.588517: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f2ba402d5a0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 07:23:39.588585: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 07:23:39.591585: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f2c2c02fd80 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 07:23:39.591631: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 07:23:39.599034: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f4b4b82bdf0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 07:23:39.599091: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 07:23:40.919713: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 07:23:41.341930: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 07:23:41.764672: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 07:23:41.811913: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 07:23:41.817507: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 07:23:41.898881: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 07:23:41.927471: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 07:23:41.951224: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 07:23:43.819105: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 07:23:44.316626: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 07:23:44.707172: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 07:23:44.811951: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 07:23:44.862946: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 07:23:44.919000: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 07:23:44.946872: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 07:23:44.980011: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
[HCTR][07:24:07.888][ERROR][RK0][tid #139961508558592]: replica 7 reaches 1000, calling init pre replica
[HCTR][07:24:07.888][ERROR][RK0][tid #139961508558592]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][07:24:07.894][ERROR][RK0][tid #139961508558592]: coll ps creation done
[HCTR][07:24:07.894][ERROR][RK0][tid #139961508558592]: replica 7 waits for coll ps creation barrier
[HCTR][07:24:07.927][ERROR][RK0][tid #139961441449728]: replica 6 reaches 1000, calling init pre replica
[HCTR][07:24:07.928][ERROR][RK0][tid #139961441449728]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][07:24:07.933][ERROR][RK0][tid #139961441449728]: coll ps creation done
[HCTR][07:24:07.933][ERROR][RK0][tid #139961441449728]: replica 6 waits for coll ps creation barrier
[HCTR][07:24:07.934][ERROR][RK0][main]: replica 0 reaches 1000, calling init pre replica
[HCTR][07:24:07.934][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][07:24:07.942][ERROR][RK0][main]: coll ps creation done
[HCTR][07:24:07.942][ERROR][RK0][main]: replica 0 waits for coll ps creation barrier
[HCTR][07:24:07.952][ERROR][RK0][main]: replica 1 reaches 1000, calling init pre replica
[HCTR][07:24:07.952][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][07:24:07.953][ERROR][RK0][main]: replica 4 reaches 1000, calling init pre replica
[HCTR][07:24:07.953][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][07:24:07.960][ERROR][RK0][main]: coll ps creation done
[HCTR][07:24:07.960][ERROR][RK0][main]: replica 1 waits for coll ps creation barrier
[HCTR][07:24:07.961][ERROR][RK0][main]: coll ps creation done
[HCTR][07:24:07.961][ERROR][RK0][main]: replica 4 waits for coll ps creation barrier
[HCTR][07:24:07.968][ERROR][RK0][main]: replica 3 reaches 1000, calling init pre replica
[HCTR][07:24:07.968][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][07:24:07.972][ERROR][RK0][main]: coll ps creation done
[HCTR][07:24:07.972][ERROR][RK0][main]: replica 3 waits for coll ps creation barrier
[HCTR][07:24:07.974][ERROR][RK0][main]: replica 5 reaches 1000, calling init pre replica
[HCTR][07:24:07.974][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][07:24:07.978][ERROR][RK0][main]: coll ps creation done
[HCTR][07:24:07.978][ERROR][RK0][main]: replica 5 waits for coll ps creation barrier
[HCTR][07:24:07.989][ERROR][RK0][main]: replica 2 reaches 1000, calling init pre replica
[HCTR][07:24:07.989][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][07:24:07.994][ERROR][RK0][main]: coll ps creation done
[HCTR][07:24:07.994][ERROR][RK0][main]: replica 2 waits for coll ps creation barrier
[HCTR][07:24:07.994][ERROR][RK0][main]: replica 0 preparing frequency
[HCTR][07:24:08.860][ERROR][RK0][main]: replica 0 preparing frequency done
[HCTR][07:24:08.907][ERROR][RK0][main]: replica 4 calling init per replica
[HCTR][07:24:08.907][ERROR][RK0][main]: replica 0 calling init per replica
[HCTR][07:24:08.907][ERROR][RK0][main]: replica 1 calling init per replica
[HCTR][07:24:08.907][ERROR][RK0][main]: replica 2 calling init per replica
[HCTR][07:24:08.907][ERROR][RK0][main]: replica 5 calling init per replica
[HCTR][07:24:08.907][ERROR][RK0][tid #139961508558592]: replica 7 calling init per replica
[HCTR][07:24:08.907][ERROR][RK0][main]: replica 3 calling init per replica
[HCTR][07:24:08.907][ERROR][RK0][tid #139961441449728]: replica 6 calling init per replica
[HCTR][07:24:08.907][ERROR][RK0][main]: Calling build_v2
[HCTR][07:24:08.907][ERROR][RK0][main]: Calling build_v2
[HCTR][07:24:08.907][ERROR][RK0][main]: Calling build_v2
[HCTR][07:24:08.907][ERROR][RK0][main]: Calling build_v2
[HCTR][07:24:08.907][ERROR][RK0][main]: Calling build_v2
[HCTR][07:24:08.907][ERROR][RK0][tid #139961508558592]: Calling build_v2
[HCTR][07:24:08.907][ERROR][RK0][main]: Calling build_v2
[HCTR][07:24:08.907][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][07:24:08.907][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][07:24:08.907][ERROR][RK0][tid #139961441449728]: Calling build_v2
[HCTR][07:24:08.907][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][07:24:08.907][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][07:24:08.907][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][07:24:08.907][ERROR][RK0][tid #139961508558592]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][07:24:08.907][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][07:24:08.907][ERROR][RK0][tid #139961441449728]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[2022-12-12 07:24:08.911905: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:178] [v100x8, slow pcie
2022-12-12 07:24:08[.2022-12-12 07:24:08911948.: 911982E:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc [:/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc178:] 196v100x8, slow pcie] 2022-12-12 07:24:08
assigning 0 to cpu.
911993: [E2022-12-12 07:24:08 ./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[912040:: 178E]  2022-12-12 07:24:08v100x8, slow pcie/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.[
:9120382022-12-12 07:24:08[196: .[] E9120772022-12-12 07:24:08assigning 0 to cpu2022-12-12 07:24:08 : .
./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE912103912085: : : 178/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccEE] :  v100x8, slow pcie212[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[
] ::2022-12-12 07:24:08build asymm link desc with 8X Tesla V100-SXM2-32GB out of 82022-12-12 07:24:08196178.[[
.] ] 9121692022-12-12 07:24:08912142assigning 0 to cpu[2022-12-12 07:24:08v100x8, slow pcie: .[: 
.
E9121962022-12-12 07:24:08E9121792022-12-12 07:24:08 : .[ : ./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE9122462022-12-12 07:24:08/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE912223[: : .: : 2022-12-12 07:24:08212/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE912293178/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE.] : : ] : 912327: build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8196/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccEv100x8, slow pcie178/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE
] : 
] : assigning 0 to cpu213/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[v100x8, slow pcie178/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[
] :2022-12-12 07:24:08
] :2022-12-12 07:24:08remote time is 8.68421196.v100x8, slow pcie212[.
] 912455
] 2022-12-12 07:24:08912458[assigning 0 to cpu: [build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8.[: 2022-12-12 07:24:08
E2022-12-12 07:24:08
9125072022-12-12 07:24:08E. .: .[ 912533/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc912542E9125482022-12-12 07:24:08[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: ::  : .2022-12-12 07:24:08:E213E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE912600.196 ]  : : 912621] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccremote time is 8.68421/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc196/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE: assigning 0 to cpu:
:] : E
212214[assigning 0 to cpu196/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc ] ] 2022-12-12 07:24:08
] :/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccbuild asymm link desc with 8X Tesla V100-SXM2-32GB out of 8cpu time is 97.0588.assigning 0 to cpu213:

912761
[] 212: [2022-12-12 07:24:08remote time is 8.68421] E2022-12-12 07:24:08.
[build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8 [.9128192022-12-12 07:24:08[
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 07:24:08912836: .2022-12-12 07:24:08:.: E[912856.214912859E 2022-12-12 07:24:08: 912878] :  /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.E: cpu time is 97.0588E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:912910 E
 :212: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc213] E:/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:] build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8 212:212remote time is 8.68421
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] 214] 
:build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8] build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8[213[
cpu time is 97.0588
2022-12-12 07:24:08] 2022-12-12 07:24:08
.remote time is 8.68421[[.913097
2022-12-12 07:24:082022-12-12 07:24:08[913103: ..2022-12-12 07:24:08: E913139913143.E : : 913188 /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccEE: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:  E:213/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc 214] ::/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] remote time is 8.68421213213:cpu time is 97.0588
] ] 214
remote time is 8.68421remote time is 8.68421[] 

2022-12-12 07:24:08cpu time is 97.0588[.
[2022-12-12 07:24:089133232022-12-12 07:24:08.: .913354E913358:  /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: EE:  214/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] ::cpu time is 97.0588214214
] ] cpu time is 97.0588cpu time is 97.0588

[2022-12-12 07:25:27.913230: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:118] solver created. now build & solve
[2022-12-12 07:25:27.953291: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:123] solver built. now solve
block 0 storage is 00010001
	access is	0	0	0	0	4	4	4	4	
block 1 storage is 00100010
	access is	1	1	1	1	5	5	5	5	
block 2 storage is 01000100
	access is	2	2	2	2	6	6	6	6	
block 3 storage is 10001000
	access is	3	3	3	3	7	7	7	7	
block 4 storage is 00000000
	access is	8	8	8	8	8	8	8	8	
[2022-12-12 07:25:28. 61862: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:127] solver solved
[2022-12-12 07:25:28. 61925: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:195] 0 solved master
[2022-12-12 07:25:28. 82726: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 0 solved
[2022-12-12 07:25:28. 82760: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 0 initing device 0
[2022-12-12 07:25:28. 83250: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-12 07:25:28. 83298: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 07:25:28. 85367: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 07:25:28. 86173: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 07:25:28. 97993: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 5 solved
[2022-12-12 07:25:28. 98055: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 5 initing device 5
[[2022-12-12 07:25:282022-12-12 07:25:28.. 98211 98224: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc::202202] ] 4 solved7 solved

[[2022-12-12 07:25:282022-12-12 07:25:28.. 98300 98302: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc::205205] ] worker 0 thread 4 initing device 4worker 0 thread 7 initing device 7

[2022-12-12 07:25:28. 98450: E[ 2022-12-12 07:25:28/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc.: 98480202: ] E1 solved 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] [Building Coll Cache with ... num gpu device is 8[2022-12-12 07:25:28
2022-12-12 07:25:28.. 98520 98510: : EE [ /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc2022-12-12 07:25:28/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:.:205 98557202] : ] worker 0 thread 1 initing device 1E2 solved
 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] [eager alloc mem 381.47 MB2022-12-12 07:25:28
. 98609: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 2 initing device 2
[2022-12-12 07:25:28. 98669: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:[2022022-12-12 07:25:28] .6 solved 98693
: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc[:2022-12-12 07:25:28[202.2022-12-12 07:25:28[]  98735.2022-12-12 07:25:28: 3 solved 98738.E
:  98745 E: /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc[ E:2022-12-12 07:25:28/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu 205.:/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu]  987801815:worker 0 thread 6 initing device 6: ] 1815
EBuilding Coll Cache with ... num gpu device is 8]  
Building Coll Cache with ... num gpu device is 8/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc
:205] worker 0 thread 3 initing device 3
[2022-12-12 07:25:28.[ 988482022-12-12 07:25:28: .E 98853 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE: 1980/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu] :eager alloc mem 381.47 MB1980
] eager alloc mem 381.47 MB
[2022-12-12 07:25:28. 98982: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-12 07:25:28. 99024: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu[:2022-12-12 07:25:281980.]  99034eager alloc mem 381.47 MB: 
E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-12 07:25:28. 99086: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 07:25:28. 99228: E [/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu2022-12-12 07:25:28:.1815 99243] : Building Coll Cache with ... num gpu device is 8E
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-12 07:25:28. 99284: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu[:2022-12-12 07:25:281980.]  99297eager alloc mem 381.47 MB: 
E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 07:25:28.102996: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 07:25:28.103236: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 07:25:28.103291: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 07:25:28.103364: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 07:25:28.103411: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 07:25:28.103944: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 07:25:28.103996: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 07:25:28.108096: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 07:25:28.108261: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 07:25:28.108310: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 07:25:28.108375: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 07:25:28.108471: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 07:25:28.108523: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 07:25:28.108998: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 07:25:28.165729: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.00 Bytes
[2022-12-12 07:25:28.171552: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5
[2022-12-12 07:25:28.171676: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 07:25:28.172525: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 114.82 MB
[2022-12-12 07:25:28.173205: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 07:25:28.174311: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 07:25:28.174366: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 95.37 MB
[2022-12-12 07:25:28.195854: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] [eager alloc mem 5.00 Bytes[2022-12-12 07:25:28[
[[2022-12-12 07:25:28.[2022-12-12 07:25:282022-12-12 07:25:282022-12-12 07:25:28.1959102022-12-12 07:25:28...195910: .195930195933195938: E195939: : : E : EEE /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE   /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu: /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:::1980] :198019801980] eager alloc mem 5.00 Bytes1980] ] ] eager alloc mem 5.00 Bytes
] eager alloc mem 5.00 Byteseager alloc mem 5.00 Byteseager alloc mem 5.00 Bytes
eager alloc mem 5.00 Bytes



[2022-12-12 07:25:28.203434: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5
[2022-12-12 07:25:28.203514[: 2022-12-12 07:25:28E. 203545/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc: :E638 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cceager release cuda mem 5:
638] eager release cuda mem 400000000
[2022-12-12 07:25:28.203588: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:[6382022-12-12 07:25:28] .eager release cuda mem 5203622
: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 07:25:28.203671: [E2022-12-12 07:25:28 ./hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc203668:: 638E]  eager release cuda mem 400000000/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc
:638] eager release cuda mem 5
[2022-12-12 07:25:28.203759: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:[6382022-12-12 07:25:28] .eager release cuda mem 400000000203763
: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5
[[2022-12-12 07:25:282022-12-12 07:25:28..203849203838: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::638638] ] eager release cuda mem 400000000
eager release cuda mem 5
[2022-12-12 07:25:28.203940[: 2022-12-12 07:25:28E. 203934/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc: :E638 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cceager release cuda mem 400000000:
638] eager release cuda mem 5
[2022-12-12 07:25:28.204027: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 07:25:28.205727: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 114.82 MB
[2022-12-12 07:25:28.206247: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 114.82 MB
[2022-12-12 07:25:28.206859: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 114.82 MB
[2022-12-12 07:25:28.207468: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 114.82 MB
[2022-12-12 07:25:28.208043: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 114.82 MB
[2022-12-12 07:25:28.208883: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 114.82 MB
[2022-12-12 07:25:28.209571: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 114.82 MB
[2022-12-12 07:25:28.211268: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 07:25:28.211301: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 07:25:28.211402: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 07:25:28.211521: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 07:25:28.211587: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 07:25:28.211641: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 07:25:28.211698: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 07:25:28.212318: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 07:25:28.[2123712022-12-12 07:25:28: .W212373 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.ccE: 43/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc] :WORKER[0] alloc host memory 95.37 MB638
] eager release cuda mem 625663
[2022-12-12 07:25:28.212439: [W2022-12-12 07:25:28 ./hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc212446:: 43E]  WORKER[0] alloc host memory 95.37 MB/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc
:638] eager release cuda mem 625663
[2022-12-12 07:25:28.212507: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 95.37 MB
[2022-12-12 07:25:28.212586: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 07:25:28.212632: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 95.37 MB[
2022-12-12 07:25:28.212651: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 07:25:28[.2022-12-12 07:25:28212698.: 212698W:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc :/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc43:] 638WORKER[0] alloc host memory 95.37 MB] 
eager release cuda mem 625663
[[2022-12-12 07:25:282022-12-12 07:25:28..212768212764: : WE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::43638] ] WORKER[0] alloc host memory 95.37 MBeager release cuda mem 625663

[2022-12-12 07:25:28.212848: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 95.37 MB
[2022-12-12 07:25:28.239959: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 07:25:28.240614: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 07:25:28.240659: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 11.93 GB
[[2022-12-12 07:25:282022-12-12 07:25:28..276254276254: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::19801980] ] eager alloc mem 25.25 KBeager alloc mem 25.25 KB

[2022-12-12 07:25:28.276735: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[[2022-12-12 07:25:282022-12-12 07:25:28..276922276923: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::638638] ] eager release cuda mem 25855eager release cuda mem 25855

[[2022-12-12 07:25:282022-12-12 07:25:28..276992276993: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::19801980] ] eager alloc mem 11.93 GBeager alloc mem 11.93 GB

[2022-12-12 07:25:28.277201: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 07:25:28.277370: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 07:25:28.277414: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 11.93 GB
[2022-12-12 07:25:28.277509: E [/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu2022-12-12 07:25:28:.1980277528] : eager alloc mem 25.25 KBE
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 07:25:28.277804: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 07:25:28.277848: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 11.93 GB
[2022-12-12 07:25:28.278129: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855[
2022-12-12 07:25:28.278150: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638[] 2022-12-12 07:25:28eager release cuda mem 25855.
278174: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 11.93 GB
[2022-12-12 07:25:28.278202: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 11.93 GB
[2022-12-12 07:25:28.278906: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 07:25:28.279517: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 07:25:28.279562: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 11.93 GB
[[[[[[[[2022-12-12 07:25:322022-12-12 07:25:322022-12-12 07:25:322022-12-12 07:25:322022-12-12 07:25:322022-12-12 07:25:322022-12-12 07:25:322022-12-12 07:25:32...655927.....655927655927: 655932655932655927655925655925: : E: : : : : EE EEEEE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu     /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::1926:::::19261926] 19261926192619261926] ] ] Device 3 init p2p of link 2] ] ] ] Device 6 init p2p of link 0Device 5 init p2p of link 6Device 0 init p2p of link 3
Device 1 init p2p of link 7Device 4 init p2p of link 5Device 7 init p2p of link 4Device 2 init p2p of link 1






[[[[[[2022-12-12 07:25:322022-12-12 07:25:32[[2022-12-12 07:25:322022-12-12 07:25:322022-12-12 07:25:322022-12-12 07:25:32..2022-12-12 07:25:322022-12-12 07:25:32....656502656502..656502656502656503656503: : 656512656515: : : : EE: : EEEE  EE    /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::::19801980::1980198019801980] ] 19801980] ] ] ] eager alloc mem 611.00 KBeager alloc mem 611.00 KB] ] eager alloc mem 611.00 KBeager alloc mem 611.00 KBeager alloc mem 611.00 KBeager alloc mem 611.00 KB

eager alloc mem 611.00 KBeager alloc mem 611.00 KB





[2022-12-12 07:25:32.657563: [E2022-12-12 07:25:32 ./hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc657573:: 638E]  eager release cuda mem 625663/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc
:638] eager release cuda mem 625663
[[2022-12-12 07:25:322022-12-12 07:25:32..657697657699: : EE[  2022-12-12 07:25:32[/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc.[2022-12-12 07:25:32[::6577202022-12-12 07:25:32.2022-12-12 07:25:32638638: .657726.] ] E657730: 657734eager release cuda mem 625663eager release cuda mem 625663 : E: 

/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccE E: /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc 638/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc] :638:eager release cuda mem 625663638] 638
] eager release cuda mem 625663] eager release cuda mem 625663
eager release cuda mem 625663

[2022-12-12 07:25:32.671220: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 6 init p2p of link 5
[2022-12-12 07:25:32.671394: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 07:25:32.671522: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 2
[2022-12-12 07:25:32.671684: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 07:25:32.672345: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 07:25:32.672643: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 07:25:32.680447: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 4 init p2p of link 7
[2022-12-12 07:25:32.680615: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 07:25:32.680671: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 7 init p2p of link 1
[2022-12-12 07:25:32.680827: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 07:25:32.681085: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 0 init p2p of link 6
[2022-12-12 07:25:32.681261: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 07:25:32.681328: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 3 init p2p of link 0
[2022-12-12 07:25:32.681503: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 07:25:32.681550: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:[6382022-12-12 07:25:32] .eager release cuda mem 625663681556
: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 2 init p2p of link 3[
2022-12-12 07:25:32.681603: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 07:25:32.681731: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 07:25:32.682036: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 07:25:32.682260: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 5 init p2p of link 4
[2022-12-12 07:25:32.682425: [E2022-12-12 07:25:32 ./hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu682433:: 1980E]  eager alloc mem 611.00 KB/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc
:638] eager release cuda mem 625663
[2022-12-12 07:25:32.682701: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 07:25:32.683411: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 07:25:32.685685: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 6 init p2p of link 4
[[2022-12-12 07:25:322022-12-12 07:25:32..685799685810: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::19261980] ] Device 1 init p2p of link 3eager alloc mem 611.00 KB

[2022-12-12 07:25:32.685943: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 07:25:32.686767: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 07:25:32.686868: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 07:25:32.694422: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 7 init p2p of link 6
[2022-12-12 07:25:32.694534: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 07:25:32.694768: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 0 init p2p of link 1
[2022-12-12 07:25:32.694889: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 07:25:32.695504: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 07:25:32.695857: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 07:25:32.702809: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 4 init p2p of link 2
[2022-12-12 07:25:32.702939: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 07:25:32.703694: E [/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu2022-12-12 07:25:32:.1926703723] : Device 3 init p2p of link 5E
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 07:25:32.703849: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 07:25:32.703891: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 2 init p2p of link 0
[2022-12-12 07:25:32.704019: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 07:25:32.704625: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 07:25:32.704755: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 5 init p2p of link 7
[2022-12-12 07:25:32.704871: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 07:25:32.704954: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 07:25:32.705796: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 07:25:32.710246: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 4 init p2p of link 6
[2022-12-12 07:25:32.710365: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 07:25:32.711320: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 07:25:32.712601: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 3 init p2p of link 1
[2022-12-12 07:25:32.712730: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 07:25:32.713692: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 07:25:32.715499: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 6 init p2p of link 7
[2022-12-12 07:25:32.715624: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 07:25:32.715852: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 0
[2022-12-12 07:25:32.715975: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 07:25:32.716584: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 07:25:32.716915: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 07:25:32.718122: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 7 init p2p of link 5
[2022-12-12 07:25:32.718233: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 07:25:32.718509: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 0 init p2p of link 2
[2022-12-12 07:25:32.718626: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 07:25:32.719183: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 07:25:32.719575: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 07:25:32.731231: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 2 init p2p of link 4
[2022-12-12 07:25:32.731356: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 07:25:32.731694: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 5 init p2p of link 3
[2022-12-12 07:25:32.731810: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 07:25:32.732132: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 07:25:32.732588: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 07:25:32.733441: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 120400004
[2022-12-12 07:25:32.734177: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 25000000 / 100000000 nodes ( 25.00 %~30.00 %) | remote 75000000 / 100000000 nodes ( 75.00 %) | cpu 0 / 100000000 nodes ( 0.00 %) | 11.93 GB | 4.63533 secs 
[2022-12-12 07:25:32.734425: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 120400004
[2022-12-12 07:25:32.735787: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 25000000 / 100000000 nodes ( 25.00 %~30.00 %) | remote 75000000 / 100000000 nodes ( 75.00 %) | cpu 0 / 100000000 nodes ( 0.00 %) | 11.93 GB | 4.63649 secs 
[2022-12-12 07:25:32.736992: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 120400004
[2022-12-12 07:25:32.737329: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 120400004
[2022-12-12 07:25:32.737748: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 25000000 / 100000000 nodes ( 25.00 %~30.00 %) | remote 75000000 / 100000000 nodes ( 75.00 %) | cpu 0 / 100000000 nodes ( 0.00 %) | 11.93 GB | 4.63867 secs 
[2022-12-12 07:25:32.738056: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 25000000 / 100000000 nodes ( 25.00 %~30.00 %) | remote 75000000 / 100000000 nodes ( 75.00 %) | cpu 0 / 100000000 nodes ( 0.00 %) | 11.93 GB | 4.63951 secs 
[2022-12-12 07:25:32.741178: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 120400004
[2022-12-12 07:25:32.741619: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 120400004[
2022-12-12 07:25:32.741651: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 25000000 / 100000000 nodes ( 25.00 %~30.00 %) | remote 75000000 / 100000000 nodes ( 75.00 %) | cpu 0 / 100000000 nodes ( 0.00 %) | 11.93 GB | 4.64237 secs 
[2022-12-12 07:25:32.742079: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 25000000 / 100000000 nodes ( 25.00 %~30.00 %) | remote 75000000 / 100000000 nodes ( 75.00 %) | cpu 0 / 100000000 nodes ( 0.00 %) | 11.93 GB | 4.64306 secs 
[2022-12-12 07:25:32.743553: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 120400004
[2022-12-12 07:25:32.743809: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 120400004
[2022-12-12 07:25:32.744006: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 25000000 / 100000000 nodes ( 25.00 %~30.00 %) | remote 75000000 / 100000000 nodes ( 75.00 %) | cpu 0 / 100000000 nodes ( 0.00 %) | 11.93 GB | 4.64516 secs 
[2022-12-12 07:25:32.744372: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 25000000 / 100000000 nodes ( 25.00 %~30.00 %) | remote 75000000 / 100000000 nodes ( 75.00 %) | cpu 0 / 100000000 nodes ( 0.00 %) | 11.93 GB | 4.66108 secs 
[HCTR][07:25:32.744][ERROR][RK0][main]: replica 0 calling init per replica done, doing barrier
[HCTR][07:25:32.744][ERROR][RK0][tid #139961441449728]: replica 6 calling init per replica done, doing barrier
[HCTR][07:25:32.744][ERROR][RK0][main]: replica 4 calling init per replica done, doing barrier
[HCTR][07:25:32.744][ERROR][RK0][main]: replica 2 calling init per replica done, doing barrier
[HCTR][07:25:32.744][ERROR][RK0][main]: replica 1 calling init per replica done, doing barrier
[HCTR][07:25:32.744][ERROR][RK0][tid #139961508558592]: replica 7 calling init per replica done, doing barrier
[HCTR][07:25:32.744][ERROR][RK0][main]: replica 5 calling init per replica done, doing barrier
[HCTR][07:25:32.744][ERROR][RK0][main]: replica 3 calling init per replica done, doing barrier
[HCTR][07:25:32.744][ERROR][RK0][tid #139961441449728]: replica 6 calling init per replica done, doing barrier done
[HCTR][07:25:32.744][ERROR][RK0][main]: replica 5 calling init per replica done, doing barrier done
[HCTR][07:25:32.744][ERROR][RK0][main]: replica 0 calling init per replica done, doing barrier done
[HCTR][07:25:32.744][ERROR][RK0][main]: replica 3 calling init per replica done, doing barrier done
[HCTR][07:25:32.744][ERROR][RK0][main]: replica 2 calling init per replica done, doing barrier done
[HCTR][07:25:32.744][ERROR][RK0][main]: replica 1 calling init per replica done, doing barrier done
[HCTR][07:25:32.744][ERROR][RK0][tid #139961508558592]: replica 7 calling init per replica done, doing barrier done
[HCTR][07:25:32.744][ERROR][RK0][main]: replica 4 calling init per replica done, doing barrier done
[HCTR][07:25:32.744][ERROR][RK0][tid #139961441449728]: init per replica done
[HCTR][07:25:32.744][ERROR][RK0][main]: init per replica done
[HCTR][07:25:32.744][ERROR][RK0][main]: init per replica done
[HCTR][07:25:32.744][ERROR][RK0][main]: init per replica done
[HCTR][07:25:32.744][ERROR][RK0][main]: init per replica done
[HCTR][07:25:32.744][ERROR][RK0][main]: init per replica done
[HCTR][07:25:32.744][ERROR][RK0][tid #139961508558592]: init per replica done
[HCTR][07:25:32.747][ERROR][RK0][main]: init per replica done








