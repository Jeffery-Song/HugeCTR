2022-12-12 04:48:33.873599: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:33.881416: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:33.889699: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:33.989941: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:33.994815: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.007571: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.016160: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.021102: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.079601: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.085955: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.088592: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.090813: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.091479: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.092422: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.093201: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.093904: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.094862: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.095528: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.096488: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.097119: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.098267: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.098835: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.100133: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.100521: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.102240: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.102395: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.104106: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.105061: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.106020: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.106975: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.108009: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.109054: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.110956: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.112409: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.113377: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.114322: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.115295: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.116250: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.117273: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.118288: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.123663: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 04:48:34.125023: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.126194: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.127305: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.128607: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.130291: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.130914: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.131845: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.132823: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.132875: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.133988: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.135159: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.135609: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.136760: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.137625: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.137904: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.138043: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.140455: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.140509: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.142953: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.143000: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.145701: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.145846: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.146613: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.147906: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.148463: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.148730: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.150140: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.151465: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.152352: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.153937: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.154776: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.154777: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.155786: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.157420: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.158012: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.158338: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.158647: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.159433: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.160902: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.161115: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.161806: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.162158: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.163873: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.163974: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.164650: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.164857: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.166222: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.166590: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.175531: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.175768: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.176856: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.177503: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.178175: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.178497: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.179190: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.180975: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.181159: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.182874: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.196867: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.206372: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.215252: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.218027: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.218065: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.218104: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.218179: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.218231: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.218904: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.222574: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.222631: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.222703: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.223030: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.223079: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.223671: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.227875: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.227941: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.228041: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.228379: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.229148: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.231378: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.231439: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.231512: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.231823: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.232585: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.235163: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.235235: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.235918: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.236125: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.237076: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.239907: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.239965: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.240189: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.240314: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.241542: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.243607: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.243840: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.244065: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.244831: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.244974: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.246860: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.247075: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.247270: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.248510: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.248824: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.250367: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.250585: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.250799: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.251934: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.252257: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.254307: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.254518: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.254688: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.255570: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.256000: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.257833: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.258023: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.258209: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.259248: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.259844: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.261405: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.261613: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.261799: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.262749: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.263277: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.265000: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.265297: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.265558: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.266353: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.266769: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.267249: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.268498: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.269046: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.269389: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.270408: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.270906: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.271360: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.271415: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.272683: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.273283: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.273643: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.275016: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.276192: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.276209: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.278149: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.278663: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.278783: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.278911: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.279625: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.279676: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.280040: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 04:48:34.281612: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.282540: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.282596: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.282876: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.283579: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.283681: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.285747: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.286532: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.286881: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.287546: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.287560: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.288060: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.289314: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.289400: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.290265: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.290682: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.291686: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.292002: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.292509: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.294128: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.294155: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.295052: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.295622: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.296531: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.296777: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.297408: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.298671: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.298696: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.299910: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.300393: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.301157: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.301297: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.302269: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.303445: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.304734: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.304992: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.305553: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.305618: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.306478: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.307836: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.309345: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.309630: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.309825: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.309990: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.310821: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.314133: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.314277: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.314909: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.316583: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 04:48:34.317178: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.317230: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.317553: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 04:48:34.317614: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 04:48:34.317693: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.320285: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.320426: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.322897: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.323215: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.324373: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 04:48:34.325411: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.325641: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.325790: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.326975: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.327063: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.328965: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.329474: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.330115: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.331066: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.331172: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.333340: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.333655: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.334630: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.334751: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.335591: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.335678: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.337323: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.346489: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.346760: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.350512: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.351966: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.380907: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.384955: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.385457: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.418129: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.418567: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.423834: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.424240: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.431818: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.433225: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.438825: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.441810: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 04:48:34.447631: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 04:48:34.450899: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.456657: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.473053: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.480165: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.480876: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:34.492825: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:35.480332: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:35.480966: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:35.481498: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:35.482119: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 04:48:35.482179: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-12 04:48:35.500098: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:35.500946: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:35.501868: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:35.502453: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:35.502988: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:35.503505: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-12 04:48:35.548535: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 04:48:35.548720: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 04:48:35.579898: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12340
2022-12-12 04:48:35.697614: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:35.698616: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:35.699397: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:35.700290: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 04:48:35.700347: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-12 04:48:35.718148: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:35.719001: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:35.719528: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:35.720244: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:35.720788: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:35.721376: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:1/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-12 04:48:35.737669: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:35.738614: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:35.739193: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:35.739660: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 04:48:35.739718: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-12 04:48:35.756569: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:35.757458: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:35.758196: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:35.758784: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:35.759547: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:35.760183: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:2/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-12 04:48:35.778511: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:35.778934: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:35.779318: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:35.779769: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:35.779999: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:35.780747: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:35.781367: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:35.781544: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:35.782180: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:35.782272: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 04:48:35.782331: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-12 04:48:35.783222: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:35.783359: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 04:48:35.783434: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-12 04:48:35.784039: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:35.784414: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 04:48:35.784463: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-12 04:48:35.785002: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:35.785465: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 04:48:35.785510: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-12 04:48:35.800501: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:35.801190: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:35.801457: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:35.802011: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:35.802016: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:35.802583: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:35.803517: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:35.803706: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:35.804035: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:35.804225: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:35.805503: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:35.805703: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:35.805874: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 04:48:35.806032: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:35.806054: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 04:48:35.806203: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:35.807638: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:4/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-12 04:48:35.807671: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:35.807987: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12341
2022-12-12 04:48:35.807995: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:35.808085: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:35.809126: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:35.809489: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:3/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-12 04:48:35.809505: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:35.810352: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:7/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-12 04:48:35.810639: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:35.811149: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:6/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-12 04:48:35.828094: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:35.828716: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:35.829250: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:35.829715: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 04:48:35.829771: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-12 04:48:35.840249: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 04:48:35.840440: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 04:48:35.842344: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12342
2022-12-12 04:48:35.849171: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:35.849824: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:35.850339: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:35.850927: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:35.851484: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:48:35.851965: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:5/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-12 04:48:35.852468: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 04:48:35.852635: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 04:48:35.854357: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12344
2022-12-12 04:48:35.854714: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 04:48:35.854862: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 04:48:35.854922: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 04:48:35.855049: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 04:48:35.856644: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12347
2022-12-12 04:48:35.856799: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12343
2022-12-12 04:48:35.857992: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 04:48:35.858111: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 04:48:35.859063: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12346
2022-12-12 04:48:35.897698: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 04:48:35.897904: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 04:48:35.898824: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12345
[HCTR][04:48:36.188][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][04:48:37.164][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][04:48:37.164][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][04:48:37.164][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][04:48:37.164][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][04:48:37.164][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][04:48:37.164][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][04:48:37.189][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 1it [00:01,  1.56s/it]warmup run: 96it [00:01, 80.36it/s]warmup run: 1it [00:01,  1.53s/it]warmup run: 1it [00:01,  1.51s/it]warmup run: 192it [00:01, 174.50it/s]warmup run: 98it [00:01, 83.57it/s]warmup run: 97it [00:01, 83.40it/s]warmup run: 1it [00:01,  1.51s/it]warmup run: 1it [00:01,  1.52s/it]warmup run: 289it [00:01, 279.89it/s]warmup run: 1it [00:01,  1.49s/it]warmup run: 197it [00:01, 182.23it/s]warmup run: 194it [00:01, 180.58it/s]warmup run: 1it [00:01,  1.49s/it]warmup run: 97it [00:01, 83.30it/s]warmup run: 1it [00:01,  1.49s/it]warmup run: 386it [00:01, 389.64it/s]warmup run: 98it [00:01, 83.96it/s]warmup run: 101it [00:01, 87.96it/s]warmup run: 297it [00:01, 292.07it/s]warmup run: 293it [00:01, 289.88it/s]warmup run: 98it [00:01, 85.64it/s]warmup run: 95it [00:01, 82.58it/s]warmup run: 194it [00:01, 180.35it/s]warmup run: 483it [00:02, 496.75it/s]warmup run: 197it [00:01, 182.88it/s]warmup run: 203it [00:01, 191.21it/s]warmup run: 396it [00:01, 404.43it/s]warmup run: 391it [00:01, 401.64it/s]warmup run: 195it [00:01, 184.04it/s]warmup run: 292it [00:01, 288.27it/s]warmup run: 193it [00:01, 181.85it/s]warmup run: 581it [00:02, 597.70it/s]warmup run: 297it [00:01, 293.16it/s]warmup run: 304it [00:01, 303.10it/s]warmup run: 494it [00:02, 512.11it/s]warmup run: 491it [00:02, 513.11it/s]warmup run: 294it [00:01, 294.25it/s]warmup run: 295it [00:01, 296.08it/s]warmup run: 389it [00:01, 398.10it/s]warmup run: 680it [00:02, 686.50it/s]warmup run: 398it [00:01, 408.75it/s]warmup run: 406it [00:01, 419.81it/s]warmup run: 593it [00:02, 612.89it/s]warmup run: 591it [00:02, 614.88it/s]warmup run: 389it [00:01, 401.54it/s]warmup run: 396it [00:01, 412.30it/s]warmup run: 487it [00:02, 506.78it/s]warmup run: 778it [00:02, 758.04it/s]warmup run: 499it [00:02, 520.60it/s]warmup run: 506it [00:01, 529.08it/s]warmup run: 693it [00:02, 701.28it/s]warmup run: 688it [00:02, 696.04it/s]warmup run: 486it [00:01, 508.82it/s]warmup run: 497it [00:01, 524.75it/s]warmup run: 591it [00:02, 618.19it/s]warmup run: 881it [00:02, 827.40it/s]warmup run: 602it [00:02, 627.80it/s]warmup run: 793it [00:02, 775.41it/s]warmup run: 608it [00:02, 632.07it/s]warmup run: 785it [00:02, 763.93it/s]warmup run: 579it [00:02, 598.35it/s]warmup run: 600it [00:02, 631.12it/s]warmup run: 694it [00:02, 712.26it/s]warmup run: 984it [00:02, 881.66it/s]warmup run: 706it [00:02, 722.55it/s]warmup run: 894it [00:02, 834.90it/s]warmup run: 710it [00:02, 720.62it/s]warmup run: 885it [00:02, 823.63it/s]warmup run: 676it [00:02, 684.34it/s]warmup run: 703it [00:02, 723.16it/s]warmup run: 794it [00:02, 783.45it/s]warmup run: 1087it [00:02, 922.19it/s]warmup run: 809it [00:02, 798.40it/s]warmup run: 995it [00:02, 881.08it/s]warmup run: 811it [00:02, 792.62it/s]warmup run: 985it [00:02, 870.12it/s]warmup run: 772it [00:02, 752.82it/s]warmup run: 806it [00:02, 798.71it/s]warmup run: 896it [00:02, 844.37it/s]warmup run: 1190it [00:02, 952.86it/s]warmup run: 912it [00:02, 857.05it/s]warmup run: 1095it [00:02, 914.21it/s]warmup run: 913it [00:02, 850.44it/s]warmup run: 1084it [00:02, 903.56it/s]warmup run: 868it [00:02, 806.13it/s]warmup run: 909it [00:02, 857.88it/s]warmup run: 997it [00:02, 887.83it/s]warmup run: 1293it [00:02, 973.96it/s]warmup run: 1014it [00:02, 900.65it/s]warmup run: 1195it [00:02, 936.82it/s]warmup run: 1014it [00:02, 892.77it/s]warmup run: 1183it [00:02, 927.84it/s]warmup run: 964it [00:02, 847.09it/s]warmup run: 1011it [00:02, 901.32it/s]warmup run: 1397it [00:02, 991.13it/s]warmup run: 1116it [00:02, 933.83it/s]warmup run: 1097it [00:02, 915.36it/s]warmup run: 1297it [00:02, 958.85it/s]warmup run: 1115it [00:02, 923.30it/s]warmup run: 1282it [00:02, 945.76it/s]warmup run: 1060it [00:02, 878.40it/s]warmup run: 1113it [00:02, 933.74it/s]warmup run: 1500it [00:03, 1001.93it/s]warmup run: 1220it [00:02, 961.28it/s]warmup run: 1197it [00:02, 923.16it/s]warmup run: 1398it [00:02, 971.52it/s]warmup run: 1217it [00:02, 950.30it/s]warmup run: 1381it [00:02, 951.60it/s]warmup run: 1156it [00:02, 899.61it/s]warmup run: 1216it [00:02, 959.51it/s]warmup run: 1322it [00:02, 977.63it/s]warmup run: 1603it [00:03, 1004.03it/s]warmup run: 1295it [00:02, 935.15it/s]warmup run: 1499it [00:03, 981.55it/s]warmup run: 1320it [00:02, 971.54it/s]warmup run: 1479it [00:03, 954.71it/s]warmup run: 1252it [00:02, 917.02it/s]warmup run: 1320it [00:02, 980.34it/s]warmup run: 1425it [00:02, 990.49it/s]warmup run: 1705it [00:03, 1005.07it/s]warmup run: 1396it [00:02, 956.00it/s]warmup run: 1602it [00:03, 994.94it/s]warmup run: 1425it [00:02, 991.97it/s]warmup run: 1577it [00:03, 957.22it/s]warmup run: 1348it [00:02, 925.94it/s]warmup run: 1423it [00:02, 994.04it/s]warmup run: 1528it [00:03, 999.62it/s]warmup run: 1807it [00:03, 1004.83it/s]warmup run: 1498it [00:03, 974.17it/s]warmup run: 1704it [00:03, 999.87it/s]warmup run: 1529it [00:03, 1004.13it/s]warmup run: 1675it [00:03, 958.40it/s]warmup run: 1444it [00:02, 929.91it/s]warmup run: 1527it [00:03, 1005.33it/s]warmup run: 1630it [00:03, 1004.95it/s]warmup run: 1909it [00:03, 1007.01it/s]warmup run: 1600it [00:03, 987.58it/s]warmup run: 1633it [00:03, 1014.20it/s]warmup run: 1805it [00:03, 994.47it/s]warmup run: 1772it [00:03, 959.52it/s]warmup run: 1540it [00:03, 938.37it/s]warmup run: 1630it [00:03, 1010.96it/s]warmup run: 1732it [00:03, 1008.79it/s]warmup run: 2012it [00:03, 1012.46it/s]warmup run: 1702it [00:03, 996.58it/s]warmup run: 1737it [00:03, 1021.16it/s]warmup run: 1906it [00:03, 994.37it/s]warmup run: 1869it [00:03, 959.35it/s]warmup run: 1636it [00:03, 943.08it/s]warmup run: 1733it [00:03, 1015.43it/s]warmup run: 1834it [00:03, 1011.54it/s]warmup run: 2132it [00:03, 1066.77it/s]warmup run: 1804it [00:03, 1003.46it/s]warmup run: 1841it [00:03, 1022.19it/s]warmup run: 2007it [00:03, 998.55it/s]warmup run: 1966it [00:03, 960.95it/s]warmup run: 1733it [00:03, 950.77it/s]warmup run: 1837it [00:03, 1020.14it/s]warmup run: 1937it [00:03, 1016.16it/s]warmup run: 2252it [00:03, 1104.90it/s]warmup run: 1906it [00:03, 1007.14it/s]warmup run: 1946it [00:03, 1028.06it/s]warmup run: 2129it [00:03, 1061.98it/s]warmup run: 2078it [00:03, 1007.36it/s]warmup run: 1830it [00:03, 956.04it/s]warmup run: 1940it [00:03, 1022.57it/s]warmup run: 2047it [00:03, 1040.21it/s]warmup run: 2373it [00:03, 1135.92it/s]warmup run: 2008it [00:03, 1010.22it/s]warmup run: 2058it [00:03, 1053.40it/s]warmup run: 2251it [00:03, 1108.94it/s]warmup run: 2200it [00:03, 1068.59it/s]warmup run: 1927it [00:03, 958.31it/s]warmup run: 2051it [00:03, 1046.45it/s]warmup run: 2170it [00:03, 1096.58it/s]warmup run: 2495it [00:03, 1159.61it/s]warmup run: 2126it [00:03, 1060.57it/s]warmup run: 2179it [00:03, 1098.58it/s]warmup run: 2374it [00:03, 1144.54it/s]warmup run: 2322it [00:03, 1112.57it/s]warmup run: 2029it [00:03, 975.92it/s]warmup run: 2172it [00:03, 1094.15it/s]warmup run: 2293it [00:03, 1135.02it/s]warmup run: 2617it [00:04, 1175.89it/s]warmup run: 2242it [00:03, 1088.09it/s]warmup run: 2300it [00:03, 1130.01it/s]warmup run: 2497it [00:03, 1168.67it/s]warmup run: 2444it [00:03, 1144.40it/s]warmup run: 2150it [00:03, 1043.94it/s]warmup run: 2293it [00:03, 1127.78it/s]warmup run: 2416it [00:03, 1161.34it/s]warmup run: 2738it [00:04, 1183.29it/s]warmup run: 2362it [00:03, 1119.99it/s]warmup run: 2421it [00:03, 1152.65it/s]warmup run: 2620it [00:04, 1185.76it/s]warmup run: 2566it [00:04, 1166.39it/s]warmup run: 2271it [00:03, 1092.32it/s]warmup run: 2414it [00:03, 1151.70it/s]warmup run: 2539it [00:03, 1180.15it/s]warmup run: 2859it [00:04, 1189.32it/s]warmup run: 2482it [00:03, 1142.92it/s]warmup run: 2542it [00:03, 1168.89it/s]warmup run: 2742it [00:04, 1194.54it/s]warmup run: 2687it [00:04, 1176.64it/s]warmup run: 2393it [00:03, 1128.26it/s]warmup run: 2535it [00:03, 1168.03it/s]warmup run: 2662it [00:04, 1194.31it/s]warmup run: 2981it [00:04, 1197.79it/s]warmup run: 2602it [00:04, 1158.90it/s]warmup run: 3000it [00:04, 682.44it/s] warmup run: 2661it [00:04, 1174.38it/s]warmup run: 2865it [00:04, 1203.43it/s]warmup run: 2809it [00:04, 1187.27it/s]warmup run: 2514it [00:03, 1151.23it/s]warmup run: 2656it [00:04, 1178.63it/s]warmup run: 2783it [00:04, 1197.99it/s]warmup run: 2722it [00:04, 1170.74it/s]warmup run: 2781it [00:04, 1181.68it/s]warmup run: 2988it [00:04, 1210.41it/s]warmup run: 3000it [00:04, 688.30it/s] warmup run: 2930it [00:04, 1191.43it/s]warmup run: 2634it [00:04, 1164.62it/s]warmup run: 2777it [00:04, 1187.19it/s]warmup run: 2906it [00:04, 1205.70it/s]warmup run: 2844it [00:04, 1184.27it/s]warmup run: 3000it [00:04, 681.89it/s] warmup run: 2901it [00:04, 1185.55it/s]warmup run: 3000it [00:04, 695.53it/s] warmup run: 2754it [00:04, 1174.79it/s]warmup run: 2897it [00:04, 1188.31it/s]warmup run: 2966it [00:04, 1193.04it/s]warmup run: 3000it [00:04, 686.77it/s] warmup run: 3000it [00:04, 698.42it/s] warmup run: 2872it [00:04, 1174.29it/s]warmup run: 3000it [00:04, 696.59it/s] warmup run: 2991it [00:04, 1178.42it/s]warmup run: 3000it [00:04, 679.64it/s] 
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]





warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   6%|         | 167/3000 [00:00<00:01, 1669.26it/s]warmup should be done:   5%|         | 161/3000 [00:00<00:01, 1609.63it/s]warmup should be done:   6%|         | 166/3000 [00:00<00:01, 1657.05it/s]warmup should be done:   6%|         | 167/3000 [00:00<00:01, 1666.12it/s]warmup should be done:   5%|         | 164/3000 [00:00<00:01, 1632.99it/s]warmup should be done:   6%|         | 165/3000 [00:00<00:01, 1646.14it/s]warmup should be done:   6%|         | 166/3000 [00:00<00:01, 1653.16it/s]warmup should be done:   6%|         | 167/3000 [00:00<00:01, 1663.47it/s]warmup should be done:  11%|         | 336/3000 [00:00<00:01, 1679.65it/s]warmup should be done:  11%|         | 324/3000 [00:00<00:01, 1619.62it/s]warmup should be done:  11%|         | 328/3000 [00:00<00:01, 1634.80it/s]warmup should be done:  11%|         | 336/3000 [00:00<00:01, 1677.41it/s]warmup should be done:  11%|         | 332/3000 [00:00<00:01, 1658.14it/s]warmup should be done:  11%|         | 334/3000 [00:00<00:01, 1666.98it/s]warmup should be done:  11%|         | 336/3000 [00:00<00:01, 1675.54it/s]warmup should be done:  11%|         | 332/3000 [00:00<00:01, 1651.19it/s]warmup should be done:  17%|        | 504/3000 [00:00<00:01, 1678.25it/s]warmup should be done:  17%|        | 498/3000 [00:00<00:01, 1658.21it/s]warmup should be done:  17%|        | 501/3000 [00:00<00:01, 1667.32it/s]warmup should be done:  17%|        | 504/3000 [00:00<00:01, 1676.47it/s]warmup should be done:  16%|        | 486/3000 [00:00<00:01, 1613.79it/s]warmup should be done:  17%|        | 504/3000 [00:00<00:01, 1674.33it/s]warmup should be done:  17%|        | 498/3000 [00:00<00:01, 1652.57it/s]warmup should be done:  16%|        | 492/3000 [00:00<00:01, 1630.73it/s]warmup should be done:  22%|       | 672/3000 [00:00<00:01, 1677.63it/s]warmup should be done:  22%|       | 668/3000 [00:00<00:01, 1666.46it/s]warmup should be done:  22%|       | 672/3000 [00:00<00:01, 1675.96it/s]warmup should be done:  22%|       | 650/3000 [00:00<00:01, 1623.26it/s]warmup should be done:  22%|       | 672/3000 [00:00<00:01, 1675.33it/s]warmup should be done:  22%|       | 664/3000 [00:00<00:01, 1652.87it/s]warmup should be done:  22%|       | 664/3000 [00:00<00:01, 1652.98it/s]warmup should be done:  22%|       | 656/3000 [00:00<00:01, 1627.23it/s]warmup should be done:  28%|       | 840/3000 [00:00<00:01, 1675.42it/s]warmup should be done:  28%|       | 840/3000 [00:00<00:01, 1673.79it/s]warmup should be done:  27%|       | 816/3000 [00:00<00:01, 1633.97it/s]warmup should be done:  28%|       | 830/3000 [00:00<00:01, 1652.89it/s]warmup should be done:  28%|       | 840/3000 [00:00<00:01, 1673.23it/s]warmup should be done:  28%|       | 835/3000 [00:00<00:01, 1655.49it/s]warmup should be done:  27%|       | 819/3000 [00:00<00:01, 1623.91it/s]warmup should be done:  28%|       | 830/3000 [00:00<00:01, 1646.08it/s]warmup should be done:  34%|      | 1008/3000 [00:00<00:01, 1671.50it/s]warmup should be done:  33%|      | 980/3000 [00:00<00:01, 1633.24it/s]warmup should be done:  34%|      | 1008/3000 [00:00<00:01, 1669.43it/s]warmup should be done:  33%|      | 996/3000 [00:00<00:01, 1649.24it/s]warmup should be done:  34%|      | 1008/3000 [00:00<00:01, 1667.59it/s]warmup should be done:  33%|      | 995/3000 [00:00<00:01, 1640.28it/s]warmup should be done:  33%|      | 982/3000 [00:00<00:01, 1616.91it/s]warmup should be done:  33%|      | 1001/3000 [00:00<00:01, 1638.81it/s]warmup should be done:  39%|      | 1176/3000 [00:00<00:01, 1672.37it/s]warmup should be done:  38%|      | 1145/3000 [00:00<00:01, 1636.65it/s]warmup should be done:  39%|      | 1176/3000 [00:00<00:01, 1670.26it/s]warmup should be done:  39%|      | 1162/3000 [00:00<00:01, 1650.14it/s]warmup should be done:  39%|      | 1175/3000 [00:00<00:01, 1665.39it/s]warmup should be done:  38%|      | 1144/3000 [00:00<00:01, 1615.93it/s]warmup should be done:  39%|      | 1160/3000 [00:00<00:01, 1633.42it/s]warmup should be done:  39%|      | 1165/3000 [00:00<00:01, 1631.23it/s]warmup should be done:  45%|     | 1344/3000 [00:00<00:00, 1673.33it/s]warmup should be done:  44%|     | 1311/3000 [00:00<00:01, 1642.03it/s]warmup should be done:  45%|     | 1344/3000 [00:00<00:00, 1670.78it/s]warmup should be done:  44%|     | 1328/3000 [00:00<00:01, 1650.25it/s]warmup should be done:  45%|     | 1342/3000 [00:00<00:00, 1664.80it/s]warmup should be done:  44%|     | 1306/3000 [00:00<00:01, 1616.16it/s]warmup should be done:  44%|     | 1324/3000 [00:00<00:01, 1633.20it/s]warmup should be done:  44%|     | 1329/3000 [00:00<00:01, 1618.27it/s]warmup should be done:  49%|     | 1476/3000 [00:00<00:00, 1644.18it/s]warmup should be done:  50%|     | 1512/3000 [00:00<00:00, 1670.66it/s]warmup should be done:  50%|     | 1512/3000 [00:00<00:00, 1669.97it/s]warmup should be done:  50%|     | 1494/3000 [00:00<00:00, 1649.37it/s]warmup should be done:  50%|     | 1509/3000 [00:00<00:00, 1663.13it/s]warmup should be done:  49%|     | 1468/3000 [00:00<00:00, 1614.42it/s]warmup should be done:  50%|     | 1490/3000 [00:00<00:00, 1640.86it/s]warmup should be done:  50%|     | 1491/3000 [00:00<00:00, 1611.23it/s]warmup should be done:  55%|    | 1641/3000 [00:01<00:00, 1641.69it/s]warmup should be done:  56%|    | 1680/3000 [00:01<00:00, 1667.43it/s]warmup should be done:  56%|    | 1676/3000 [00:01<00:00, 1664.87it/s]warmup should be done:  55%|    | 1659/3000 [00:01<00:00, 1648.11it/s]warmup should be done:  56%|    | 1679/3000 [00:01<00:00, 1664.08it/s]warmup should be done:  54%|    | 1630/3000 [00:01<00:00, 1610.41it/s]warmup should be done:  55%|    | 1655/3000 [00:01<00:00, 1637.59it/s]warmup should be done:  55%|    | 1655/3000 [00:01<00:00, 1617.67it/s]warmup should be done:  60%|    | 1806/3000 [00:01<00:00, 1639.55it/s]warmup should be done:  61%|   | 1843/3000 [00:01<00:00, 1665.39it/s]warmup should be done:  62%|   | 1848/3000 [00:01<00:00, 1668.56it/s]warmup should be done:  61%|    | 1824/3000 [00:01<00:00, 1647.75it/s]warmup should be done:  62%|   | 1846/3000 [00:01<00:00, 1665.05it/s]warmup should be done:  60%|    | 1792/3000 [00:01<00:00, 1610.55it/s]warmup should be done:  61%|    | 1821/3000 [00:01<00:00, 1641.95it/s]warmup should be done:  61%|    | 1818/3000 [00:01<00:00, 1621.37it/s]warmup should be done:  66%|   | 1970/3000 [00:01<00:00, 1638.46it/s]warmup should be done:  67%|   | 2016/3000 [00:01<00:00, 1670.69it/s]warmup should be done:  67%|   | 2010/3000 [00:01<00:00, 1664.24it/s]warmup should be done:  66%|   | 1990/3000 [00:01<00:00, 1649.11it/s]warmup should be done:  67%|   | 2014/3000 [00:01<00:00, 1666.55it/s]warmup should be done:  65%|   | 1954/3000 [00:01<00:00, 1607.95it/s]warmup should be done:  66%|   | 1987/3000 [00:01<00:00, 1645.72it/s]warmup should be done:  66%|   | 1983/3000 [00:01<00:00, 1627.17it/s]warmup should be done:  71%|   | 2135/3000 [00:01<00:00, 1640.02it/s]warmup should be done:  72%|  | 2155/3000 [00:01<00:00, 1649.36it/s]warmup should be done:  73%|  | 2184/3000 [00:01<00:00, 1672.05it/s]warmup should be done:  73%|  | 2177/3000 [00:01<00:00, 1663.15it/s]warmup should be done:  73%|  | 2182/3000 [00:01<00:00, 1668.04it/s]warmup should be done:  71%|   | 2116/3000 [00:01<00:00, 1609.66it/s]warmup should be done:  72%|  | 2153/3000 [00:01<00:00, 1649.15it/s]warmup should be done:  72%|  | 2148/3000 [00:01<00:00, 1633.45it/s]warmup should be done:  77%|  | 2300/3000 [00:01<00:00, 1638.03it/s]warmup should be done:  78%|  | 2352/3000 [00:01<00:00, 1670.21it/s]warmup should be done:  77%|  | 2320/3000 [00:01<00:00, 1644.10it/s]warmup should be done:  78%|  | 2349/3000 [00:01<00:00, 1666.06it/s]warmup should be done:  77%|  | 2319/3000 [00:01<00:00, 1652.30it/s]warmup should be done:  76%|  | 2277/3000 [00:01<00:00, 1608.33it/s]warmup should be done:  78%|  | 2344/3000 [00:01<00:00, 1646.34it/s]warmup should be done:  77%|  | 2312/3000 [00:01<00:00, 1631.03it/s]warmup should be done:  82%| | 2465/3000 [00:01<00:00, 1638.66it/s]warmup should be done:  84%| | 2520/3000 [00:01<00:00, 1671.34it/s]warmup should be done:  83%| | 2485/3000 [00:01<00:00, 1645.13it/s]warmup should be done:  84%| | 2516/3000 [00:01<00:00, 1666.83it/s]warmup should be done:  81%| | 2439/3000 [00:01<00:00, 1608.93it/s]warmup should be done:  84%| | 2510/3000 [00:01<00:00, 1649.45it/s]warmup should be done:  83%| | 2485/3000 [00:01<00:00, 1648.91it/s]warmup should be done:  83%| | 2476/3000 [00:01<00:00, 1622.63it/s]warmup should be done:  88%| | 2650/3000 [00:01<00:00, 1643.58it/s]warmup should be done:  90%| | 2688/3000 [00:01<00:00, 1669.84it/s]warmup should be done:  88%| | 2629/3000 [00:01<00:00, 1634.24it/s]warmup should be done:  89%| | 2683/3000 [00:01<00:00, 1664.43it/s]warmup should be done:  87%| | 2600/3000 [00:01<00:00, 1609.10it/s]warmup should be done:  89%| | 2676/3000 [00:01<00:00, 1652.09it/s]warmup should be done:  88%| | 2651/3000 [00:01<00:00, 1651.36it/s]warmup should be done:  88%| | 2639/3000 [00:01<00:00, 1623.51it/s]warmup should be done:  95%|| 2856/3000 [00:01<00:00, 1671.34it/s]warmup should be done:  94%|| 2815/3000 [00:01<00:00, 1641.53it/s]warmup should be done:  95%|| 2851/3000 [00:01<00:00, 1666.36it/s]warmup should be done:  93%|| 2793/3000 [00:01<00:00, 1631.10it/s]warmup should be done:  92%|| 2761/3000 [00:01<00:00, 1608.88it/s]warmup should be done:  94%|| 2818/3000 [00:01<00:00, 1654.29it/s]warmup should be done:  95%|| 2842/3000 [00:01<00:00, 1650.74it/s]warmup should be done:  93%|| 2802/3000 [00:01<00:00, 1623.81it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1672.27it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1669.04it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1661.33it/s]warmup should be done:  99%|| 2982/3000 [00:01<00:00, 1648.45it/s]warmup should be done:  99%|| 2959/3000 [00:01<00:00, 1638.06it/s]warmup should be done:  97%|| 2924/3000 [00:01<00:00, 1615.16it/s]warmup should be done:  99%|| 2984/3000 [00:01<00:00, 1655.89it/s]warmup should be done:  99%|| 2968/3000 [00:01<00:00, 1633.16it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1648.46it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1647.40it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1635.40it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1632.82it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1615.03it/s]






warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   6%|         | 172/3000 [00:00<00:01, 1718.94it/s]warmup should be done:   6%|         | 166/3000 [00:00<00:01, 1655.23it/s]warmup should be done:   6%|         | 172/3000 [00:00<00:01, 1712.64it/s]warmup should be done:   6%|         | 170/3000 [00:00<00:01, 1691.62it/s]warmup should be done:   6%|         | 171/3000 [00:00<00:01, 1701.36it/s]warmup should be done:   6%|         | 171/3000 [00:00<00:01, 1701.93it/s]warmup should be done:   6%|         | 168/3000 [00:00<00:01, 1671.49it/s]warmup should be done:   6%|         | 170/3000 [00:00<00:01, 1691.89it/s]warmup should be done:  12%|        | 345/3000 [00:00<00:01, 1720.52it/s]warmup should be done:  11%|        | 340/3000 [00:00<00:01, 1696.57it/s]warmup should be done:  11%|         | 334/3000 [00:00<00:01, 1664.47it/s]warmup should be done:  11%|        | 342/3000 [00:00<00:01, 1703.48it/s]warmup should be done:  11%|         | 337/3000 [00:00<00:01, 1678.97it/s]warmup should be done:  11%|        | 343/3000 [00:00<00:01, 1708.76it/s]warmup should be done:  11%|        | 344/3000 [00:00<00:01, 1710.74it/s]warmup should be done:  11%|        | 343/3000 [00:00<00:01, 1706.70it/s]warmup should be done:  17%|        | 511/3000 [00:00<00:01, 1701.85it/s]warmup should be done:  17%|        | 518/3000 [00:00<00:01, 1721.96it/s]warmup should be done:  17%|        | 506/3000 [00:00<00:01, 1683.15it/s]warmup should be done:  17%|        | 515/3000 [00:00<00:01, 1711.63it/s]warmup should be done:  17%|        | 502/3000 [00:00<00:01, 1667.20it/s]warmup should be done:  17%|        | 516/3000 [00:00<00:01, 1715.25it/s]warmup should be done:  17%|        | 513/3000 [00:00<00:01, 1701.19it/s]warmup should be done:  17%|        | 516/3000 [00:00<00:01, 1710.02it/s]warmup should be done:  23%|       | 691/3000 [00:00<00:01, 1724.45it/s]warmup should be done:  23%|       | 683/3000 [00:00<00:01, 1705.34it/s]warmup should be done:  22%|       | 675/3000 [00:00<00:01, 1683.97it/s]warmup should be done:  23%|       | 688/3000 [00:00<00:01, 1716.65it/s]warmup should be done:  23%|       | 687/3000 [00:00<00:01, 1712.19it/s]warmup should be done:  22%|       | 669/3000 [00:00<00:01, 1665.10it/s]warmup should be done:  23%|       | 684/3000 [00:00<00:01, 1701.70it/s]warmup should be done:  23%|       | 688/3000 [00:00<00:01, 1712.02it/s]warmup should be done:  29%|       | 864/3000 [00:00<00:01, 1725.90it/s]warmup should be done:  29%|       | 860/3000 [00:00<00:01, 1717.55it/s]warmup should be done:  28%|       | 844/3000 [00:00<00:01, 1682.38it/s]warmup should be done:  28%|       | 854/3000 [00:00<00:01, 1702.72it/s]warmup should be done:  29%|       | 860/3000 [00:00<00:01, 1713.77it/s]warmup should be done:  28%|       | 836/3000 [00:00<00:01, 1663.63it/s]warmup should be done:  29%|       | 859/3000 [00:00<00:01, 1709.84it/s]warmup should be done:  28%|       | 855/3000 [00:00<00:01, 1688.47it/s]warmup should be done:  35%|      | 1037/3000 [00:00<00:01, 1726.26it/s]warmup should be done:  34%|      | 1033/3000 [00:00<00:01, 1719.51it/s]warmup should be done:  33%|      | 1003/3000 [00:00<00:01, 1664.73it/s]warmup should be done:  34%|      | 1032/3000 [00:00<00:01, 1713.37it/s]warmup should be done:  34%|      | 1013/3000 [00:00<00:01, 1680.96it/s]warmup should be done:  34%|      | 1025/3000 [00:00<00:01, 1700.66it/s]warmup should be done:  34%|      | 1031/3000 [00:00<00:01, 1710.15it/s]warmup should be done:  34%|      | 1026/3000 [00:00<00:01, 1693.89it/s]warmup should be done:  40%|      | 1205/3000 [00:00<00:01, 1719.46it/s]warmup should be done:  40%|      | 1210/3000 [00:00<00:01, 1723.56it/s]warmup should be done:  39%|      | 1170/3000 [00:00<00:01, 1665.08it/s]warmup should be done:  39%|      | 1182/3000 [00:00<00:01, 1680.81it/s]warmup should be done:  40%|      | 1196/3000 [00:00<00:01, 1700.02it/s]warmup should be done:  40%|      | 1204/3000 [00:00<00:01, 1709.61it/s]warmup should be done:  40%|      | 1203/3000 [00:00<00:01, 1708.63it/s]warmup should be done:  40%|      | 1196/3000 [00:00<00:01, 1694.02it/s]warmup should be done:  46%|     | 1378/3000 [00:00<00:00, 1720.85it/s]warmup should be done:  46%|     | 1384/3000 [00:00<00:00, 1726.28it/s]warmup should be done:  46%|     | 1376/3000 [00:00<00:00, 1712.77it/s]warmup should be done:  45%|     | 1351/3000 [00:00<00:00, 1680.79it/s]warmup should be done:  46%|     | 1367/3000 [00:00<00:00, 1699.67it/s]warmup should be done:  46%|     | 1374/3000 [00:00<00:00, 1706.45it/s]warmup should be done:  45%|     | 1337/3000 [00:00<00:01, 1658.25it/s]warmup should be done:  46%|     | 1367/3000 [00:00<00:00, 1698.40it/s]warmup should be done:  52%|    | 1557/3000 [00:00<00:00, 1726.71it/s]warmup should be done:  52%|    | 1551/3000 [00:00<00:00, 1721.27it/s]warmup should be done:  51%|     | 1520/3000 [00:00<00:00, 1682.88it/s]warmup should be done:  52%|    | 1548/3000 [00:00<00:00, 1712.70it/s]warmup should be done:  50%|     | 1503/3000 [00:00<00:00, 1656.02it/s]warmup should be done:  51%|     | 1537/3000 [00:00<00:00, 1693.29it/s]warmup should be done:  52%|    | 1545/3000 [00:00<00:00, 1702.58it/s]warmup should be done:  51%|    | 1538/3000 [00:00<00:00, 1699.77it/s]warmup should be done:  58%|    | 1730/3000 [00:01<00:00, 1726.09it/s]warmup should be done:  57%|    | 1724/3000 [00:01<00:00, 1721.36it/s]warmup should be done:  56%|    | 1690/3000 [00:01<00:00, 1685.22it/s]warmup should be done:  57%|    | 1720/3000 [00:01<00:00, 1712.52it/s]warmup should be done:  56%|    | 1669/3000 [00:01<00:00, 1656.89it/s]warmup should be done:  57%|    | 1716/3000 [00:01<00:00, 1700.84it/s]warmup should be done:  57%|    | 1709/3000 [00:01<00:00, 1699.92it/s]warmup should be done:  57%|    | 1707/3000 [00:01<00:00, 1689.05it/s]warmup should be done:  63%|   | 1904/3000 [00:01<00:00, 1728.49it/s]warmup should be done:  63%|   | 1897/3000 [00:01<00:00, 1722.41it/s]warmup should be done:  62%|   | 1863/3000 [00:01<00:00, 1697.19it/s]warmup should be done:  63%|   | 1893/3000 [00:01<00:00, 1714.96it/s]warmup should be done:  61%|    | 1835/3000 [00:01<00:00, 1656.46it/s]warmup should be done:  63%|   | 1880/3000 [00:01<00:00, 1702.92it/s]warmup should be done:  63%|   | 1887/3000 [00:01<00:00, 1700.36it/s]warmup should be done:  63%|   | 1876/3000 [00:01<00:00, 1686.21it/s]warmup should be done:  69%|   | 2078/3000 [00:01<00:00, 1728.96it/s]warmup should be done:  69%|   | 2070/3000 [00:01<00:00, 1722.34it/s]warmup should be done:  68%|   | 2036/3000 [00:01<00:00, 1705.12it/s]warmup should be done:  69%|   | 2065/3000 [00:01<00:00, 1715.98it/s]warmup should be done:  67%|   | 2001/3000 [00:01<00:00, 1655.31it/s]warmup should be done:  68%|   | 2051/3000 [00:01<00:00, 1704.05it/s]warmup should be done:  69%|   | 2058/3000 [00:01<00:00, 1699.88it/s]warmup should be done:  68%|   | 2047/3000 [00:01<00:00, 1690.36it/s]warmup should be done:  75%|  | 2251/3000 [00:01<00:00, 1728.13it/s]warmup should be done:  75%|  | 2243/3000 [00:01<00:00, 1721.47it/s]warmup should be done:  74%|  | 2208/3000 [00:01<00:00, 1707.97it/s]warmup should be done:  75%|  | 2237/3000 [00:01<00:00, 1715.05it/s]warmup should be done:  72%|  | 2167/3000 [00:01<00:00, 1654.86it/s]warmup should be done:  74%|  | 2222/3000 [00:01<00:00, 1703.01it/s]warmup should be done:  74%|  | 2217/3000 [00:01<00:00, 1690.72it/s]warmup should be done:  74%|  | 2228/3000 [00:01<00:00, 1691.07it/s]warmup should be done:  81%|  | 2424/3000 [00:01<00:00, 1726.59it/s]warmup should be done:  81%|  | 2416/3000 [00:01<00:00, 1720.16it/s]warmup should be done:  79%|  | 2380/3000 [00:01<00:00, 1710.76it/s]warmup should be done:  78%|  | 2333/3000 [00:01<00:00, 1656.30it/s]warmup should be done:  80%|  | 2409/3000 [00:01<00:00, 1713.28it/s]warmup should be done:  80%|  | 2393/3000 [00:01<00:00, 1702.46it/s]warmup should be done:  80%|  | 2387/3000 [00:01<00:00, 1687.60it/s]warmup should be done:  80%|  | 2398/3000 [00:01<00:00, 1671.69it/s]warmup should be done:  87%| | 2597/3000 [00:01<00:00, 1722.25it/s]warmup should be done:  86%| | 2589/3000 [00:01<00:00, 1719.46it/s]warmup should be done:  85%| | 2552/3000 [00:01<00:00, 1709.80it/s]warmup should be done:  83%| | 2499/3000 [00:01<00:00, 1655.13it/s]warmup should be done:  86%| | 2581/3000 [00:01<00:00, 1713.31it/s]warmup should be done:  85%| | 2564/3000 [00:01<00:00, 1700.64it/s]warmup should be done:  85%| | 2558/3000 [00:01<00:00, 1691.38it/s]warmup should be done:  86%| | 2567/3000 [00:01<00:00, 1674.59it/s]warmup should be done:  92%|| 2770/3000 [00:01<00:00, 1723.42it/s]warmup should be done:  92%|| 2762/3000 [00:01<00:00, 1720.28it/s]warmup should be done:  92%|| 2753/3000 [00:01<00:00, 1714.19it/s]warmup should be done:  91%| | 2723/3000 [00:01<00:00, 1705.12it/s]warmup should be done:  91%| | 2735/3000 [00:01<00:00, 1702.18it/s]warmup should be done:  89%| | 2665/3000 [00:01<00:00, 1645.43it/s]warmup should be done:  91%| | 2729/3000 [00:01<00:00, 1694.95it/s]warmup should be done:  91%| | 2736/3000 [00:01<00:00, 1679.10it/s]warmup should be done:  98%|| 2944/3000 [00:01<00:00, 1725.66it/s]warmup should be done:  98%|| 2935/3000 [00:01<00:00, 1720.56it/s]warmup should be done:  98%|| 2925/3000 [00:01<00:00, 1714.65it/s]warmup should be done:  96%|| 2894/3000 [00:01<00:00, 1697.33it/s]warmup should be done:  97%|| 2906/3000 [00:01<00:00, 1701.34it/s]warmup should be done:  94%|| 2830/3000 [00:01<00:00, 1638.02it/s]warmup should be done:  97%|| 2899/3000 [00:01<00:00, 1678.88it/s]warmup should be done:  97%|| 2904/3000 [00:01<00:00, 1676.38it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1725.06it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1719.02it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1713.09it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1699.84it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1692.91it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1692.51it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1691.17it/s]warmup should be done: 100%|| 2994/3000 [00:01<00:00, 1636.29it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1652.75it/s]2022-12-12 04:50:12.100523: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7eddd002d8b0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 04:50:12.100583: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 04:50:13.078812: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7edd7402d880 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 04:50:13.078884: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 04:50:13.113750: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7edd0402d9b0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 04:50:13.113819: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 04:50:13.113903: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7efb9f0c1cc0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 04:50:13.113962: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 04:50:13.115086: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7edde4031d70 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 04:50:13.115147: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 04:50:13.603913: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7efb9f82c8a0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 04:50:13.603983: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 04:50:13.604805: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7efba382cc00 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 04:50:13.604872: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 04:50:13.658432: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7edd24031df0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 04:50:13.658503: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 04:50:14.315060: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 04:50:15.347124: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 04:50:15.405603: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 04:50:15.406212: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 04:50:15.442394: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 04:50:15.907846: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 04:50:15.956418: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 04:50:15.958048: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 04:50:17.124827: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 04:50:18.226597: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 04:50:18.284426: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 04:50:18.325083: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 04:50:18.344340: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 04:50:18.791290: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 04:50:18.829888: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 04:50:18.872242: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
[HCTR][04:50:43.465][ERROR][RK0][main]: replica 5 reaches 1000, calling init pre replica
[HCTR][04:50:43.466][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][04:50:43.469][ERROR][RK0][tid #139619639219968]: replica 2 reaches 1000, calling init pre replica
[HCTR][04:50:43.469][ERROR][RK0][tid #139619639219968]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][04:50:43.472][ERROR][RK0][main]: coll ps creation done
[HCTR][04:50:43.472][ERROR][RK0][main]: replica 5 waits for coll ps creation barrier
[HCTR][04:50:43.473][ERROR][RK0][tid #139620041873152]: replica 4 reaches 1000, calling init pre replica
[HCTR][04:50:43.473][ERROR][RK0][tid #139620041873152]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][04:50:43.477][ERROR][RK0][tid #139620041873152]: replica 7 reaches 1000, calling init pre replica
[HCTR][04:50:43.477][ERROR][RK0][tid #139620041873152]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][04:50:43.478][ERROR][RK0][tid #139619639219968]: coll ps creation done
[HCTR][04:50:43.478][ERROR][RK0][tid #139619639219968]: replica 2 waits for coll ps creation barrier
[HCTR][04:50:43.481][ERROR][RK0][main]: replica 6 reaches 1000, calling init pre replica
[HCTR][04:50:43.481][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][04:50:43.482][ERROR][RK0][tid #139620041873152]: coll ps creation done
[HCTR][04:50:43.482][ERROR][RK0][tid #139620041873152]: replica 4 waits for coll ps creation barrier
[HCTR][04:50:43.482][ERROR][RK0][tid #139620041873152]: coll ps creation done
[HCTR][04:50:43.482][ERROR][RK0][tid #139620041873152]: replica 7 waits for coll ps creation barrier
[HCTR][04:50:43.486][ERROR][RK0][main]: replica 3 reaches 1000, calling init pre replica
[HCTR][04:50:43.486][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][04:50:43.489][ERROR][RK0][main]: coll ps creation done
[HCTR][04:50:43.489][ERROR][RK0][main]: replica 6 waits for coll ps creation barrier
[HCTR][04:50:43.491][ERROR][RK0][main]: coll ps creation done
[HCTR][04:50:43.491][ERROR][RK0][main]: replica 3 waits for coll ps creation barrier
[HCTR][04:50:43.558][ERROR][RK0][main]: replica 1 reaches 1000, calling init pre replica
[HCTR][04:50:43.558][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][04:50:43.563][ERROR][RK0][main]: coll ps creation done
[HCTR][04:50:43.563][ERROR][RK0][main]: replica 1 waits for coll ps creation barrier
[HCTR][04:50:43.578][ERROR][RK0][tid #139619505002240]: replica 0 reaches 1000, calling init pre replica
[HCTR][04:50:43.578][ERROR][RK0][tid #139619505002240]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][04:50:43.582][ERROR][RK0][tid #139619505002240]: coll ps creation done
[HCTR][04:50:43.582][ERROR][RK0][tid #139619505002240]: replica 0 waits for coll ps creation barrier
[HCTR][04:50:43.582][ERROR][RK0][tid #139619505002240]: replica 0 preparing frequency
[HCTR][04:50:44.429][ERROR][RK0][tid #139619505002240]: replica 0 preparing frequency done
[HCTR][04:50:44.479][ERROR][RK0][tid #139619505002240]: replica 0 calling init per replica
[HCTR][04:50:44.479][ERROR][RK0][main]: replica 6 calling init per replica
[HCTR][04:50:44.479][ERROR][RK0][main]: replica 3 calling init per replica
[HCTR][04:50:44.479][ERROR][RK0][tid #139620041873152]: replica 7 calling init per replica
[HCTR][04:50:44.479][ERROR][RK0][main]: replica 1 calling init per replica
[HCTR][04:50:44.479][ERROR][RK0][tid #139620041873152]: replica 4 calling init per replica
[HCTR][04:50:44.479][ERROR][RK0][tid #139619639219968]: replica 2 calling init per replica
[HCTR][04:50:44.479][ERROR][RK0][main]: replica 5 calling init per replica
[HCTR][04:50:44.479][ERROR][RK0][tid #139619505002240]: Calling build_v2
[HCTR][04:50:44.479][ERROR][RK0][main]: Calling build_v2
[HCTR][04:50:44.479][ERROR][RK0][main]: Calling build_v2
[HCTR][04:50:44.479][ERROR][RK0][tid #139620041873152]: Calling build_v2
[HCTR][04:50:44.479][ERROR][RK0][main]: Calling build_v2
[HCTR][04:50:44.479][ERROR][RK0][tid #139620041873152]: Calling build_v2
[HCTR][04:50:44.479][ERROR][RK0][tid #139619639219968]: Calling build_v2
[HCTR][04:50:44.479][ERROR][RK0][tid #139619505002240]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][04:50:44.479][ERROR][RK0][tid #139620041873152]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][04:50:44.479][ERROR][RK0][main]: Calling build_v2
[HCTR][04:50:44.479][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][04:50:44.479][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][04:50:44.479][ERROR][RK0][tid #139620041873152]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][04:50:44.479][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][04:50:44.479][ERROR][RK0][tid #139619639219968]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][04:50:44.479][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[2022-12-12 04:50:44.483740: E [/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:178] v100x8, slow pcie2022-12-12 04:50:44
.483781: E [/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 04:50:44:[.178483829] : v100x8, slow pcieE2022-12-12 04:50:44
 ./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc483827:: [196E2022-12-12 04:50:44]  .assigning 0 to cpu/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[483872
:: 178E] 2022-12-12 04:50:44 v100x8, slow pcie./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
483878:: 196E] [ assigning 0 to cpu2022-12-12 04:50:44/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[[
.:2022-12-12 04:50:44483935178.: 2022-12-12 04:50:44] 483956E[.v100x8, slow pcie:  483928
E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[2022-12-12 04:50:44: [ :[2022-12-12 04:50:44.2022-12-12 04:50:44E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc1962022-12-12 04:50:44.483973. :] [.484033: 484016/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc212assigning 0 to cpu484036: E: :] 
: E E178build asymm link desc with 8X Tesla V100-SXM2-32GB out of 82022-12-12 04:50:44E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc ] 
. /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccv100x8, slow pcie484062/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:178:
[: [:] 2121782022-12-12 04:50:44E2022-12-12 04:50:44[196v100x8, slow pcie] ] . .2022-12-12 04:50:44] 
build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8v100x8, slow pcie484182/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc484184.assigning 0 to cpu

[: :[: 484208
2022-12-12 04:50:44E178[2022-12-12 04:50:44E: . ] 2022-12-12 04:50:44. E484278/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccv100x8, slow pcie.[484308/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc : :
4843122022-12-12 04:50:44: :/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE213: .E[212: ] E484362 2022-12-12 04:50:44] 196/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccremote time is 8.68421 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8] :
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE:484412
assigning 0 to cpu196: [196: 
] 213[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 04:50:44] Eassigning 0 to cpu] 2022-12-12 04:50:44:.assigning 0 to cpu 
[remote time is 8.68421.212484512
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 04:50:44
484531[] : :.: 2022-12-12 04:50:44[build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8E196484574E.2022-12-12 04:50:44
 ] : [[ 484624./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccassigning 0 to cpuE2022-12-12 04:50:442022-12-12 04:50:44/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: 484629:
 ..:E: 214/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc484664484689213 E] :: : ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc cpu time is 97.0588212[EEremote time is 8.68421:/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
] 2022-12-12 04:50:44  
212:build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] [214
484782::build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8[2022-12-12 04:50:44] : 212213
2022-12-12 04:50:44.cpu time is 97.0588E] ] .484859
 [build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8remote time is 8.68421484906: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 04:50:44

: E:[.E [2122022-12-12 04:50:44484943 /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 04:50:44] .: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:.build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8485000E:214484999
:  213] : E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] [cpu time is 97.0588E :remote time is 8.684212022-12-12 04:50:44
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc213
./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:] 485078:[213remote time is 8.68421: 2142022-12-12 04:50:44] 
E] .remote time is 8.68421 /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cccpu time is 97.0588[485140
:
2022-12-12 04:50:44: 213[.E] 2022-12-12 04:50:44485182 remote time is 8.68421.: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
485215E::  214E[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc]  2022-12-12 04:50:44:cpu time is 97.0588/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.214
:485268] 214: cpu time is 97.0588] E
cpu time is 97.0588 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:214] cpu time is 97.0588
[2022-12-12 04:52:03.805088: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:118] solver created. now build & solve
[2022-12-12 04:52:03.845259: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:123] solver built. now solve
block 0 storage is 00010001
	access is	0	0	0	0	4	4	4	4	
block 1 storage is 00100010
	access is	1	1	1	1	5	5	5	5	
block 2 storage is 01000100
	access is	2	2	2	2	6	6	6	6	
block 3 storage is 10001000
	access is	3	3	3	3	7	7	7	7	
block 4 storage is 00000000
	access is	8	8	8	8	8	8	8	8	
[2022-12-12 04:52:03.971861: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:127] solver solved
[2022-12-12 04:52:03.971923: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:195] 0 solved master
[2022-12-12 04:52:03.971956: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 0 solved
[2022-12-12 04:52:03.971986: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 0 initing device 0
[2022-12-12 04:52:03.972471: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-12 04:52:03.972517: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 04:52:03.973407: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 04:52:03.974060: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 04:52:03.987237: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 4 solved
[2022-12-12 04:52:03.987303: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 4 initing device 4
[[2022-12-12 04:52:032022-12-12 04:52:03..987624987633: : [EE2022-12-12 04:52:03  ./hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc[987659::2022-12-12 04:52:03: 202202.E] ] 987681[ 5 solved1 solved: 2022-12-12 04:52:03/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc

E.: 987724[[202/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc: 2022-12-12 04:52:032022-12-12 04:52:03] :E..2 solved202 987758987759
] /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu: : 6 solved:E[E
1815 2022-12-12 04:52:03 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc.[/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.ccBuilding Coll Cache with ... num gpu device is 8:9878092022-12-12 04:52:03:
205: .205] E987830] worker 0 thread 5 initing device 5 : worker 0 thread 1 initing device 1[
/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.ccE
2022-12-12 04:52:03: .205/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc987874] :: worker 0 thread 2 initing device 2205E
]  worker 0 thread 6 initing device 6/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu
:1980] eager alloc mem 381.47 MB
[[2022-12-12 04:52:03.2022-12-12 04:52:03987944.: 987944E:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc :/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc202:] 202] 3 solved
7 solved
[2022-12-12 04:52:03[.2022-12-12 04:52:03988047.: 988050E:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc :/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc205:] 205worker 0 thread 3 initing device 3] 
worker 0 thread 7 initing device 7
[2022-12-12 04:52:03[.2022-12-12 04:52:03988298.: 988305E:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu [:/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu2022-12-12 04:52:031815:.] [1815988329Building Coll Cache with ... num gpu device is 82022-12-12 04:52:03] : 
.Building Coll Cache with ... num gpu device is 8E988343
 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE:[ 18152022-12-12 04:52:03/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu] [.:Building Coll Cache with ... num gpu device is 82022-12-12 04:52:039883831815
.: ] 988392EBuilding Coll Cache with ... num gpu device is 8:  
E/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu[ :2022-12-12 04:52:03/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu1980.:[] 98842919802022-12-12 04:52:03eager alloc mem 381.47 MB: ] .
Eeager alloc mem 381.47 MB988445 
: /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE: 1980/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu] :eager alloc mem 381.47 MB1980
] eager alloc mem 381.47 MB
[[2022-12-12 04:52:032022-12-12 04:52:03..988524988524: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::18151815] ] Building Coll Cache with ... num gpu device is 8Building Coll Cache with ... num gpu device is 8

[[2022-12-12 04:52:032022-12-12 04:52:03..988600988600: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::19801980] ] eager alloc mem 381.47 MBeager alloc mem 381.47 MB

[2022-12-12 04:52:03.992274: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 04:52:03.992706: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 04:52:03.992769: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 04:52:03.992816: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 04:52:03.993375: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 04:52:03.993435: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 04:52:03.993485: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 04:52:03.996847: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 04:52:03.997147: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 04:52:03.997202: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 04:52:03.997247: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 04:52:03.997796: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 04:52:03.997845: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 04:52:03.997900: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 04:52:04. 51880: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.00 Bytes
[2022-12-12 04:52:04. 57394: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5
[2022-12-12 04:52:04. 57521: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 04:52:04. 58472: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 76.68 MB
[2022-12-12 04:52:04. 59194: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:52:04. 60294: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:52:04. 60341: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 76.29 MB
[2022-12-12 04:52:04. 83542[[: [[2022-12-12 04:52:042022-12-12 04:52:04E2022-12-12 04:52:042022-12-12 04:52:04.. .. 83583 83583/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu 83600 83602: : :: : EE1980EE  ]   /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cueager alloc mem 5.00 Bytes/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu
/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::::1980198019801980] ] ] ] eager alloc mem 5.00 Byteseager alloc mem 5.00 Byteseager alloc mem 5.00 Byteseager alloc mem 5.00 Bytes



[[2022-12-12 04:52:042022-12-12 04:52:04.. 86057 86056: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::19801980] ] eager alloc mem 5.00 Byteseager alloc mem 5.00 Bytes

[2022-12-12 04:52:04. 89686: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5
[2022-12-12 04:52:04. 89773: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc[:2022-12-12 04:52:04638.]  89771eager release cuda mem 400000000: 
E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5
[[2022-12-12 04:52:042022-12-12 04:52:04.. 89859 89847: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::638638] ] eager release cuda mem 400000000eager release cuda mem 5

[2022-12-12 04:52:04[.2022-12-12 04:52:04 89946.:  89936E:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc :/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc638:] 638eager release cuda mem 400000000] 
eager release cuda mem 5
[[2022-12-12 04:52:042022-12-12 04:52:04.. 90032 90019: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::638638] ] eager release cuda mem 400000000eager release cuda mem 5

[2022-12-12 04:52:04. 90125: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 04:52:04. 90648: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 76.68 MB
[2022-12-12 04:52:04. 91152: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5
[2022-12-12 04:52:04. 91237: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638[] 2022-12-12 04:52:04eager release cuda mem 400000000.
 91239: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5
[2022-12-12 04:52:04. 91329: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 04:52:04. 96959: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 76.68 MB
[2022-12-12 04:52:04. 97702: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 76.68 MB
[2022-12-12 04:52:04. 98630: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 76.68 MB
[2022-12-12 04:52:04. 99268: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 76.68 MB
[2022-12-12 04:52:04.100015: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:52:04.100443: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 76.68 MB
[2022-12-12 04:52:04.100949: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 76.68 MB
[2022-12-12 04:52:04.101055: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:52:04.101107: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 76.29 MB
[2022-12-12 04:52:04.101826: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:52:04.101935: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:52:04.102200: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:52:04.102244: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:52:04.102542: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:52:04.102599: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:52:04.102859: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:52:04.102905: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 76.29 MB
[2022-12-12 04:52:04.102970: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:52:04.103014: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 76.29 MB
[2022-12-12 04:52:04.103245: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:52:04.103292[: 2022-12-12 04:52:04W. 103295/hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc: :E43 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccWORKER[0] alloc host memory 76.29 MB:
638] eager release cuda mem 625663
[2022-12-12 04:52:04.103359: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 76.29 MB
[2022-12-12 04:52:04.103638: E [/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc2022-12-12 04:52:04:.638103655] : eager release cuda mem 625663E
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:52:04.103705: W[ 2022-12-12 04:52:04/hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc.:10371643: ] WWORKER[0] alloc host memory 76.29 MB 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 76.29 MB
[2022-12-12 04:52:04.111783: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 04:52:04.112403: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 04:52:04.112448: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 9.54 GB
[2022-12-12 04:52:04.151277: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 04:52:04.151906: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 04:52:04.151950: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 9.54 GB
[2022-12-12 04:52:04.152711: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 04:52:04.153199: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 04:52:04.153324: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 04:52:04.153369: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 9.54 GB
[2022-12-12 04:52:04.153642: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 04:52:04.153752: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 04:52:04.153804: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638[] 2022-12-12 04:52:04eager release cuda mem 25855.
153809: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] [eager alloc mem 25.25 KB2022-12-12 04:52:04
.153857: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 9.54 GB
[2022-12-12 04:52:04.154253: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 04:52:04.154298: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 9.54 GB
[2022-12-12 04:52:04.154366: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 04:52:04.154411: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 9.54 GB
[2022-12-12 04:52:04.154435: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 04:52:04.154479: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 9.54 GB
[2022-12-12 04:52:04.154991: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 04:52:04.155600: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 04:52:04.155645: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 9.54 GB
[[[[[[[[2022-12-12 04:52:072022-12-12 04:52:072022-12-12 04:52:072022-12-12 04:52:072022-12-12 04:52:072022-12-12 04:52:072022-12-12 04:52:072022-12-12 04:52:07........752066752073752065752066752066752066752066752066: : : : : : : : EEEEEEEE        /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::::::::19261926192619261926192619261926] ] ] ] ] ] ] ] Device 4 init p2p of link 5Device 0 init p2p of link 3Device 2 init p2p of link 1Device 6 init p2p of link 0Device 1 init p2p of link 7Device 7 init p2p of link 4Device 3 init p2p of link 2Device 5 init p2p of link 6







[[[[[[2022-12-12 04:52:072022-12-12 04:52:072022-12-12 04:52:07[2022-12-12 04:52:072022-12-12 04:52:072022-12-12 04:52:07[...2022-12-12 04:52:07...2022-12-12 04:52:07752623752623752623.752627752633752625.: : : 752633: : : 752645EEE: EEE:    E   E/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu :::/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:::/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu198019801980:1980] 19801980:] ] ] 1980eager alloc mem 611.00 KB] ] 1980eager alloc mem 611.00 KBeager alloc mem 611.00 KBeager alloc mem 611.00 KB] 
eager alloc mem 611.00 KBeager alloc mem 611.00 KB] 


eager alloc mem 611.00 KB

eager alloc mem 611.00 KB

[2022-12-12 04:52:07.753738[: 2022-12-12 04:52:07E. 753748/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc: :E638 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cceager release cuda mem 625663:
638] eager release cuda mem 625663
[[2022-12-12 04:52:072022-12-12 04:52:07.[.7538842022-12-12 04:52:07[753886[: .[2022-12-12 04:52:07: 2022-12-12 04:52:07E7538922022-12-12 04:52:07.E. : .753903 753906/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccE753911: /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc: : : E:E638/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccE 638 ] : /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc] /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cceager release cuda mem 625663638/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:eager release cuda mem 625663:
] :638
638eager release cuda mem 625663638] ] 
] eager release cuda mem 625663eager release cuda mem 625663eager release cuda mem 625663


[2022-12-12 04:52:07.767208: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 2
[2022-12-12 04:52:07.767366: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:52:07.767456: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 6 init p2p of link 5
[2022-12-12 04:52:07.767610: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:52:07.768291: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:52:07.768529: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:52:07.775942: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 4 init p2p of link 7
[2022-12-12 04:52:07.776102: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:52:07.776417: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 3 init p2p of link 0
[2022-12-12 04:52:07.776580: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:52:07.776650: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 7 init p2p of link 1
[2022-12-12 04:52:07.776734: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 2 init p2p of link 3
[2022-12-12 04:52:07.776807: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:52:07.776887: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:52:07.777021[: 2022-12-12 04:52:07E. 777017/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc: :E638 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cueager release cuda mem 625663:
1926] Device 0 init p2p of link 6
[2022-12-12 04:52:07.777208: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:52:07.777441: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 5 init p2p of link 4
[2022-12-12 04:52:07.777499: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:52:07.777582: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663[
2022-12-12 04:52:07.777613: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:52:07.777830: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:52:07.777984: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:52:07.778565: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:52:07.780874: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 3
[2022-12-12 04:52:07.781000: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:52:07.781203: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 6 init p2p of link 4
[2022-12-12 04:52:07.781325: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:52:07.781901: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:52:07.782227: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:52:07.789992: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 7 init p2p of link 6
[2022-12-12 04:52:07.790119: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:52:07.790700: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 0 init p2p of link 1
[2022-12-12 04:52:07.790845: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:52:07.791064: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:52:07.791798: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:52:07.797698: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 4 init p2p of link 2
[2022-12-12 04:52:07.797835: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:52:07.798002: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 3 init p2p of link 5
[2022-12-12 04:52:07.798130: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:52:07.798482: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 2 init p2p of link 0
[2022-12-12 04:52:07.798613[: 2022-12-12 04:52:07E. 798619/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu: :E1980 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cceager alloc mem 611.00 KB:
638] eager release cuda mem 625663
[2022-12-12 04:52:07.798912: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:52:07.799535: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:52:07.799580: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 5 init p2p of link 7
[2022-12-12 04:52:07.799708: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:52:07.800609: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:52:07.806081: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 4 init p2p of link 6
[2022-12-12 04:52:07.806208: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:52:07.806361: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 3 init p2p of link 1
[2022-12-12 04:52:07.806493: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:52:07.807147: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:52:07.807423: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:52:07.809782: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 0
[2022-12-12 04:52:07.809902: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:52:07.810107: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 6 init p2p of link 7
[2022-12-12 04:52:07.810224: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:52:07.810828: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:52:07.811152: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:52:07.812661: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 7 init p2p of link 5
[2022-12-12 04:52:07.812779: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:52:07.812987: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 0 init p2p of link 2
[2022-12-12 04:52:07.813112: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:52:07.813697: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:52:07.814034: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:52:07.823919: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 2 init p2p of link 4
[2022-12-12 04:52:07.824043: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:52:07.824299: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 5 init p2p of link 3
[2022-12-12 04:52:07.824420: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:52:07.824821: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:52:07.825193: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:52:07.827672: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 80400000
[2022-12-12 04:52:07.828745: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 80400000
[2022-12-12 04:52:07.828825: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 20000000 / 100000000 nodes ( 20.00 %~20.00 %) | remote 60000000 / 100000000 nodes ( 60.00 %) | cpu 20000000 / 100000000 nodes ( 20.00 %) | 9.54 GB | 3.84096 secs 
[2022-12-12 04:52:07.829117: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 80400000
[2022-12-12 04:52:07.829562: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 80400000
[2022-12-12 04:52:07.830018: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 20000000 / 100000000 nodes ( 20.00 %~20.00 %) | remote 60000000 / 100000000 nodes ( 60.00 %) | cpu 20000000 / 100000000 nodes ( 20.00 %) | 9.54 GB | 3.84143 secs 
[2022-12-12 04:52:07.830182: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 20000000 / 100000000 nodes ( 20.00 %~20.00 %) | remote 60000000 / 100000000 nodes ( 60.00 %) | cpu 20000000 / 100000000 nodes ( 20.00 %) | 9.54 GB | 3.84176 secs 
[2022-12-12 04:52:07.830441: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 20000000 / 100000000 nodes ( 20.00 %~20.00 %) | remote 60000000 / 100000000 nodes ( 60.00 %) | cpu 20000000 / 100000000 nodes ( 20.00 %) | 9.54 GB | 3.84207 secs 
[2022-12-12 04:52:07.833995: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 80400000
[2022-12-12 04:52:07.834419: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 20000000 / 100000000 nodes ( 20.00 %~20.00 %) | remote 60000000 / 100000000 nodes ( 60.00 %) | cpu 20000000 / 100000000 nodes ( 20.00 %) | 9.54 GB | 3.84603 secs 
[2022-12-12 04:52:07.834851: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 80400000
[2022-12-12 04:52:07.835272: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 20000000 / 100000000 nodes ( 20.00 %~20.00 %) | remote 60000000 / 100000000 nodes ( 60.00 %) | cpu 20000000 / 100000000 nodes ( 20.00 %) | 9.54 GB | 3.84683 secs 
[2022-12-12 04:52:07.836818: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 80400000
[2022-12-12 04:52:07.837079: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 80400000
[2022-12-12 04:52:07.837244: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 20000000 / 100000000 nodes ( 20.00 %~20.00 %) | remote 60000000 / 100000000 nodes ( 60.00 %) | cpu 20000000 / 100000000 nodes ( 20.00 %) | 9.54 GB | 3.84866 secs 
[2022-12-12 04:52:07.837575: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 20000000 / 100000000 nodes ( 20.00 %~20.00 %) | remote 60000000 / 100000000 nodes ( 60.00 %) | cpu 20000000 / 100000000 nodes ( 20.00 %) | 9.54 GB | 3.86507 secs 
[HCTR][04:52:07.837][ERROR][RK0][main]: replica 3 calling init per replica done, doing barrier
[HCTR][04:52:07.837][ERROR][RK0][main]: replica 5 calling init per replica done, doing barrier
[HCTR][04:52:07.837][ERROR][RK0][tid #139620041873152]: replica 7 calling init per replica done, doing barrier
[HCTR][04:52:07.837][ERROR][RK0][tid #139619639219968]: replica 2 calling init per replica done, doing barrier
[HCTR][04:52:07.837][ERROR][RK0][main]: replica 6 calling init per replica done, doing barrier
[HCTR][04:52:07.837][ERROR][RK0][main]: replica 1 calling init per replica done, doing barrier
[HCTR][04:52:07.837][ERROR][RK0][tid #139620041873152]: replica 4 calling init per replica done, doing barrier
[HCTR][04:52:07.837][ERROR][RK0][tid #139619505002240]: replica 0 calling init per replica done, doing barrier
[HCTR][04:52:07.837][ERROR][RK0][main]: replica 6 calling init per replica done, doing barrier done
[HCTR][04:52:07.837][ERROR][RK0][tid #139619505002240]: replica 0 calling init per replica done, doing barrier done
[HCTR][04:52:07.837][ERROR][RK0][main]: replica 1 calling init per replica done, doing barrier done
[HCTR][04:52:07.837][ERROR][RK0][main]: replica 3 calling init per replica done, doing barrier done
[HCTR][04:52:07.837][ERROR][RK0][tid #139619639219968]: replica 2 calling init per replica done, doing barrier done
[HCTR][04:52:07.837][ERROR][RK0][main]: replica 5 calling init per replica done, doing barrier done
[HCTR][04:52:07.837][ERROR][RK0][tid #139620041873152]: replica 4 calling init per replica done, doing barrier done
[HCTR][04:52:07.837][ERROR][RK0][tid #139620041873152]: replica 7 calling init per replica done, doing barrier done
[HCTR][04:52:07.837][ERROR][RK0][main]: init per replica done
[HCTR][04:52:07.837][ERROR][RK0][main]: init per replica done
[HCTR][04:52:07.837][ERROR][RK0][main]: init per replica done
[HCTR][04:52:07.837][ERROR][RK0][tid #139619639219968]: init per replica done
[HCTR][04:52:07.837][ERROR][RK0][main]: init per replica done
[HCTR][04:52:07.837][ERROR][RK0][tid #139620041873152]: init per replica done
[HCTR][04:52:07.837][ERROR][RK0][tid #139620041873152]: init per replica done
[HCTR][04:52:07.840][ERROR][RK0][tid #139619505002240]: init per replica done








