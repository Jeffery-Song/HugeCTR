2022-12-11 20:44:53.820452: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:53.831884: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:53.836486: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:53.841130: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:53.846648: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:53.857109: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:53.871903: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:53.877905: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:53.927481: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:53.932778: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:53.935527: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:53.936485: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:53.937448: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:53.938375: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:53.939541: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:53.940819: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:53.942729: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:53.943731: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:53.943793: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:53.945488: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:53.945688: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:53.946967: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:53.947449: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:53.948504: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:53.949063: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:53.950106: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:53.950651: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:53.951646: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:53.952443: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:53.953226: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:53.954166: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:53.955395: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:53.955459: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:53.956968: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:53.957957: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:53.958840: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:53.959753: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:53.960637: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:53.961593: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:53.962603: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:53.967848: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 20:44:53.969403: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:53.970472: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:53.971488: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:53.972438: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:53.973453: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:53.974499: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:53.975432: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:53.976374: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:53.977530: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:53.978712: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:53.980452: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:53.981126: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:53.983353: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:53.983492: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:53.985987: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:53.986116: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:53.986358: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:53.988711: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:53.988953: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:53.989431: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:53.991906: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:53.992130: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:53.992628: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:53.993213: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:53.994896: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:53.995010: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:53.995503: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:53.996371: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:53.998071: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:53.998115: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:53.998594: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:53.999646: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:54.000619: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:54.001074: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:54.001297: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:54.001558: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:54.001753: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:54.002928: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:54.004272: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:54.004889: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:54.005200: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:54.005309: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:54.006397: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:54.007484: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:54.008114: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:54.008291: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:54.013473: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:54.015274: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:54.015809: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:54.017129: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:54.017627: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:54.017981: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:54.019826: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:54.020276: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:54.021809: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:54.049145: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:54.051748: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:54.053799: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:54.057286: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:54.057299: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:54.057328: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:54.057402: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:54.057403: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:54.059712: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:54.061839: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:54.061877: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:54.061918: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:54.062086: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:54.062545: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:54.065124: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:54.066616: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:54.067619: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:54.067709: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:54.067842: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:54.069667: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:54.070838: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:54.071400: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:54.071444: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:54.071653: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:54.073287: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:54.075518: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:54.075614: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:54.075755: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:54.075998: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:54.077355: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:54.079824: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:54.079863: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:54.080062: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:54.080254: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:54.081639: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:54.083736: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:54.083777: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:54.083952: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:54.084293: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:54.085378: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:54.087435: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:54.087476: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:54.087586: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:54.088025: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:54.089005: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:54.091042: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:54.091203: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:54.091292: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:54.091807: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:54.092926: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:54.095094: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:54.095149: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:54.095390: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:54.096024: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:54.096844: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:54.098856: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:54.098989: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:54.099220: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:54.100595: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:54.100714: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:54.102462: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:54.102601: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:54.102847: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:54.104044: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:54.104079: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:54.105784: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:54.106284: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:54.106420: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:54.107791: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:54.107892: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:54.108656: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:54.109784: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:54.110114: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:54.110158: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:54.111554: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:54.111829: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:54.112538: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:54.113047: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:54.114136: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:54.114267: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:54.114406: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:54.116023: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:54.116497: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:54.117193: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:54.117624: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:54.119597: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:54.119673: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:54.120135: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:54.120309: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:54.120832: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:54.121373: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:54.121932: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:54.124203: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:54.124285: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:54.124945: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:54.125654: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:54.125950: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:54.126138: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:54.126757: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:54.128472: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:54.128650: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:54.129619: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:54.130325: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:54.130644: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:54.130728: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:54.131487: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:54.133372: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:54.133581: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:54.134323: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:54.135052: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:54.135367: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:54.135960: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:54.137438: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:54.137478: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:54.138441: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:54.139034: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:54.139123: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:54.139732: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 20:44:54.139920: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:54.141423: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:54.141512: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:54.142211: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:54.142816: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:54.142926: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:54.143640: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:54.145328: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:54.146117: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:54.146214: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:54.146584: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:54.146710: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:54.147641: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:54.149014: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:54.149692: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:54.149860: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:54.150045: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:54.150689: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:54.150792: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:54.151729: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:54.154333: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:54.154955: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:54.155039: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:54.155750: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:54.157539: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 20:44:54.157627: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:54.157894: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 20:44:54.157906: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 20:44:54.158302: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:54.158641: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:54.161004: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:54.161130: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:54.161370: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 20:44:54.163491: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:54.163570: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:54.165788: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:54.165819: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:54.167197: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:54.167697: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:54.167793: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:54.169001: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:54.169747: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:54.170840: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:54.171170: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:54.172544: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:54.172641: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:54.174638: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:54.174898: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:54.176025: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:54.176040: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:54.176769: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:54.177244: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:54.179088: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:54.179438: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:54.180388: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:54.183287: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:54.211603: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:54.215768: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:54.216197: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:54.220577: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:54.221025: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:54.225037: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:54.225509: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:54.233511: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:54.233971: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:54.239484: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:54.242527: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 20:44:54.251999: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:54.274568: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:54.276126: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 20:44:54.278849: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:54.285259: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:54.295010: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:54.303217: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:55.287234: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:55.288080: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:55.288812: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:55.289292: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 20:44:55.289347: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-11 20:44:55.307340: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:55.307993: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:55.308506: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:55.309232: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:55.310130: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:55.310691: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-11 20:44:55.358910: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 20:44:55.359121: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 20:44:55.420000: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12340
2022-12-11 20:44:55.513874: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:55.514564: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:55.515377: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:55.516199: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 20:44:55.516254: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-11 20:44:55.533472: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:55.534120: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:55.534632: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:55.535233: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:55.535764: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:55.536236: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:1/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-11 20:44:55.576314: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:55.577198: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:55.577394: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:55.578164: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:55.578302: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:55.579090: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:55.579172: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 20:44:55.579241: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-11 20:44:55.579759: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 20:44:55.579803: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-11 20:44:55.596444: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:55.596543: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:55.597612: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:55.597634: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:55.598592: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:55.598629: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:55.599663: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:55.599717: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:55.600814: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:55.600958: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:55.602122: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:2/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-11 20:44:55.602147: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:3/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-11 20:44:55.606772: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:55.606801: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:55.608107: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:55.608119: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:55.609246: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:55.609286: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:55.610698: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 20:44:55.610739: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 20:44:55.610768: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-11 20:44:55.610797: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-11 20:44:55.616591: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 20:44:55.616756: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 20:44:55.617814: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12341
2022-12-11 20:44:55.628109: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:55.628594: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:55.628750: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:55.629692: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:55.629782: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:55.630682: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:55.630913: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:55.631209: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:55.631272: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:55.632152: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:55.632742: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:55.633270: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:55.633364: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:55.634126: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:55.634760: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:6/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-11 20:44:55.635199: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:55.635277: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:55.635738: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:4/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-11 20:44:55.636300: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 20:44:55.636348: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-11 20:44:55.636385: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 20:44:55.636433: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-11 20:44:55.647452: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 20:44:55.647656: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 20:44:55.648695: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12343
2022-12-11 20:44:55.653466: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:55.654135: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:55.654206: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:55.655066: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:55.655246: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:55.656162: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:55.656171: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:55.657219: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:55.657228: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:55.658152: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:7/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-11 20:44:55.658237: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:44:55.658700: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:5/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-11 20:44:55.679532: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 20:44:55.679741: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 20:44:55.681436: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12346
2022-12-11 20:44:55.684203: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 20:44:55.684398: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 20:44:55.686172: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12344
2022-12-11 20:44:55.699595: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 20:44:55.699781: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 20:44:55.700745: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12342
2022-12-11 20:44:55.704248: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 20:44:55.704426: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 20:44:55.705029: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 20:44:55.705175: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 20:44:55.705513: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12347
2022-12-11 20:44:55.706959: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12345
[HCTR][20:44:56.940][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][20:44:56.955][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][20:44:56.955][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][20:44:56.963][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][20:44:56.963][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][20:44:56.963][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][20:44:56.997][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][20:44:57.017][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 1it [00:01,  1.55s/it]warmup run: 1it [00:01,  1.50s/it]warmup run: 1it [00:01,  1.52s/it]warmup run: 97it [00:01, 81.36it/s]warmup run: 93it [00:01, 80.41it/s]warmup run: 1it [00:01,  1.54s/it]warmup run: 1it [00:01,  1.55s/it]warmup run: 1it [00:01,  1.56s/it]warmup run: 1it [00:01,  1.54s/it]warmup run: 1it [00:01,  1.55s/it]warmup run: 94it [00:01, 80.36it/s]warmup run: 194it [00:01, 176.71it/s]warmup run: 187it [00:01, 175.16it/s]warmup run: 98it [00:01, 83.02it/s]warmup run: 97it [00:01, 81.38it/s]warmup run: 92it [00:01, 77.72it/s]warmup run: 93it [00:01, 77.98it/s]warmup run: 98it [00:01, 82.00it/s]warmup run: 189it [00:01, 175.17it/s]warmup run: 293it [00:01, 284.44it/s]warmup run: 283it [00:01, 281.73it/s]warmup run: 195it [00:01, 178.88it/s]warmup run: 195it [00:01, 177.71it/s]warmup run: 189it [00:01, 172.59it/s]warmup run: 198it [00:01, 180.19it/s]warmup run: 185it [00:01, 169.60it/s]warmup run: 286it [00:01, 282.02it/s]warmup run: 390it [00:01, 393.39it/s]warmup run: 380it [00:01, 393.17it/s]warmup run: 290it [00:01, 281.88it/s]warmup run: 290it [00:01, 280.10it/s]warmup run: 286it [00:01, 278.46it/s]warmup run: 299it [00:01, 290.09it/s]warmup run: 279it [00:01, 272.22it/s]warmup run: 384it [00:01, 394.57it/s]warmup run: 488it [00:02, 501.21it/s]warmup run: 476it [00:02, 499.31it/s]warmup run: 391it [00:01, 398.67it/s]warmup run: 385it [00:01, 386.53it/s]warmup run: 384it [00:01, 390.34it/s]warmup run: 395it [00:01, 396.45it/s]warmup run: 373it [00:01, 378.60it/s]warmup run: 482it [00:02, 503.87it/s]warmup run: 587it [00:02, 602.85it/s]warmup run: 572it [00:02, 595.60it/s]warmup run: 493it [00:02, 513.86it/s]warmup run: 483it [00:02, 496.48it/s]warmup run: 483it [00:02, 500.66it/s]warmup run: 490it [00:02, 498.24it/s]warmup run: 465it [00:02, 478.88it/s]warmup run: 578it [00:02, 599.89it/s]warmup run: 687it [00:02, 692.95it/s]warmup run: 668it [00:02, 679.99it/s]warmup run: 593it [00:02, 615.97it/s]warmup run: 585it [00:02, 604.90it/s]warmup run: 583it [00:02, 604.94it/s]warmup run: 587it [00:02, 596.63it/s]warmup run: 557it [00:02, 570.39it/s]warmup run: 678it [00:02, 691.90it/s]warmup run: 783it [00:02, 754.40it/s]warmup run: 763it [00:02, 746.46it/s]warmup run: 692it [00:02, 701.41it/s]warmup run: 684it [00:02, 693.26it/s]warmup run: 683it [00:02, 695.44it/s]warmup run: 688it [00:02, 691.25it/s]warmup run: 653it [00:02, 659.90it/s]warmup run: 779it [00:02, 769.01it/s]warmup run: 881it [00:02, 812.81it/s]warmup run: 862it [00:02, 809.16it/s]warmup run: 792it [00:02, 774.65it/s]warmup run: 788it [00:02, 767.42it/s]warmup run: 783it [00:02, 769.30it/s]warmup run: 752it [00:02, 741.31it/s]warmup run: 780it [00:02, 753.04it/s]warmup run: 878it [00:02, 826.48it/s]warmup run: 981it [00:02, 863.40it/s]warmup run: 960it [00:02, 855.21it/s]warmup run: 890it [00:02, 821.22it/s]warmup run: 882it [00:02, 825.32it/s]warmup run: 849it [00:02, 800.51it/s]warmup run: 875it [00:02, 798.75it/s]warmup run: 885it [00:02, 809.56it/s]warmup run: 976it [00:02, 868.09it/s]warmup run: 1083it [00:02, 904.90it/s]warmup run: 1058it [00:02, 888.00it/s]warmup run: 987it [00:02, 856.44it/s]warmup run: 981it [00:02, 869.05it/s]warmup run: 944it [00:02, 840.13it/s]warmup run: 973it [00:02, 847.16it/s]warmup run: 982it [00:02, 852.44it/s]warmup run: 1074it [00:02, 897.42it/s]warmup run: 1185it [00:02, 937.52it/s]warmup run: 1160it [00:02, 923.44it/s]warmup run: 1084it [00:02, 878.68it/s]warmup run: 1042it [00:02, 878.37it/s]warmup run: 1079it [00:02, 896.20it/s]warmup run: 1070it [00:02, 880.70it/s]warmup run: 1083it [00:02, 894.01it/s]warmup run: 1172it [00:02, 913.05it/s]warmup run: 1286it [00:02, 957.02it/s]warmup run: 1260it [00:02, 945.13it/s]warmup run: 1180it [00:02, 900.18it/s]warmup run: 1178it [00:02, 922.11it/s]warmup run: 1138it [00:02, 898.95it/s]warmup run: 1171it [00:02, 916.12it/s]warmup run: 1181it [00:02, 907.51it/s]warmup run: 1270it [00:02, 931.98it/s]warmup run: 1387it [00:02, 971.80it/s]warmup run: 1361it [00:02, 963.73it/s]warmup run: 1280it [00:02, 928.67it/s]warmup run: 1277it [00:02, 940.17it/s]warmup run: 1234it [00:02, 914.82it/s]warmup run: 1269it [00:02, 931.16it/s]warmup run: 1278it [00:02, 920.52it/s]warmup run: 1370it [00:02, 949.46it/s]warmup run: 1489it [00:03, 984.37it/s]warmup run: 1461it [00:03, 973.33it/s]warmup run: 1381it [00:02, 950.54it/s]warmup run: 1330it [00:02, 921.43it/s]warmup run: 1369it [00:02, 950.08it/s]warmup run: 1375it [00:02, 929.00it/s]warmup run: 1375it [00:02, 932.31it/s]warmup run: 1591it [00:03, 994.49it/s]warmup run: 1468it [00:03, 956.03it/s]warmup run: 1561it [00:03, 979.99it/s]warmup run: 1482it [00:03, 967.22it/s]warmup run: 1472it [00:03, 973.35it/s]warmup run: 1426it [00:03, 931.60it/s]warmup run: 1474it [00:03, 948.06it/s]warmup run: 1471it [00:03, 922.84it/s]warmup run: 1568it [00:03, 968.64it/s]warmup run: 1693it [00:03, 1000.75it/s]warmup run: 1662it [00:03, 988.64it/s]warmup run: 1582it [00:03, 976.58it/s]warmup run: 1575it [00:03, 988.76it/s]warmup run: 1523it [00:03, 940.84it/s]warmup run: 1576it [00:03, 967.34it/s]warmup run: 1573it [00:03, 949.96it/s]warmup run: 1795it [00:03, 1004.58it/s]warmup run: 1667it [00:03, 969.48it/s]warmup run: 1763it [00:03, 991.61it/s]warmup run: 1684it [00:03, 987.65it/s]warmup run: 1620it [00:03, 948.53it/s]warmup run: 1676it [00:03, 988.90it/s]warmup run: 1679it [00:03, 985.40it/s]warmup run: 1670it [00:03, 924.49it/s]warmup run: 1897it [00:03, 1008.68it/s]warmup run: 1769it [00:03, 981.55it/s]warmup run: 1786it [00:03, 996.70it/s]warmup run: 1863it [00:03, 941.86it/s]warmup run: 1718it [00:03, 955.05it/s]warmup run: 1778it [00:03, 997.33it/s]warmup run: 1782it [00:03, 996.56it/s]warmup run: 1772it [00:03, 950.59it/s]warmup run: 1999it [00:03, 1006.00it/s]warmup run: 1871it [00:03, 991.60it/s]warmup run: 1887it [00:03, 987.98it/s]warmup run: 1966it [00:03, 966.34it/s]warmup run: 1817it [00:03, 964.09it/s]warmup run: 1881it [00:03, 1006.23it/s]warmup run: 1884it [00:03, 1003.46it/s]warmup run: 1873it [00:03, 965.88it/s]warmup run: 2120it [00:03, 1064.96it/s]warmup run: 1972it [00:03, 995.57it/s]warmup run: 2081it [00:03, 1019.05it/s]warmup run: 1987it [00:03, 976.14it/s]warmup run: 1917it [00:03, 971.93it/s]warmup run: 1983it [00:03, 1004.06it/s]warmup run: 1986it [00:03, 1006.48it/s]warmup run: 1971it [00:03, 953.53it/s]warmup run: 2240it [00:03, 1104.69it/s]warmup run: 2086it [00:03, 1036.68it/s]warmup run: 2203it [00:03, 1076.31it/s]warmup run: 2104it [00:03, 1031.74it/s]warmup run: 2019it [00:03, 985.44it/s]warmup run: 2100it [00:03, 1050.82it/s]warmup run: 2102it [00:03, 1050.31it/s]warmup run: 2073it [00:03, 971.18it/s]warmup run: 2362it [00:03, 1138.49it/s]warmup run: 2205it [00:03, 1080.45it/s]warmup run: 2324it [00:03, 1115.21it/s]warmup run: 2226it [00:03, 1085.36it/s]warmup run: 2141it [00:03, 1054.98it/s]warmup run: 2221it [00:03, 1095.55it/s]warmup run: 2222it [00:03, 1092.75it/s]warmup run: 2178it [00:03, 991.96it/s]warmup run: 2485it [00:03, 1163.08it/s]warmup run: 2325it [00:03, 1114.61it/s]warmup run: 2445it [00:03, 1141.97it/s]warmup run: 2348it [00:03, 1122.97it/s]warmup run: 2264it [00:03, 1104.68it/s]warmup run: 2341it [00:03, 1126.24it/s]warmup run: 2342it [00:03, 1122.24it/s]warmup run: 2300it [00:03, 1057.73it/s]warmup run: 2607it [00:04, 1178.99it/s]warmup run: 2446it [00:03, 1140.74it/s]warmup run: 2565it [00:04, 1159.13it/s]warmup run: 2469it [00:03, 1148.07it/s]warmup run: 2387it [00:03, 1140.24it/s]warmup run: 2459it [00:03, 1140.87it/s]warmup run: 2461it [00:03, 1142.38it/s]warmup run: 2423it [00:03, 1106.09it/s]warmup run: 2729it [00:04, 1190.72it/s]warmup run: 2565it [00:04, 1153.80it/s]warmup run: 2685it [00:04, 1170.75it/s]warmup run: 2590it [00:04, 1165.91it/s]warmup run: 2510it [00:04, 1165.59it/s]warmup run: 2580it [00:04, 1158.73it/s]warmup run: 2581it [00:04, 1157.12it/s]warmup run: 2546it [00:04, 1140.69it/s]warmup run: 2850it [00:04, 1195.55it/s]warmup run: 2684it [00:04, 1164.58it/s]warmup run: 2803it [00:04, 1173.18it/s]warmup run: 2712it [00:04, 1179.27it/s]warmup run: 2633it [00:04, 1183.54it/s]warmup run: 2701it [00:04, 1172.72it/s]warmup run: 2701it [00:04, 1168.32it/s]warmup run: 2668it [00:04, 1164.15it/s]warmup run: 2973it [00:04, 1203.88it/s]warmup run: 2801it [00:04, 1159.16it/s]warmup run: 3000it [00:04, 681.99it/s] warmup run: 2922it [00:04, 1177.64it/s]warmup run: 2832it [00:04, 1183.25it/s]warmup run: 2756it [00:04, 1194.74it/s]warmup run: 2820it [00:04, 1177.73it/s]warmup run: 2820it [00:04, 1173.62it/s]warmup run: 2789it [00:04, 1176.59it/s]warmup run: 2919it [00:04, 1163.92it/s]warmup run: 3000it [00:04, 680.91it/s] warmup run: 2953it [00:04, 1190.97it/s]warmup run: 2876it [00:04, 1195.72it/s]warmup run: 2942it [00:04, 1187.77it/s]warmup run: 2942it [00:04, 1185.49it/s]warmup run: 3000it [00:04, 679.24it/s] warmup run: 2908it [00:04, 1180.04it/s]warmup run: 3000it [00:04, 680.06it/s] warmup run: 3000it [00:04, 677.24it/s] warmup run: 3000it [00:04, 675.90it/s] warmup run: 2996it [00:04, 1173.99it/s]warmup run: 3000it [00:04, 669.06it/s] warmup run: 3000it [00:04, 670.23it/s] 


warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]




warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   5%|▌         | 163/3000 [00:00<00:01, 1629.20it/s]warmup should be done:   5%|▌         | 164/3000 [00:00<00:01, 1638.51it/s]warmup should be done:   6%|▌         | 165/3000 [00:00<00:01, 1648.38it/s]warmup should be done:   6%|▌         | 165/3000 [00:00<00:01, 1646.26it/s]warmup should be done:   5%|▌         | 160/3000 [00:00<00:01, 1593.13it/s]warmup should be done:   6%|▌         | 166/3000 [00:00<00:01, 1652.71it/s]warmup should be done:   5%|▌         | 163/3000 [00:00<00:01, 1621.22it/s]warmup should be done:   5%|▌         | 161/3000 [00:00<00:01, 1586.48it/s]warmup should be done:  11%|█         | 332/3000 [00:00<00:01, 1659.17it/s]warmup should be done:  11%|█         | 321/3000 [00:00<00:01, 1601.59it/s]warmup should be done:  11%|█         | 332/3000 [00:00<00:01, 1658.18it/s]warmup should be done:  11%|█         | 328/3000 [00:00<00:01, 1636.43it/s]warmup should be done:  11%|█         | 332/3000 [00:00<00:01, 1657.21it/s]warmup should be done:  11%|█         | 333/3000 [00:00<00:01, 1660.85it/s]warmup should be done:  11%|█         | 327/3000 [00:00<00:01, 1630.15it/s]warmup should be done:  11%|█         | 326/3000 [00:00<00:01, 1619.92it/s]warmup should be done:  17%|█▋        | 499/3000 [00:00<00:01, 1663.15it/s]warmup should be done:  17%|█▋        | 500/3000 [00:00<00:01, 1663.52it/s]warmup should be done:  17%|█▋        | 498/3000 [00:00<00:01, 1653.92it/s]warmup should be done:  16%|█▋        | 492/3000 [00:00<00:01, 1632.17it/s]warmup should be done:  16%|█▌        | 482/3000 [00:00<00:01, 1596.67it/s]warmup should be done:  17%|█▋        | 500/3000 [00:00<00:01, 1657.60it/s]warmup should be done:  16%|█▋        | 491/3000 [00:00<00:01, 1626.75it/s]warmup should be done:  16%|█▋        | 489/3000 [00:00<00:01, 1623.73it/s]warmup should be done:  22%|██▏       | 666/3000 [00:00<00:01, 1663.75it/s]warmup should be done:  22%|██▏       | 664/3000 [00:00<00:01, 1655.07it/s]warmup should be done:  22%|██▏       | 667/3000 [00:00<00:01, 1663.62it/s]warmup should be done:  22%|██▏       | 656/3000 [00:00<00:01, 1631.51it/s]warmup should be done:  21%|██▏       | 643/3000 [00:00<00:01, 1599.69it/s]warmup should be done:  22%|██▏       | 666/3000 [00:00<00:01, 1656.44it/s]warmup should be done:  22%|██▏       | 653/3000 [00:00<00:01, 1629.32it/s]warmup should be done:  22%|██▏       | 654/3000 [00:00<00:01, 1624.50it/s]warmup should be done:  28%|██▊       | 833/3000 [00:00<00:01, 1660.98it/s]warmup should be done:  28%|██▊       | 830/3000 [00:00<00:01, 1653.85it/s]warmup should be done:  28%|██▊       | 832/3000 [00:00<00:01, 1655.73it/s]warmup should be done:  28%|██▊       | 834/3000 [00:00<00:01, 1659.77it/s]warmup should be done:  27%|██▋       | 817/3000 [00:00<00:01, 1630.08it/s]warmup should be done:  27%|██▋       | 804/3000 [00:00<00:01, 1599.78it/s]warmup should be done:  27%|██▋       | 820/3000 [00:00<00:01, 1630.20it/s]warmup should be done:  27%|██▋       | 817/3000 [00:00<00:01, 1620.14it/s]warmup should be done:  33%|███▎      | 998/3000 [00:00<00:01, 1654.50it/s]warmup should be done:  33%|███▎      | 996/3000 [00:00<00:01, 1650.15it/s]warmup should be done:  33%|███▎      | 1000/3000 [00:00<00:01, 1657.78it/s]warmup should be done:  32%|███▏      | 964/3000 [00:00<00:01, 1595.87it/s]warmup should be done:  33%|███▎      | 984/3000 [00:00<00:01, 1628.97it/s]warmup should be done:  33%|███▎      | 1000/3000 [00:00<00:01, 1653.99it/s]warmup should be done:  33%|███▎      | 981/3000 [00:00<00:01, 1627.24it/s]warmup should be done:  33%|███▎      | 980/3000 [00:00<00:01, 1614.99it/s]warmup should be done:  39%|███▉      | 1164/3000 [00:00<00:01, 1650.93it/s]warmup should be done:  38%|███▊      | 1145/3000 [00:00<00:01, 1628.68it/s]warmup should be done:  39%|███▉      | 1166/3000 [00:00<00:01, 1650.68it/s]warmup should be done:  39%|███▉      | 1166/3000 [00:00<00:01, 1650.41it/s]warmup should be done:  39%|███▊      | 1162/3000 [00:00<00:01, 1643.75it/s]warmup should be done:  37%|███▋      | 1124/3000 [00:00<00:01, 1588.75it/s]warmup should be done:  38%|███▊      | 1147/3000 [00:00<00:01, 1614.76it/s]warmup should be done:  38%|███▊      | 1142/3000 [00:00<00:01, 1605.87it/s]warmup should be done:  44%|████▍     | 1330/3000 [00:00<00:01, 1650.28it/s]warmup should be done:  44%|████▎     | 1310/3000 [00:00<00:01, 1633.60it/s]warmup should be done:  44%|████▍     | 1332/3000 [00:00<00:01, 1651.82it/s]warmup should be done:  44%|████▍     | 1328/3000 [00:00<00:01, 1646.38it/s]warmup should be done:  44%|████▍     | 1332/3000 [00:00<00:01, 1641.71it/s]warmup should be done:  43%|████▎     | 1283/3000 [00:00<00:01, 1579.30it/s]warmup should be done:  43%|████▎     | 1303/3000 [00:00<00:01, 1602.23it/s]warmup should be done:  44%|████▎     | 1309/3000 [00:00<00:01, 1601.65it/s]warmup should be done:  49%|████▉     | 1475/3000 [00:00<00:00, 1637.85it/s]warmup should be done:  50%|████▉     | 1498/3000 [00:00<00:00, 1651.72it/s]warmup should be done:  50%|████▉     | 1496/3000 [00:00<00:00, 1649.65it/s]warmup should be done:  50%|████▉     | 1494/3000 [00:00<00:00, 1649.78it/s]warmup should be done:  48%|████▊     | 1442/3000 [00:00<00:00, 1581.95it/s]warmup should be done:  50%|████▉     | 1498/3000 [00:00<00:00, 1644.27it/s]warmup should be done:  49%|████▉     | 1464/3000 [00:00<00:00, 1604.15it/s]warmup should be done:  49%|████▉     | 1470/3000 [00:00<00:00, 1596.60it/s]warmup should be done:  55%|█████▌    | 1664/3000 [00:01<00:00, 1653.82it/s]warmup should be done:  55%|█████▌    | 1661/3000 [00:01<00:00, 1649.27it/s]warmup should be done:  55%|█████▍    | 1641/3000 [00:01<00:00, 1641.63it/s]warmup should be done:  55%|█████▌    | 1659/3000 [00:01<00:00, 1647.55it/s]warmup should be done:  53%|█████▎    | 1602/3000 [00:01<00:00, 1586.02it/s]warmup should be done:  55%|█████▌    | 1664/3000 [00:01<00:00, 1648.33it/s]warmup should be done:  54%|█████▍    | 1625/3000 [00:01<00:00, 1605.33it/s]warmup should be done:  54%|█████▍    | 1630/3000 [00:01<00:00, 1592.45it/s]warmup should be done:  61%|██████    | 1830/3000 [00:01<00:00, 1655.33it/s]warmup should be done:  61%|██████    | 1826/3000 [00:01<00:00, 1647.88it/s]warmup should be done:  60%|██████    | 1807/3000 [00:01<00:00, 1644.78it/s]warmup should be done:  61%|██████    | 1824/3000 [00:01<00:00, 1644.51it/s]warmup should be done:  61%|██████    | 1830/3000 [00:01<00:00, 1651.68it/s]warmup should be done:  59%|█████▉    | 1764/3000 [00:01<00:00, 1593.61it/s]warmup should be done:  60%|█████▉    | 1786/3000 [00:01<00:00, 1606.59it/s]warmup should be done:  60%|█████▉    | 1790/3000 [00:01<00:00, 1591.06it/s]warmup should be done:  67%|██████▋   | 1996/3000 [00:01<00:00, 1656.34it/s]warmup should be done:  66%|██████▋   | 1991/3000 [00:01<00:00, 1648.38it/s]warmup should be done:  66%|██████▌   | 1973/3000 [00:01<00:00, 1647.43it/s]warmup should be done:  66%|██████▋   | 1990/3000 [00:01<00:00, 1648.00it/s]warmup should be done:  64%|██████▍   | 1926/3000 [00:01<00:00, 1599.79it/s]warmup should be done:  67%|██████▋   | 1996/3000 [00:01<00:00, 1649.09it/s]warmup should be done:  65%|██████▍   | 1948/3000 [00:01<00:00, 1607.63it/s]warmup should be done:  65%|██████▌   | 1953/3000 [00:01<00:00, 1601.03it/s]warmup should be done:  72%|███████▏  | 2162/3000 [00:01<00:00, 1655.07it/s]warmup should be done:  72%|███████▏  | 2156/3000 [00:01<00:00, 1647.51it/s]warmup should be done:  71%|███████▏  | 2138/3000 [00:01<00:00, 1646.65it/s]warmup should be done:  72%|███████▏  | 2156/3000 [00:01<00:00, 1651.46it/s]warmup should be done:  70%|██████▉   | 2088/3000 [00:01<00:00, 1605.60it/s]warmup should be done:  70%|███████   | 2109/3000 [00:01<00:00, 1608.02it/s]warmup should be done:  71%|███████   | 2116/3000 [00:01<00:00, 1607.76it/s]warmup should be done:  72%|███████▏  | 2161/3000 [00:01<00:00, 1611.06it/s]warmup should be done:  78%|███████▊  | 2328/3000 [00:01<00:00, 1656.03it/s]warmup should be done:  77%|███████▋  | 2303/3000 [00:01<00:00, 1646.79it/s]warmup should be done:  77%|███████▋  | 2322/3000 [00:01<00:00, 1653.51it/s]warmup should be done:  77%|███████▋  | 2321/3000 [00:01<00:00, 1642.05it/s]warmup should be done:  75%|███████▌  | 2250/3000 [00:01<00:00, 1608.42it/s]warmup should be done:  76%|███████▌  | 2271/3000 [00:01<00:00, 1609.94it/s]warmup should be done:  76%|███████▌  | 2278/3000 [00:01<00:00, 1611.27it/s]warmup should be done:  78%|███████▊  | 2327/3000 [00:01<00:00, 1623.15it/s]warmup should be done:  83%|████████▎ | 2494/3000 [00:01<00:00, 1653.17it/s]warmup should be done:  82%|████████▏ | 2468/3000 [00:01<00:00, 1645.42it/s]warmup should be done:  83%|████████▎ | 2488/3000 [00:01<00:00, 1652.47it/s]warmup should be done:  80%|████████  | 2411/3000 [00:01<00:00, 1607.95it/s]warmup should be done:  81%|████████  | 2432/3000 [00:01<00:00, 1607.75it/s]warmup should be done:  81%|████████▏ | 2440/3000 [00:01<00:00, 1607.83it/s]warmup should be done:  83%|████████▎ | 2492/3000 [00:01<00:00, 1630.93it/s]warmup should be done:  83%|████████▎ | 2486/3000 [00:01<00:00, 1594.39it/s]warmup should be done:  89%|████████▊ | 2660/3000 [00:01<00:00, 1653.04it/s]warmup should be done:  88%|████████▊ | 2633/3000 [00:01<00:00, 1646.56it/s]warmup should be done:  88%|████████▊ | 2655/3000 [00:01<00:00, 1655.06it/s]warmup should be done:  86%|████████▌ | 2573/3000 [00:01<00:00, 1609.25it/s]warmup should be done:  86%|████████▋ | 2593/3000 [00:01<00:00, 1593.78it/s]warmup should be done:  87%|████████▋ | 2601/3000 [00:01<00:00, 1605.13it/s]warmup should be done:  89%|████████▊ | 2658/3000 [00:01<00:00, 1638.61it/s]warmup should be done:  88%|████████▊ | 2651/3000 [00:01<00:00, 1609.34it/s]warmup should be done:  93%|█████████▎| 2799/3000 [00:01<00:00, 1649.11it/s]warmup should be done:  94%|█████████▍| 2827/3000 [00:01<00:00, 1655.96it/s]warmup should be done:  94%|█████████▍| 2822/3000 [00:01<00:00, 1657.96it/s]warmup should be done:  91%|█████████ | 2735/3000 [00:01<00:00, 1610.65it/s]warmup should be done:  92%|█████████▏| 2755/3000 [00:01<00:00, 1599.37it/s]warmup should be done:  92%|█████████▏| 2762/3000 [00:01<00:00, 1604.08it/s]warmup should be done:  94%|█████████▍| 2825/3000 [00:01<00:00, 1645.96it/s]warmup should be done:  94%|█████████▍| 2817/3000 [00:01<00:00, 1622.51it/s]warmup should be done:  99%|█████████▉| 2966/3000 [00:01<00:00, 1655.25it/s]warmup should be done: 100%|█████████▉| 2993/3000 [00:01<00:00, 1653.69it/s]warmup should be done: 100%|█████████▉| 2990/3000 [00:01<00:00, 1661.59it/s]warmup should be done:  97%|█████████▋| 2898/3000 [00:01<00:00, 1615.97it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1654.69it/s]warmup should be done:  97%|█████████▋| 2918/3000 [00:01<00:00, 1608.11it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1652.70it/s]warmup should be done:  97%|█████████▋| 2924/3000 [00:01<00:00, 1607.79it/s]warmup should be done: 100%|█████████▉| 2992/3000 [00:01<00:00, 1651.53it/s]warmup should be done:  99%|█████████▉| 2984/3000 [00:01<00:00, 1635.43it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1645.59it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1640.75it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1640.64it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1610.21it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1609.17it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1601.39it/s]





warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   6%|▌         | 171/3000 [00:00<00:01, 1709.19it/s]warmup should be done:   6%|▌         | 166/3000 [00:00<00:01, 1657.03it/s]warmup should be done:   6%|▌         | 165/3000 [00:00<00:01, 1647.13it/s]warmup should be done:   6%|▌         | 167/3000 [00:00<00:01, 1665.44it/s]warmup should be done:   6%|▌         | 170/3000 [00:00<00:01, 1696.17it/s]warmup should be done:   6%|▌         | 167/3000 [00:00<00:01, 1663.82it/s]warmup should be done:   6%|▌         | 171/3000 [00:00<00:01, 1700.76it/s]warmup should be done:   6%|▌         | 171/3000 [00:00<00:01, 1701.43it/s]warmup should be done:  11%|█         | 332/3000 [00:00<00:01, 1658.36it/s]warmup should be done:  11%|█▏        | 340/3000 [00:00<00:01, 1697.90it/s]warmup should be done:  11%|█▏        | 343/3000 [00:00<00:01, 1710.05it/s]warmup should be done:  11%|█         | 331/3000 [00:00<00:01, 1649.06it/s]warmup should be done:  11%|█         | 335/3000 [00:00<00:01, 1668.58it/s]warmup should be done:  11%|█▏        | 342/3000 [00:00<00:01, 1703.27it/s]warmup should be done:  11%|█         | 334/3000 [00:00<00:01, 1660.76it/s]warmup should be done:  11%|█▏        | 342/3000 [00:00<00:01, 1697.75it/s]warmup should be done:  17%|█▋        | 511/3000 [00:00<00:01, 1699.36it/s]warmup should be done:  17%|█▋        | 497/3000 [00:00<00:01, 1652.76it/s]warmup should be done:  17%|█▋        | 498/3000 [00:00<00:01, 1653.14it/s]warmup should be done:  17%|█▋        | 513/3000 [00:00<00:01, 1703.99it/s]warmup should be done:  17%|█▋        | 503/3000 [00:00<00:01, 1669.52it/s]warmup should be done:  17%|█▋        | 515/3000 [00:00<00:01, 1707.85it/s]warmup should be done:  17%|█▋        | 501/3000 [00:00<00:01, 1662.07it/s]warmup should be done:  17%|█▋        | 512/3000 [00:00<00:01, 1697.24it/s]warmup should be done:  22%|██▏       | 664/3000 [00:00<00:01, 1658.62it/s]warmup should be done:  23%|██▎       | 682/3000 [00:00<00:01, 1701.91it/s]warmup should be done:  22%|██▏       | 671/3000 [00:00<00:01, 1673.11it/s]warmup should be done:  23%|██▎       | 685/3000 [00:00<00:01, 1707.26it/s]warmup should be done:  23%|██▎       | 687/3000 [00:00<00:01, 1711.18it/s]warmup should be done:  22%|██▏       | 668/3000 [00:00<00:01, 1664.24it/s]warmup should be done:  22%|██▏       | 664/3000 [00:00<00:01, 1651.22it/s]warmup should be done:  23%|██▎       | 682/3000 [00:00<00:01, 1696.34it/s]warmup should be done:  28%|██▊       | 831/3000 [00:00<00:01, 1662.65it/s]warmup should be done:  28%|██▊       | 853/3000 [00:00<00:01, 1703.99it/s]warmup should be done:  29%|██▊       | 856/3000 [00:00<00:01, 1707.87it/s]warmup should be done:  28%|██▊       | 835/3000 [00:00<00:01, 1664.58it/s]warmup should be done:  28%|██▊       | 840/3000 [00:00<00:01, 1675.80it/s]warmup should be done:  29%|██▊       | 859/3000 [00:00<00:01, 1711.38it/s]warmup should be done:  28%|██▊       | 852/3000 [00:00<00:01, 1695.45it/s]warmup should be done:  28%|██▊       | 830/3000 [00:00<00:01, 1648.92it/s]warmup should be done:  33%|███▎      | 998/3000 [00:00<00:01, 1665.08it/s]warmup should be done:  34%|███▍      | 1024/3000 [00:00<00:01, 1704.50it/s]warmup should be done:  34%|███▍      | 1028/3000 [00:00<00:01, 1709.33it/s]warmup should be done:  34%|███▎      | 1008/3000 [00:00<00:01, 1676.59it/s]warmup should be done:  34%|███▍      | 1022/3000 [00:00<00:01, 1695.12it/s]warmup should be done:  34%|███▍      | 1031/3000 [00:00<00:01, 1710.74it/s]warmup should be done:  33%|███▎      | 1002/3000 [00:00<00:01, 1661.60it/s]warmup should be done:  33%|███▎      | 996/3000 [00:00<00:01, 1649.69it/s]warmup should be done:  39%|███▉      | 1165/3000 [00:00<00:01, 1664.46it/s]warmup should be done:  40%|███▉      | 1195/3000 [00:00<00:01, 1701.65it/s]warmup should be done:  39%|███▉      | 1176/3000 [00:00<00:01, 1674.71it/s]warmup should be done:  40%|███▉      | 1199/3000 [00:00<00:01, 1706.25it/s]warmup should be done:  40%|███▉      | 1192/3000 [00:00<00:01, 1693.23it/s]warmup should be done:  39%|███▉      | 1163/3000 [00:00<00:01, 1655.00it/s]warmup should be done:  39%|███▉      | 1169/3000 [00:00<00:01, 1658.99it/s]warmup should be done:  40%|████      | 1203/3000 [00:00<00:01, 1693.12it/s]warmup should be done:  46%|████▌     | 1371/3000 [00:00<00:00, 1709.72it/s]warmup should be done:  46%|████▌     | 1367/3000 [00:00<00:00, 1704.47it/s]warmup should be done:  44%|████▍     | 1332/3000 [00:00<00:01, 1657.98it/s]warmup should be done:  45%|████▍     | 1345/3000 [00:00<00:00, 1676.93it/s]warmup should be done:  45%|████▌     | 1363/3000 [00:00<00:00, 1697.71it/s]warmup should be done:  44%|████▍     | 1330/3000 [00:00<00:01, 1656.40it/s]warmup should be done:  45%|████▍     | 1336/3000 [00:00<00:01, 1659.64it/s]warmup should be done:  46%|████▌     | 1373/3000 [00:00<00:00, 1694.67it/s]warmup should be done:  51%|█████▏    | 1542/3000 [00:00<00:00, 1708.97it/s]warmup should be done:  50%|████▉     | 1498/3000 [00:00<00:00, 1656.87it/s]warmup should be done:  51%|█████▏    | 1538/3000 [00:00<00:00, 1703.56it/s]warmup should be done:  50%|█████     | 1513/3000 [00:00<00:00, 1676.11it/s]warmup should be done:  51%|█████     | 1534/3000 [00:00<00:00, 1699.65it/s]warmup should be done:  50%|████▉     | 1497/3000 [00:00<00:00, 1659.68it/s]warmup should be done:  50%|█████     | 1503/3000 [00:00<00:00, 1660.02it/s]warmup should be done:  52%|█████▏    | 1545/3000 [00:00<00:00, 1700.14it/s]warmup should be done:  57%|█████▋    | 1713/3000 [00:01<00:00, 1709.13it/s]warmup should be done:  55%|█████▌    | 1664/3000 [00:01<00:00, 1657.68it/s]warmup should be done:  56%|█████▌    | 1681/3000 [00:01<00:00, 1676.20it/s]warmup should be done:  57%|█████▋    | 1709/3000 [00:01<00:00, 1703.38it/s]warmup should be done:  57%|█████▋    | 1705/3000 [00:01<00:00, 1700.66it/s]warmup should be done:  56%|█████▌    | 1665/3000 [00:01<00:00, 1662.92it/s]warmup should be done:  56%|█████▌    | 1670/3000 [00:01<00:00, 1661.18it/s]warmup should be done:  57%|█████▋    | 1717/3000 [00:01<00:00, 1703.40it/s]warmup should be done:  63%|██████▎   | 1884/3000 [00:01<00:00, 1708.50it/s]warmup should be done:  61%|██████    | 1830/3000 [00:01<00:00, 1658.05it/s]warmup should be done:  62%|██████▏   | 1849/3000 [00:01<00:00, 1677.01it/s]warmup should be done:  63%|██████▎   | 1880/3000 [00:01<00:00, 1703.23it/s]warmup should be done:  63%|██████▎   | 1876/3000 [00:01<00:00, 1701.79it/s]warmup should be done:  61%|██████    | 1833/3000 [00:01<00:00, 1665.23it/s]warmup should be done:  61%|██████    | 1837/3000 [00:01<00:00, 1662.02it/s]warmup should be done:  63%|██████▎   | 1889/3000 [00:01<00:00, 1705.80it/s]warmup should be done:  68%|██████▊   | 2051/3000 [00:01<00:00, 1703.46it/s]warmup should be done:  67%|██████▋   | 2017/3000 [00:01<00:00, 1673.54it/s]warmup should be done:  68%|██████▊   | 2055/3000 [00:01<00:00, 1701.60it/s]warmup should be done:  67%|██████▋   | 1996/3000 [00:01<00:00, 1652.51it/s]warmup should be done:  68%|██████▊   | 2047/3000 [00:01<00:00, 1701.87it/s]warmup should be done:  67%|██████▋   | 2000/3000 [00:01<00:00, 1663.45it/s]warmup should be done:  67%|██████▋   | 2004/3000 [00:01<00:00, 1659.01it/s]warmup should be done:  69%|██████▊   | 2060/3000 [00:01<00:00, 1705.20it/s]warmup should be done:  74%|███████▍  | 2222/3000 [00:01<00:00, 1703.74it/s]warmup should be done:  73%|███████▎  | 2185/3000 [00:01<00:00, 1673.59it/s]warmup should be done:  74%|███████▍  | 2227/3000 [00:01<00:00, 1704.47it/s]warmup should be done:  74%|███████▍  | 2218/3000 [00:01<00:00, 1700.71it/s]warmup should be done:  72%|███████▏  | 2162/3000 [00:01<00:00, 1647.12it/s]warmup should be done:  72%|███████▏  | 2167/3000 [00:01<00:00, 1662.65it/s]warmup should be done:  72%|███████▏  | 2171/3000 [00:01<00:00, 1661.17it/s]warmup should be done:  74%|███████▍  | 2231/3000 [00:01<00:00, 1697.87it/s]warmup should be done:  78%|███████▊  | 2353/3000 [00:01<00:00, 1673.11it/s]warmup should be done:  80%|███████▉  | 2393/3000 [00:01<00:00, 1701.14it/s]warmup should be done:  80%|███████▉  | 2389/3000 [00:01<00:00, 1700.61it/s]warmup should be done:  78%|███████▊  | 2328/3000 [00:01<00:00, 1650.42it/s]warmup should be done:  78%|███████▊  | 2334/3000 [00:01<00:00, 1664.64it/s]warmup should be done:  80%|███████▉  | 2398/3000 [00:01<00:00, 1696.67it/s]warmup should be done:  78%|███████▊  | 2338/3000 [00:01<00:00, 1662.13it/s]warmup should be done:  80%|████████  | 2401/3000 [00:01<00:00, 1691.68it/s]warmup should be done:  85%|████████▌ | 2564/3000 [00:01<00:00, 1703.54it/s]warmup should be done:  84%|████████▍ | 2522/3000 [00:01<00:00, 1675.26it/s]warmup should be done:  85%|████████▌ | 2560/3000 [00:01<00:00, 1702.10it/s]warmup should be done:  83%|████████▎ | 2502/3000 [00:01<00:00, 1667.12it/s]warmup should be done:  83%|████████▎ | 2494/3000 [00:01<00:00, 1649.20it/s]warmup should be done:  84%|████████▎ | 2505/3000 [00:01<00:00, 1662.69it/s]warmup should be done:  86%|████████▌ | 2568/3000 [00:01<00:00, 1690.27it/s]warmup should be done:  86%|████████▌ | 2571/3000 [00:01<00:00, 1689.63it/s]warmup should be done:  91%|█████████ | 2735/3000 [00:01<00:00, 1704.29it/s]warmup should be done:  90%|████████▉ | 2690/3000 [00:01<00:00, 1675.20it/s]warmup should be done:  91%|█████████ | 2731/3000 [00:01<00:00, 1702.79it/s]warmup should be done:  89%|████████▉ | 2669/3000 [00:01<00:00, 1667.41it/s]warmup should be done:  89%|████████▊ | 2661/3000 [00:01<00:00, 1654.19it/s]warmup should be done:  89%|████████▉ | 2675/3000 [00:01<00:00, 1672.51it/s]warmup should be done:  91%|█████████▏| 2738/3000 [00:01<00:00, 1685.76it/s]warmup should be done:  91%|█████████▏| 2740/3000 [00:01<00:00, 1687.31it/s]warmup should be done:  97%|█████████▋| 2906/3000 [00:01<00:00, 1704.19it/s]warmup should be done:  95%|█████████▍| 2836/3000 [00:01<00:00, 1668.09it/s]warmup should be done:  97%|█████████▋| 2902/3000 [00:01<00:00, 1702.60it/s]warmup should be done:  95%|█████████▌| 2858/3000 [00:01<00:00, 1671.65it/s]warmup should be done:  94%|█████████▍| 2828/3000 [00:01<00:00, 1657.82it/s]warmup should be done:  95%|█████████▍| 2845/3000 [00:01<00:00, 1680.63it/s]warmup should be done:  97%|█████████▋| 2907/3000 [00:01<00:00, 1686.35it/s]warmup should be done:  97%|█████████▋| 2909/3000 [00:01<00:00, 1684.18it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1702.94it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1699.71it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1699.70it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1696.76it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1673.53it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1667.59it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1661.93it/s]warmup should be done: 100%|█████████▉| 2995/3000 [00:01<00:00, 1659.65it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1656.18it/s]
WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7fca8b753310>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7fca8b753100>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7fca8b743100>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7fca8b7512b0>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7fca8ba9ddf0>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7fca8ba96730>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7fca8ba95e80>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7fca8b743070>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
2022-12-11 20:46:26.283062: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fc5c70291b0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 20:46:26.283141: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 20:46:26.292747: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 20:46:26.298622: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fc5c2833e20 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 20:46:26.298666: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 20:46:26.307390: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 20:46:26.324948: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fc5be833a50 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 20:46:26.325004: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 20:46:26.333527: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 20:46:26.417260: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fc5b68332f0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 20:46:26.417314: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 20:46:26.425646: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 20:46:26.899381: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fc5ca82be70 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 20:46:26.899442: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 20:46:26.908161: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 20:46:26.922285: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fc5c2833ed0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 20:46:26.922325: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 20:46:26.931776: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 20:46:27.035429: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fc5c7031780 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 20:46:27.035496: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 20:46:27.037153: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fc5c2795b10 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 20:46:27.037201: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 20:46:27.045839: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 20:46:27.046204: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 20:46:33.328734: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 20:46:33.411852: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 20:46:33.413201: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 20:46:33.440253: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 20:46:33.554797: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 20:46:33.678873: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 20:46:33.805781: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 20:46:33.866547: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
[HCTR][20:47:38.881][ERROR][RK0][main]: replica 5 reaches 1000, calling init pre replica
[HCTR][20:47:38.881][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][20:47:38.891][ERROR][RK0][main]: coll ps creation done
[HCTR][20:47:38.891][ERROR][RK0][main]: replica 5 waits for coll ps creation barrier
[HCTR][20:47:39.115][ERROR][RK0][tid #140487751091968]: replica 7 reaches 1000, calling init pre replica
[HCTR][20:47:39.115][ERROR][RK0][tid #140487751091968]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][20:47:39.120][ERROR][RK0][tid #140487751091968]: coll ps creation done
[HCTR][20:47:39.120][ERROR][RK0][tid #140487751091968]: replica 7 waits for coll ps creation barrier
[HCTR][20:47:39.184][ERROR][RK0][tid #140487809808128]: replica 3 reaches 1000, calling init pre replica
[HCTR][20:47:39.184][ERROR][RK0][tid #140487809808128]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][20:47:39.189][ERROR][RK0][tid #140487809808128]: coll ps creation done
[HCTR][20:47:39.189][ERROR][RK0][tid #140487809808128]: replica 3 waits for coll ps creation barrier
[HCTR][20:47:39.248][ERROR][RK0][main]: replica 4 reaches 1000, calling init pre replica
[HCTR][20:47:39.248][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][20:47:39.255][ERROR][RK0][main]: coll ps creation done
[HCTR][20:47:39.255][ERROR][RK0][main]: replica 4 waits for coll ps creation barrier
[HCTR][20:47:39.315][ERROR][RK0][tid #140487809808128]: replica 2 reaches 1000, calling init pre replica
[HCTR][20:47:39.315][ERROR][RK0][tid #140487809808128]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][20:47:39.323][ERROR][RK0][tid #140487809808128]: coll ps creation done
[HCTR][20:47:39.323][ERROR][RK0][tid #140487809808128]: replica 2 waits for coll ps creation barrier
[HCTR][20:47:39.483][ERROR][RK0][main]: replica 6 reaches 1000, calling init pre replica
[HCTR][20:47:39.483][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][20:47:39.486][ERROR][RK0][tid #140487868524288]: replica 1 reaches 1000, calling init pre replica
[HCTR][20:47:39.486][ERROR][RK0][tid #140487868524288]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][20:47:39.488][ERROR][RK0][main]: coll ps creation done
[HCTR][20:47:39.488][ERROR][RK0][main]: replica 6 waits for coll ps creation barrier
[HCTR][20:47:39.489][ERROR][RK0][tid #140487868524288]: coll ps creation done
[HCTR][20:47:39.489][ERROR][RK0][tid #140487868524288]: replica 1 waits for coll ps creation barrier
[HCTR][20:47:39.524][ERROR][RK0][tid #140487809808128]: replica 0 reaches 1000, calling init pre replica
[HCTR][20:47:39.524][ERROR][RK0][tid #140487809808128]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][20:47:39.529][ERROR][RK0][tid #140487809808128]: coll ps creation done
[HCTR][20:47:39.529][ERROR][RK0][tid #140487809808128]: replica 0 waits for coll ps creation barrier
[HCTR][20:47:39.529][ERROR][RK0][tid #140487809808128]: replica 0 preparing frequency
[HCTR][20:47:40.445][ERROR][RK0][tid #140487809808128]: replica 0 preparing frequency done
[HCTR][20:47:40.476][ERROR][RK0][tid #140487809808128]: replica 0 calling init per replica
[HCTR][20:47:40.476][ERROR][RK0][main]: replica 6 calling init per replica
[HCTR][20:47:40.476][ERROR][RK0][tid #140487868524288]: replica 1 calling init per replica
[HCTR][20:47:40.476][ERROR][RK0][main]: replica 4 calling init per replica
[HCTR][20:47:40.476][ERROR][RK0][main]: replica 5 calling init per replica
[HCTR][20:47:40.476][ERROR][RK0][tid #140487751091968]: replica 7 calling init per replica
[HCTR][20:47:40.476][ERROR][RK0][tid #140487809808128]: replica 3 calling init per replica
[HCTR][20:47:40.476][ERROR][RK0][tid #140487809808128]: replica 2 calling init per replica
[HCTR][20:47:40.476][ERROR][RK0][tid #140487809808128]: Calling build_v2
[HCTR][20:47:40.476][ERROR][RK0][main]: Calling build_v2
[HCTR][20:47:40.476][ERROR][RK0][tid #140487868524288]: Calling build_v2
[HCTR][20:47:40.476][ERROR][RK0][main]: Calling build_v2
[HCTR][20:47:40.476][ERROR][RK0][main]: Calling build_v2
[HCTR][20:47:40.476][ERROR][RK0][tid #140487751091968]: Calling build_v2
[HCTR][20:47:40.476][ERROR][RK0][tid #140487809808128]: Calling build_v2
[HCTR][20:47:40.476][ERROR][RK0][tid #140487809808128]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][20:47:40.476][ERROR][RK0][tid #140487809808128]: Calling build_v2
[HCTR][20:47:40.476][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][20:47:40.476][ERROR][RK0][tid #140487868524288]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][20:47:40.476][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][20:47:40.476][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][20:47:40.476][ERROR][RK0][tid #140487751091968]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][20:47:40.476][ERROR][RK0][tid #140487809808128]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][20:47:40.476][ERROR][RK0][tid #140487809808128]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[2022-12-11 20:47:40.491456: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:178] v100x8, slow pcie[
[2022-12-11 20:47:402022-12-11 20:47:40..491500491530: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc::178196] [] v100x8, slow pcieassigning 0 to cpu

2022-12-11 20:47:40.491546[: 2022-12-11 20:47:40E. 491590[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: :E178 ] 2022-12-11 20:47:40/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccv100x8, slow pcie[.:
2022-12-11 20:47:40491591196.: ] 491624[Eassigning 0 to cpu: 2022-12-11 20:47:40 
E./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc 491644:/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: [178:E] 212 v100x8, slow pcie] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8:2022-12-11 20:47:40
196[[[.] 2022-12-11 20:47:402022-12-11 20:47:40491659assigning 0 to cpu[..: 
2022-12-11 20:47:40491714491714E.: 2022-12-11 20:47:40:  491737E.E[: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc 491698 E:/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-11 20:47:40 [:178E[:./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-11 20:47:402022-12-11 20:47:40212]  196491751:..] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccv100x8, slow pcie] : 213491807491796build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8:assigning 0 to cpu
E] : : 
178
 remote time is 8.68421E[E] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
[ /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-11 20:47:40 v100x8, slow pcie:2022-12-11 20:47:40:[[./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
178.2122022-12-11 20:47:402022-12-11 20:47:40491944:] 491966] [..: 178v100x8, slow pcie: build asymm link desc with 8X Tesla V100-SXM2-32GB out of 82022-12-11 20:47:40491992491992E] 
E
.: :  v100x8, slow pcie 492037E[E[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:  2022-12-11 20:47:40[ 2022-12-11 20:47:40::E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.2022-12-11 20:47:40/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.196213 :492106.:492115] ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc212: 492156214: assigning 0 to cpuremote time is 8.68421:] E: ] E

196build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8 Ecpu time is 97.0588 ] 
[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccassigning 0 to cpu2022-12-11 20:47:40:/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[:
.196:2022-12-11 20:47:40213[492264] 196.] 2022-12-11 20:47:40: assigning 0 to cpu] 492290remote time is 8.68421.
E[assigning 0 to cpu: 
492306 : 2022-12-11 20:47:40
E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[E.[ :2022-12-11 20:47:40 4923612022-12-11 20:47:40/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc214./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: .:] 492397[:E492405213cpu time is 97.0588: 2022-12-11 20:47:40212 : ] 
E.] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccEremote time is 8.68421 492448build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8: 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: 
212/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:E[[] :214 2022-12-11 20:47:402022-12-11 20:47:40build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8.212] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.
492585] cpu time is 97.0588:492567: build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8[
212: E
2022-12-11 20:47:40] E .build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8 [/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc492649
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-11 20:47:40:: [:.213E2022-12-11 20:47:40214492708]  .] : remote time is 8.68421/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc492749cpu time is 97.0588E
:: 
 213E[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc]  2022-12-11 20:47:40:remote time is 8.68421/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.213
:492838] 213[: remote time is 8.68421] 2022-12-11 20:47:40E
remote time is 8.68421. 
492904[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: [2022-12-11 20:47:40:E2022-12-11 20:47:40.214 .492946] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc492963: cpu time is 97.0588:: E
214E ]  /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cccpu time is 97.0588/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:
:214214] ] cpu time is 97.0588cpu time is 97.0588

[2022-12-11 20:48:59.655354: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:118] solver created. now build & solve
[2022-12-11 20:48:59.695209: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:123] solver built. now solve
block 0 storage is 00010001
	access is	0	0	0	0	4	4	4	4	
block 1 storage is 00100010
	access is	1	1	1	1	5	5	5	5	
block 2 storage is 01000100
	access is	2	2	2	2	6	6	6	6	
block 3 storage is 10001000
	access is	3	3	3	3	7	7	7	7	
block 4 storage is 00000000
	access is	8	8	8	8	8	8	8	8	
[2022-12-11 20:48:59.830763: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:127] solver solved
[2022-12-11 20:48:59.830825: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:195] 0 solved master
[2022-12-11 20:48:59.830854: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 0 solved
[2022-12-11 20:48:59.830882: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 0 initing device 0
[2022-12-11 20:48:59.831411: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-11 20:48:59.831460: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 20:48:59.832415: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 20:48:59.833254: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 20:48:59.846087: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 6 solved
[2022-12-11 20:48:59.846150: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 6 initing device 6
[[2022-12-11 20:48:59[2022-12-11 20:48:59.2022-12-11 20:48:59.846479.846486: 846500: E: E E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:[/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202:2022-12-11 20:48:59202[] 202.] 2022-12-11 20:48:59] 3 solved8465455 solved.7 solved
: 
846575
E: [[ E2022-12-11 20:48:59[2022-12-11 20:48:59/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc .2022-12-11 20:48:59.:/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu846621.846617202:: 846625: ] 1815E: E1 solved]  E 
Building Coll Cache with ... num gpu device is 8/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc
:/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc[:205:2022-12-11 20:48:59205] 205.] worker 0 thread 5 initing device 5worker 0 thread 3 initing device 3] 846692[

worker 0 thread 7 initing device 7: 2022-12-11 20:48:59
E. 846717/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc: :E205 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuworker 0 thread 1 initing device 1:
1980] eager alloc mem 381.47 MB
[2022-12-11 20:48:59.847108: E [/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu2022-12-11 20:48:59:.[18158471232022-12-11 20:48:59] : .Building Coll Cache with ... num gpu device is 8E847141
[ : 2022-12-11 20:48:59/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE.: 8471611815/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu: [] :E2022-12-11 20:48:59Building Coll Cache with ... num gpu device is 81815 .
] /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu847188Building Coll Cache with ... num gpu device is 8:: 
1815E]  Building Coll Cache with ... num gpu device is 8[/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu
2022-12-11 20:48:59:.1980[847232] 2022-12-11 20:48:59: eager alloc mem 381.47 MB.E[2022-12-11 20:48:59
847245 .: /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu847260E::  1980E/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu]  :eager alloc mem 381.47 MB/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu1980
:] 1980eager alloc mem 381.47 MB] 
eager alloc mem 381.47 MB
[2022-12-11 20:48:59.849913: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 20:48:59.850229: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 20:48:59.850285: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 20:48:59.850786: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 20:48:59.850844: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 20:48:59.853261: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 20:48:59.853430: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 20:48:59.853484: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 20:48:59.853581: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 20:48:59.854057: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[[2022-12-11 20:48:592022-12-11 20:48:59..855981855981: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc::202202] ] 2 solved4 solved

[[2022-12-11 20:48:592022-12-11 20:48:59..856060856060: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc::205205] ] worker 0 thread 2 initing device 2worker 0 thread 4 initing device 4

[[2022-12-11 20:48:592022-12-11 20:48:59..856596856595: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::18151815] ] Building Coll Cache with ... num gpu device is 8Building Coll Cache with ... num gpu device is 8

[[2022-12-11 20:48:592022-12-11 20:48:59..856668856668: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::19801980] ] eager alloc mem 381.47 MBeager alloc mem 381.47 MB

[2022-12-11 20:48:59.858305: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 20:48:59.858367: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 20:48:59.859885: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 20:48:59.859936: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 20:48:59.910449: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.00 Bytes
[2022-12-11 20:48:59.915987: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5
[2022-12-11 20:48:59.916125: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-11 20:48:59.916962: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 4.20 MB
[2022-12-11 20:48:59.917558: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 20:48:59.918543: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 20:48:59.919687: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 20:48:59.920431: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 20:48:59.920477: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 493.16 MB
[[[[[2022-12-11 20:48:592022-12-11 20:48:592022-12-11 20:48:592022-12-11 20:48:592022-12-11 20:48:59.....938420938420938420938420938449: : : : : EEEEE     /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:::::19801980198019801980] ] ] ] ] eager alloc mem 5.00 Byteseager alloc mem 5.00 Byteseager alloc mem 5.00 Byteseager alloc mem 5.00 Byteseager alloc mem 5.00 Bytes




[[2022-12-11 20:48:592022-12-11 20:48:59..941129941130: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::19801980] ] eager alloc mem 5.00 Byteseager alloc mem 5.00 Bytes

[2022-12-11 20:48:59.944626: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5
[2022-12-11 20:48:59.944720: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-11 20:48:59.944811: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5
[2022-12-11 20:48:59.944895: E[ 2022-12-11 20:48:59/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc.:944892638: ] Eeager release cuda mem 400000000 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5
[[2022-12-11 20:48:592022-12-11 20:48:59..944976944991: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::638638] ] eager release cuda mem 5eager release cuda mem 400000000

[2022-12-11 20:48:59.945058[: 2022-12-11 20:48:59E. 945084/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc: :E638 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cceager release cuda mem 5:
638] eager release cuda mem 400000000
[2022-12-11 20:48:59.945159: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-11 20:48:59.945506: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 4.20 MB
[2022-12-11 20:48:59.946083: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5
[2022-12-11 20:48:59[.2022-12-11 20:48:59946160.: 946152E:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc :/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc638:] 638eager release cuda mem 400000000] 
eager release cuda mem 5
[2022-12-11 20:48:59.946248: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-11 20:48:59.957168: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 4.20 MB
[2022-12-11 20:48:59.957852: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 4.20 MB
[2022-12-11 20:48:59.958361: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 4.20 MB
[2022-12-11 20:48:59.958869: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 4.20 MB
[2022-12-11 20:48:59.959574: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 4.20 MB
[2022-12-11 20:48:59.960079: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 4.20 MB
[2022-12-11 20:48:59.960529: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 20:48:59.961305: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[[2022-12-11 20:48:592022-12-11 20:48:59..961415961423: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::1980638] ] eager alloc mem 611.00 KBeager release cuda mem 625663

[2022-12-11 20:48:59.961513: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 20:48:59.961619: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 20:48:59.961660: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 20:48:59.961705: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 20:48:59.962264: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 20:48:59.962395: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 20:48:59.962455: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 20:48:59.962566: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 20:48:59.962607: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 20:48:59.962652: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 20:48:59.963468: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[[2022-12-11 20:48:592022-12-11 20:48:59..963585963591: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::19801980] ] eager alloc mem 25.25 KBeager alloc mem 25.25 KB

[2022-12-11 20:48:59.964169: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 20:48:59.964211: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 493.16 MB
[2022-12-11 20:48:59.964314[: 2022-12-11 20:48:59E. 964321/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc: :E638 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cceager release cuda mem 25855:
638] eager release cuda mem 25855
[2022-12-11 20:48:59.964378[: 2022-12-11 20:48:59E. 964385/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu: :E1980 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cueager alloc mem 493.16 MB:
1980] eager alloc mem 493.16 MB
[2022-12-11 20:48:59.964989: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 20:48:59.965586: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 20:48:59.965629: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 493.16 MB
[2022-12-11 20:48:59.966214: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 20:48:59.966608: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 20:48:59.966741: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 20:48:59.966923: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 20:48:59.966967: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 493.16 MB
[2022-12-11 20:48:59.967318: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 20:48:59.967364: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 493.16 MB
[2022-12-11 20:48:59.967457: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 20:48:59.967502: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 493.16 MB
[[[[[[[[2022-12-11 20:49:002022-12-11 20:49:002022-12-11 20:49:002022-12-11 20:49:002022-12-11 20:49:002022-12-11 20:49:002022-12-11 20:49:002022-12-11 20:49:00........157135157135157135157136157135157135157134157136: : : : : : : : EEEEEEEE        /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::::::::19261926192619261926192619261926] ] ] ] ] ] ] ] Device 7 init p2p of link 4Device 6 init p2p of link 0Device 2 init p2p of link 1Device 3 init p2p of link 2Device 4 init p2p of link 5Device 1 init p2p of link 7Device 5 init p2p of link 6Device 0 init p2p of link 3







[[[[2022-12-11 20:49:00[2022-12-11 20:49:00[[2022-12-11 20:49:002022-12-11 20:49:00.2022-12-11 20:49:00.2022-12-11 20:49:002022-12-11 20:49:00..157543.157544..157545[157544: 157546: 157548157548: 2022-12-11 20:49:00: E: E: : E.E E EE 157581 /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu: /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:E:1980:1980::1980 1980] 1980] 19801980] /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu] eager alloc mem 611.00 KB] eager alloc mem 611.00 KB] ] eager alloc mem 611.00 KB:eager alloc mem 611.00 KB
eager alloc mem 611.00 KB
eager alloc mem 611.00 KBeager alloc mem 611.00 KB
1980



] eager alloc mem 611.00 KB
[2022-12-11 20:49:00[.[2022-12-11 20:49:001584992022-12-11 20:49:00.: .[[158504E1585052022-12-11 20:49:002022-12-11 20:49:00[:  : .[.2022-12-11 20:49:00E/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccE1585222022-12-11 20:49:00[158519. : : .2022-12-11 20:49:00: 158528/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc638/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccE158537.E: :] : : 158552 E638eager release cuda mem 625663638/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccE: /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc ] 
] : E:/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cceager release cuda mem 625663eager release cuda mem 625663638/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc 638:

] :/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc] 638eager release cuda mem 625663638:eager release cuda mem 625663] 
] 638
eager release cuda mem 625663eager release cuda mem 625663] 

eager release cuda mem 625663
[2022-12-11 20:49:00.171318: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 6 init p2p of link 5
[2022-12-11 20:49:00.171471: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 20:49:00.171900: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 5 init p2p of link 4
[2022-12-11 20:49:00.172047: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 20:49:00.172129: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 3 init p2p of link 0
[2022-12-11 20:49:00.172233: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 2[[
2022-12-11 20:49:002022-12-11 20:49:00..172278172280: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::6381980] ] eager release cuda mem 625663eager alloc mem 611.00 KB

[2022-12-11 20:49:00[.2022-12-11 20:49:00172403.: 172397E:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu :/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu1980:] 1926eager alloc mem 611.00 KB] 
Device 7 init p2p of link 1
[2022-12-11 20:49:00.172566: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 20:49:00[.2022-12-11 20:49:00172818.: 172835E:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu :/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc1926:] 638Device 4 init p2p of link 7] 
eager release cuda mem 625663
[2022-12-11 20:49:00.172979: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 20:49:00.173060: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 2 init p2p of link 3
[2022-12-11 20:49:00.173108: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 20:49:00.173191: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 20:49:00[.2022-12-11 20:49:00173233.: 173252E:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu :/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc1926:] 638Device 0 init p2p of link 6] 
eager release cuda mem 625663
[2022-12-11 20:49:00.173365: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 20:49:00.173412: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 20:49:00.173791: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 20:49:00.173994: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 20:49:00.174211: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 20:49:00.190095: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 6 init p2p of link 4
[2022-12-11 20:49:00.190220: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 20:49:00.190499: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 5 init p2p of link 7
[2022-12-11 20:49:00.190614: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 20:49:00.190686: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 7 init p2p of link 6
[2022-12-11 20:49:00.190730: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 3 init p2p of link 5
[2022-12-11 20:49:00.190804: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 20:49:00.190850: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 20:49:00.190912: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 3
[2022-12-11 20:49:00.191000: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 20:49:00.191029: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[[2022-12-11 20:49:002022-12-11 20:49:00..191383191391: : E[E 2022-12-11 20:49:00 /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu./hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:191406:1926: 638] E] Device 4 init p2p of link 2 eager release cuda mem 625663
/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu
:1926] Device 0 init p2p of link 1
[2022-12-11 20:49:00.191554: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:[19802022-12-11 20:49:00[] .2022-12-11 20:49:00eager alloc mem 611.00 KB191569.
: 191564E:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu :/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu1980:] 1926eager alloc mem 611.00 KB] 
Device 2 init p2p of link 0
[2022-12-11 20:49:00[.2022-12-11 20:49:00191631.: 191635E:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc :/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc638:] 638eager release cuda mem 625663] 
eager release cuda mem 625663
[2022-12-11 20:49:00.191710: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 20:49:00.191835: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 20:49:00.192366: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 20:49:00.192418: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 20:49:00.192487: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 20:49:00.206167: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 6 init p2p of link 7
[2022-12-11 20:49:00.206291: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 20:49:00.206515: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 3 init p2p of link 1
[2022-12-11 20:49:00.206632: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 20:49:00.206666: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 5 init p2p of link 3
[2022-12-11 20:49:00.206780: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 20:49:00.207006: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 7 init p2p of link 5
[2022-12-11 20:49:00.207078: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 20:49:00.207122: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 20:49:00.207216: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 4 init p2p of link 6
[2022-12-11 20:49:00.207333: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 20:49:00.207432: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 20:49:00.207548: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:[6382022-12-11 20:49:00] .eager release cuda mem 625663207556
: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 0
[2022-12-11 20:49:00.207678: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 20:49:00.207942: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 20:49:00.208140: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 20:49:00.208267: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 0 init p2p of link 2
[2022-12-11 20:49:00.208381: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 20:49:00.208491: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 20:49:00.208597: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 2 init p2p of link 4
[2022-12-11 20:49:00.208712: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 20:49:00.209185: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 20:49:00.209475: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 20:49:00.222544: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 4399996
[2022-12-11 20:49:00.222664: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 4399996
[2022-12-11 20:49:00.222922: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 4399996
[2022-12-11 20:49:00.223733: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 4399996
[2022-12-11 20:49:00.224024: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 4399996
[2022-12-11 20:49:00.224205: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 4399996
[2022-12-11 20:49:00.224354: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 4399996
[2022-12-11 20:49:00.224685: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 4399996
[2022-12-11 20:49:00.225676: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 999999 / 100000000 nodes ( 1.00 %~1.00 %) | remote 2999997 / 100000000 nodes ( 3.00 %) | cpu 96000004 / 100000000 nodes ( 96.00 %) | 493.16 MB | 0.37897 secs 
[2022-12-11 20:49:00.226126: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 999999 / 100000000 nodes ( 1.00 %~1.00 %) | remote 2999997 / 100000000 nodes ( 3.00 %) | cpu 96000004 / 100000000 nodes ( 96.00 %) | 493.16 MB | 0.378949 secs 
[2022-12-11 20:49:00.226289: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 999999 / 100000000 nodes ( 1.00 %~1.00 %) | remote 2999997 / 100000000 nodes ( 3.00 %) | cpu 96000004 / 100000000 nodes ( 96.00 %) | 493.16 MB | 0.379055 secs 
[2022-12-11 20:49:00.226364: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 999999 / 100000000 nodes ( 1.00 %~1.00 %) | remote 2999997 / 100000000 nodes ( 3.00 %) | cpu 96000004 / 100000000 nodes ( 96.00 %) | 493.16 MB | 0.379144 secs 
[2022-12-11 20:49:00.226768: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 999999 / 100000000 nodes ( 1.00 %~1.00 %) | remote 2999997 / 100000000 nodes ( 3.00 %) | cpu 96000004 / 100000000 nodes ( 96.00 %) | 493.16 MB | 0.370115 secs 
[2022-12-11 20:49:00.227244: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 999999 / 100000000 nodes ( 1.00 %~1.00 %) | remote 2999997 / 100000000 nodes ( 3.00 %) | cpu 96000004 / 100000000 nodes ( 96.00 %) | 493.16 MB | 0.370589 secs 
[2022-12-11 20:49:00.227412: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 999999 / 100000000 nodes ( 1.00 %~1.00 %) | remote 2999997 / 100000000 nodes ( 3.00 %) | cpu 96000004 / 100000000 nodes ( 96.00 %) | 493.16 MB | 0.380163 secs 
[2022-12-11 20:49:00.227878: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 999999 / 100000000 nodes ( 1.00 %~1.00 %) | remote 2999997 / 100000000 nodes ( 3.00 %) | cpu 96000004 / 100000000 nodes ( 96.00 %) | 493.16 MB | 0.396432 secs 
[HCTR][20:49:00.228][ERROR][RK0][tid #140487809808128]: replica 0 calling init per replica done, doing barrier
[HCTR][20:49:00.228][ERROR][RK0][tid #140487809808128]: replica 2 calling init per replica done, doing barrier
[HCTR][20:49:00.228][ERROR][RK0][tid #140487751091968]: replica 7 calling init per replica done, doing barrier
[HCTR][20:49:00.228][ERROR][RK0][main]: replica 4 calling init per replica done, doing barrier
[HCTR][20:49:00.228][ERROR][RK0][tid #140487868524288]: replica 1 calling init per replica done, doing barrier
[HCTR][20:49:00.228][ERROR][RK0][tid #140487809808128]: replica 3 calling init per replica done, doing barrier
[HCTR][20:49:00.228][ERROR][RK0][main]: replica 6 calling init per replica done, doing barrier
[HCTR][20:49:00.228][ERROR][RK0][main]: replica 5 calling init per replica done, doing barrier
[HCTR][20:49:00.228][ERROR][RK0][main]: replica 5 calling init per replica done, doing barrier done
[HCTR][20:49:00.228][ERROR][RK0][main]: replica 6 calling init per replica done, doing barrier done
[HCTR][20:49:00.228][ERROR][RK0][main]: replica 4 calling init per replica done, doing barrier done
[HCTR][20:49:00.228][ERROR][RK0][tid #140487751091968]: replica 7 calling init per replica done, doing barrier done
[HCTR][20:49:00.228][ERROR][RK0][tid #140487809808128]: replica 2 calling init per replica done, doing barrier done
[HCTR][20:49:00.228][ERROR][RK0][tid #140487809808128]: replica 3 calling init per replica done, doing barrier done
[HCTR][20:49:00.228][ERROR][RK0][tid #140487868524288]: replica 1 calling init per replica done, doing barrier done
[HCTR][20:49:00.228][ERROR][RK0][tid #140487809808128]: replica 0 calling init per replica done, doing barrier done
[HCTR][20:49:00.228][ERROR][RK0][main]: init per replica done
[HCTR][20:49:00.228][ERROR][RK0][main]: init per replica done
[HCTR][20:49:00.228][ERROR][RK0][main]: init per replica done
[HCTR][20:49:00.228][ERROR][RK0][tid #140487751091968]: init per replica done
[HCTR][20:49:00.228][ERROR][RK0][tid #140487809808128]: init per replica done
[HCTR][20:49:00.228][ERROR][RK0][tid #140487809808128]: init per replica done
[HCTR][20:49:00.228][ERROR][RK0][tid #140487868524288]: init per replica done
[HCTR][20:49:00.230][ERROR][RK0][tid #140487809808128]: init per replica done
