2022-12-12 00:33:17.781211: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:17.791865: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:17.797397: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:17.802298: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:17.814762: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:17.827148: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:17.835881: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:17.839633: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:17.894178: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:17.901076: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:17.903996: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:17.905010: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:17.906061: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:17.907076: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:17.908340: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:17.910076: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:17.911054: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:17.911181: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:17.912802: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:17.912882: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:17.914469: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:17.914707: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:17.916499: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:17.917098: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:17.918229: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:17.918714: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:17.919990: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:17.920432: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:17.921408: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:17.922158: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:17.922862: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:17.923900: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:17.925119: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:17.926186: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:17.927325: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:17.928265: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:17.929193: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:17.930211: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:17.931274: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:17.932306: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:17.937800: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 00:33:17.940044: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:17.941166: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:17.942243: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:17.943615: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:17.945574: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:17.946046: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:17.947572: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:17.947747: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:17.948471: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:17.950374: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:17.950487: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:17.951027: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:17.951167: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:17.952888: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:17.952932: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:17.953850: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:17.954009: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:17.956741: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:17.956995: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:17.959778: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:17.960170: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:17.961145: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:17.962582: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:17.962846: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:17.964365: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:17.965842: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:17.966202: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:17.967866: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:17.968843: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:17.969526: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:17.970813: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:17.970839: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:17.972252: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:17.972739: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:17.972737: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:17.973879: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:17.974360: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:17.975864: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:17.976460: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:17.977064: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:17.977649: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:17.978728: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:17.979263: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:17.979674: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:17.980426: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:17.981351: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:17.981881: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:17.981975: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:17.985766: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:17.987795: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:17.988304: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:17.988441: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:17.990465: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:17.990607: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:17.990878: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:17.992945: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:17.993003: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:18.000938: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:18.013923: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:18.021104: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:18.026200: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:18.028968: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:18.029401: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:18.029506: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:18.029578: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:18.030097: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:18.032845: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:18.033191: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:18.033284: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:18.033330: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:18.033959: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:18.036652: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:18.037777: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:18.037820: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:18.037909: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:18.038597: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:18.040564: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:18.040878: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:18.040965: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:18.041134: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:18.041922: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:18.044136: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:18.044403: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:18.044624: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:18.044801: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:18.045449: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:18.047973: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:18.048114: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:18.048295: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:18.048467: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:18.049223: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:18.051337: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:18.051562: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:18.051697: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:18.051838: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:18.052548: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:18.054572: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:18.054776: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:18.054866: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:18.055062: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:18.055754: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:18.057946: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:18.058055: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:18.058195: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:18.058645: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:18.058880: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:18.061378: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:18.061553: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:18.061653: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:18.062239: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:18.062296: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:18.064600: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:18.064730: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:18.064924: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:18.065579: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:18.065629: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:18.067922: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:18.068114: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:18.068307: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:18.068621: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:18.068716: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:18.071322: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:18.071514: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:18.071692: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:18.072051: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:18.072132: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:18.074337: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:18.074539: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:18.074856: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:18.074998: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:18.075475: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:18.075496: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:18.078278: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:18.078475: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:18.078675: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:18.078685: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:18.078851: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:18.079714: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:18.079764: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:18.083292: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:18.083943: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:18.084270: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:18.084490: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:18.084538: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:18.084773: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:18.085160: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:18.087813: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:18.088495: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:18.088869: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:18.089388: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:18.089511: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:18.089958: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:18.092070: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:18.092539: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:18.093196: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:18.093286: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 00:33:18.093573: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:18.093847: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:18.093984: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:18.096214: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:18.096534: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:18.097125: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:18.097633: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:18.098025: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:18.098148: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:18.100047: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:18.100407: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:18.101188: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:18.101608: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:18.102139: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:18.102240: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:18.103408: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:18.104157: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:18.104627: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:18.105324: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:18.105918: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:18.106561: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:18.106915: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:18.107849: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:18.108733: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:18.109342: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:18.110179: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:18.110744: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:18.111252: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:18.111521: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:18.112883: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:18.113493: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:18.113998: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:18.114835: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:18.115430: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:18.116381: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:18.116514: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:18.118031: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:18.118566: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:18.121604: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:18.121888: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:18.122144: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:18.123844: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 00:33:18.123844: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 00:33:18.124455: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 00:33:18.124519: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:18.124825: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:18.125088: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:18.127723: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:18.128012: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:18.129989: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:18.130434: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:18.130796: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 00:33:18.132419: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:18.132629: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:18.133716: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:18.133718: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:18.134629: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:18.135918: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:18.136777: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:18.137219: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:18.137415: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:18.138241: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:18.140394: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:18.140473: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:18.140791: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:18.141064: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:18.141351: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:18.142281: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:18.144830: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:18.145103: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:18.145381: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:18.177640: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:18.178014: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:18.178349: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:18.190741: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:18.191071: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:18.196413: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:18.196736: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:18.230121: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:18.230530: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:18.235342: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:18.237112: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:18.242468: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:18.246333: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 00:33:18.251521: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 00:33:18.255975: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:18.261377: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:18.263782: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:18.270072: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:18.270422: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:18.279299: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:19.254400: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:19.255038: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:19.255790: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:19.256451: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 00:33:19.256511: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-12 00:33:19.274579: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:19.275250: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:19.275968: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:19.276744: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:19.277270: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:19.277747: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-12 00:33:19.325231: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 00:33:19.325435: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 00:33:19.359830: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12340
2022-12-12 00:33:19.514421: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:19.515946: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:19.517567: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:19.518435: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 00:33:19.518496: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-12 00:33:19.536532: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:19.537177: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:19.537678: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:19.538255: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:19.538990: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:19.539827: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:1/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-12 00:33:19.555755: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:19.556727: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:19.557616: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:19.558088: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 00:33:19.558146: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-12 00:33:19.573647: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:19.574384: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:19.574927: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:19.575400: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 00:33:19.575452: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-12 00:33:19.575561: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:19.576493: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:19.577198: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:19.577753: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:19.578476: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:19.578961: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:2/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-12 00:33:19.580162: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:19.580764: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:19.581292: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:19.581747: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 00:33:19.581798: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-12 00:33:19.583660: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:19.584272: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:19.584786: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:19.585326: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 00:33:19.585377: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-12 00:33:19.592844: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:19.593465: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:19.593976: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:19.594532: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:19.595297: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:19.595768: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:6/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-12 00:33:19.599359: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:19.599986: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:19.600504: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:19.601076: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:19.601585: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:19.602058: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:4/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-12 00:33:19.603300: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:19.603926: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:19.604440: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:19.605336: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:19.605896: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:19.606381: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:3/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-12 00:33:19.620365: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:19.620991: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:19.621519: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:19.621596: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 00:33:19.621795: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 00:33:19.622126: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 00:33:19.622186: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-12 00:33:19.622643: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12341
2022-12-12 00:33:19.636679: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:19.637324: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:19.637840: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:19.638302: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 00:33:19.638361: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-12 00:33:19.639833: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:19.640436: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:19.640947: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:19.641513: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:19.642039: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:19.642098: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 00:33:19.642244: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 00:33:19.642507: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:7/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-12 00:33:19.643177: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12346
2022-12-12 00:33:19.648167: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 00:33:19.648342: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 00:33:19.649257: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12344
2022-12-12 00:33:19.653210: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 00:33:19.653364: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 00:33:19.654298: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12343
2022-12-12 00:33:19.655531: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:19.656255: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:19.656763: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:19.657347: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:19.657850: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:33:19.658324: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:5/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-12 00:33:19.676197: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 00:33:19.676370: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 00:33:19.677255: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12342
2022-12-12 00:33:19.689096: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 00:33:19.689268: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 00:33:19.690993: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12347
2022-12-12 00:33:19.704865: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 00:33:19.705044: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 00:33:19.706105: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12345
[HCTR][00:33:20.981][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][00:33:20.981][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][00:33:20.981][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][00:33:20.981][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][00:33:20.981][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][00:33:20.981][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][00:33:20.981][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][00:33:20.981][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 1it [00:00,  2.07it/s]warmup run: 95it [00:00, 215.57it/s]warmup run: 190it [00:00, 395.63it/s]warmup run: 285it [00:00, 539.39it/s]warmup run: 381it [00:00, 652.52it/s]warmup run: 478it [00:00, 739.00it/s]warmup run: 574it [00:01, 800.69it/s]warmup run: 671it [00:01, 848.82it/s]warmup run: 1it [00:01,  1.54s/it]warmup run: 1it [00:01,  1.53s/it]warmup run: 765it [00:01, 866.44it/s]warmup run: 100it [00:01, 84.79it/s]warmup run: 93it [00:01, 79.04it/s]warmup run: 1it [00:01,  1.52s/it]warmup run: 1it [00:01,  1.51s/it]warmup run: 1it [00:01,  1.53s/it]warmup run: 1it [00:01,  1.49s/it]warmup run: 858it [00:01, 869.94it/s]warmup run: 1it [00:01,  1.50s/it]warmup run: 200it [00:01, 183.92it/s]warmup run: 190it [00:01, 175.71it/s]warmup run: 100it [00:01, 85.60it/s]warmup run: 98it [00:01, 84.23it/s]warmup run: 86it [00:01, 72.98it/s]warmup run: 100it [00:01, 87.17it/s]warmup run: 950it [00:01, 871.07it/s]warmup run: 98it [00:01, 84.88it/s]warmup run: 289it [00:01, 284.94it/s]warmup run: 301it [00:01, 294.35it/s]warmup run: 200it [00:01, 185.36it/s]warmup run: 198it [00:01, 184.48it/s]warmup run: 173it [00:01, 159.39it/s]warmup run: 199it [00:01, 187.33it/s]warmup run: 1040it [00:01, 865.94it/s]warmup run: 199it [00:01, 186.82it/s]warmup run: 382it [00:01, 388.44it/s]warmup run: 402it [00:01, 408.66it/s]warmup run: 297it [00:01, 291.13it/s]warmup run: 299it [00:01, 295.83it/s]warmup run: 265it [00:01, 261.26it/s]warmup run: 298it [00:01, 297.08it/s]warmup run: 1130it [00:01, 873.92it/s]warmup run: 299it [00:01, 297.21it/s]warmup run: 476it [00:02, 490.86it/s]warmup run: 502it [00:02, 518.66it/s]warmup run: 398it [00:01, 406.97it/s]warmup run: 400it [00:01, 410.77it/s]warmup run: 359it [00:01, 369.93it/s]warmup run: 399it [00:01, 413.22it/s]warmup run: 1225it [00:01, 894.86it/s]warmup run: 397it [00:01, 407.79it/s]warmup run: 572it [00:02, 589.41it/s]warmup run: 605it [00:02, 624.87it/s]warmup run: 497it [00:02, 515.96it/s]warmup run: 499it [00:02, 519.15it/s]warmup run: 454it [00:02, 477.35it/s]warmup run: 498it [00:01, 521.60it/s]warmup run: 1323it [00:01, 918.01it/s]warmup run: 494it [00:02, 513.44it/s]warmup run: 676it [00:02, 692.06it/s]warmup run: 708it [00:02, 716.86it/s]warmup run: 596it [00:02, 614.85it/s]warmup run: 601it [00:02, 623.97it/s]warmup run: 549it [00:02, 576.08it/s]warmup run: 593it [00:02, 612.52it/s]warmup run: 1418it [00:02, 926.90it/s]warmup run: 594it [00:02, 615.55it/s]warmup run: 779it [00:02, 774.41it/s]warmup run: 811it [00:02, 792.61it/s]warmup run: 695it [00:02, 701.25it/s]warmup run: 700it [00:02, 707.60it/s]warmup run: 639it [00:02, 641.54it/s]warmup run: 695it [00:02, 706.72it/s]warmup run: 1516it [00:02, 940.63it/s]warmup run: 695it [00:02, 706.22it/s]warmup run: 881it [00:02, 837.87it/s]warmup run: 913it [00:02, 851.39it/s]warmup run: 798it [00:02, 781.58it/s]warmup run: 797it [00:02, 766.03it/s]warmup run: 796it [00:02, 782.33it/s]warmup run: 727it [00:02, 670.84it/s]warmup run: 1616it [00:02, 957.18it/s]warmup run: 796it [00:02, 781.69it/s]warmup run: 984it [00:02, 888.86it/s]warmup run: 1014it [00:02, 894.18it/s]warmup run: 897it [00:02, 832.76it/s]warmup run: 893it [00:02, 813.82it/s]warmup run: 897it [00:02, 841.67it/s]warmup run: 827it [00:02, 752.05it/s]warmup run: 1718it [00:02, 974.05it/s]warmup run: 896it [00:02, 838.44it/s]warmup run: 1116it [00:02, 928.33it/s]warmup run: 1085it [00:02, 920.14it/s]warmup run: 995it [00:02, 861.27it/s]warmup run: 992it [00:02, 860.08it/s]warmup run: 996it [00:02, 882.00it/s]warmup run: 921it [00:02, 800.06it/s]warmup run: 1820it [00:02, 984.97it/s]warmup run: 995it [00:02, 875.88it/s]warmup run: 1218it [00:02, 951.61it/s]warmup run: 1185it [00:02, 930.60it/s]warmup run: 1094it [00:02, 902.78it/s]warmup run: 1092it [00:02, 880.54it/s]warmup run: 1095it [00:02, 911.82it/s]warmup run: 1022it [00:02, 857.04it/s]warmup run: 1919it [00:02, 976.55it/s]warmup run: 1095it [00:02, 908.32it/s]warmup run: 1319it [00:02, 967.28it/s]warmup run: 1284it [00:02, 930.25it/s]warmup run: 1195it [00:02, 932.47it/s]warmup run: 1190it [00:02, 907.69it/s]warmup run: 1195it [00:02, 934.70it/s]warmup run: 1124it [00:02, 900.92it/s]warmup run: 2021it [00:02, 987.95it/s]warmup run: 1197it [00:02, 938.77it/s]warmup run: 1421it [00:02, 981.16it/s]warmup run: 1381it [00:02, 931.88it/s]warmup run: 1291it [00:02, 934.37it/s]warmup run: 1294it [00:02, 916.29it/s]warmup run: 1295it [00:02, 952.35it/s]warmup run: 1227it [00:02, 935.72it/s]warmup run: 2139it [00:02, 1042.82it/s]warmup run: 1299it [00:02, 960.69it/s]warmup run: 1522it [00:03, 987.33it/s]warmup run: 1482it [00:03, 953.42it/s]warmup run: 1393it [00:02, 958.32it/s]warmup run: 1390it [00:02, 908.46it/s]warmup run: 1396it [00:02, 968.29it/s]warmup run: 1327it [00:02, 952.32it/s]warmup run: 2257it [00:02, 1082.89it/s]warmup run: 1399it [00:02, 966.61it/s]warmup run: 1623it [00:03, 971.04it/s]warmup run: 1583it [00:03, 969.44it/s]warmup run: 1495it [00:03, 976.22it/s]warmup run: 1498it [00:02, 983.33it/s]warmup run: 1484it [00:03, 904.26it/s]warmup run: 1426it [00:03, 951.62it/s]warmup run: 2376it [00:02, 1114.63it/s]warmup run: 1499it [00:03, 975.95it/s]warmup run: 1722it [00:03, 974.98it/s]warmup run: 1687it [00:03, 987.80it/s]warmup run: 1598it [00:03, 990.36it/s]warmup run: 1601it [00:03, 995.96it/s]warmup run: 1578it [00:03, 914.19it/s]warmup run: 1528it [00:03, 971.31it/s]warmup run: 2497it [00:03, 1140.72it/s]warmup run: 1600it [00:03, 983.42it/s]warmup run: 1821it [00:03, 977.89it/s]warmup run: 1790it [00:03, 998.46it/s]warmup run: 1701it [00:03, 1001.29it/s]warmup run: 1702it [00:03, 999.76it/s]warmup run: 1672it [00:03, 919.88it/s]warmup run: 1627it [00:03, 975.75it/s]warmup run: 2619it [00:03, 1164.29it/s]warmup run: 1700it [00:03, 983.28it/s]warmup run: 1921it [00:03, 981.52it/s]warmup run: 1891it [00:03, 995.33it/s]warmup run: 1803it [00:03, 996.50it/s] warmup run: 1806it [00:03, 1009.69it/s]warmup run: 1766it [00:03, 924.55it/s]warmup run: 1726it [00:03, 968.43it/s]warmup run: 2740it [00:03, 1176.16it/s]warmup run: 1800it [00:03, 982.50it/s]warmup run: 2024it [00:03, 994.63it/s]warmup run: 1906it [00:03, 1004.28it/s]warmup run: 1992it [00:03, 990.24it/s]warmup run: 1860it [00:03, 928.85it/s]warmup run: 1908it [00:03, 1002.78it/s]warmup run: 2860it [00:03, 1182.72it/s]warmup run: 1824it [00:03, 963.21it/s]warmup run: 1899it [00:03, 984.43it/s]warmup run: 2144it [00:03, 1054.75it/s]warmup run: 2011it [00:03, 1015.63it/s]warmup run: 2107it [00:03, 1036.22it/s]warmup run: 1954it [00:03, 929.15it/s]warmup run: 2009it [00:03, 993.22it/s] warmup run: 2981it [00:03, 1188.23it/s]warmup run: 1921it [00:03, 958.97it/s]warmup run: 2000it [00:03, 991.07it/s]warmup run: 3000it [00:03, 871.80it/s] warmup run: 2265it [00:03, 1098.73it/s]warmup run: 2131it [00:03, 1069.07it/s]warmup run: 2225it [00:03, 1076.22it/s]warmup run: 2060it [00:03, 966.36it/s]warmup run: 2121it [00:03, 1028.58it/s]warmup run: 2019it [00:03, 963.99it/s]warmup run: 2121it [00:03, 1055.85it/s]warmup run: 2387it [00:03, 1134.12it/s]warmup run: 2251it [00:03, 1107.44it/s]warmup run: 2343it [00:03, 1105.64it/s]warmup run: 2180it [00:03, 1035.06it/s]warmup run: 2234it [00:03, 1056.06it/s]warmup run: 2141it [00:03, 1038.41it/s]warmup run: 2244it [00:03, 1105.65it/s]warmup run: 2510it [00:03, 1160.00it/s]warmup run: 2372it [00:03, 1136.56it/s]warmup run: 2461it [00:03, 1125.63it/s]warmup run: 2300it [00:03, 1084.00it/s]warmup run: 2347it [00:03, 1076.54it/s]warmup run: 2262it [00:03, 1088.89it/s]warmup run: 2367it [00:03, 1140.82it/s]warmup run: 2633it [00:04, 1178.24it/s]warmup run: 2493it [00:03, 1156.42it/s]warmup run: 2579it [00:04, 1140.37it/s]warmup run: 2421it [00:03, 1119.08it/s]warmup run: 2460it [00:03, 1090.49it/s]warmup run: 2384it [00:03, 1126.71it/s]warmup run: 2490it [00:03, 1164.61it/s]warmup run: 2754it [00:04, 1186.64it/s]warmup run: 2613it [00:04, 1168.80it/s]warmup run: 2697it [00:04, 1151.79it/s]warmup run: 2542it [00:04, 1143.54it/s]warmup run: 2573it [00:04, 1100.05it/s]warmup run: 2506it [00:04, 1153.61it/s]warmup run: 2607it [00:04, 1160.47it/s]warmup run: 2876it [00:04, 1196.54it/s]warmup run: 2733it [00:04, 1177.41it/s]warmup run: 2814it [00:04, 1155.17it/s]warmup run: 2662it [00:04, 1159.44it/s]warmup run: 2686it [00:04, 1107.75it/s]warmup run: 2628it [00:04, 1172.41it/s]warmup run: 2729it [00:04, 1177.06it/s]warmup run: 2999it [00:04, 1203.59it/s]warmup run: 3000it [00:04, 687.05it/s] warmup run: 2853it [00:04, 1181.29it/s]warmup run: 2931it [00:04, 1158.50it/s]warmup run: 2780it [00:04, 1164.79it/s]warmup run: 2798it [00:04, 1108.75it/s]warmup run: 2750it [00:04, 1185.04it/s]warmup run: 2848it [00:04, 1179.70it/s]warmup run: 3000it [00:04, 677.29it/s] warmup run: 2974it [00:04, 1188.42it/s]warmup run: 2899it [00:04, 1171.51it/s]warmup run: 2909it [00:04, 1100.82it/s]warmup run: 3000it [00:04, 685.86it/s] warmup run: 2869it [00:04, 1185.95it/s]warmup run: 2968it [00:04, 1183.03it/s]warmup run: 3000it [00:04, 688.49it/s] warmup run: 3000it [00:04, 674.57it/s] warmup run: 3000it [00:04, 681.69it/s] warmup run: 2990it [00:04, 1192.74it/s]warmup run: 3000it [00:04, 667.20it/s] 
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]






warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   6%|▌         | 167/3000 [00:00<00:01, 1663.86it/s]warmup should be done:   5%|▌         | 164/3000 [00:00<00:01, 1638.69it/s]warmup should be done:   5%|▌         | 152/3000 [00:00<00:01, 1517.02it/s]warmup should be done:   5%|▌         | 162/3000 [00:00<00:01, 1616.13it/s]warmup should be done:   5%|▌         | 162/3000 [00:00<00:01, 1616.62it/s]warmup should be done:   5%|▌         | 162/3000 [00:00<00:01, 1616.04it/s]warmup should be done:   5%|▌         | 164/3000 [00:00<00:01, 1634.97it/s]warmup should be done:   6%|▌         | 166/3000 [00:00<00:01, 1655.51it/s]warmup should be done:  11%|█         | 329/3000 [00:00<00:01, 1645.13it/s]warmup should be done:  10%|█         | 304/3000 [00:00<00:01, 1517.01it/s]warmup should be done:  11%|█         | 325/3000 [00:00<00:01, 1622.87it/s]warmup should be done:  11%|█         | 334/3000 [00:00<00:01, 1663.98it/s]warmup should be done:  11%|█         | 325/3000 [00:00<00:01, 1619.93it/s]warmup should be done:  11%|█         | 326/3000 [00:00<00:01, 1625.27it/s]warmup should be done:  11%|█         | 333/3000 [00:00<00:01, 1659.68it/s]warmup should be done:  11%|█         | 330/3000 [00:00<00:01, 1644.13it/s]warmup should be done:  16%|█▋        | 494/3000 [00:00<00:01, 1641.63it/s]warmup should be done:  17%|█▋        | 499/3000 [00:00<00:01, 1658.26it/s]warmup should be done:  15%|█▌        | 456/3000 [00:00<00:01, 1513.27it/s]warmup should be done:  16%|█▌        | 487/3000 [00:00<00:01, 1616.78it/s]warmup should be done:  16%|█▋        | 488/3000 [00:00<00:01, 1620.46it/s]warmup should be done:  16%|█▋        | 495/3000 [00:00<00:01, 1642.55it/s]warmup should be done:  16%|█▋        | 489/3000 [00:00<00:01, 1621.22it/s]warmup should be done:  17%|█▋        | 501/3000 [00:00<00:01, 1655.88it/s]warmup should be done:  22%|██▏       | 665/3000 [00:00<00:01, 1658.52it/s]warmup should be done:  22%|██▏       | 659/3000 [00:00<00:01, 1638.40it/s]warmup should be done:  22%|██▏       | 651/3000 [00:00<00:01, 1617.87it/s]warmup should be done:  22%|██▏       | 660/3000 [00:00<00:01, 1640.04it/s]warmup should be done:  22%|██▏       | 652/3000 [00:00<00:01, 1617.22it/s]warmup should be done:  22%|██▏       | 667/3000 [00:00<00:01, 1651.45it/s]warmup should be done:  20%|██        | 608/3000 [00:00<00:01, 1501.57it/s]warmup should be done:  22%|██▏       | 649/3000 [00:00<00:01, 1602.25it/s]warmup should be done:  27%|██▋       | 813/3000 [00:00<00:01, 1616.50it/s]warmup should be done:  27%|██▋       | 823/3000 [00:00<00:01, 1634.39it/s]warmup should be done:  28%|██▊       | 825/3000 [00:00<00:01, 1637.83it/s]warmup should be done:  27%|██▋       | 814/3000 [00:00<00:01, 1614.58it/s]warmup should be done:  25%|██▌       | 759/3000 [00:00<00:01, 1502.84it/s]warmup should be done:  27%|██▋       | 810/3000 [00:00<00:01, 1603.59it/s]warmup should be done:  28%|██▊       | 831/3000 [00:00<00:01, 1642.06it/s]warmup should be done:  28%|██▊       | 833/3000 [00:00<00:01, 1645.52it/s]warmup should be done:  32%|███▎      | 975/3000 [00:00<00:01, 1613.22it/s]warmup should be done:  33%|███▎      | 987/3000 [00:00<00:01, 1630.51it/s]warmup should be done:  33%|███▎      | 989/3000 [00:00<00:01, 1635.26it/s]warmup should be done:  33%|███▎      | 996/3000 [00:00<00:01, 1640.67it/s]warmup should be done:  33%|███▎      | 976/3000 [00:00<00:01, 1606.77it/s]warmup should be done:  32%|███▏      | 971/3000 [00:00<00:01, 1598.12it/s]warmup should be done:  30%|███       | 910/3000 [00:00<00:01, 1497.94it/s]warmup should be done:  33%|███▎      | 998/3000 [00:00<00:01, 1631.94it/s]warmup should be done:  38%|███▊      | 1137/3000 [00:00<00:01, 1609.13it/s]warmup should be done:  39%|███▊      | 1161/3000 [00:00<00:01, 1641.51it/s]warmup should be done:  38%|███▊      | 1151/3000 [00:00<00:01, 1625.38it/s]warmup should be done:  35%|███▌      | 1060/3000 [00:00<00:01, 1496.90it/s]warmup should be done:  38%|███▊      | 1132/3000 [00:00<00:01, 1599.18it/s]warmup should be done:  38%|███▊      | 1137/3000 [00:00<00:01, 1602.73it/s]warmup should be done:  38%|███▊      | 1153/3000 [00:00<00:01, 1622.10it/s]warmup should be done:  39%|███▊      | 1162/3000 [00:00<00:01, 1630.80it/s]warmup should be done:  43%|████▎     | 1298/3000 [00:00<00:01, 1608.84it/s]warmup should be done:  44%|████▍     | 1314/3000 [00:00<00:01, 1626.11it/s]warmup should be done:  44%|████▍     | 1327/3000 [00:00<00:01, 1645.00it/s]warmup should be done:  43%|████▎     | 1295/3000 [00:00<00:01, 1606.86it/s]warmup should be done:  40%|████      | 1210/3000 [00:00<00:01, 1493.07it/s]warmup should be done:  44%|████▍     | 1317/3000 [00:00<00:01, 1626.55it/s]warmup should be done:  43%|████▎     | 1298/3000 [00:00<00:01, 1600.87it/s]warmup should be done:  44%|████▍     | 1326/3000 [00:00<00:01, 1633.60it/s]warmup should be done:  49%|████▊     | 1460/3000 [00:00<00:00, 1609.55it/s]warmup should be done:  49%|████▉     | 1478/3000 [00:00<00:00, 1628.03it/s]warmup should be done:  49%|████▊     | 1458/3000 [00:00<00:00, 1612.76it/s]warmup should be done:  45%|████▌     | 1360/3000 [00:00<00:01, 1494.80it/s]warmup should be done:  49%|████▉     | 1481/3000 [00:00<00:00, 1629.95it/s]warmup should be done:  49%|████▊     | 1460/3000 [00:00<00:00, 1605.16it/s]warmup should be done:  50%|████▉     | 1492/3000 [00:00<00:00, 1639.12it/s]warmup should be done:  50%|████▉     | 1492/3000 [00:00<00:01, 1507.24it/s]warmup should be done:  54%|█████▍    | 1622/3000 [00:01<00:00, 1610.11it/s]warmup should be done:  55%|█████▍    | 1642/3000 [00:01<00:00, 1629.19it/s]warmup should be done:  54%|█████▍    | 1621/3000 [00:01<00:00, 1615.97it/s]warmup should be done:  50%|█████     | 1510/3000 [00:01<00:00, 1495.13it/s]warmup should be done:  55%|█████▍    | 1645/3000 [00:01<00:00, 1632.63it/s]warmup should be done:  54%|█████▍    | 1622/3000 [00:01<00:00, 1609.60it/s]warmup should be done:  55%|█████▌    | 1657/3000 [00:01<00:00, 1642.37it/s]warmup should be done:  55%|█████▌    | 1656/3000 [00:01<00:00, 1543.46it/s]warmup should be done:  59%|█████▉    | 1784/3000 [00:01<00:00, 1607.48it/s]warmup should be done:  60%|██████    | 1806/3000 [00:01<00:00, 1629.93it/s]warmup should be done:  59%|█████▉    | 1784/3000 [00:01<00:00, 1612.56it/s]warmup should be done:  55%|█████▌    | 1660/3000 [00:01<00:00, 1494.52it/s]warmup should be done:  60%|██████    | 1809/3000 [00:01<00:00, 1630.17it/s]warmup should be done:  59%|█████▉    | 1783/3000 [00:01<00:00, 1611.19it/s]warmup should be done:  61%|██████    | 1822/3000 [00:01<00:00, 1644.11it/s]warmup should be done:  61%|██████    | 1820/3000 [00:01<00:00, 1569.04it/s]warmup should be done:  66%|██████▌   | 1969/3000 [00:01<00:00, 1629.55it/s]warmup should be done:  65%|██████▍   | 1946/3000 [00:01<00:00, 1612.93it/s]warmup should be done:  65%|██████▍   | 1945/3000 [00:01<00:00, 1602.51it/s]warmup should be done:  60%|██████    | 1810/3000 [00:01<00:00, 1493.43it/s]warmup should be done:  66%|██████▌   | 1987/3000 [00:01<00:00, 1644.17it/s]warmup should be done:  66%|██████▌   | 1973/3000 [00:01<00:00, 1628.27it/s]warmup should be done:  65%|██████▍   | 1945/3000 [00:01<00:00, 1607.23it/s]warmup should be done:  66%|██████▌   | 1979/3000 [00:01<00:00, 1557.33it/s]warmup should be done:  71%|███████   | 2133/3000 [00:01<00:00, 1630.01it/s]warmup should be done:  70%|███████   | 2108/3000 [00:01<00:00, 1612.04it/s]warmup should be done:  65%|██████▌   | 1960/3000 [00:01<00:00, 1492.41it/s]warmup should be done:  71%|███████   | 2136/3000 [00:01<00:00, 1627.81it/s]warmup should be done:  70%|███████   | 2106/3000 [00:01<00:00, 1595.12it/s]warmup should be done:  70%|███████   | 2106/3000 [00:01<00:00, 1604.23it/s]warmup should be done:  72%|███████▏  | 2152/3000 [00:01<00:00, 1637.79it/s]warmup should be done:  71%|███████▏  | 2143/3000 [00:01<00:00, 1579.19it/s]warmup should be done:  77%|███████▋  | 2297/3000 [00:01<00:00, 1629.85it/s]warmup should be done:  70%|███████   | 2110/3000 [00:01<00:00, 1491.75it/s]warmup should be done:  77%|███████▋  | 2299/3000 [00:01<00:00, 1626.72it/s]warmup should be done:  76%|███████▌  | 2270/3000 [00:01<00:00, 1608.04it/s]warmup should be done:  77%|███████▋  | 2317/3000 [00:01<00:00, 1640.56it/s]warmup should be done:  76%|███████▌  | 2267/3000 [00:01<00:00, 1600.11it/s]warmup should be done:  76%|███████▌  | 2266/3000 [00:01<00:00, 1588.87it/s]warmup should be done:  77%|███████▋  | 2307/3000 [00:01<00:00, 1595.15it/s]warmup should be done:  82%|████████▏ | 2460/3000 [00:01<00:00, 1626.95it/s]warmup should be done:  75%|███████▌  | 2260/3000 [00:01<00:00, 1491.70it/s]warmup should be done:  82%|████████▏ | 2462/3000 [00:01<00:00, 1623.02it/s]warmup should be done:  81%|████████  | 2431/3000 [00:01<00:00, 1603.82it/s]warmup should be done:  83%|████████▎ | 2482/3000 [00:01<00:00, 1640.23it/s]warmup should be done:  81%|████████  | 2425/3000 [00:01<00:00, 1582.57it/s]warmup should be done:  81%|████████  | 2428/3000 [00:01<00:00, 1593.58it/s]warmup should be done:  82%|████████▏ | 2470/3000 [00:01<00:00, 1603.65it/s]warmup should be done:  87%|████████▋ | 2623/3000 [00:01<00:00, 1627.08it/s]warmup should be done:  80%|████████  | 2410/3000 [00:01<00:00, 1489.36it/s]warmup should be done:  88%|████████▊ | 2626/3000 [00:01<00:00, 1625.34it/s]warmup should be done:  86%|████████▋ | 2592/3000 [00:01<00:00, 1603.18it/s]warmup should be done:  88%|████████▊ | 2648/3000 [00:01<00:00, 1643.34it/s]warmup should be done:  86%|████████▌ | 2584/3000 [00:01<00:00, 1579.83it/s]warmup should be done:  86%|████████▋ | 2588/3000 [00:01<00:00, 1588.87it/s]warmup should be done:  88%|████████▊ | 2634/3000 [00:01<00:00, 1612.30it/s]warmup should be done:  93%|█████████▎| 2786/3000 [00:01<00:00, 1627.90it/s]warmup should be done:  85%|████████▌ | 2559/3000 [00:01<00:00, 1489.15it/s]warmup should be done:  93%|█████████▎| 2791/3000 [00:01<00:00, 1631.68it/s]warmup should be done:  92%|█████████▏| 2753/3000 [00:01<00:00, 1603.45it/s]warmup should be done:  91%|█████████▏| 2743/3000 [00:01<00:00, 1580.75it/s]warmup should be done:  92%|█████████▏| 2747/3000 [00:01<00:00, 1582.90it/s]warmup should be done:  94%|█████████▍| 2813/3000 [00:01<00:00, 1605.83it/s]warmup should be done:  93%|█████████▎| 2798/3000 [00:01<00:00, 1618.38it/s]warmup should be done:  98%|█████████▊| 2951/3000 [00:01<00:00, 1634.48it/s]warmup should be done:  90%|█████████ | 2709/3000 [00:01<00:00, 1490.66it/s]warmup should be done:  99%|█████████▊| 2958/3000 [00:01<00:00, 1641.90it/s]warmup should be done:  97%|█████████▋| 2914/3000 [00:01<00:00, 1598.40it/s]warmup should be done:  97%|█████████▋| 2904/3000 [00:01<00:00, 1588.84it/s]warmup should be done:  97%|█████████▋| 2907/3000 [00:01<00:00, 1586.22it/s]warmup should be done:  99%|█████████▉| 2974/3000 [00:01<00:00, 1592.84it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1632.82it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1631.08it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1629.27it/s]warmup should be done:  99%|█████████▉| 2963/3000 [00:01<00:00, 1625.98it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1608.03it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1606.68it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1600.73it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1598.69it/s]warmup should be done:  95%|█████████▌| 2860/3000 [00:01<00:00, 1495.98it/s]warmup should be done: 100%|██████████| 3000/3000 [00:02<00:00, 1497.05it/s]


warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]

warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   6%|▌         | 166/3000 [00:00<00:01, 1658.74it/s]warmup should be done:   6%|▌         | 168/3000 [00:00<00:01, 1676.27it/s]warmup should be done:   6%|▌         | 165/3000 [00:00<00:01, 1649.75it/s]warmup should be done:   5%|▌         | 155/3000 [00:00<00:01, 1543.88it/s]warmup should be done:   6%|▌         | 170/3000 [00:00<00:01, 1693.38it/s]warmup should be done:   5%|▌         | 163/3000 [00:00<00:01, 1625.95it/s]warmup should be done:   6%|▌         | 168/3000 [00:00<00:01, 1675.21it/s]warmup should be done:   6%|▌         | 166/3000 [00:00<00:01, 1655.10it/s]warmup should be done:  11%|█         | 332/3000 [00:00<00:01, 1657.57it/s]warmup should be done:  11%|█         | 333/3000 [00:00<00:01, 1666.94it/s]warmup should be done:  10%|█         | 310/3000 [00:00<00:01, 1547.07it/s]warmup should be done:  11%|█         | 337/3000 [00:00<00:01, 1682.89it/s]warmup should be done:  11%|█         | 336/3000 [00:00<00:01, 1675.17it/s]warmup should be done:  11%|█         | 327/3000 [00:00<00:01, 1628.41it/s]warmup should be done:  11%|█         | 335/3000 [00:00<00:01, 1669.93it/s]warmup should be done:  11%|█▏        | 340/3000 [00:00<00:01, 1691.07it/s]warmup should be done:  17%|█▋        | 498/3000 [00:00<00:01, 1658.25it/s]warmup should be done:  17%|█▋        | 504/3000 [00:00<00:01, 1676.44it/s]warmup should be done:  16%|█▌        | 466/3000 [00:00<00:01, 1550.07it/s]warmup should be done:  17%|█▋        | 507/3000 [00:00<00:01, 1686.59it/s]warmup should be done:  17%|█▋        | 504/3000 [00:00<00:01, 1677.10it/s]warmup should be done:  17%|█▋        | 510/3000 [00:00<00:01, 1692.27it/s]warmup should be done:  16%|█▋        | 492/3000 [00:00<00:01, 1633.39it/s]warmup should be done:  17%|█▋        | 500/3000 [00:00<00:01, 1623.69it/s]warmup should be done:  22%|██▏       | 665/3000 [00:00<00:01, 1660.44it/s]warmup should be done:  22%|██▏       | 673/3000 [00:00<00:01, 1679.26it/s]warmup should be done:  23%|██▎       | 677/3000 [00:00<00:01, 1691.53it/s]warmup should be done:  21%|██        | 623/3000 [00:00<00:01, 1555.65it/s]warmup should be done:  22%|██▏       | 657/3000 [00:00<00:01, 1639.65it/s]warmup should be done:  22%|██▏       | 673/3000 [00:00<00:01, 1678.29it/s]warmup should be done:  23%|██▎       | 681/3000 [00:00<00:01, 1696.01it/s]warmup should be done:  22%|██▏       | 671/3000 [00:00<00:01, 1654.11it/s]warmup should be done:  28%|██▊       | 832/3000 [00:00<00:01, 1660.76it/s]warmup should be done:  28%|██▊       | 847/3000 [00:00<00:01, 1692.67it/s]warmup should be done:  26%|██▌       | 779/3000 [00:00<00:01, 1554.51it/s]warmup should be done:  28%|██▊       | 842/3000 [00:00<00:01, 1679.81it/s]warmup should be done:  28%|██▊       | 841/3000 [00:00<00:01, 1678.16it/s]warmup should be done:  27%|██▋       | 822/3000 [00:00<00:01, 1640.28it/s]warmup should be done:  28%|██▊       | 852/3000 [00:00<00:01, 1698.45it/s]warmup should be done:  28%|██▊       | 842/3000 [00:00<00:01, 1670.99it/s]warmup should be done:  34%|███▎      | 1010/3000 [00:00<00:01, 1678.97it/s]warmup should be done:  31%|███       | 935/3000 [00:00<00:01, 1555.17it/s]warmup should be done:  33%|███▎      | 999/3000 [00:00<00:01, 1659.87it/s]warmup should be done:  34%|███▍      | 1017/3000 [00:00<00:01, 1691.51it/s]warmup should be done:  34%|███▎      | 1009/3000 [00:00<00:01, 1676.36it/s]warmup should be done:  34%|███▍      | 1022/3000 [00:00<00:01, 1697.03it/s]warmup should be done:  33%|███▎      | 987/3000 [00:00<00:01, 1634.45it/s]warmup should be done:  34%|███▍      | 1013/3000 [00:00<00:01, 1683.87it/s]warmup should be done:  39%|███▉      | 1178/3000 [00:00<00:01, 1676.53it/s]warmup should be done:  36%|███▋      | 1092/3000 [00:00<00:01, 1557.69it/s]warmup should be done:  39%|███▉      | 1166/3000 [00:00<00:01, 1660.82it/s]warmup should be done:  39%|███▉      | 1178/3000 [00:00<00:01, 1678.16it/s]warmup should be done:  40%|███▉      | 1187/3000 [00:00<00:01, 1687.71it/s]warmup should be done:  38%|███▊      | 1151/3000 [00:00<00:01, 1635.49it/s]warmup should be done:  40%|███▉      | 1192/3000 [00:00<00:01, 1684.70it/s]warmup should be done:  39%|███▉      | 1183/3000 [00:00<00:01, 1686.54it/s]warmup should be done:  45%|████▍     | 1346/3000 [00:00<00:00, 1677.37it/s]warmup should be done:  45%|████▍     | 1346/3000 [00:00<00:00, 1677.68it/s]warmup should be done:  44%|████▍     | 1333/3000 [00:00<00:01, 1658.59it/s]warmup should be done:  42%|████▏     | 1248/3000 [00:00<00:01, 1552.68it/s]warmup should be done:  45%|████▌     | 1357/3000 [00:00<00:00, 1686.12it/s]warmup should be done:  44%|████▍     | 1315/3000 [00:00<00:01, 1632.99it/s]warmup should be done:  45%|████▌     | 1361/3000 [00:00<00:00, 1679.88it/s]warmup should be done:  45%|████▌     | 1352/3000 [00:00<00:00, 1681.16it/s]warmup should be done:  50%|█████     | 1514/3000 [00:00<00:00, 1673.66it/s]warmup should be done:  50%|█████     | 1515/3000 [00:00<00:00, 1680.38it/s]warmup should be done:  50%|████▉     | 1499/3000 [00:00<00:00, 1658.58it/s]warmup should be done:  47%|████▋     | 1405/3000 [00:00<00:01, 1555.15it/s]warmup should be done:  49%|████▉     | 1480/3000 [00:00<00:00, 1635.77it/s]warmup should be done:  51%|█████     | 1526/3000 [00:00<00:00, 1679.03it/s]warmup should be done:  51%|█████     | 1529/3000 [00:00<00:00, 1677.68it/s]warmup should be done:  51%|█████     | 1521/3000 [00:00<00:00, 1674.86it/s]warmup should be done:  56%|█████▌    | 1665/3000 [00:01<00:00, 1658.43it/s]warmup should be done:  56%|█████▌    | 1684/3000 [00:01<00:00, 1680.62it/s]warmup should be done:  52%|█████▏    | 1561/3000 [00:01<00:00, 1554.09it/s]warmup should be done:  56%|█████▌    | 1682/3000 [00:01<00:00, 1667.40it/s]warmup should be done:  55%|█████▍    | 1644/3000 [00:01<00:00, 1634.23it/s]warmup should be done:  57%|█████▋    | 1696/3000 [00:01<00:00, 1683.93it/s]warmup should be done:  57%|█████▋    | 1697/3000 [00:01<00:00, 1676.95it/s]warmup should be done:  56%|█████▋    | 1690/3000 [00:01<00:00, 1678.81it/s]warmup should be done:  61%|██████    | 1832/3000 [00:01<00:00, 1660.78it/s]warmup should be done:  62%|██████▏   | 1854/3000 [00:01<00:00, 1684.23it/s]warmup should be done:  57%|█████▋    | 1717/3000 [00:01<00:00, 1553.68it/s]warmup should be done:  60%|██████    | 1809/3000 [00:01<00:00, 1637.31it/s]warmup should be done:  62%|██████▏   | 1849/3000 [00:01<00:00, 1661.22it/s]warmup should be done:  62%|██████▏   | 1866/3000 [00:01<00:00, 1688.21it/s]warmup should be done:  62%|██████▏   | 1865/3000 [00:01<00:00, 1677.23it/s]warmup should be done:  62%|██████▏   | 1861/3000 [00:01<00:00, 1686.30it/s]warmup should be done:  67%|██████▋   | 1999/3000 [00:01<00:00, 1659.85it/s]warmup should be done:  67%|██████▋   | 2023/3000 [00:01<00:00, 1684.04it/s]warmup should be done:  62%|██████▏   | 1874/3000 [00:01<00:00, 1556.28it/s]warmup should be done:  68%|██████▊   | 2036/3000 [00:01<00:00, 1690.72it/s]warmup should be done:  66%|██████▌   | 1973/3000 [00:01<00:00, 1635.89it/s]warmup should be done:  68%|██████▊   | 2033/3000 [00:01<00:00, 1677.53it/s]warmup should be done:  67%|██████▋   | 2016/3000 [00:01<00:00, 1654.82it/s]warmup should be done:  68%|██████▊   | 2032/3000 [00:01<00:00, 1690.65it/s]warmup should be done:  72%|███████▏  | 2165/3000 [00:01<00:00, 1659.50it/s]warmup should be done:  73%|███████▎  | 2192/3000 [00:01<00:00, 1682.06it/s]warmup should be done:  68%|██████▊   | 2030/3000 [00:01<00:00, 1556.72it/s]warmup should be done:  71%|███████   | 2137/3000 [00:01<00:00, 1636.77it/s]warmup should be done:  74%|███████▎  | 2206/3000 [00:01<00:00, 1688.99it/s]warmup should be done:  73%|███████▎  | 2201/3000 [00:01<00:00, 1676.77it/s]warmup should be done:  73%|███████▎  | 2182/3000 [00:01<00:00, 1649.52it/s]warmup should be done:  73%|███████▎  | 2202/3000 [00:01<00:00, 1693.18it/s]warmup should be done:  78%|███████▊  | 2332/3000 [00:01<00:00, 1661.23it/s]warmup should be done:  73%|███████▎  | 2186/3000 [00:01<00:00, 1554.82it/s]warmup should be done:  77%|███████▋  | 2301/3000 [00:01<00:00, 1634.19it/s]warmup should be done:  79%|███████▉  | 2369/3000 [00:01<00:00, 1676.70it/s]warmup should be done:  79%|███████▉  | 2375/3000 [00:01<00:00, 1681.87it/s]warmup should be done:  79%|███████▊  | 2361/3000 [00:01<00:00, 1663.87it/s]warmup should be done:  78%|███████▊  | 2347/3000 [00:01<00:00, 1647.99it/s]warmup should be done:  79%|███████▉  | 2373/3000 [00:01<00:00, 1695.21it/s]warmup should be done:  83%|████████▎ | 2499/3000 [00:01<00:00, 1661.93it/s]warmup should be done:  78%|███████▊  | 2342/3000 [00:01<00:00, 1556.19it/s]warmup should be done:  85%|████████▍ | 2539/3000 [00:01<00:00, 1681.46it/s]warmup should be done:  82%|████████▏ | 2465/3000 [00:01<00:00, 1631.73it/s]warmup should be done:  85%|████████▍ | 2544/3000 [00:01<00:00, 1679.79it/s]warmup should be done:  84%|████████▍ | 2530/3000 [00:01<00:00, 1670.06it/s]warmup should be done:  84%|████████▎ | 2512/3000 [00:01<00:00, 1647.49it/s]warmup should be done:  85%|████████▍ | 2544/3000 [00:01<00:00, 1698.17it/s]warmup should be done:  89%|████████▉ | 2666/3000 [00:01<00:00, 1660.31it/s]warmup should be done:  83%|████████▎ | 2499/3000 [00:01<00:00, 1558.12it/s]warmup should be done:  90%|█████████ | 2710/3000 [00:01<00:00, 1687.58it/s]warmup should be done:  88%|████████▊ | 2629/3000 [00:01<00:00, 1628.07it/s]warmup should be done:  90%|█████████ | 2714/3000 [00:01<00:00, 1683.94it/s]warmup should be done:  90%|████████▉ | 2699/3000 [00:01<00:00, 1673.86it/s]warmup should be done:  89%|████████▉ | 2677/3000 [00:01<00:00, 1644.64it/s]warmup should be done:  90%|█████████ | 2715/3000 [00:01<00:00, 1700.34it/s]warmup should be done:  94%|█████████▍| 2833/3000 [00:01<00:00, 1660.16it/s]warmup should be done:  88%|████████▊ | 2655/3000 [00:01<00:00, 1555.21it/s]warmup should be done:  96%|█████████▌| 2880/3000 [00:01<00:00, 1689.67it/s]warmup should be done:  93%|█████████▎| 2793/3000 [00:01<00:00, 1630.48it/s]warmup should be done:  96%|█████████▌| 2883/3000 [00:01<00:00, 1684.47it/s]warmup should be done:  96%|█████████▌| 2867/3000 [00:01<00:00, 1674.97it/s]warmup should be done:  95%|█████████▍| 2842/3000 [00:01<00:00, 1642.36it/s]warmup should be done:  96%|█████████▌| 2886/3000 [00:01<00:00, 1700.25it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1685.76it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1685.63it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1684.53it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1676.51it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1661.24it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1660.09it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1659.92it/s]warmup should be done:  99%|█████████▊| 2958/3000 [00:01<00:00, 1633.78it/s]warmup should be done:  94%|█████████▎| 2811/3000 [00:01<00:00, 1534.19it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1633.85it/s]warmup should be done:  99%|█████████▉| 2979/3000 [00:01<00:00, 1575.68it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1557.73it/s]
WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f21d289e730>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f21d256b160>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f21d257a0d0>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f21d256b1f0>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f21d25791c0>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f21d28a0d30>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f21d256d2b0>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f21d289de80>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
2022-12-12 00:34:50.252872: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f1d0a82f330 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 00:34:50.252970: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 00:34:50.261654: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 00:34:50.364622: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f1d070286d0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 00:34:50.364678: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 00:34:50.373881: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 00:34:50.408395: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f1d0b0306f0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 00:34:50.408452: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 00:34:50.417365: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 00:34:50.993292: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f1d0282fb10 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 00:34:50.993357: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 00:34:51.002896: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 00:34:51.085551: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f1d0a82fdc0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 00:34:51.085621: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 00:34:51.085628: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f1d070284d0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 00:34:51.085676: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 00:34:51.095079: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 00:34:51.095173: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 00:34:51.108899: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f1d02837290 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 00:34:51.108967: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 00:34:51.112731: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f1cfef91c60 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 00:34:51.112784: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 00:34:51.117020: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 00:34:51.120236: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 00:34:57.412544: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 00:34:57.532488: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 00:34:57.583289: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 00:34:57.749367: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 00:34:57.806009: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 00:34:57.816860: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 00:34:58.054138: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 00:34:58.056257: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
[HCTR][00:35:55.876][ERROR][RK0][tid #139763034076928]: replica 7 reaches 1000, calling init pre replica
[HCTR][00:35:55.876][ERROR][RK0][tid #139763034076928]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][00:35:55.886][ERROR][RK0][tid #139763034076928]: coll ps creation done
[HCTR][00:35:55.886][ERROR][RK0][tid #139763034076928]: replica 7 waits for coll ps creation barrier
[HCTR][00:35:55.979][ERROR][RK0][main]: replica 5 reaches 1000, calling init pre replica
[HCTR][00:35:55.979][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][00:35:55.988][ERROR][RK0][main]: coll ps creation done
[HCTR][00:35:55.988][ERROR][RK0][main]: replica 5 waits for coll ps creation barrier
[HCTR][00:35:56.044][ERROR][RK0][main]: replica 4 reaches 1000, calling init pre replica
[HCTR][00:35:56.044][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][00:35:56.051][ERROR][RK0][main]: coll ps creation done
[HCTR][00:35:56.051][ERROR][RK0][main]: replica 4 waits for coll ps creation barrier
[HCTR][00:35:56.162][ERROR][RK0][main]: replica 6 reaches 1000, calling init pre replica
[HCTR][00:35:56.163][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][00:35:56.166][ERROR][RK0][main]: replica 1 reaches 1000, calling init pre replica
[HCTR][00:35:56.166][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][00:35:56.167][ERROR][RK0][main]: coll ps creation done
[HCTR][00:35:56.167][ERROR][RK0][main]: replica 6 waits for coll ps creation barrier
[HCTR][00:35:56.174][ERROR][RK0][main]: coll ps creation done
[HCTR][00:35:56.174][ERROR][RK0][main]: replica 1 waits for coll ps creation barrier
[HCTR][00:35:56.200][ERROR][RK0][tid #139763168294656]: replica 0 reaches 1000, calling init pre replica
[HCTR][00:35:56.200][ERROR][RK0][tid #139763168294656]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][00:35:56.208][ERROR][RK0][tid #139763168294656]: coll ps creation done
[HCTR][00:35:56.208][ERROR][RK0][tid #139763168294656]: replica 0 waits for coll ps creation barrier
[HCTR][00:35:56.372][ERROR][RK0][main]: replica 3 reaches 1000, calling init pre replica
[HCTR][00:35:56.372][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][00:35:56.380][ERROR][RK0][main]: coll ps creation done
[HCTR][00:35:56.380][ERROR][RK0][main]: replica 3 waits for coll ps creation barrier
[HCTR][00:35:56.381][ERROR][RK0][tid #139763763881728]: replica 2 reaches 1000, calling init pre replica
[HCTR][00:35:56.381][ERROR][RK0][tid #139763763881728]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][00:35:56.388][ERROR][RK0][tid #139763763881728]: coll ps creation done
[HCTR][00:35:56.388][ERROR][RK0][tid #139763763881728]: replica 2 waits for coll ps creation barrier
[HCTR][00:35:56.388][ERROR][RK0][tid #139763168294656]: replica 0 preparing frequency
[HCTR][00:35:57.272][ERROR][RK0][tid #139763168294656]: replica 0 preparing frequency done
[HCTR][00:35:57.306][ERROR][RK0][main]: replica 1 calling init per replica
[HCTR][00:35:57.306][ERROR][RK0][main]: replica 4 calling init per replica
[HCTR][00:35:57.306][ERROR][RK0][tid #139763168294656]: replica 0 calling init per replica
[HCTR][00:35:57.306][ERROR][RK0][tid #139763763881728]: replica 2 calling init per replica
[HCTR][00:35:57.306][ERROR][RK0][tid #139763034076928]: replica 7 calling init per replica
[HCTR][00:35:57.306][ERROR][RK0][main]: replica 3 calling init per replica
[HCTR][00:35:57.306][ERROR][RK0][main]: replica 6 calling init per replica
[HCTR][00:35:57.306][ERROR][RK0][main]: replica 5 calling init per replica
[HCTR][00:35:57.306][ERROR][RK0][main]: Calling build_v2
[HCTR][00:35:57.306][ERROR][RK0][main]: Calling build_v2
[HCTR][00:35:57.306][ERROR][RK0][tid #139763168294656]: Calling build_v2
[HCTR][00:35:57.306][ERROR][RK0][tid #139763763881728]: Calling build_v2
[HCTR][00:35:57.306][ERROR][RK0][tid #139763034076928]: Calling build_v2
[HCTR][00:35:57.306][ERROR][RK0][main]: Calling build_v2
[HCTR][00:35:57.306][ERROR][RK0][main]: Calling build_v2
[HCTR][00:35:57.306][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][00:35:57.306][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][00:35:57.306][ERROR][RK0][tid #139763168294656]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][00:35:57.306][ERROR][RK0][main]: Calling build_v2
[HCTR][00:35:57.306][ERROR][RK0][tid #139763763881728]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][00:35:57.306][ERROR][RK0][tid #139763034076928]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][00:35:57.306][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][00:35:57.306][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][00:35:57.306][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[2022-12-12 00:35:57.310819: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:[178] v100x8, slow pcie
2022-12-12 00:35:57.310859[: 2022-12-12 00:35:57E. 310898/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: [:E178 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 00:35:57v100x8, slow pcie:.
196310902] : assigning 0 to cpuE[
 [2022-12-12 00:35:57/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.:310953178: ] E2022-12-12 00:35:57v100x8, slow pcie .
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc310945:[[: 196[2022-12-12 00:35:57E] 2022-12-12 00:35:57. assigning 0 to cpu.311000/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
311002: :: 2022-12-12 00:35:57E178E. ] [ 310987/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccv100x8, slow pcie/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: :
:[E2022-12-12 00:35:57[2121962022-12-12 00:35:57 .2022-12-12 00:35:57] ] .[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc311035.build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8assigning 0 to cpu3110742022-12-12 00:35:57:: 311086

: .178E: [E311086]  v100x8, slow pcieE[ [: 2022-12-12 00:35:57/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
 2022-12-12 00:35:57/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 00:35:57E.:/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.:[. 311126178:3111692122022-12-12 00:35:57311180/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: ] 196: ] .: :Ev100x8, slow pcie] Ebuild asymm link desc with 8X Tesla V100-SXM2-32GB out of 8311218E178 
assigning 0 to cpu 
:  ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccv100x8, slow pcie:[: 2022-12-12 00:35:57:
1782022-12-12 00:35:57213/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.212] [.] :[311335] v100x8, slow pcie2022-12-12 00:35:57311347remote time is 8.684211962022-12-12 00:35:57: build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8
.: 
] .E
311381E[assigning 0 to cpu311386[ :  2022-12-12 00:35:57[
: 2022-12-12 00:35:57/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.2022-12-12 00:35:57E.: :311456.[ 311464196/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc213: 3114802022-12-12 00:35:57/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: ] :] E: .:Eassigning 0 to cpu212remote time is 8.68421 E311537196 
] 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc : ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccbuild asymm link desc with 8X Tesla V100-SXM2-32GB out of 8:/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[Eassigning 0 to cpu:[
196:2022-12-12 00:35:57 
2142022-12-12 00:35:57] 213.[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] .assigning 0 to cpu] 3116592022-12-12 00:35:57:cpu time is 97.0588311672
[remote time is 8.68421: .212
: 
2022-12-12 00:35:57E311712] E. [: build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8 311765[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 00:35:57E
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: 2022-12-12 00:35:57:[. :E.2142022-12-12 00:35:57311794/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc212 311805] .: :] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: cpu time is 97.0588311843E213build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8:E
:  ] 
212 E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccremote time is 8.68421] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc :[
build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8:/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2142022-12-12 00:35:57
[212:] .2022-12-12 00:35:57] 213cpu time is 97.0588311941[.build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8] 
: 2022-12-12 00:35:57311969
remote time is 8.68421E.: 
 311999E[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: [ 2022-12-12 00:35:57:E2022-12-12 00:35:57/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.213 .:312063] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc312072214: remote time is 8.68421:: ] E
213E[cpu time is 97.0588 ]  2022-12-12 00:35:57
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccremote time is 8.68421/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.:
:312192213214: [] ] E2022-12-12 00:35:57remote time is 8.68421cpu time is 97.0588 .

/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc312241:[: 2142022-12-12 00:35:57E] . cpu time is 97.0588312290/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
: :E214 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cccpu time is 97.0588:
214] cpu time is 97.0588
[2022-12-12 00:37:15.635706: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:118] solver created. now build & solve
[2022-12-12 00:37:15.675978: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:123] solver built. now solve
[2022-12-12 00:37:15.676045: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:919] num_cached_nodes = 3999999
[2022-12-12 00:37:15.792442: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:127] solver solved
[2022-12-12 00:37:15.792529: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:195] 0 solved master
[2022-12-12 00:37:15.861946: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 0 solved
[2022-12-12 00:37:15.861983: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 0 initing device 0
[2022-12-12 00:37:15.862556: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 00:37:15.863519: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 00:37:15.864373: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 00:37:15.877238: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 3 solved
[2022-12-12 00:37:15.877311: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 3 initing device 3
[2022-12-12 00:37:15.877702: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[[[2022-12-12 00:37:152022-12-12 00:37:15[2022-12-12 00:37:15..2022-12-12 00:37:15.877854877856.877863: : 877883: EE: E  E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc::/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:[202202:2022022-12-12 00:37:15] ] 202] .5 solved4 solved] 2 solved877953

1 solved
: 
[[E[2022-12-12 00:37:152022-12-12 00:37:15[ 2022-12-12 00:37:15..2022-12-12 00:37:15/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc.878050878052.:878058: : 878066202: EE: ] E  E6 solved /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc::/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205205[:205] ] 2022-12-12 00:37:15205] worker 0 thread 5 initing device 5worker 0 thread 4 initing device 4.] worker 0 thread 2 initing device 2

878192worker 0 thread 1 initing device 1
: 
E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 6 initing device 6
[[2022-12-12 00:37:152022-12-12 00:37:15..878594878593: : [EE[2022-12-12 00:37:15  2022-12-12 00:37:15./hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu.878614::878622: 19801980: E] ] E eager alloc mem 381.47 MB[eager alloc mem 381.47 MB /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu
2022-12-12 00:37:15
/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:.:19808786711980] : ] eager alloc mem 381.47 MBEeager alloc mem 381.47 MB
 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 00:37:15.881519: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 7 solved
[2022-12-12 00:37:15.881576: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 7 initing device 7
[2022-12-12 00:37:15.881728: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 00:37:15.881981: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 00:37:15.882237: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 00:37:15.882789: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 00:37:15.882850: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 00:37:15.883391: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 00:37:15.883450: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 00:37:15.886334: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 00:37:15.886593: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 00:37:15.886752: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 00:37:15.887306: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 00:37:15.887356: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 00:37:15.887405: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 00:37:15.887912: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 00:37:15.891047: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 00:37:15.945560: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 2.00 Bytes
[2022-12-12 00:37:15.945923: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1024.00 Bytes
[2022-12-12 00:37:15.958718: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-12 00:37:15.958813: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 2
[2022-12-12 00:37:15.958863: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 00:37:15.959752: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 15.64 MB
[2022-12-12 00:37:15.960523: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:37:15.961494: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:37:15.961583: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 00:37:15.962260: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 00:37:15.962301: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1.91 GB
[[2022-12-12 00:37:152022-12-12 00:37:15..971314971314: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::19801980] ] eager alloc mem 2.00 Byteseager alloc mem 2.00 Bytes

[2022-12-12 00:37:15.971656: [E2022-12-12 00:37:15 ./hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu971664:: 1980E]  eager alloc mem 1024.00 Bytes/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu
:1980] eager alloc mem 1024.00 Bytes
[2022-12-12 00:37:15.976712: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-12 00:37:15.[9767782022-12-12 00:37:15: .E976771 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccE: 638/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc] :eager release cuda mem 2638
] eager release cuda mem 1024
[2022-12-12 00:37:15.976848: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000[
2022-12-12 00:37:15.976868: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 2
[2022-12-12 00:37:15.976911: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[[[[[2022-12-12 00:37:152022-12-12 00:37:152022-12-12 00:37:152022-12-12 00:37:152022-12-12 00:37:15....976918.976925976919976919: 976918: : : E: EEE E   /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:::1980:198019801980] 1980] ] ] eager alloc mem 2.00 Bytes] eager alloc mem 2.00 Byteseager alloc mem 2.00 Byteseager alloc mem 2.00 Bytes
eager alloc mem 2.00 Bytes



[2022-12-12 00:37:15[.[[[2022-12-12 00:37:159773482022-12-12 00:37:152022-12-12 00:37:152022-12-12 00:37:15.: ...977350E977352977352977353:  : : : E/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuEEE :   /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu1980/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:] :::1980eager alloc mem 1024.00 Bytes198019801980] 
] ] ] eager alloc mem 1024.00 Byteseager alloc mem 1024.00 Byteseager alloc mem 1024.00 Byteseager alloc mem 1024.00 Bytes



[2022-12-12 00:37:15.977703: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 15.64 MB
[2022-12-12 00:37:15.978209: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 15.64 MB
[2022-12-12 00:37:15.978634: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:37:15.978712: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:37:15.979578: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[[2022-12-12 00:37:152022-12-12 00:37:15..979660979664: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::6381980] ] eager release cuda mem 625663eager alloc mem 25.25 KB

[2022-12-12 00:37:15.979772: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 00:37:15.980366: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 00:37:15.980405: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1.91 GB
[2022-12-12 00:37:15.980443: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 00:37:15.980482: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1.91 GB
[2022-12-12 00:37:15.987879: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-12 00:37:15.987958: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 2
[2022-12-12 00:37:15.988003: E[ 2022-12-12 00:37:15/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc.:987997638: ] Eeager release cuda mem 400000000 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-12 00:37:15.988082[: 2022-12-12 00:37:15E. 988076/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc: :E638 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cceager release cuda mem 2:
638] eager release cuda mem 1024
[2022-12-12 00:37:15.988152: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000[[
2022-12-12 00:37:152022-12-12 00:37:15..988172988161: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::638638] ] eager release cuda mem 2eager release cuda mem 1024

[2022-12-12 00:37:15[.2022-12-12 00:37:15988249.: 988252E:  [E/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc2022-12-12 00:37:15 :./hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc638988256:] : 638eager release cuda mem 400000000E] 
 eager release cuda mem 2/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc
:638] eager release cuda mem 1024
[2022-12-12 00:37:15.988343: E[ 2022-12-12 00:37:15/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc.:988353638: ] Eeager release cuda mem 400000000 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 2
[2022-12-12 00:37:15.988412: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 00:37:15.988849: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 15.64 MB
[2022-12-12 00:37:15.989757: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 15.64 MB
[2022-12-12 00:37:15.990518: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 15.64 MB
[2022-12-12 00:37:15.991092: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 15.64 MB
[2022-12-12 00:37:15.991631: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 15.64 MB
[2022-12-12 00:37:15.992051: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:37:15.992361: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:37:15.992587: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:37:15.992622: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:37:15.992685: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:37:15.993004: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:37:15.993086: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 00:37:15.993311: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:37:15.993393: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 00:37:15.993532: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:37:15.993575: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:37:15.993611: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 00:37:15.993644: E [/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc2022-12-12 00:37:15:.638993658] : eager release cuda mem 625663E
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 00:37:15.993745: E [/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu2022-12-12 00:37:15:.1980993757] : eager alloc mem 25.25 KBE
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 00:37:15.993810: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1.91 GB
[2022-12-12 00:37:15.994060: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 00:37:15.994108: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1.91 GB
[2022-12-12 00:37:15.994280: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 00:37:15.994320: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1.91 GB
[2022-12-12 00:37:15.994343: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 00:37:15.994383: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1.91 GB
[2022-12-12 00:37:15.994433: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 00:37:15.994473: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1.91 GB
[[[[[[2022-12-12 00:37:16[2022-12-12 00:37:16[2022-12-12 00:37:162022-12-12 00:37:162022-12-12 00:37:162022-12-12 00:37:16.2022-12-12 00:37:16.2022-12-12 00:37:16....375222.375221.375222375221375232375229: 375245: 375256: : : : E: E: EEEE E E    /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::::1980:1980:1980198019801980] 1980] 1980] ] ] ] eager alloc mem 611.00 KB] eager alloc mem 611.00 KB] eager alloc mem 611.00 KBeager alloc mem 611.00 KBeager alloc mem 611.00 KBeager alloc mem 611.00 KB
eager alloc mem 611.00 KB
eager alloc mem 611.00 KB





[[[2022-12-12 00:37:16[2022-12-12 00:37:162022-12-12 00:37:16.[2022-12-12 00:37:16..3764442022-12-12 00:37:16.[376445376449: .3764502022-12-12 00:37:16[: : E376456: .2022-12-12 00:37:16EE [: E376465.  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc2022-12-12 00:37:16E : 376485/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638. /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccE: ::] 376510/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc: E638638eager release cuda mem 625663: :638/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc ] ] 
E638] eager release cuda mem 625663:/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cceager release cuda mem 625663eager release cuda mem 625663 ] 
638:

/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc[eager release cuda mem 625663] 638:2022-12-12 00:37:16
eager release cuda mem 625663] 638.
eager release cuda mem 625663] [376735
[eager release cuda mem 625663[2022-12-12 00:37:16: 2022-12-12 00:37:16
2022-12-12 00:37:16.E[..376771 2022-12-12 00:37:16376782376785: [/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu.: : E[2022-12-12 00:37:16:376813EE 2022-12-12 00:37:16.1980:   /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu.376830] [E/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:376847: eager alloc mem 611.00 KB2022-12-12 00:37:16 ::1980: E
./hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu19801980] E 376889:] eager alloc mem 611.00 KB]  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu: 1980eager alloc mem 611.00 KB
/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cueager alloc mem 611.00 KB:E] 
:
1980 eager alloc mem 611.00 KB1980] /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu
] eager alloc mem 611.00 KB:eager alloc mem 611.00 KB
1980
] eager alloc mem 611.00 KB
[2022-12-12 00:37:16.377706: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:37:16.377753: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:37:16.377778[: 2022-12-12 00:37:16E. 377784[/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu: 2022-12-12 00:37:16:E.1980 377796] /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc: eager alloc mem 611.00 KB[:[E
2022-12-12 00:37:166382022-12-12 00:37:16 [.] ./hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc[2022-12-12 00:37:16377822eager release cuda mem 625663377825:[2022-12-12 00:37:16.: 
: 6382022-12-12 00:37:16..377836EE] 377851377876:   eager release cuda mem 625663: : E/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu[
EE ::2022-12-12 00:37:16  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc6381980./hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:] ] 377943:[:638eager release cuda mem 625663eager alloc mem 611.00 KB: 6382022-12-12 00:37:16638] 

E] .] eager release cuda mem 625663 eager release cuda mem 625663378010eager release cuda mem 625663
/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu[
: 
:2022-12-12 00:37:16E1980. ] 378114/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cueager alloc mem 611.00 KB: :
E1980[ ] 2022-12-12 00:37:16[/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cueager alloc mem 611.00 KB.2022-12-12 00:37:16:[
378157.19802022-12-12 00:37:16: 378169] .E: eager alloc mem 611.00 KB 378186E
/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:  :E/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu1980 :] /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu1980eager alloc mem 611.00 KB:] 
1980eager alloc mem 611.00 KB] 
eager alloc mem 611.00 KB
[2022-12-12 00:37:16.378563: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:37:16.378629: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:37:16.378800: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:37:16.378866: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] [eager alloc mem 611.00 KB2022-12-12 00:37:16
.378884: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:37:16.378941: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc[:2022-12-12 00:37:16638.] 378956eager release cuda mem 625663: 
E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:37:16.379001: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638[] 2022-12-12 00:37:16eager release cuda mem 625663.
379019: E[ 2022-12-12 00:37:16/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu.:[37903519802022-12-12 00:37:16: ] .[Eeager alloc mem 611.00 KB[3790512022-12-12 00:37:16 
2022-12-12 00:37:16: ./hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc.E379075:379080 : 638: /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccE] E: eager release cuda mem 625663 638/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu
/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc] ::eager release cuda mem 6256631980638
] ] eager alloc mem 611.00 KBeager release cuda mem 625663
[
2022-12-12 00:37:16.379263: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] [eager alloc mem 611.00 KB2022-12-12 00:37:16
.379292: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:37:16.379332: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:37:16.379379: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:37:16.379449: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:37:16.379616: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:37:16.379683: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:37:16.379724: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:37:16.379795: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:37:16.379853: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:37:16.379921: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:37:16.380000: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:37:16.380027: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663[
2022-12-12 00:37:16.380053: E[ 2022-12-12 00:37:16/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc.:380068638: ] E[eager release cuda mem 625663 2022-12-12 00:37:16[
/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu.2022-12-12 00:37:16:380102.1980: 380110] E: eager alloc mem 611.00 KB[ E
2022-12-12 00:37:16/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu [.:/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc2022-12-12 00:37:163801761980:.: ] 638380197Eeager alloc mem 611.00 KB] :  
eager release cuda mem 625663E/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu
 :/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc1980:] 638eager alloc mem 611.00 KB] 
eager release cuda mem 625663
[2022-12-12 00:37:16.380350: E [/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu2022-12-12 00:37:16:.1980380378] : eager alloc mem 611.00 KBE
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu[:2022-12-12 00:37:161980.] 380436eager alloc mem 611.00 KB: 
E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:37:16.380511: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:37:16.380542: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:37:16.380607: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:37:16.380663: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:37:16.380733: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:37:16.380926: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:37:16.380992: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980[] 2022-12-12 00:37:16eager alloc mem 611.00 KB.
381005: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:37:16.381057: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:37:16.381092: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:37:16.381123: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[[2022-12-12 00:37:162022-12-12 00:37:16..381175381180: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:[:6382022-12-12 00:37:16638] .] eager release cuda mem 625663381256eager release cuda mem 625663
: 
E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:37:16.381353: [E2022-12-12 00:37:16 [./hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc[2022-12-12 00:37:16381362:2022-12-12 00:37:16.: 638.381365E] 381373:  eager release cuda mem 625663: E/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu
E : /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu1980/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu[:] :2022-12-12 00:37:161980eager alloc mem 611.00 KB[1980.] 
2022-12-12 00:37:16] 381474eager alloc mem 611.00 KB.eager alloc mem 611.00 KB: 
381494
E:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc :/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu638:] 1980eager release cuda mem 625663] 
eager alloc mem 611.00 KB
[2022-12-12 00:37:16.381621: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:37:16.381754: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:37:16.381821: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:37:16.381841: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:37:16.381874: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:37:16.381911: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:37:16.381940: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:37:16.382247: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:37:16.382275: [E2022-12-12 00:37:16 .[/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc3822842022-12-12 00:37:16:: [.638E2022-12-12 00:37:16382290]  .: eager release cuda mem 625663/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc382310E
:[:  6382022-12-12 00:37:16E[/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc] . 2022-12-12 00:37:16:eager release cuda mem 16399996382366/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc.638
: :382388] E638: eager release cuda mem 625663 ] E
/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cceager release cuda mem 625663 :
/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc638:] 638eager release cuda mem 625663] [
eager release cuda mem 163999962022-12-12 00:37:16
.382508: [E2022-12-12 00:37:16 ./hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc382525:: 638E] [ eager release cuda mem 16399996[2022-12-12 00:37:16/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc
2022-12-12 00:37:16.:.382541638382560: ] : Eeager release cuda mem 16399996E 
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::1980[638] 2022-12-12 00:37:16] eager alloc mem 611.00 KB.[eager release cuda mem 625663
3826572022-12-12 00:37:16
: .E382687 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc[E:2022-12-12 00:37:16 638./hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc] 382731:eager release cuda mem 625663: 638
E]  eager release cuda mem 625663/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc[
:2022-12-12 00:37:16638.] [382801eager release cuda mem 163999962022-12-12 00:37:16: 
.E382827 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccE: 638/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc] :eager release cuda mem 16399996638
] eager release cuda mem 16399996
[2022-12-12 00:37:16.383050: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 3999999 / 100000000 nodes ( 4.00 %~4.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 96000001 / 100000000 nodes ( 96.00 %) | 1.91 GB | 0.504431 secs 
[2022-12-12 00:37:16.383507: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:37:16.383561: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 16399996
[2022-12-12 00:37:16.383625: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 3999999 / 100000000 nodes ( 4.00 %~4.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 96000001 / 100000000 nodes ( 96.00 %) | 1.91 GB | 0.505037 secs 
[2022-12-12 00:37:16.384269: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 3999999 / 100000000 nodes ( 4.00 %~4.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 96000001 / 100000000 nodes ( 96.00 %) | 1.91 GB | 0.506579 secs 
[2022-12-12 00:37:16.385042: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 3999999 / 100000000 nodes ( 4.00 %~4.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 96000001 / 100000000 nodes ( 96.00 %) | 1.91 GB | 0.506456 secs 
[2022-12-12 00:37:16.385133: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 3999999 / 100000000 nodes ( 4.00 %~4.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 96000001 / 100000000 nodes ( 96.00 %) | 1.91 GB | 0.506525 secs 
[2022-12-12 00:37:16.385654: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 3999999 / 100000000 nodes ( 4.00 %~4.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 96000001 / 100000000 nodes ( 96.00 %) | 1.91 GB | 0.523103 secs 
[2022-12-12 00:37:16.386062: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 3999999 / 100000000 nodes ( 4.00 %~4.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 96000001 / 100000000 nodes ( 96.00 %) | 1.91 GB | 0.504087 secs 
[2022-12-12 00:37:16.386468: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 3999999 / 100000000 nodes ( 4.00 %~4.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 96000001 / 100000000 nodes ( 96.00 %) | 1.91 GB | 0.507803 secs 
[HCTR][00:37:16.386][ERROR][RK0][main]: replica 6 calling init per replica done, doing barrier
[HCTR][00:37:16.386][ERROR][RK0][tid #139763034076928]: replica 7 calling init per replica done, doing barrier
[HCTR][00:37:16.386][ERROR][RK0][main]: replica 5 calling init per replica done, doing barrier
[HCTR][00:37:16.386][ERROR][RK0][main]: replica 4 calling init per replica done, doing barrier
[HCTR][00:37:16.386][ERROR][RK0][tid #139763168294656]: replica 0 calling init per replica done, doing barrier
[HCTR][00:37:16.386][ERROR][RK0][tid #139763763881728]: replica 2 calling init per replica done, doing barrier
[HCTR][00:37:16.386][ERROR][RK0][main]: replica 1 calling init per replica done, doing barrier
[HCTR][00:37:16.386][ERROR][RK0][main]: replica 3 calling init per replica done, doing barrier
[HCTR][00:37:16.386][ERROR][RK0][main]: replica 3 calling init per replica done, doing barrier done
[HCTR][00:37:16.386][ERROR][RK0][tid #139763168294656]: replica 0 calling init per replica done, doing barrier done
[HCTR][00:37:16.386][ERROR][RK0][main]: replica 1 calling init per replica done, doing barrier done
[HCTR][00:37:16.386][ERROR][RK0][main]: replica 6 calling init per replica done, doing barrier done
[HCTR][00:37:16.386][ERROR][RK0][main]: replica 4 calling init per replica done, doing barrier done
[HCTR][00:37:16.386][ERROR][RK0][tid #139763763881728]: replica 2 calling init per replica done, doing barrier done
[HCTR][00:37:16.386][ERROR][RK0][tid #139763034076928]: replica 7 calling init per replica done, doing barrier done
[HCTR][00:37:16.386][ERROR][RK0][main]: replica 5 calling init per replica done, doing barrier done
[HCTR][00:37:16.386][ERROR][RK0][main]: init per replica done
[HCTR][00:37:16.386][ERROR][RK0][main]: init per replica done
[HCTR][00:37:16.386][ERROR][RK0][main]: init per replica done
[HCTR][00:37:16.386][ERROR][RK0][main]: init per replica done
[HCTR][00:37:16.386][ERROR][RK0][tid #139763763881728]: init per replica done
[HCTR][00:37:16.386][ERROR][RK0][tid #139763034076928]: init per replica done
[HCTR][00:37:16.386][ERROR][RK0][main]: init per replica done
[HCTR][00:37:16.389][ERROR][RK0][tid #139763168294656]: init per replica done
