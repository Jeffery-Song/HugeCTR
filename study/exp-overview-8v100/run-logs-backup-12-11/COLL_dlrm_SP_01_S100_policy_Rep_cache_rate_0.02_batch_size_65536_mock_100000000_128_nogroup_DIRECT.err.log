2022-12-11 21:36:05.111711: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.118063: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.125562: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.129227: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.134172: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.146064: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.155484: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.165766: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.217918: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.223550: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.226852: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.228133: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.228543: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.229320: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.230207: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.230848: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.231905: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.232326: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.233613: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.233749: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.235362: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.235464: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.236787: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.236954: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.238287: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.238510: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.239708: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.240123: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.241076: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.242349: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.243258: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.244598: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.246392: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.247427: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.248410: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.249498: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.250454: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.251472: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.252482: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.253424: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.256323: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.257452: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.258525: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.258921: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 21:36:05.259515: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.260603: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.261703: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.262793: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.264432: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.266759: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.267907: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.269077: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.269589: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.270456: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.271179: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.272094: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.272637: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.273992: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.276830: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.277801: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.279427: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.280677: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.280978: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.283170: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.283451: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.285318: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.285649: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.286393: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.287711: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.288176: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.289025: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.290401: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.290916: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.291780: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.293507: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.294334: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.295064: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.296756: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.296846: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.297437: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.297685: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.305747: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.306144: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.306174: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.306465: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.308397: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.308501: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.309130: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.310111: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.310973: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.311029: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.325483: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.346005: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.346949: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.347857: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.347934: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.348031: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.349207: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.349396: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.350356: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.352020: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.352156: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.352274: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.352645: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.353319: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.353719: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.355477: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.357510: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.357561: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.358650: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.359288: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.359400: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.360329: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.360744: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.362454: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.362882: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.363053: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.364049: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.364131: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.365170: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.365292: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.367652: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.367803: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.368013: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.368815: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.370213: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.370727: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.372516: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.372608: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.372752: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.373374: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.374812: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.376457: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.376645: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.376897: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.377276: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.378326: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.379869: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.380113: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.380297: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.380742: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.382001: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.383498: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.383620: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.383971: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.384120: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.385164: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.387646: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.387880: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.388718: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.388870: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.390235: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.391662: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.391972: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.392459: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.392542: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.393710: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.395546: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.396038: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.396415: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.396537: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.397454: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.398970: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.399355: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.399841: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.400056: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.400754: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.402175: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.402663: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.403222: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.403492: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.403984: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.405480: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.405886: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.406548: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.406815: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.408313: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.409728: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.409816: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.410809: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.410946: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.412892: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.412932: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.414073: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.414337: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.416478: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.416654: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.416729: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.417631: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.418230: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.418897: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.419477: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 21:36:05.419904: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.420018: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.420075: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.421393: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.421987: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.422691: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.423639: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.423723: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.423818: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.425555: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.425903: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.427084: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.429127: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.429268: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.429305: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.430129: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.431217: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.431366: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.432333: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.433936: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.434079: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.434081: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.434930: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.436155: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.436673: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.437025: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.438868: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.438955: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.438993: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.439495: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.441011: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.441597: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.441832: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.443632: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.443857: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.445308: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.446325: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.447398: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.447564: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.447813: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 21:36:05.449148: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 21:36:05.450225: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.451421: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.452383: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.454743: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.455266: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.455895: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 21:36:05.457039: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 21:36:05.457078: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.457577: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.458552: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.459530: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.459557: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.460339: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.461649: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.463659: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.463673: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.464865: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.466248: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.466691: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.467465: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.467500: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.467577: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.468636: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.470866: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.471576: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.471703: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.472925: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.474955: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.475536: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.475721: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.478376: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.508042: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.513103: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.515875: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.518019: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.521598: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.522894: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.555370: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.557803: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.561783: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.563239: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.566446: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.569406: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.571752: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.573327: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.576122: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.582832: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 21:36:05.585811: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.593336: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.593746: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 21:36:05.604072: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.655234: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.657442: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.661255: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:05.690563: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:06.582519: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:06.583227: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:06.583978: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:06.584439: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 21:36:06.584498: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-11 21:36:06.602191: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:06.602823: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:06.603573: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:06.604165: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:06.604676: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:06.605154: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-11 21:36:06.651980: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 21:36:06.652177: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 21:36:06.719641: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12340
2022-12-11 21:36:06.827196: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:06.827820: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:06.828449: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:06.829021: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 21:36:06.829082: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-11 21:36:06.846905: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:06.847556: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:06.848070: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:06.848827: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:06.849622: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:06.850098: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:1/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-11 21:36:06.869743: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:06.870372: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:06.870896: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:06.871945: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 21:36:06.872008: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-11 21:36:06.879882: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:06.880577: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:06.881116: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:06.881581: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 21:36:06.881637: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-11 21:36:06.888892: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:06.889522: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:06.890043: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:06.890610: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:06.891144: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:06.892292: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:7/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-11 21:36:06.899717: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:06.900348: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:06.901088: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:06.901669: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:06.902193: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:06.902665: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:2/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-11 21:36:06.907444: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:06.908209: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:06.908728: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:06.909193: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 21:36:06.909245: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-11 21:36:06.912886: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:06.912956: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:06.914183: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:06.914257: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:06.915186: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:06.915265: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:06.916077: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 21:36:06.916144: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-11 21:36:06.916154: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 21:36:06.916203: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-11 21:36:06.926716: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:06.927408: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:06.927930: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:06.929043: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:06.929615: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:06.930102: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:6/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-11 21:36:06.933634: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:06.933934: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:06.934483: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:06.934940: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:06.935578: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:06.936139: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:06.936580: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:06.937021: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 21:36:06.937045: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:06.937157: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 21:36:06.937446: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:06.937733: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 21:36:06.937902: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 21:36:06.937996: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:06.938315: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:3/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-11 21:36:06.938625: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:4/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-11 21:36:06.938787: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12347
2022-12-11 21:36:06.939669: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12341
2022-12-11 21:36:06.957319: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:06.957959: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:06.958481: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:06.958954: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 21:36:06.959012: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-11 21:36:06.975207: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 21:36:06.975405: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 21:36:06.977042: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:06.977154: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12346
2022-12-11 21:36:06.977722: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:06.978240: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:06.978819: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:06.979368: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:36:06.979847: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:5/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-11 21:36:06.984053: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 21:36:06.984202: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 21:36:06.984569: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 21:36:06.984701: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 21:36:06.985946: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12344
2022-12-11 21:36:06.986503: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12343
2022-12-11 21:36:06.988462: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 21:36:06.988620: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 21:36:06.990475: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12342
2022-12-11 21:36:07.025032: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 21:36:07.025235: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 21:36:07.026797: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12345
[HCTR][21:36:08.284][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][21:36:08.284][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][21:36:08.286][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][21:36:08.287][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][21:36:08.287][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][21:36:08.287][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][21:36:08.312][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][21:36:08.313][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 1it [00:01,  1.56s/it]warmup run: 94it [00:01, 78.42it/s]warmup run: 1it [00:01,  1.53s/it]warmup run: 189it [00:01, 171.47it/s]warmup run: 100it [00:01, 85.15it/s]warmup run: 1it [00:01,  1.52s/it]warmup run: 1it [00:01,  1.51s/it]warmup run: 284it [00:01, 274.31it/s]warmup run: 1it [00:01,  1.49s/it]warmup run: 1it [00:01,  1.49s/it]warmup run: 198it [00:01, 182.44it/s]warmup run: 1it [00:01,  1.49s/it]warmup run: 98it [00:01, 83.82it/s]warmup run: 98it [00:01, 84.45it/s]warmup run: 379it [00:01, 381.27it/s]warmup run: 97it [00:01, 84.64it/s]warmup run: 99it [00:01, 86.55it/s]warmup run: 296it [00:01, 289.35it/s]warmup run: 99it [00:01, 86.04it/s]warmup run: 195it [00:01, 180.57it/s]warmup run: 196it [00:01, 182.79it/s]warmup run: 475it [00:02, 487.47it/s]warmup run: 194it [00:01, 183.02it/s]warmup run: 200it [00:01, 189.21it/s]warmup run: 1it [00:01,  1.48s/it]warmup run: 394it [00:01, 400.48it/s]warmup run: 198it [00:01, 186.02it/s]warmup run: 293it [00:01, 288.13it/s]warmup run: 295it [00:01, 292.14it/s]warmup run: 291it [00:01, 290.75it/s]warmup run: 565it [00:02, 552.27it/s]warmup run: 300it [00:01, 300.26it/s]warmup run: 78it [00:01, 68.16it/s]warmup run: 491it [00:02, 506.27it/s]warmup run: 297it [00:01, 295.51it/s]warmup run: 391it [00:01, 399.19it/s]warmup run: 394it [00:01, 404.98it/s]warmup run: 389it [00:01, 403.38it/s]warmup run: 662it [00:02, 644.88it/s]warmup run: 401it [00:01, 415.82it/s]warmup run: 175it [00:01, 168.46it/s]warmup run: 589it [00:02, 605.87it/s]warmup run: 397it [00:01, 409.76it/s]warmup run: 489it [00:02, 507.85it/s]warmup run: 492it [00:02, 512.36it/s]warmup run: 485it [00:01, 508.72it/s]warmup run: 757it [00:02, 717.26it/s]warmup run: 501it [00:01, 526.04it/s]warmup run: 275it [00:01, 282.25it/s]warmup run: 688it [00:02, 693.45it/s]warmup run: 495it [00:01, 516.65it/s]warmup run: 589it [00:02, 610.76it/s]warmup run: 592it [00:02, 615.26it/s]warmup run: 586it [00:02, 614.72it/s]warmup run: 851it [00:02, 773.77it/s]warmup run: 603it [00:02, 630.78it/s]warmup run: 373it [00:01, 396.42it/s]warmup run: 787it [00:02, 765.41it/s]warmup run: 594it [00:02, 616.06it/s]warmup run: 689it [00:02, 700.75it/s]warmup run: 692it [00:02, 703.33it/s]warmup run: 684it [00:02, 699.81it/s]warmup run: 945it [00:02, 817.49it/s]warmup run: 705it [00:02, 721.40it/s]warmup run: 470it [00:01, 504.60it/s]warmup run: 886it [00:02, 822.62it/s]warmup run: 692it [00:02, 700.40it/s]warmup run: 788it [00:02, 770.82it/s]warmup run: 791it [00:02, 774.40it/s]warmup run: 781it [00:02, 766.27it/s]warmup run: 1038it [00:02, 848.42it/s]warmup run: 805it [00:02, 789.80it/s]warmup run: 567it [00:02, 603.15it/s]warmup run: 985it [00:02, 866.39it/s]warmup run: 792it [00:02, 773.98it/s]warmup run: 886it [00:02, 825.43it/s]warmup run: 890it [00:02, 830.32it/s]warmup run: 878it [00:02, 818.99it/s]warmup run: 1131it [00:02, 871.18it/s]warmup run: 906it [00:02, 847.44it/s]warmup run: 667it [00:02, 694.07it/s]warmup run: 1084it [00:02, 899.43it/s]warmup run: 889it [00:02, 818.19it/s]warmup run: 985it [00:02, 869.23it/s]warmup run: 989it [00:02, 871.70it/s]warmup run: 975it [00:02, 858.48it/s]warmup run: 1226it [00:02, 891.79it/s]warmup run: 1008it [00:02, 892.69it/s]warmup run: 768it [00:02, 771.36it/s]warmup run: 1183it [00:02, 924.45it/s]warmup run: 985it [00:02, 845.61it/s]warmup run: 1083it [00:02, 898.28it/s]warmup run: 1090it [00:02, 908.76it/s]warmup run: 1072it [00:02, 889.08it/s]warmup run: 1321it [00:02, 906.92it/s]warmup run: 1111it [00:02, 929.52it/s]warmup run: 867it [00:02, 826.73it/s]warmup run: 1282it [00:02, 943.15it/s]warmup run: 1082it [00:02, 878.63it/s]warmup run: 1181it [00:02, 917.40it/s]warmup run: 1190it [00:02, 932.06it/s]warmup run: 1169it [00:02, 906.27it/s]warmup run: 1417it [00:03, 922.06it/s]warmup run: 1214it [00:02, 955.72it/s]warmup run: 965it [00:02, 866.35it/s]warmup run: 1381it [00:02, 953.77it/s]warmup run: 1183it [00:02, 914.33it/s]warmup run: 1279it [00:02, 929.54it/s]warmup run: 1289it [00:02, 942.17it/s]warmup run: 1265it [00:02, 921.62it/s]warmup run: 1513it [00:03, 932.19it/s]warmup run: 1318it [00:02, 978.33it/s]warmup run: 1062it [00:02, 887.99it/s]warmup run: 1480it [00:03, 961.37it/s]warmup run: 1284it [00:02, 939.04it/s]warmup run: 1376it [00:02, 939.61it/s]warmup run: 1388it [00:02, 947.61it/s]warmup run: 1361it [00:02, 931.98it/s]warmup run: 1610it [00:03, 940.70it/s]warmup run: 1422it [00:02, 993.74it/s]warmup run: 1579it [00:03, 965.00it/s]warmup run: 1159it [00:02, 903.78it/s]warmup run: 1385it [00:02, 957.31it/s]warmup run: 1476it [00:03, 956.42it/s]warmup run: 1486it [00:03, 955.27it/s]warmup run: 1459it [00:02, 945.38it/s]warmup run: 1706it [00:03, 946.22it/s]warmup run: 1525it [00:02, 1003.02it/s]warmup run: 1678it [00:03, 970.28it/s]warmup run: 1258it [00:02, 928.03it/s]warmup run: 1486it [00:03, 972.29it/s]warmup run: 1579it [00:03, 976.08it/s]warmup run: 1586it [00:03, 968.15it/s]warmup run: 1558it [00:03, 956.82it/s]warmup run: 1803it [00:03, 952.25it/s]warmup run: 1628it [00:03, 1004.07it/s]warmup run: 1360it [00:02, 953.71it/s]warmup run: 1778it [00:03, 975.97it/s]warmup run: 1588it [00:03, 984.34it/s]warmup run: 1682it [00:03, 989.54it/s]warmup run: 1686it [00:03, 977.34it/s]warmup run: 1657it [00:03, 964.40it/s]warmup run: 1900it [00:03, 954.45it/s]warmup run: 1730it [00:03, 999.82it/s] warmup run: 1463it [00:02, 975.68it/s]warmup run: 1877it [00:03, 978.99it/s]warmup run: 1689it [00:03, 990.12it/s]warmup run: 1785it [00:03, 1001.42it/s]warmup run: 1787it [00:03, 984.56it/s]warmup run: 1756it [00:03, 971.88it/s]warmup run: 1996it [00:03, 952.27it/s]warmup run: 1831it [00:03, 1000.60it/s]warmup run: 1567it [00:03, 992.51it/s]warmup run: 1976it [00:03, 980.53it/s]warmup run: 1790it [00:03, 993.89it/s]warmup run: 1887it [00:03, 1006.64it/s]warmup run: 1887it [00:03, 987.48it/s]warmup run: 1854it [00:03, 973.67it/s]warmup run: 2112it [00:03, 1012.61it/s]warmup run: 1932it [00:03, 1002.38it/s]warmup run: 1668it [00:03, 997.25it/s]warmup run: 2089it [00:03, 1024.03it/s]warmup run: 1891it [00:03, 995.40it/s]warmup run: 1989it [00:03, 1008.03it/s]warmup run: 1987it [00:03, 981.37it/s]warmup run: 1952it [00:03, 973.46it/s]warmup run: 2230it [00:03, 1060.14it/s]warmup run: 2039it [00:03, 1020.71it/s]warmup run: 1770it [00:03, 1001.75it/s]warmup run: 2207it [00:03, 1068.99it/s]warmup run: 1992it [00:03, 997.42it/s]warmup run: 2109it [00:03, 1063.80it/s]warmup run: 2102it [00:03, 1030.49it/s]warmup run: 2061it [00:03, 1006.44it/s]warmup run: 2348it [00:03, 1095.11it/s]warmup run: 2159it [00:03, 1073.51it/s]warmup run: 1872it [00:03, 1005.81it/s]warmup run: 2325it [00:03, 1099.46it/s]warmup run: 2111it [00:03, 1052.30it/s]warmup run: 2232it [00:03, 1110.91it/s]warmup run: 2220it [00:03, 1072.99it/s]warmup run: 2182it [00:03, 1066.82it/s]warmup run: 2465it [00:04, 1115.45it/s]warmup run: 2279it [00:03, 1110.98it/s]warmup run: 1974it [00:03, 1009.96it/s]warmup run: 2443it [00:03, 1122.71it/s]warmup run: 2233it [00:03, 1100.39it/s]warmup run: 2355it [00:03, 1143.93it/s]warmup run: 2339it [00:03, 1107.23it/s]warmup run: 2303it [00:03, 1108.73it/s]warmup run: 2584it [00:04, 1135.33it/s]warmup run: 2400it [00:03, 1138.67it/s]warmup run: 2091it [00:03, 1057.51it/s]warmup run: 2561it [00:04, 1139.68it/s]warmup run: 2355it [00:03, 1134.25it/s]warmup run: 2474it [00:03, 1155.40it/s]warmup run: 2460it [00:03, 1136.42it/s]warmup run: 2424it [00:03, 1138.65it/s]warmup run: 2701it [00:04, 1145.42it/s]warmup run: 2521it [00:03, 1158.16it/s]warmup run: 2214it [00:03, 1107.52it/s]warmup run: 2678it [00:04, 1148.60it/s]warmup run: 2477it [00:03, 1158.15it/s]warmup run: 2594it [00:04, 1166.19it/s]warmup run: 2581it [00:04, 1156.35it/s]warmup run: 2545it [00:03, 1158.29it/s]warmup run: 2819it [00:04, 1155.27it/s]warmup run: 2640it [00:04, 1167.17it/s]warmup run: 2337it [00:03, 1141.83it/s]warmup run: 2796it [00:04, 1156.50it/s]warmup run: 2599it [00:04, 1173.98it/s]warmup run: 2715it [00:04, 1178.58it/s]warmup run: 2700it [00:04, 1165.66it/s]warmup run: 2665it [00:04, 1168.81it/s]warmup run: 2937it [00:04, 1162.18it/s]warmup run: 2760it [00:04, 1176.72it/s]warmup run: 2458it [00:03, 1161.35it/s]warmup run: 2914it [00:04, 1162.05it/s]warmup run: 2720it [00:04, 1183.54it/s]warmup run: 3000it [00:04, 658.07it/s] warmup run: 2836it [00:04, 1187.11it/s]warmup run: 2820it [00:04, 1173.78it/s]warmup run: 2785it [00:04, 1175.75it/s]warmup run: 3000it [00:04, 678.18it/s] warmup run: 2881it [00:04, 1184.76it/s]warmup run: 2578it [00:04, 1171.47it/s]warmup run: 2839it [00:04, 1182.96it/s]warmup run: 2956it [00:04, 1189.28it/s]warmup run: 2940it [00:04, 1180.32it/s]warmup run: 2905it [00:04, 1180.60it/s]warmup run: 3000it [00:04, 684.21it/s] warmup run: 3000it [00:04, 696.52it/s] warmup run: 2697it [00:04, 1175.53it/s]warmup run: 3000it [00:04, 683.76it/s] warmup run: 2958it [00:04, 1183.98it/s]warmup run: 3000it [00:04, 683.78it/s] warmup run: 3000it [00:04, 687.59it/s] warmup run: 2818it [00:04, 1184.55it/s]warmup run: 2939it [00:04, 1189.83it/s]warmup run: 3000it [00:04, 687.93it/s] 
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]



warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]

warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   6%|         | 165/3000 [00:00<00:01, 1648.10it/s]warmup should be done:   5%|         | 163/3000 [00:00<00:01, 1627.41it/s]warmup should be done:   6%|         | 167/3000 [00:00<00:01, 1663.17it/s]warmup should be done:   5%|         | 160/3000 [00:00<00:01, 1594.31it/s]warmup should be done:   6%|         | 165/3000 [00:00<00:01, 1641.96it/s]warmup should be done:   5%|         | 162/3000 [00:00<00:01, 1610.55it/s]warmup should be done:   5%|         | 155/3000 [00:00<00:01, 1541.09it/s]warmup should be done:   4%|         | 105/3000 [00:00<00:02, 1041.43it/s]warmup should be done:  11%|         | 332/3000 [00:00<00:01, 1656.59it/s]warmup should be done:  11%|         | 326/3000 [00:00<00:01, 1630.38it/s]warmup should be done:  11%|         | 329/3000 [00:00<00:01, 1641.47it/s]warmup should be done:  11%|         | 336/3000 [00:00<00:01, 1674.70it/s]warmup should be done:  10%|         | 312/3000 [00:00<00:01, 1555.59it/s]warmup should be done:  11%|         | 327/3000 [00:00<00:01, 1629.14it/s]warmup should be done:  11%|         | 331/3000 [00:00<00:01, 1646.86it/s]warmup should be done:   9%|         | 263/3000 [00:00<00:02, 1353.56it/s]warmup should be done:  17%|        | 498/3000 [00:00<00:01, 1652.36it/s]warmup should be done:  16%|        | 491/3000 [00:00<00:01, 1635.91it/s]warmup should be done:  17%|        | 504/3000 [00:00<00:01, 1672.14it/s]warmup should be done:  16%|        | 491/3000 [00:00<00:01, 1630.79it/s]warmup should be done:  16%|        | 494/3000 [00:00<00:01, 1638.59it/s]warmup should be done:  17%|        | 496/3000 [00:00<00:01, 1643.32it/s]warmup should be done:  14%|        | 426/3000 [00:00<00:01, 1475.78it/s]warmup should be done:  16%|        | 468/3000 [00:00<00:01, 1543.56it/s]warmup should be done:  22%|       | 656/3000 [00:00<00:01, 1640.99it/s]warmup should be done:  22%|       | 664/3000 [00:00<00:01, 1651.58it/s]warmup should be done:  22%|       | 672/3000 [00:00<00:01, 1672.98it/s]warmup should be done:  22%|       | 655/3000 [00:00<00:01, 1631.38it/s]warmup should be done:  20%|        | 592/3000 [00:00<00:01, 1547.36it/s]warmup should be done:  22%|       | 659/3000 [00:00<00:01, 1639.72it/s]warmup should be done:  22%|       | 661/3000 [00:00<00:01, 1642.11it/s]warmup should be done:  21%|        | 624/3000 [00:00<00:01, 1549.40it/s]warmup should be done:  27%|       | 821/3000 [00:00<00:01, 1642.17it/s]warmup should be done:  25%|       | 756/3000 [00:00<00:01, 1580.42it/s]warmup should be done:  27%|       | 823/3000 [00:00<00:01, 1638.76it/s]warmup should be done:  27%|       | 819/3000 [00:00<00:01, 1631.93it/s]warmup should be done:  28%|       | 840/3000 [00:00<00:01, 1670.43it/s]warmup should be done:  28%|       | 830/3000 [00:00<00:01, 1648.96it/s]warmup should be done:  28%|       | 826/3000 [00:00<00:01, 1640.21it/s]warmup should be done:  26%|       | 779/3000 [00:00<00:01, 1547.30it/s]warmup should be done:  33%|      | 986/3000 [00:00<00:01, 1639.15it/s]warmup should be done:  31%|       | 920/3000 [00:00<00:01, 1597.49it/s]warmup should be done:  33%|      | 983/3000 [00:00<00:01, 1631.25it/s]warmup should be done:  33%|      | 987/3000 [00:00<00:01, 1633.94it/s]warmup should be done:  33%|      | 995/3000 [00:00<00:01, 1642.83it/s]warmup should be done:  34%|      | 1008/3000 [00:00<00:01, 1664.19it/s]warmup should be done:  31%|       | 934/3000 [00:00<00:01, 1543.14it/s]warmup should be done:  33%|      | 991/3000 [00:00<00:01, 1633.52it/s]warmup should be done:  38%|      | 1151/3000 [00:00<00:01, 1642.10it/s]warmup should be done:  36%|      | 1080/3000 [00:00<00:01, 1595.62it/s]warmup should be done:  38%|      | 1152/3000 [00:00<00:01, 1637.40it/s]warmup should be done:  38%|      | 1147/3000 [00:00<00:01, 1628.35it/s]warmup should be done:  39%|      | 1160/3000 [00:00<00:01, 1641.48it/s]warmup should be done:  36%|      | 1093/3000 [00:00<00:01, 1556.27it/s]warmup should be done:  39%|      | 1175/3000 [00:00<00:01, 1662.31it/s]warmup should be done:  38%|      | 1155/3000 [00:00<00:01, 1631.69it/s]warmup should be done:  44%|     | 1316/3000 [00:00<00:01, 1643.16it/s]warmup should be done:  42%|     | 1245/3000 [00:00<00:01, 1611.86it/s]warmup should be done:  44%|     | 1310/3000 [00:00<00:01, 1628.83it/s]warmup should be done:  44%|     | 1316/3000 [00:00<00:01, 1633.32it/s]warmup should be done:  42%|     | 1253/3000 [00:00<00:01, 1569.56it/s]warmup should be done:  44%|     | 1325/3000 [00:00<00:01, 1641.04it/s]warmup should be done:  45%|     | 1342/3000 [00:00<00:00, 1658.20it/s]warmup should be done:  44%|     | 1319/3000 [00:00<00:01, 1630.77it/s]warmup should be done:  49%|     | 1481/3000 [00:00<00:00, 1644.33it/s]warmup should be done:  47%|     | 1410/3000 [00:00<00:00, 1623.07it/s]warmup should be done:  49%|     | 1474/3000 [00:00<00:00, 1629.46it/s]warmup should be done:  49%|     | 1480/3000 [00:00<00:00, 1632.87it/s]warmup should be done:  47%|     | 1413/3000 [00:00<00:01, 1577.60it/s]warmup should be done:  50%|     | 1490/3000 [00:00<00:00, 1640.13it/s]warmup should be done:  49%|     | 1483/3000 [00:00<00:00, 1627.75it/s]warmup should be done:  50%|     | 1508/3000 [00:00<00:00, 1645.72it/s]warmup should be done:  55%|    | 1646/3000 [00:01<00:00, 1645.07it/s]warmup should be done:  52%|    | 1574/3000 [00:01<00:00, 1625.51it/s]warmup should be done:  55%|    | 1638/3000 [00:01<00:00, 1629.96it/s]warmup should be done:  52%|    | 1573/3000 [00:01<00:00, 1583.74it/s]warmup should be done:  55%|    | 1644/3000 [00:01<00:00, 1629.82it/s]warmup should be done:  55%|    | 1655/3000 [00:01<00:00, 1639.04it/s]warmup should be done:  55%|    | 1649/3000 [00:01<00:00, 1636.48it/s]warmup should be done:  56%|    | 1673/3000 [00:01<00:00, 1638.97it/s]warmup should be done:  60%|    | 1811/3000 [00:01<00:00, 1644.99it/s]warmup should be done:  58%|    | 1737/3000 [00:01<00:00, 1623.60it/s]warmup should be done:  60%|    | 1802/3000 [00:01<00:00, 1630.45it/s]warmup should be done:  58%|    | 1733/3000 [00:01<00:00, 1587.33it/s]warmup should be done:  60%|    | 1807/3000 [00:01<00:00, 1626.92it/s]warmup should be done:  61%|    | 1819/3000 [00:01<00:00, 1636.63it/s]warmup should be done:  60%|    | 1815/3000 [00:01<00:00, 1643.24it/s]warmup should be done:  61%|    | 1837/3000 [00:01<00:00, 1633.79it/s]warmup should be done:  66%|   | 1976/3000 [00:01<00:00, 1644.10it/s]warmup should be done:  63%|   | 1900/3000 [00:01<00:00, 1619.25it/s]warmup should be done:  63%|   | 1893/3000 [00:01<00:00, 1588.89it/s]warmup should be done:  66%|   | 1966/3000 [00:01<00:00, 1630.50it/s]warmup should be done:  66%|   | 1983/3000 [00:01<00:00, 1637.13it/s]warmup should be done:  66%|   | 1970/3000 [00:01<00:00, 1624.79it/s]warmup should be done:  66%|   | 1981/3000 [00:01<00:00, 1647.77it/s]warmup should be done:  67%|   | 2001/3000 [00:01<00:00, 1629.65it/s]warmup should be done:  71%|  | 2141/3000 [00:01<00:00, 1644.40it/s]warmup should be done:  68%|   | 2053/3000 [00:01<00:00, 1589.53it/s]warmup should be done:  71%|   | 2130/3000 [00:01<00:00, 1630.82it/s]warmup should be done:  69%|   | 2062/3000 [00:01<00:00, 1615.86it/s]warmup should be done:  72%|  | 2147/3000 [00:01<00:00, 1637.69it/s]warmup should be done:  71%|   | 2133/3000 [00:01<00:00, 1624.98it/s]warmup should be done:  72%|  | 2148/3000 [00:01<00:00, 1651.63it/s]warmup should be done:  72%|  | 2164/3000 [00:01<00:00, 1627.01it/s]warmup should be done:  77%|  | 2306/3000 [00:01<00:00, 1640.54it/s]warmup should be done:  74%|  | 2213/3000 [00:01<00:00, 1589.92it/s]warmup should be done:  76%|  | 2294/3000 [00:01<00:00, 1630.28it/s]warmup should be done:  74%|  | 2224/3000 [00:01<00:00, 1613.88it/s]warmup should be done:  77%|  | 2311/3000 [00:01<00:00, 1634.93it/s]warmup should be done:  77%|  | 2296/3000 [00:01<00:00, 1622.40it/s]warmup should be done:  77%|  | 2314/3000 [00:01<00:00, 1650.36it/s]warmup should be done:  78%|  | 2327/3000 [00:01<00:00, 1622.58it/s]warmup should be done:  82%| | 2471/3000 [00:01<00:00, 1640.38it/s]warmup should be done:  79%|  | 2372/3000 [00:01<00:00, 1587.92it/s]warmup should be done:  80%|  | 2386/3000 [00:01<00:00, 1610.52it/s]warmup should be done:  83%| | 2476/3000 [00:01<00:00, 1636.99it/s]warmup should be done:  82%| | 2461/3000 [00:01<00:00, 1628.50it/s]warmup should be done:  82%| | 2458/3000 [00:01<00:00, 1626.15it/s]warmup should be done:  83%| | 2480/3000 [00:01<00:00, 1650.99it/s]warmup should be done:  83%| | 2490/3000 [00:01<00:00, 1622.52it/s]warmup should be done:  88%| | 2636/3000 [00:01<00:00, 1641.12it/s]warmup should be done:  84%| | 2531/3000 [00:01<00:00, 1583.53it/s]warmup should be done:  88%| | 2641/3000 [00:01<00:00, 1638.99it/s]warmup should be done:  85%| | 2548/3000 [00:01<00:00, 1611.43it/s]warmup should be done:  87%| | 2621/3000 [00:01<00:00, 1626.35it/s]warmup should be done:  88%| | 2628/3000 [00:01<00:00, 1638.38it/s]warmup should be done:  88%| | 2646/3000 [00:01<00:00, 1652.37it/s]warmup should be done:  88%| | 2654/3000 [00:01<00:00, 1625.88it/s]warmup should be done:  93%|| 2802/3000 [00:01<00:00, 1645.74it/s]warmup should be done:  90%| | 2710/3000 [00:01<00:00, 1613.22it/s]warmup should be done:  93%|| 2784/3000 [00:01<00:00, 1626.06it/s]warmup should be done:  94%|| 2806/3000 [00:01<00:00, 1639.88it/s]warmup should be done:  93%|| 2794/3000 [00:01<00:00, 1644.67it/s]warmup should be done:  90%| | 2690/3000 [00:01<00:00, 1580.14it/s]warmup should be done:  94%|| 2813/3000 [00:01<00:00, 1656.60it/s]warmup should be done:  94%|| 2819/3000 [00:01<00:00, 1631.10it/s]warmup should be done:  99%|| 2969/3000 [00:01<00:00, 1650.65it/s]warmup should be done:  96%|| 2876/3000 [00:01<00:00, 1625.84it/s]warmup should be done:  99%|| 2972/3000 [00:01<00:00, 1645.73it/s]warmup should be done:  98%|| 2949/3000 [00:01<00:00, 1631.13it/s]warmup should be done:  99%|| 2962/3000 [00:01<00:00, 1652.82it/s]warmup should be done:  95%|| 2849/3000 [00:01<00:00, 1581.79it/s]warmup should be done:  99%|| 2979/3000 [00:01<00:00, 1653.31it/s]warmup should be done:  99%|| 2984/3000 [00:01<00:00, 1636.54it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1644.52it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1643.21it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1642.80it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1642.08it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1636.73it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1629.05it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1592.34it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1574.13it/s]


warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]



warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   6%|         | 168/3000 [00:00<00:01, 1677.58it/s]warmup should be done:   6%|         | 170/3000 [00:00<00:01, 1699.09it/s]warmup should be done:   5%|         | 164/3000 [00:00<00:01, 1638.47it/s]warmup should be done:   5%|         | 161/3000 [00:00<00:01, 1601.45it/s]warmup should be done:   6%|         | 169/3000 [00:00<00:01, 1680.55it/s]warmup should be done:   6%|         | 171/3000 [00:00<00:01, 1699.87it/s]warmup should be done:   6%|         | 166/3000 [00:00<00:01, 1651.26it/s]warmup should be done:   6%|         | 167/3000 [00:00<00:01, 1660.58it/s]warmup should be done:  11%|        | 340/3000 [00:00<00:01, 1697.77it/s]warmup should be done:  11%|         | 337/3000 [00:00<00:01, 1682.71it/s]warmup should be done:  11%|         | 333/3000 [00:00<00:01, 1661.35it/s]warmup should be done:  11%|         | 331/3000 [00:00<00:01, 1651.57it/s]warmup should be done:  11%|        | 341/3000 [00:00<00:01, 1699.76it/s]warmup should be done:  11%|         | 322/3000 [00:00<00:01, 1601.82it/s]warmup should be done:  11%|        | 343/3000 [00:00<00:01, 1707.35it/s]warmup should be done:  11%|         | 334/3000 [00:00<00:01, 1659.66it/s]warmup should be done:  17%|        | 510/3000 [00:00<00:01, 1698.44it/s]warmup should be done:  17%|        | 506/3000 [00:00<00:01, 1685.39it/s]warmup should be done:  17%|        | 498/3000 [00:00<00:01, 1658.41it/s]warmup should be done:  17%|        | 501/3000 [00:00<00:01, 1667.46it/s]warmup should be done:  17%|        | 513/3000 [00:00<00:01, 1707.54it/s]warmup should be done:  16%|        | 484/3000 [00:00<00:01, 1608.03it/s]warmup should be done:  17%|        | 515/3000 [00:00<00:01, 1710.39it/s]warmup should be done:  17%|        | 501/3000 [00:00<00:01, 1663.30it/s]warmup should be done:  23%|       | 680/3000 [00:00<00:01, 1697.80it/s]warmup should be done:  22%|       | 675/3000 [00:00<00:01, 1682.81it/s]warmup should be done:  22%|       | 645/3000 [00:00<00:01, 1608.54it/s]warmup should be done:  22%|       | 669/3000 [00:00<00:01, 1669.73it/s]warmup should be done:  22%|       | 665/3000 [00:00<00:01, 1659.82it/s]warmup should be done:  23%|       | 685/3000 [00:00<00:01, 1708.78it/s]warmup should be done:  22%|       | 669/3000 [00:00<00:01, 1666.12it/s]warmup should be done:  23%|       | 687/3000 [00:00<00:01, 1708.15it/s]warmup should be done:  28%|       | 851/3000 [00:00<00:01, 1699.36it/s]warmup should be done:  27%|       | 806/3000 [00:00<00:01, 1606.48it/s]warmup should be done:  29%|       | 856/3000 [00:00<00:01, 1708.34it/s]warmup should be done:  28%|       | 832/3000 [00:00<00:01, 1660.75it/s]warmup should be done:  28%|       | 837/3000 [00:00<00:01, 1670.00it/s]warmup should be done:  28%|       | 837/3000 [00:00<00:01, 1670.77it/s]warmup should be done:  29%|       | 858/3000 [00:00<00:01, 1703.82it/s]warmup should be done:  28%|       | 844/3000 [00:00<00:01, 1672.74it/s]warmup should be done:  34%|      | 1021/3000 [00:00<00:01, 1697.91it/s]warmup should be done:  33%|      | 1000/3000 [00:00<00:01, 1666.30it/s]warmup should be done:  34%|      | 1007/3000 [00:00<00:01, 1678.82it/s]warmup should be done:  34%|      | 1028/3000 [00:00<00:01, 1709.74it/s]warmup should be done:  32%|      | 968/3000 [00:00<00:01, 1607.65it/s]warmup should be done:  34%|      | 1005/3000 [00:00<00:01, 1671.31it/s]warmup should be done:  34%|      | 1029/3000 [00:00<00:01, 1704.85it/s]warmup should be done:  34%|      | 1012/3000 [00:00<00:01, 1668.37it/s]warmup should be done:  40%|      | 1191/3000 [00:00<00:01, 1695.59it/s]warmup should be done:  39%|      | 1167/3000 [00:00<00:01, 1667.33it/s]warmup should be done:  40%|      | 1199/3000 [00:00<00:01, 1709.61it/s]warmup should be done:  38%|      | 1130/3000 [00:00<00:01, 1610.33it/s]warmup should be done:  39%|      | 1178/3000 [00:00<00:01, 1686.10it/s]warmup should be done:  39%|      | 1173/3000 [00:00<00:01, 1668.74it/s]warmup should be done:  40%|      | 1200/3000 [00:00<00:01, 1704.27it/s]warmup should be done:  39%|      | 1179/3000 [00:00<00:01, 1636.70it/s]warmup should be done:  44%|     | 1334/3000 [00:00<00:00, 1667.66it/s]warmup should be done:  45%|     | 1362/3000 [00:00<00:00, 1697.20it/s]warmup should be done:  46%|     | 1371/3000 [00:00<00:00, 1711.04it/s]warmup should be done:  45%|     | 1340/3000 [00:00<00:00, 1668.80it/s]warmup should be done:  46%|     | 1372/3000 [00:00<00:00, 1706.44it/s]warmup should be done:  43%|     | 1292/3000 [00:00<00:01, 1604.64it/s]warmup should be done:  45%|     | 1345/3000 [00:00<00:01, 1643.34it/s]warmup should be done:  45%|     | 1347/3000 [00:00<00:01, 1587.06it/s]warmup should be done:  51%|     | 1532/3000 [00:00<00:00, 1696.51it/s]warmup should be done:  50%|     | 1501/3000 [00:00<00:00, 1662.76it/s]warmup should be done:  51%|    | 1543/3000 [00:00<00:00, 1708.92it/s]warmup should be done:  50%|     | 1507/3000 [00:00<00:00, 1667.46it/s]warmup should be done:  51%|    | 1543/3000 [00:00<00:00, 1706.16it/s]warmup should be done:  48%|     | 1454/3000 [00:00<00:00, 1607.50it/s]warmup should be done:  50%|     | 1510/3000 [00:00<00:00, 1644.11it/s]warmup should be done:  51%|     | 1516/3000 [00:00<00:00, 1616.40it/s]warmup should be done:  57%|    | 1702/3000 [00:01<00:00, 1696.80it/s]warmup should be done:  56%|    | 1669/3000 [00:01<00:00, 1666.37it/s]warmup should be done:  57%|    | 1715/3000 [00:01<00:00, 1709.40it/s]warmup should be done:  56%|    | 1676/3000 [00:01<00:00, 1673.04it/s]warmup should be done:  54%|    | 1616/3000 [00:01<00:00, 1610.89it/s]warmup should be done:  57%|    | 1714/3000 [00:01<00:00, 1705.32it/s]warmup should be done:  56%|    | 1676/3000 [00:01<00:00, 1648.57it/s]warmup should be done:  56%|    | 1679/3000 [00:01<00:00, 1545.68it/s]warmup should be done:  62%|   | 1873/3000 [00:01<00:00, 1698.61it/s]warmup should be done:  61%|    | 1837/3000 [00:01<00:00, 1668.30it/s]warmup should be done:  63%|   | 1887/3000 [00:01<00:00, 1711.23it/s]warmup should be done:  62%|   | 1845/3000 [00:01<00:00, 1676.60it/s]warmup should be done:  63%|   | 1885/3000 [00:01<00:00, 1705.96it/s]warmup should be done:  59%|    | 1779/3000 [00:01<00:00, 1614.47it/s]warmup should be done:  61%|   | 1843/3000 [00:01<00:00, 1653.86it/s]warmup should be done:  62%|   | 1849/3000 [00:01<00:00, 1590.22it/s]warmup should be done:  68%|   | 2043/3000 [00:01<00:00, 1698.69it/s]warmup should be done:  67%|   | 2004/3000 [00:01<00:00, 1668.65it/s]warmup should be done:  67%|   | 2014/3000 [00:01<00:00, 1679.95it/s]warmup should be done:  69%|   | 2059/3000 [00:01<00:00, 1711.47it/s]warmup should be done:  65%|   | 1943/3000 [00:01<00:00, 1621.60it/s]warmup should be done:  69%|   | 2057/3000 [00:01<00:00, 1707.15it/s]warmup should be done:  67%|   | 2009/3000 [00:01<00:00, 1655.68it/s]warmup should be done:  67%|   | 2010/3000 [00:01<00:00, 1527.90it/s]warmup should be done:  74%|  | 2213/3000 [00:01<00:00, 1697.40it/s]warmup should be done:  72%|  | 2171/3000 [00:01<00:00, 1667.86it/s]warmup should be done:  70%|   | 2108/3000 [00:01<00:00, 1628.99it/s]warmup should be done:  74%|  | 2231/3000 [00:01<00:00, 1709.72it/s]warmup should be done:  74%|  | 2228/3000 [00:01<00:00, 1705.12it/s]warmup should be done:  73%|  | 2182/3000 [00:01<00:00, 1645.54it/s]warmup should be done:  72%|  | 2175/3000 [00:01<00:00, 1654.04it/s]warmup should be done:  73%|  | 2179/3000 [00:01<00:00, 1574.17it/s]warmup should be done:  79%|  | 2383/3000 [00:01<00:00, 1697.47it/s]warmup should be done:  78%|  | 2339/3000 [00:01<00:00, 1668.63it/s]warmup should be done:  80%|  | 2402/3000 [00:01<00:00, 1709.71it/s]warmup should be done:  76%|  | 2272/3000 [00:01<00:00, 1631.33it/s]warmup should be done:  80%|  | 2399/3000 [00:01<00:00, 1704.62it/s]warmup should be done:  78%|  | 2348/3000 [00:01<00:00, 1649.62it/s]warmup should be done:  78%|  | 2341/3000 [00:01<00:00, 1654.66it/s]warmup should be done:  78%|  | 2338/3000 [00:01<00:00, 1509.76it/s]warmup should be done:  85%| | 2554/3000 [00:01<00:00, 1698.78it/s]warmup should be done:  84%| | 2507/3000 [00:01<00:00, 1669.79it/s]warmup should be done:  86%| | 2574/3000 [00:01<00:00, 1710.95it/s]warmup should be done:  81%|  | 2436/3000 [00:01<00:00, 1632.93it/s]warmup should be done:  86%| | 2571/3000 [00:01<00:00, 1706.19it/s]warmup should be done:  84%| | 2517/3000 [00:01<00:00, 1660.26it/s]warmup should be done:  84%| | 2507/3000 [00:01<00:00, 1655.70it/s]warmup should be done:  84%| | 2508/3000 [00:01<00:00, 1563.12it/s]warmup should be done:  91%| | 2725/3000 [00:01<00:00, 1700.04it/s]warmup should be done:  89%| | 2674/3000 [00:01<00:00, 1669.55it/s]warmup should be done:  92%|| 2746/3000 [00:01<00:00, 1712.41it/s]warmup should be done:  91%|| 2743/3000 [00:01<00:00, 1707.49it/s]warmup should be done:  90%| | 2685/3000 [00:01<00:00, 1664.78it/s]warmup should be done:  87%| | 2600/3000 [00:01<00:00, 1604.20it/s]warmup should be done:  89%| | 2673/3000 [00:01<00:00, 1653.08it/s]warmup should be done:  89%| | 2666/3000 [00:01<00:00, 1506.13it/s]warmup should be done:  97%|| 2896/3000 [00:01<00:00, 1699.28it/s]warmup should be done:  95%|| 2841/3000 [00:01<00:00, 1668.78it/s]warmup should be done:  97%|| 2918/3000 [00:01<00:00, 1712.70it/s]warmup should be done:  97%|| 2914/3000 [00:01<00:00, 1707.05it/s]warmup should be done:  95%|| 2853/3000 [00:01<00:00, 1668.64it/s]warmup should be done:  92%|| 2765/3000 [00:01<00:00, 1617.10it/s]warmup should be done:  95%|| 2839/3000 [00:01<00:00, 1650.72it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1709.48it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1705.96it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1698.13it/s]warmup should be done:  95%|| 2837/3000 [00:01<00:00, 1562.75it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1667.03it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1666.06it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1656.47it/s]warmup should be done:  98%|| 2927/3000 [00:01<00:00, 1607.26it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1613.26it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1595.59it/s]2022-12-11 21:37:43.310454: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f284f8332e0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 21:37:43.310523: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 21:37:43.532073: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f284f832fa0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 21:37:43.532137: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 21:37:43.943405: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f285782c6a0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 21:37:43.943467: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 21:37:43.943821: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f284f8309e0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 21:37:43.943864: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 21:37:43.983811: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f2857796260 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 21:37:43.983875: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 21:37:44.087150: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f284f830b20 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 21:37:44.087218: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 21:37:44.313849: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f0bfc02e830 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 21:37:44.313931: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 21:37:44.314982: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f0c9c029f90 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 21:37:44.315051: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 21:37:45.572608: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 21:37:45.848327: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 21:37:46.184094: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 21:37:46.188445: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 21:37:46.229277: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 21:37:46.433177: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 21:37:46.579888: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 21:37:46.633764: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 21:37:48.447358: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 21:37:48.751320: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 21:37:49.056673: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 21:37:49.066957: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 21:37:49.158200: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 21:37:49.339087: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 21:37:49.410140: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 21:37:49.512948: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
[HCTR][21:38:29.290][ERROR][RK0][main]: replica 5 reaches 1000, calling init pre replica
[HCTR][21:38:29.290][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][21:38:29.298][ERROR][RK0][main]: coll ps creation done
[HCTR][21:38:29.298][ERROR][RK0][main]: replica 5 waits for coll ps creation barrier
[HCTR][21:38:29.308][ERROR][RK0][main]: replica 4 reaches 1000, calling init pre replica
[HCTR][21:38:29.308][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][21:38:29.317][ERROR][RK0][main]: coll ps creation done
[HCTR][21:38:29.317][ERROR][RK0][main]: replica 4 waits for coll ps creation barrier
[HCTR][21:38:29.378][ERROR][RK0][tid #139811738326784]: replica 2 reaches 1000, calling init pre replica
[HCTR][21:38:29.378][ERROR][RK0][tid #139811738326784]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][21:38:29.379][ERROR][RK0][main]: replica 7 reaches 1000, calling init pre replica
[HCTR][21:38:29.379][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][21:38:29.382][ERROR][RK0][main]: replica 1 reaches 1000, calling init pre replica
[HCTR][21:38:29.382][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][21:38:29.383][ERROR][RK0][main]: coll ps creation done
[HCTR][21:38:29.383][ERROR][RK0][main]: replica 7 waits for coll ps creation barrier
[HCTR][21:38:29.383][ERROR][RK0][tid #139811738326784]: coll ps creation done
[HCTR][21:38:29.383][ERROR][RK0][tid #139811738326784]: replica 2 waits for coll ps creation barrier
[HCTR][21:38:29.390][ERROR][RK0][main]: coll ps creation done
[HCTR][21:38:29.390][ERROR][RK0][main]: replica 1 waits for coll ps creation barrier
[HCTR][21:38:29.394][ERROR][RK0][tid #139811746719488]: replica 3 reaches 1000, calling init pre replica
[HCTR][21:38:29.394][ERROR][RK0][tid #139811746719488]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][21:38:29.394][ERROR][RK0][main]: replica 6 reaches 1000, calling init pre replica
[HCTR][21:38:29.394][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][21:38:29.398][ERROR][RK0][main]: replica 0 reaches 1000, calling init pre replica
[HCTR][21:38:29.398][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][21:38:29.403][ERROR][RK0][tid #139811746719488]: coll ps creation done
[HCTR][21:38:29.403][ERROR][RK0][tid #139811746719488]: replica 3 waits for coll ps creation barrier
[HCTR][21:38:29.403][ERROR][RK0][main]: coll ps creation done
[HCTR][21:38:29.403][ERROR][RK0][main]: replica 6 waits for coll ps creation barrier
[HCTR][21:38:29.406][ERROR][RK0][main]: coll ps creation done
[HCTR][21:38:29.407][ERROR][RK0][main]: replica 0 waits for coll ps creation barrier
[HCTR][21:38:29.407][ERROR][RK0][main]: replica 0 preparing frequency
[HCTR][21:38:30.294][ERROR][RK0][main]: replica 0 preparing frequency done
[HCTR][21:38:30.330][ERROR][RK0][main]: replica 0 calling init per replica
[HCTR][21:38:30.330][ERROR][RK0][main]: replica 1 calling init per replica
[HCTR][21:38:30.330][ERROR][RK0][main]: replica 4 calling init per replica
[HCTR][21:38:30.330][ERROR][RK0][main]: replica 7 calling init per replica
[HCTR][21:38:30.330][ERROR][RK0][tid #139811738326784]: replica 2 calling init per replica
[HCTR][21:38:30.330][ERROR][RK0][main]: replica 5 calling init per replica
[HCTR][21:38:30.330][ERROR][RK0][main]: replica 6 calling init per replica
[HCTR][21:38:30.330][ERROR][RK0][tid #139811746719488]: replica 3 calling init per replica
[HCTR][21:38:30.330][ERROR][RK0][main]: Calling build_v2
[HCTR][21:38:30.330][ERROR][RK0][main]: Calling build_v2
[HCTR][21:38:30.330][ERROR][RK0][main]: Calling build_v2
[HCTR][21:38:30.330][ERROR][RK0][main]: Calling build_v2
[HCTR][21:38:30.330][ERROR][RK0][tid #139811738326784]: Calling build_v2
[HCTR][21:38:30.330][ERROR][RK0][main]: Calling build_v2
[HCTR][21:38:30.330][ERROR][RK0][main]: Calling build_v2
[HCTR][21:38:30.330][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][21:38:30.330][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][21:38:30.330][ERROR][RK0][tid #139811746719488]: Calling build_v2
[HCTR][21:38:30.330][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][21:38:30.330][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][21:38:30.330][ERROR][RK0][tid #139811738326784]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][21:38:30.330][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][21:38:30.330][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][21:38:30.330][ERROR][RK0][tid #139811746719488]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[2022-12-11 21:38:30.334257: E [/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:178] v100x8, slow pcie2022-12-11 21:38:30
.334308: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[:2022-12-11 21:38:30[178.] 3343672022-12-11 21:38:30v100x8, slow pcie: .
E334359 [: [/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-11 21:38:30E2022-12-11 21:38:30:. .196334406[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc334426] : :: assigning 0 to cpu2022-12-11 21:38:30E178E
[. ]  2022-12-11 21:38:30334457/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccv100x8, slow pcie/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[.: :
[:334504E[1782022-12-11 21:38:302022-12-11 21:38:30196:  [] .2022-12-11 21:38:30.] E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-11 21:38:30v100x8, slow pcie334543.334577assigning 0 to cpu :.
: 334591: 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc178334600E: [E:] :  E2022-12-11 21:38:30 178v100x8, slow pcieE/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc ./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] 
 :/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[334692:v100x8, slow pcie/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc178:[2022-12-11 21:38:30: 212
:] 1782022-12-11 21:38:30.E] 196v100x8, slow pcie] [.334757 build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8] 
v100x8, slow pcie2022-12-11 21:38:30334777: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
assigning 0 to cpu
.: E[:
334821E 2022-12-11 21:38:30[196[:  /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.2022-12-11 21:38:30] 2022-12-11 21:38:30E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:334865.assigning 0 to cpu. [:212: 334885
334890/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-11 21:38:30196] E: : :.] build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8 EE196334931assigning 0 to cpu
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc [ ] : 
:/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-11 21:38:30/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccassigning 0 to cpu[E196:.:
2022-12-11 21:38:30 ] 196335009[213./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccassigning 0 to cpu] : 2022-12-11 21:38:30] 335042:
assigning 0 to cpuE[.remote time is 8.68421: 212
 2022-12-11 21:38:30335079
E] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.:  build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8[:335120[E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
2022-12-11 21:38:30212[: 2022-12-11 21:38:30 :.] 2022-12-11 21:38:30E[./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc213335195build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8. 2022-12-11 21:38:30335196:] : 
335214/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.: 212remote time is 8.68421E: :335243E[] 
 E212:  2022-12-11 21:38:30build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc [] E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.
:/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-11 21:38:30build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8 :335308212:.
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[214: ] 212335337:2022-12-11 21:38:30[] Ebuild asymm link desc with 8X Tesla V100-SXM2-32GB out of 8] : 213.2022-12-11 21:38:30cpu time is 97.0588 
build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8E] 335380.
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
 remote time is 8.68421[: 335408:/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
2022-12-11 21:38:30[E: 213:.2022-12-11 21:38:30 [E] 214335468./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-11 21:38:30 remote time is 8.68421] : 335491:./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
cpu time is 97.0588E: 213335516:[
 E] : 2132022-12-11 21:38:30/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc remote time is 8.68421E] .:/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
 remote time is 8.68421335579[213:/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
: 2022-12-11 21:38:30] 213:E.remote time is 8.68421[] 214 335648
2022-12-11 21:38:30remote time is 8.68421] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: .
[cpu time is 97.0588:E3356702022-12-11 21:38:30[
214 : .2022-12-11 21:38:30] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE335704.cpu time is 97.0588: : 335721
214/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE: ] : Ecpu time is 97.0588214/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc 
] :/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cccpu time is 97.0588214:
] 214cpu time is 97.0588] 
cpu time is 97.0588
[2022-12-11 21:39:47.406842: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:118] solver created. now build & solve
[2022-12-11 21:39:47.446898: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:123] solver built. now solve
[2022-12-11 21:39:47.446962: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:919] num_cached_nodes = 1999999
[2022-12-11 21:39:47.565755: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:127] solver solved
[2022-12-11 21:39:47.565859: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:195] 0 solved master
[2022-12-11 21:39:47.565901: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 0 solved
[2022-12-11 21:39:47.565938: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 0 initing device 0
[2022-12-11 21:39:47.566421: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 21:39:47.567329: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 21:39:47.568004: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 21:39:47.581100: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 6 solved
[2022-12-11 21:39:47.581167: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 6 initing device 6
[2022-12-11 21:39:47.581402: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 7 solved
[2022-12-11 21:39:47.581460: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 7 initing device 7
[2022-12-11 21:39:47.581474: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 2 solved
[2022-12-11 21:39:47.581537: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 2 initing device 2
[2022-12-11 21:39:47.581568: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 21:39:47.581812: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:[2022022-12-11 21:39:47] .4 solved581848
: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu[:2022-12-11 21:39:471980.] 581876eager alloc mem 381.47 MB: 
E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 4 initing device 4
[2022-12-11 21:39:47.581934: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 21:39:47.581980: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 1 solved
[2022-12-11 21:39:47.582036: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 1 initing device 1
[2022-12-11 21:39:47.582274: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 21:39:47.582418: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 21:39:47.585321: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 21:39:47.585559: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB[
2022-12-11 21:39:47.585596: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 21:39:47.585850: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 3 solved
[2022-12-11 21:39:47.585903: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 3 initing device 3
[2022-12-11 21:39:47.585964: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 5 solved
[2022-12-11 21:39:47.586035: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 5 initing device 5
[2022-12-11 21:39:47.586140: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 21:39:47.586203: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 21:39:47.586266: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 21:39:47.586439: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 21:39:47.589721: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 21:39:47.590122: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 21:39:47.590162: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 21:39:47.590211: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 21:39:47.590770: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 21:39:47.590836: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 21:39:47.590897: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 21:39:47.594876: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 21:39:47.594938: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 21:39:47.645953: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 2.00 Bytes
[2022-12-11 21:39:47.646372: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1024.00 Bytes
[2022-12-11 21:39:47.658851: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-11 21:39:47.658961: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 2
[2022-12-11 21:39:47.659011: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-11 21:39:47.664643: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 8.01 MB
[2022-12-11 21:39:47.671803: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:39:47.672813: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:39:47.672908: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 21:39:47.673585: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 21:39:47.673628: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 976.56 MB
[2022-12-11 21:39:47.[[676194[[[2022-12-11 21:39:472022-12-11 21:39:47: 2022-12-11 21:39:472022-12-11 21:39:472022-12-11 21:39:47..E...676213676212 676212676221676220: : /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu: : : EE:EEE  1980   /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu] /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::eager alloc mem 2.00 Bytes:::19801980
198019801980] ] ] ] ] eager alloc mem 2.00 Byteseager alloc mem 2.00 Byteseager alloc mem 2.00 Byteseager alloc mem 2.00 Byteseager alloc mem 2.00 Bytes




[2022-12-11 21:39:47.676629: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu[:[2022-12-11 21:39:471980[[2022-12-11 21:39:47.[] 2022-12-11 21:39:472022-12-11 21:39:47.6766452022-12-11 21:39:47eager alloc mem 1024.00 Bytes..676646: .
676650676650: E676655: : E : EE /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu: /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::1980] :19801980] eager alloc mem 1024.00 Bytes1980] ] eager alloc mem 1024.00 Bytes
] eager alloc mem 1024.00 Byteseager alloc mem 1024.00 Bytes
eager alloc mem 1024.00 Bytes


[2022-12-11 21:39:47.680968: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 2.00 Bytes
[2022-12-11 21:39:47.681296: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1024.00 Bytes
[2022-12-11 21:39:47.686738: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-11 21:39:47.686814: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 2
[2022-12-11 21:39:47.686858: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-11 21:39:47.687070: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-11 21:39:47.687154: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 2
[[2022-12-11 21:39:472022-12-11 21:39:47..687180687200: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::638638] ] eager release cuda mem 1024eager release cuda mem 400000000

[2022-12-11 21:39:47.687268: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 2
[2022-12-11 21:39:47.687316: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-11 21:39:47.687367: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-11 21:39:47.687432: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 2
[2022-12-11 21:39:47.687441: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:[6382022-12-11 21:39:47] .eager release cuda mem 1024687477
: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-11 21:39:47.687515: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:[6382022-12-11 21:39:47] .eager release cuda mem 2687518
: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-11 21:39:47.687577: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] [eager release cuda mem 4000000002022-12-11 21:39:47
.[6875952022-12-11 21:39:47: .E687592 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccE: [638/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc2022-12-11 21:39:47] :.eager release cuda mem 2638687660
] : eager release cuda mem 1024E
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu[:2022-12-11 21:39:471980.] 687729eager alloc mem 8.01 MB: 
E[ 2022-12-11 21:39:47/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc.:687748638: ] Eeager release cuda mem 400000000 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 2
[2022-12-11 21:39:47.687825: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-11 21:39:47.688901: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 8.01 MB
[2022-12-11 21:39:47.689406: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 8.01 MB
[2022-12-11 21:39:47.690081: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 8.01 MB
[2022-12-11 21:39:47.690587: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 8.01 MB
[2022-12-11 21:39:47.691408: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 8.01 MB
[2022-12-11 21:39:47.692056: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 8.01 MB
[2022-12-11 21:39:47.692282: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:39:47.692866: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:39:47.692974: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:39:47.693035: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:39:47.693225: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:39:47.693277: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:39:47.693311: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 21:39:47.693807: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:39:47.693889: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 21:39:47.693910: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:39:47.693975: [E2022-12-11 21:39:47 [./hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc2022-12-11 21:39:47693986:.: 638693993E] :  eager release cuda mem 625663E/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc
 :/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu638:] 1980eager release cuda mem 25855] 
eager alloc mem 25.25 KB
[2022-12-11 21:39:47.694066: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 976.56 MB[
2022-12-11 21:39:47.694086: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 21:39:47.694209: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:39:47.694290: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 21:39:47.694564: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 21:39:47.694605: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 976.56 MB
[2022-12-11 21:39:47.694701: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 21:39:47.694752: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:[19802022-12-11 21:39:47] .eager alloc mem 976.56 MB694766
: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 21:39:47.694815: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 976.56 MB
[2022-12-11 21:39:47.694964: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 21:39:47.695005: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 976.56 MB
[2022-12-11 21:39:47.697867: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:39:47.698813: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:39:47.698893: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 21:39:47.699559: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 21:39:47.699601: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 976.56 MB
[2022-12-11 21:39:47.703343: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:39:47.704268: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:39:47.704352: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 21:39:47.704906: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 21:39:47.704948: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 976.56 MB
[2022-12-11 21:39:47[[.[[[[2022-12-11 21:39:47[2022-12-11 21:39:478870262022-12-11 21:39:472022-12-11 21:39:472022-12-11 21:39:472022-12-11 21:39:47.2022-12-11 21:39:47.: ....887039.887038E887035887038887044887041: 887056:  : : : : E: E/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuEEEE E :    /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu1980/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:] ::::1980:1980eager alloc mem 611.00 KB1980198019801980] 1980] 
] ] ] ] eager alloc mem 611.00 KB] eager alloc mem 611.00 KBeager alloc mem 611.00 KBeager alloc mem 611.00 KBeager alloc mem 611.00 KBeager alloc mem 611.00 KB
eager alloc mem 611.00 KB





[2022-12-11 21:39:47.888101: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc[:2022-12-11 21:39:47638[.[] [2022-12-11 21:39:47888117[[2022-12-11 21:39:47[eager release cuda mem 6256632022-12-11 21:39:47.: 2022-12-11 21:39:472022-12-11 21:39:47.2022-12-11 21:39:47
.888128E..888130.888132:  888141888141: 888144: E/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc: : E: E :EE E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc638  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc [/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:] /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc2022-12-11 21:39:47:638eager release cuda mem 625663::638:.638] 
638638] 638888240] eager release cuda mem 625663] ] eager release cuda mem 625663] : eager release cuda mem 625663
eager release cuda mem 625663eager release cuda mem 625663
eager release cuda mem 625663E


[
 2022-12-11 21:39:47/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu.:8883571980[: [] 2022-12-11 21:39:47E2022-12-11 21:39:47eager alloc mem 611.00 KB. [.[
[888394/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu[2022-12-11 21:39:478883972022-12-11 21:39:472022-12-11 21:39:47: :2022-12-11 21:39:47.: ..E1980.888417E888421888424 ] 888429:  : : /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cueager alloc mem 611.00 KB: E/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuEE:
E :  1980 /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu1980/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu] /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:] ::eager alloc mem 611.00 KB:1980eager alloc mem 611.00 KB19801980
1980] 
] ] ] eager alloc mem 611.00 KBeager alloc mem 611.00 KBeager alloc mem 611.00 KBeager alloc mem 611.00 KB



[2022-12-11 21:39:47.889200: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:39:47.889244: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:39:47.889271: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:[19802022-12-11 21:39:47] .eager alloc mem 611.00 KB889288
: E[ 2022-12-11 21:39:47/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc[.:2022-12-11 21:39:47889316638.: [] 889323[E[2022-12-11 21:39:47[eager release cuda mem 625663: 2022-12-11 21:39:47 2022-12-11 21:39:47.2022-12-11 21:39:47
E./hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu.889335. 889337:889339: 889343/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc: 1980: E: :E] E E638 eager alloc mem 611.00 KB /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc ] [/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc
/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cceager release cuda mem 6256632022-12-11 21:39:47::638:
.638638] 638889415] ] eager release cuda mem 625663] : eager release cuda mem 625663eager release cuda mem 625663
eager release cuda mem 625663E

 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:39:47.889494: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:39:47.[8895232022-12-11 21:39:47[: [.2022-12-11 21:39:47E2022-12-11 21:39:47889528. .: 889532/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu889534E: ::  E1980E/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu ]  :/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cueager alloc mem 611.00 KB/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu1980:
:] 19801980eager alloc mem 611.00 KB] ] 
eager alloc mem 611.00 KBeager alloc mem 611.00 KB

[2022-12-11 21:39:47.890063: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:39:47.890131: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:39:47.890156: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:39:47.890225: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:39:47.890277: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:39:47.890344: E[ 2022-12-11 21:39:47/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu.:890353[1980: 2022-12-11 21:39:47] [E[.eager alloc mem 611.00 KB2022-12-11 21:39:47 2022-12-11 21:39:47890366
./hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc.: 890372:890374E: 638:  E] E/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc eager release cuda mem 625663 :/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc
/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc638::] 638638eager release cuda mem 625663] ] 
eager release cuda mem 625663eager release cuda mem 625663

[2022-12-11 21:39:47.890505: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980[] 2022-12-11 21:39:47eager alloc mem 611.00 KB.[[
8905272022-12-11 21:39:472022-12-11 21:39:47: ..E890533890534 : : /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuEE:  1980/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu] ::eager alloc mem 611.00 KB19801980
] ] eager alloc mem 611.00 KBeager alloc mem 611.00 KB

[2022-12-11 21:39:47.890907: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[[2022-12-11 21:39:472022-12-11 21:39:47..890972890976: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::6381980] ] eager release cuda mem 625663eager alloc mem 611.00 KB

[2022-12-11 21:39:47.891088: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:39:47.891160: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:39:47.891230: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:39:47.891297: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:39:47.891347: E[ [2022-12-11 21:39:47/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc[2022-12-11 21:39:47.:2022-12-11 21:39:47.891357638.891358: ] 891364: Eeager release cuda mem 625663: E 
E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:638:638] 1980] eager release cuda mem 625663] eager release cuda mem 625663
eager alloc mem 611.00 KB

[2022-12-11 21:39:47.891446: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:39:47.[8914782022-12-11 21:39:47: .E891482 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE: 1980/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu] :eager alloc mem 611.00 KB1980
] eager alloc mem 611.00 KB
[2022-12-11 21:39:47.891795: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:39:47.891849: E [/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc2022-12-11 21:39:47:.638891864] : eager release cuda mem 625663E
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:39:47.891944: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:39:47.892006: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:39:47.892073: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:39:47.892198: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:39:47.892225: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:39:47.892265: E [/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu2022-12-11 21:39:47:[.19802022-12-11 21:39:47892276[] .: 2022-12-11 21:39:47eager alloc mem 611.00 KB892284E.
:  892291E/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:  :E/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc638 :] /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu638eager release cuda mem 625663:] 
1980eager release cuda mem 625663] 
eager alloc mem 611.00 KB
[2022-12-11 21:39:47.892399: E[ 2022-12-11 21:39:47/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu.:8924091980: ] Eeager alloc mem 611.00 KB 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:39:47.892665: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:39:47.892696: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:39:47.892732: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:39:47.892763: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:39:47.892849: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:39:47.892916: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:39:47.893065: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:39:47.893123[: 2022-12-11 21:39:47E. 893131/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc: :E638 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cueager release cuda mem 625663:
1980] eager alloc mem 611.00 KB
[2022-12-11 21:39:47.893187: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc[:[2022-12-11 21:39:476382022-12-11 21:39:47.] .893201eager release cuda mem 625663893204: 
: EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::6381980] ] eager release cuda mem 625663eager alloc mem 611.00 KB

[2022-12-11 21:39:47.893268: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:39:47.893299: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:39:47[.2022-12-11 21:39:47893504.: 893516E:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc :/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc638:] 638eager release cuda mem 625663] 
eager release cuda mem 625663
[2022-12-11 21:39:47[.2022-12-11 21:39:47893594.: 893598E:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc :/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc638:] 638eager release cuda mem 8399996] 
eager release cuda mem 8399996
[2022-12-11 21:39:47.893692: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:39:47.893728: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 8399996
[2022-12-11 21:39:47.893923: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:39:47.893959: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 8399996
[2022-12-11 21:39:47.894013: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[[2022-12-11 21:39:472022-12-11 21:39:47..894045894048: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::[6386382022-12-11 21:39:47] ] .eager release cuda mem 625663eager release cuda mem 8399996894075

: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638[] 2022-12-11 21:39:47eager release cuda mem 625663.
894111: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638[] 2022-12-11 21:39:47eager release cuda mem 8399996.
894134: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 8399996
[2022-12-11 21:39:47.894430: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 1999999 / 100000000 nodes ( 2.00 %~2.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 98000001 / 100000000 nodes ( 98.00 %) | 976.56 MB | 0.308002 secs 
[2022-12-11 21:39:47.894898: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 1999999 / 100000000 nodes ( 2.00 %~2.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 98000001 / 100000000 nodes ( 98.00 %) | 976.56 MB | 0.313055 secs 
[2022-12-11 21:39:47.895470: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 1999999 / 100000000 nodes ( 2.00 %~2.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 98000001 / 100000000 nodes ( 98.00 %) | 976.56 MB | 0.313058 secs 
[2022-12-11 21:39:47.895917: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:39:47.896030: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:39:47.896275: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 1999999 / 100000000 nodes ( 2.00 %~2.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 98000001 / 100000000 nodes ( 98.00 %) | 976.56 MB | 0.310015 secs 
[2022-12-11 21:39:47.896683: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 1999999 / 100000000 nodes ( 2.00 %~2.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 98000001 / 100000000 nodes ( 98.00 %) | 976.56 MB | 0.315124 secs 
[2022-12-11 21:39:47.896765: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:[17932022-12-11 21:39:47] .Collaborative GPU cache (policy: rep_cache) | local 1999999 / 100000000 nodes ( 2.00 %~2.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 98000001 / 100000000 nodes ( 98.00 %) | 976.56 MB | 0.314497 secs 896787
: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:39:47.896871: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:39:47.897214: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 1999999 / 100000000 nodes ( 2.00 %~2.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 98000001 / 100000000 nodes ( 98.00 %) | 976.56 MB | 0.315287 secs 
[2022-12-11 21:39:47.897622: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:39:47.897692: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:39:47.898440: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:39:47.898507: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:39:47.899257: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:39:47.899303: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 8399996
[2022-12-11 21:39:47.900067: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 1999999 / 100000000 nodes ( 2.00 %~2.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 98000001 / 100000000 nodes ( 98.00 %) | 976.56 MB | 0.333651 secs 
[HCTR][21:39:47.900][ERROR][RK0][main]: replica 1 calling init per replica done, doing barrier
[HCTR][21:39:47.900][ERROR][RK0][main]: replica 5 calling init per replica done, doing barrier
[HCTR][21:39:47.900][ERROR][RK0][tid #139811746719488]: replica 3 calling init per replica done, doing barrier
[HCTR][21:39:47.900][ERROR][RK0][main]: replica 6 calling init per replica done, doing barrier
[HCTR][21:39:47.900][ERROR][RK0][main]: replica 0 calling init per replica done, doing barrier
[HCTR][21:39:47.900][ERROR][RK0][main]: replica 7 calling init per replica done, doing barrier
[HCTR][21:39:47.900][ERROR][RK0][tid #139811738326784]: replica 2 calling init per replica done, doing barrier
[HCTR][21:39:47.900][ERROR][RK0][main]: replica 4 calling init per replica done, doing barrier
[HCTR][21:39:47.900][ERROR][RK0][main]: replica 4 calling init per replica done, doing barrier done
[HCTR][21:39:47.900][ERROR][RK0][main]: replica 6 calling init per replica done, doing barrier done
[HCTR][21:39:47.900][ERROR][RK0][main]: replica 1 calling init per replica done, doing barrier done
[HCTR][21:39:47.900][ERROR][RK0][tid #139811738326784]: replica 2 calling init per replica done, doing barrier done
[HCTR][21:39:47.900][ERROR][RK0][main]: replica 5 calling init per replica done, doing barrier done
[HCTR][21:39:47.900][ERROR][RK0][main]: replica 7 calling init per replica done, doing barrier done
[HCTR][21:39:47.900][ERROR][RK0][tid #139811746719488]: replica 3 calling init per replica done, doing barrier done
[HCTR][21:39:47.900][ERROR][RK0][main]: replica 0 calling init per replica done, doing barrier done
[HCTR][21:39:47.900][ERROR][RK0][main]: init per replica done
[HCTR][21:39:47.900][ERROR][RK0][main]: init per replica done
[HCTR][21:39:47.900][ERROR][RK0][main]: init per replica done
[HCTR][21:39:47.900][ERROR][RK0][tid #139811738326784]: init per replica done
[HCTR][21:39:47.900][ERROR][RK0][main]: init per replica done
[HCTR][21:39:47.900][ERROR][RK0][main]: init per replica done
[HCTR][21:39:47.900][ERROR][RK0][tid #139811746719488]: init per replica done
[HCTR][21:39:47.903][ERROR][RK0][main]: init per replica done








