2022-12-12 02:13:01.939270: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:01.946391: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:01.951352: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:01.957762: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:01.965140: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:01.976711: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:01.984305: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:01.989500: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.045311: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.048702: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.051288: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.059786: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.063547: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.065018: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.066845: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.068049: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.068156: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.069695: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.069915: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.071171: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.071598: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.072624: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.073216: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.074222: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.074865: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.075865: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.076546: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.077378: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.078297: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.078859: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.080113: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.080354: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.082829: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.083996: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.084914: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.085828: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.086759: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.087802: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.088818: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.089833: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.094955: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 02:13:02.096444: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.097611: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.098595: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.099611: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.100597: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.101628: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.103149: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.104080: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.105350: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.105536: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.106445: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.108073: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.108341: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.108440: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.110086: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.110358: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.112376: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.112817: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.115236: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.115922: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.116669: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.118296: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.119060: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.120182: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.120323: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.121471: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.122365: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.123674: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.123810: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.124209: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.124716: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.125533: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.126842: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.127063: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.127417: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.127611: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.129105: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.130620: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.130845: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.131104: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.131421: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.143859: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.143947: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.144033: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.144437: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.146434: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.146713: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.146750: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.146975: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.151707: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.153006: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.153634: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.153758: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.154135: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.155347: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.156002: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.156931: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.157483: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.158250: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.158871: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.171352: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.185424: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.194561: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.195071: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.195780: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.196001: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.196088: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.198475: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.199564: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.199786: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.199983: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.200934: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.202330: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.203124: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.203676: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.203726: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.205830: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.206292: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.207074: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.207371: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.207788: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.209250: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.209572: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.210875: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.211242: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.211354: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.212767: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.213103: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.214596: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.215152: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.215194: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.216628: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.217088: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.217905: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.218551: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.218643: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.219953: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.220343: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.221412: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.221886: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.221930: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.223161: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.223770: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.224668: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.225171: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.225213: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.226624: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.227248: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.229118: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.229206: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.229895: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.230489: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.230852: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.232364: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.232506: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.233398: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.233914: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.234199: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.236036: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.236202: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.237055: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.237351: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.237720: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.237750: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.239636: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.239880: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.241198: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.241430: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.241482: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.241500: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.242540: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.243705: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.244064: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.245694: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.245806: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.246072: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.246109: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.247545: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.248570: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.248783: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.250437: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.250605: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.250755: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.250936: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.252170: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.254420: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.254961: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.255111: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.255306: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.255363: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.255678: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.256591: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.259064: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.259559: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.259696: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.260058: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.260156: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.260395: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.261634: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.263768: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.264357: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.264701: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.264840: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.265691: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.266067: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.267499: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.267845: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.268143: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.268326: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.268792: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 02:13:02.270101: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.270155: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.271231: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.271817: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.272088: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.272329: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.274454: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.274513: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.275587: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.275963: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.276202: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.276544: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.278270: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.278524: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.279698: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.280138: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.280327: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.280504: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.282121: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.282160: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 02:13:02.282385: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.283625: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.284148: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.284392: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.284514: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.286371: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.286689: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.288414: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.288603: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.290041: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.290753: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.291485: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.291773: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 02:13:02.291825: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 02:13:02.291992: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.293495: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.294441: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.295489: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.296069: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.297955: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.298910: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.299343: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.300864: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.300876: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 02:13:02.300908: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.302189: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.303511: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.304790: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.304834: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.306961: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.307671: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.308998: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.309024: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.309613: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.311361: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.311775: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.314414: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.316244: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.316695: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.319083: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.321152: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.321834: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.354589: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.355432: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.366761: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.367306: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.373270: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.373620: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.378846: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.382751: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 02:13:02.391401: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.417114: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 02:13:02.418231: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.424927: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.425991: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.430836: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:02.470154: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:03.442506: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:03.443170: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:03.443833: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:03.444324: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 02:13:03.444381: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-12 02:13:03.464463: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:03.465108: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:03.465613: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:03.466200: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:03.466711: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:03.467202: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-12 02:13:03.511583: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 02:13:03.511796: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 02:13:03.555762: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12340
2022-12-12 02:13:03.695142: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:03.695756: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:03.696295: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:03.696769: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 02:13:03.696820: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-12 02:13:03.713561: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:03.714202: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:03.714701: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:03.715295: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:03.716053: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:03.716528: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:1/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-12 02:13:03.722372: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:03.722545: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:03.723357: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:03.723522: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:03.724373: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:03.724528: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:03.725214: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 02:13:03.725273: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-12 02:13:03.725643: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 02:13:03.725691: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-12 02:13:03.742731: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:03.742875: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:03.743521: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:03.743947: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:03.747106: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:03.747580: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:03.748300: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:03.748651: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:03.749103: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:03.749727: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:03.750471: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:03.750836: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:03.751571: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:03.751951: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:03.752530: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 02:13:03.752579: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-12 02:13:03.752737: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 02:13:03.752779: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-12 02:13:03.753565: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:03.753970: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:03.754537: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:3/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-12 02:13:03.754764: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:2/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-12 02:13:03.754821: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:03.755445: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:03.756161: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:03.756634: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 02:13:03.756684: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-12 02:13:03.767833: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:03.768451: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:03.768978: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:03.769447: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 02:13:03.769494: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-12 02:13:03.769568: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:03.769887: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:03.770825: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:03.771057: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:03.771761: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:03.772085: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:03.772698: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:03.772834: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:03.773403: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:03.774297: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:03.774363: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:03.774825: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:03.775842: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:03.775888: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:5/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-12 02:13:03.776226: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:4/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-12 02:13:03.776643: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:03.777170: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:03.777633: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:6/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-12 02:13:03.785718: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:03.786352: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:03.786856: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:03.787492: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:03.788096: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:13:03.788582: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:7/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-12 02:13:03.796310: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 02:13:03.796499: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 02:13:03.798337: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12343
2022-12-12 02:13:03.800486: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 02:13:03.800628: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 02:13:03.802466: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12341
2022-12-12 02:13:03.818196: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 02:13:03.818233: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 02:13:03.818405: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 02:13:03.818425: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 02:13:03.819028: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 02:13:03.819201: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 02:13:03.820300: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12345
2022-12-12 02:13:03.820391: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12344
2022-12-12 02:13:03.821006: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12346
2022-12-12 02:13:03.831695: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 02:13:03.831866: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 02:13:03.833752: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12347
2022-12-12 02:13:03.856058: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 02:13:03.856257: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 02:13:03.858069: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12342
[HCTR][02:13:05.129][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][02:13:05.129][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][02:13:05.129][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][02:13:05.130][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][02:13:05.130][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][02:13:05.130][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][02:13:05.130][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][02:13:05.130][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 1it [00:01,  1.52s/it]warmup run: 1it [00:01,  1.52s/it]warmup run: 1it [00:01,  1.51s/it]warmup run: 1it [00:01,  1.51s/it]warmup run: 96it [00:01, 82.28it/s]warmup run: 1it [00:01,  1.49s/it]warmup run: 97it [00:01, 82.90it/s]warmup run: 100it [00:01, 86.01it/s]warmup run: 99it [00:01, 85.34it/s]warmup run: 1it [00:01,  1.48s/it]warmup run: 1it [00:01,  1.52s/it]warmup run: 1it [00:01,  1.48s/it]warmup run: 196it [00:01, 182.65it/s]warmup run: 98it [00:01, 85.30it/s]warmup run: 196it [00:01, 181.69it/s]warmup run: 199it [00:01, 185.09it/s]warmup run: 199it [00:01, 185.83it/s]warmup run: 97it [00:01, 84.98it/s]warmup run: 95it [00:01, 81.58it/s]warmup run: 98it [00:01, 85.68it/s]warmup run: 297it [00:01, 294.12it/s]warmup run: 198it [00:01, 186.60it/s]warmup run: 296it [00:01, 291.56it/s]warmup run: 300it [00:01, 296.72it/s]warmup run: 301it [00:01, 298.94it/s]warmup run: 197it [00:01, 186.88it/s]warmup run: 191it [00:01, 177.67it/s]warmup run: 196it [00:01, 185.17it/s]warmup run: 398it [00:01, 409.82it/s]warmup run: 297it [00:01, 296.13it/s]warmup run: 396it [00:01, 405.77it/s]warmup run: 400it [00:01, 410.57it/s]warmup run: 403it [00:01, 415.50it/s]warmup run: 298it [00:01, 299.59it/s]warmup run: 287it [00:01, 283.37it/s]warmup run: 295it [00:01, 295.51it/s]warmup run: 500it [00:02, 523.71it/s]warmup run: 399it [00:01, 414.06it/s]warmup run: 497it [00:02, 517.90it/s]warmup run: 501it [00:02, 523.02it/s]warmup run: 505it [00:02, 528.98it/s]warmup run: 398it [00:01, 414.28it/s]warmup run: 384it [00:01, 393.92it/s]warmup run: 389it [00:01, 400.75it/s]warmup run: 605it [00:02, 633.75it/s]warmup run: 502it [00:01, 529.67it/s]warmup run: 599it [00:02, 623.07it/s]warmup run: 605it [00:02, 631.34it/s]warmup run: 608it [00:02, 634.61it/s]warmup run: 499it [00:01, 527.08it/s]warmup run: 480it [00:02, 499.60it/s]warmup run: 477it [00:01, 484.29it/s]warmup run: 709it [00:02, 727.04it/s]warmup run: 606it [00:02, 637.55it/s]warmup run: 701it [00:02, 713.45it/s]warmup run: 708it [00:02, 723.27it/s]warmup run: 712it [00:02, 727.42it/s]warmup run: 602it [00:02, 633.27it/s]warmup run: 578it [00:02, 600.02it/s]warmup run: 581it [00:02, 601.15it/s]warmup run: 813it [00:02, 803.86it/s]warmup run: 710it [00:02, 730.66it/s]warmup run: 803it [00:02, 788.86it/s]warmup run: 811it [00:02, 798.80it/s]warmup run: 816it [00:02, 803.21it/s]warmup run: 704it [00:02, 722.01it/s]warmup run: 675it [00:02, 685.48it/s]warmup run: 686it [00:02, 703.42it/s]warmup run: 914it [00:02, 848.90it/s]warmup run: 814it [00:02, 806.56it/s]warmup run: 906it [00:02, 851.05it/s]warmup run: 913it [00:02, 856.29it/s]warmup run: 919it [00:02, 861.30it/s]warmup run: 804it [00:02, 791.02it/s]warmup run: 769it [00:02, 744.44it/s]warmup run: 791it [00:02, 787.90it/s]warmup run: 917it [00:02, 864.14it/s]warmup run: 1014it [00:02, 879.01it/s]warmup run: 1007it [00:02, 888.27it/s]warmup run: 1016it [00:02, 902.20it/s]warmup run: 904it [00:02, 845.19it/s]warmup run: 1021it [00:02, 899.74it/s]warmup run: 863it [00:02, 795.32it/s]warmup run: 893it [00:02, 847.25it/s]warmup run: 1019it [00:02, 900.45it/s]warmup run: 1113it [00:02, 902.22it/s]warmup run: 1110it [00:02, 927.44it/s]warmup run: 1119it [00:02, 935.70it/s]warmup run: 1004it [00:02, 885.35it/s]warmup run: 1122it [00:02, 914.21it/s]warmup run: 959it [00:02, 839.14it/s]warmup run: 994it [00:02, 891.09it/s]warmup run: 1120it [00:02, 929.57it/s]warmup run: 1211it [00:02, 913.34it/s]warmup run: 1212it [00:02, 953.42it/s]warmup run: 1222it [00:02, 961.91it/s]warmup run: 1104it [00:02, 908.00it/s]warmup run: 1222it [00:02, 936.44it/s]warmup run: 1057it [00:02, 877.25it/s]warmup run: 1094it [00:02, 906.19it/s]warmup run: 1222it [00:02, 954.84it/s]warmup run: 1313it [00:02, 942.33it/s]warmup run: 1315it [00:02, 973.80it/s]warmup run: 1325it [00:02, 980.53it/s]warmup run: 1203it [00:02, 923.57it/s]warmup run: 1324it [00:02, 959.53it/s]warmup run: 1157it [00:02, 910.25it/s]warmup run: 1193it [00:02, 893.92it/s]warmup run: 1324it [00:02, 971.42it/s]warmup run: 1414it [00:02, 959.51it/s]warmup run: 1418it [00:02, 988.15it/s]warmup run: 1428it [00:02, 989.09it/s]warmup run: 1427it [00:02, 979.64it/s]warmup run: 1258it [00:02, 937.75it/s]warmup run: 1301it [00:02, 926.89it/s]warmup run: 1295it [00:02, 928.60it/s]warmup run: 1426it [00:02, 984.57it/s]warmup run: 1516it [00:03, 974.47it/s]warmup run: 1521it [00:03, 998.07it/s]warmup run: 1530it [00:03, 992.92it/s]warmup run: 1358it [00:02, 955.73it/s]warmup run: 1528it [00:03, 979.54it/s]warmup run: 1398it [00:02, 924.20it/s]warmup run: 1528it [00:03, 991.70it/s]warmup run: 1392it [00:02, 929.30it/s]warmup run: 1617it [00:03, 984.82it/s]warmup run: 1623it [00:03, 999.05it/s]warmup run: 1633it [00:03, 1001.12it/s]warmup run: 1460it [00:03, 972.87it/s]warmup run: 1628it [00:03, 972.09it/s]warmup run: 1493it [00:03, 925.40it/s]warmup run: 1630it [00:03, 999.44it/s]warmup run: 1494it [00:03, 952.68it/s]warmup run: 1719it [00:03, 992.25it/s]warmup run: 1725it [00:03, 1003.75it/s]warmup run: 1735it [00:03, 1000.34it/s]warmup run: 1560it [00:03, 973.45it/s]warmup run: 1727it [00:03, 972.17it/s]warmup run: 1588it [00:03, 925.29it/s]warmup run: 1732it [00:03, 999.54it/s]warmup run: 1596it [00:03, 969.49it/s]warmup run: 1821it [00:03, 998.47it/s]warmup run: 1827it [00:03, 1003.90it/s]warmup run: 1837it [00:03, 999.11it/s] warmup run: 1661it [00:03, 982.24it/s]warmup run: 1826it [00:03, 964.36it/s]warmup run: 1682it [00:03, 925.40it/s]warmup run: 1833it [00:03, 1002.54it/s]warmup run: 1697it [00:03, 978.59it/s]warmup run: 1922it [00:03, 1001.75it/s]warmup run: 1929it [00:03, 1007.65it/s]warmup run: 1938it [00:03, 998.96it/s]warmup run: 1762it [00:03, 987.61it/s]warmup run: 1924it [00:03, 968.48it/s]warmup run: 1776it [00:03, 924.18it/s]warmup run: 1934it [00:03, 1002.67it/s]warmup run: 1799it [00:03, 987.99it/s]warmup run: 2028it [00:03, 1018.43it/s]warmup run: 2036it [00:03, 1023.80it/s]warmup run: 2046it [00:03, 1021.50it/s]warmup run: 1862it [00:03, 989.00it/s]warmup run: 2030it [00:03, 993.77it/s]warmup run: 1870it [00:03, 925.17it/s]warmup run: 2042it [00:03, 1025.37it/s]warmup run: 1901it [00:03, 995.62it/s]warmup run: 2151it [00:03, 1080.31it/s]warmup run: 2156it [00:03, 1076.22it/s]warmup run: 2169it [00:03, 1081.96it/s]warmup run: 1962it [00:03, 990.36it/s]warmup run: 2150it [00:03, 1053.56it/s]warmup run: 1963it [00:03, 926.14it/s]warmup run: 2164it [00:03, 1081.35it/s]warmup run: 2003it [00:03, 1001.72it/s]warmup run: 2274it [00:03, 1122.97it/s]warmup run: 2277it [00:03, 1113.95it/s]warmup run: 2292it [00:03, 1125.46it/s]warmup run: 2074it [00:03, 1028.39it/s]warmup run: 2270it [00:03, 1095.61it/s]warmup run: 2068it [00:03, 960.92it/s]warmup run: 2286it [00:03, 1121.43it/s]warmup run: 2123it [00:03, 1059.16it/s]warmup run: 2396it [00:03, 1150.72it/s]warmup run: 2398it [00:03, 1140.24it/s]warmup run: 2415it [00:03, 1155.83it/s]warmup run: 2195it [00:03, 1081.13it/s]warmup run: 2390it [00:03, 1124.34it/s]warmup run: 2181it [00:03, 1010.48it/s]warmup run: 2409it [00:03, 1152.21it/s]warmup run: 2243it [00:03, 1100.92it/s]warmup run: 2519it [00:03, 1173.93it/s]warmup run: 2519it [00:03, 1158.50it/s]warmup run: 2538it [00:03, 1177.74it/s]warmup run: 2316it [00:03, 1118.22it/s]warmup run: 2510it [00:03, 1145.93it/s]warmup run: 2295it [00:03, 1046.72it/s]warmup run: 2530it [00:03, 1168.22it/s]warmup run: 2363it [00:03, 1129.73it/s]warmup run: 2641it [00:04, 1184.70it/s]warmup run: 2639it [00:04, 1169.10it/s]warmup run: 2661it [00:04, 1191.38it/s]warmup run: 2437it [00:03, 1143.78it/s]warmup run: 2631it [00:04, 1162.88it/s]warmup run: 2409it [00:03, 1072.58it/s]warmup run: 2650it [00:04, 1177.07it/s]warmup run: 2483it [00:03, 1149.04it/s]warmup run: 2761it [00:04, 1186.73it/s]warmup run: 2760it [00:04, 1179.94it/s]warmup run: 2785it [00:04, 1204.35it/s]warmup run: 2558it [00:04, 1162.53it/s]warmup run: 2752it [00:04, 1174.41it/s]warmup run: 2517it [00:04, 1002.00it/s]warmup run: 2772it [00:04, 1187.22it/s]warmup run: 2603it [00:04, 1162.10it/s]warmup run: 2881it [00:04, 1187.84it/s]warmup run: 2881it [00:04, 1187.20it/s]warmup run: 2908it [00:04, 1209.50it/s]warmup run: 2678it [00:04, 1172.65it/s]warmup run: 2871it [00:04, 1177.50it/s]warmup run: 2893it [00:04, 1191.82it/s]warmup run: 2721it [00:04, 1166.84it/s]warmup run: 3000it [00:04, 695.42it/s] warmup run: 3000it [00:04, 1187.23it/s]warmup run: 3000it [00:04, 689.25it/s] warmup run: 2619it [00:04, 936.01it/s] warmup run: 3000it [00:04, 689.81it/s] warmup run: 2798it [00:04, 1179.75it/s]warmup run: 2991it [00:04, 1181.87it/s]warmup run: 3000it [00:04, 688.48it/s] warmup run: 3000it [00:04, 695.91it/s] warmup run: 2840it [00:04, 1172.83it/s]warmup run: 2737it [00:04, 1002.40it/s]warmup run: 2920it [00:04, 1189.16it/s]warmup run: 2960it [00:04, 1177.98it/s]warmup run: 2859it [00:04, 1063.07it/s]warmup run: 3000it [00:04, 681.10it/s] warmup run: 3000it [00:04, 686.26it/s] warmup run: 2980it [00:04, 1102.86it/s]warmup run: 3000it [00:04, 667.82it/s] 

warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]



warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]

warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   5%|         | 161/3000 [00:00<00:01, 1609.30it/s]warmup should be done:   6%|         | 167/3000 [00:00<00:01, 1665.42it/s]warmup should be done:   6%|         | 165/3000 [00:00<00:01, 1642.33it/s]warmup should be done:   6%|         | 166/3000 [00:00<00:01, 1658.12it/s]warmup should be done:   5%|         | 164/3000 [00:00<00:01, 1634.90it/s]warmup should be done:   5%|         | 163/3000 [00:00<00:01, 1627.10it/s]warmup should be done:   6%|         | 168/3000 [00:00<00:01, 1673.03it/s]warmup should be done:   5%|         | 163/3000 [00:00<00:01, 1620.47it/s]warmup should be done:  11%|         | 334/3000 [00:00<00:01, 1670.33it/s]warmup should be done:  11%|         | 330/3000 [00:00<00:01, 1645.12it/s]warmup should be done:  11%|         | 337/3000 [00:00<00:01, 1681.41it/s]warmup should be done:  11%|         | 329/3000 [00:00<00:01, 1643.63it/s]warmup should be done:  11%|         | 329/3000 [00:00<00:01, 1646.86it/s]warmup should be done:  11%|         | 329/3000 [00:00<00:01, 1640.00it/s]warmup should be done:  11%|         | 336/3000 [00:00<00:01, 1674.64it/s]warmup should be done:  11%|         | 326/3000 [00:00<00:01, 1622.41it/s]warmup should be done:  16%|        | 495/3000 [00:00<00:01, 1652.28it/s]warmup should be done:  16%|        | 495/3000 [00:00<00:01, 1648.36it/s]warmup should be done:  16%|        | 493/3000 [00:00<00:01, 1636.77it/s]warmup should be done:  17%|        | 502/3000 [00:00<00:01, 1667.24it/s]warmup should be done:  17%|        | 504/3000 [00:00<00:01, 1672.71it/s]warmup should be done:  16%|        | 495/3000 [00:00<00:01, 1639.30it/s]warmup should be done:  16%|        | 489/3000 [00:00<00:01, 1606.40it/s]warmup should be done:  17%|        | 506/3000 [00:00<00:01, 1644.30it/s]warmup should be done:  22%|       | 662/3000 [00:00<00:01, 1656.77it/s]warmup should be done:  22%|       | 660/3000 [00:00<00:01, 1647.89it/s]warmup should be done:  22%|       | 657/3000 [00:00<00:01, 1635.98it/s]warmup should be done:  22%|       | 672/3000 [00:00<00:01, 1672.83it/s]warmup should be done:  22%|       | 659/3000 [00:00<00:01, 1634.59it/s]warmup should be done:  22%|       | 669/3000 [00:00<00:01, 1661.03it/s]warmup should be done:  22%|       | 653/3000 [00:00<00:01, 1616.21it/s]warmup should be done:  22%|       | 671/3000 [00:00<00:01, 1600.84it/s]warmup should be done:  28%|       | 829/3000 [00:00<00:01, 1659.34it/s]warmup should be done:  28%|       | 826/3000 [00:00<00:01, 1649.66it/s]warmup should be done:  27%|       | 822/3000 [00:00<00:01, 1639.74it/s]warmup should be done:  28%|       | 840/3000 [00:00<00:01, 1669.62it/s]warmup should be done:  27%|       | 823/3000 [00:00<00:01, 1631.25it/s]warmup should be done:  27%|       | 817/3000 [00:00<00:01, 1621.62it/s]warmup should be done:  28%|       | 836/3000 [00:00<00:01, 1649.85it/s]warmup should be done:  28%|       | 832/3000 [00:00<00:01, 1596.77it/s]warmup should be done:  33%|      | 995/3000 [00:00<00:01, 1656.04it/s]warmup should be done:  33%|      | 986/3000 [00:00<00:01, 1636.32it/s]warmup should be done:  33%|      | 991/3000 [00:00<00:01, 1643.70it/s]warmup should be done:  34%|      | 1007/3000 [00:00<00:01, 1663.05it/s]warmup should be done:  33%|      | 980/3000 [00:00<00:01, 1622.83it/s]warmup should be done:  33%|      | 987/3000 [00:00<00:01, 1623.56it/s]warmup should be done:  33%|      | 1001/3000 [00:00<00:01, 1638.53it/s]warmup should be done:  33%|      | 992/3000 [00:00<00:01, 1593.28it/s]warmup should be done:  39%|      | 1161/3000 [00:00<00:01, 1656.81it/s]warmup should be done:  38%|      | 1151/3000 [00:00<00:01, 1637.51it/s]warmup should be done:  39%|      | 1156/3000 [00:00<00:01, 1642.12it/s]warmup should be done:  39%|      | 1174/3000 [00:00<00:01, 1661.65it/s]warmup should be done:  38%|      | 1143/3000 [00:00<00:01, 1621.03it/s]warmup should be done:  38%|      | 1150/3000 [00:00<00:01, 1621.59it/s]warmup should be done:  39%|      | 1165/3000 [00:00<00:01, 1634.39it/s]warmup should be done:  38%|      | 1155/3000 [00:00<00:01, 1602.87it/s]warmup should be done:  44%|     | 1327/3000 [00:00<00:01, 1656.69it/s]warmup should be done:  44%|     | 1316/3000 [00:00<00:01, 1639.22it/s]warmup should be done:  45%|     | 1341/3000 [00:00<00:00, 1662.03it/s]warmup should be done:  44%|     | 1321/3000 [00:00<00:01, 1635.57it/s]warmup should be done:  44%|     | 1306/3000 [00:00<00:01, 1622.93it/s]warmup should be done:  44%|     | 1313/3000 [00:00<00:01, 1620.35it/s]warmup should be done:  44%|     | 1329/3000 [00:00<00:01, 1627.60it/s]warmup should be done:  44%|     | 1319/3000 [00:00<00:01, 1611.50it/s]warmup should be done:  50%|     | 1493/3000 [00:00<00:00, 1653.58it/s]warmup should be done:  49%|     | 1482/3000 [00:00<00:00, 1645.23it/s]warmup should be done:  50%|     | 1485/3000 [00:00<00:00, 1632.44it/s]warmup should be done:  49%|     | 1469/3000 [00:00<00:00, 1624.22it/s]warmup should be done:  49%|     | 1476/3000 [00:00<00:00, 1616.04it/s]warmup should be done:  50%|     | 1508/3000 [00:00<00:00, 1645.40it/s]warmup should be done:  50%|     | 1492/3000 [00:00<00:00, 1624.74it/s]warmup should be done:  49%|     | 1481/3000 [00:00<00:00, 1611.54it/s]warmup should be done:  55%|    | 1659/3000 [00:01<00:00, 1653.72it/s]warmup should be done:  55%|    | 1649/3000 [00:01<00:00, 1651.85it/s]warmup should be done:  54%|    | 1632/3000 [00:01<00:00, 1625.63it/s]warmup should be done:  55%|    | 1649/3000 [00:01<00:00, 1629.75it/s]warmup should be done:  56%|    | 1675/3000 [00:01<00:00, 1652.20it/s]warmup should be done:  55%|    | 1638/3000 [00:01<00:00, 1612.14it/s]warmup should be done:  55%|    | 1655/3000 [00:01<00:00, 1619.20it/s]warmup should be done:  55%|    | 1644/3000 [00:01<00:00, 1614.25it/s]warmup should be done:  61%|    | 1825/3000 [00:01<00:00, 1655.23it/s]warmup should be done:  61%|    | 1816/3000 [00:01<00:00, 1656.85it/s]warmup should be done:  60%|    | 1795/3000 [00:01<00:00, 1626.89it/s]warmup should be done:  60%|    | 1812/3000 [00:01<00:00, 1623.41it/s]warmup should be done:  61%|   | 1842/3000 [00:01<00:00, 1655.95it/s]warmup should be done:  60%|    | 1801/3000 [00:01<00:00, 1615.58it/s]warmup should be done:  61%|    | 1818/3000 [00:01<00:00, 1620.53it/s]warmup should be done:  60%|    | 1807/3000 [00:01<00:00, 1618.22it/s]warmup should be done:  66%|   | 1991/3000 [00:01<00:00, 1655.69it/s]warmup should be done:  66%|   | 1982/3000 [00:01<00:00, 1651.25it/s]warmup should be done:  65%|   | 1958/3000 [00:01<00:00, 1627.76it/s]warmup should be done:  66%|   | 1978/3000 [00:01<00:00, 1631.79it/s]warmup should be done:  67%|   | 2009/3000 [00:01<00:00, 1659.21it/s]warmup should be done:  66%|   | 1965/3000 [00:01<00:00, 1620.91it/s]warmup should be done:  66%|   | 1981/3000 [00:01<00:00, 1621.10it/s]warmup should be done:  66%|   | 1970/3000 [00:01<00:00, 1620.75it/s]warmup should be done:  72%|  | 2157/3000 [00:01<00:00, 1656.21it/s]warmup should be done:  72%|  | 2148/3000 [00:01<00:00, 1647.19it/s]warmup should be done:  71%|   | 2121/3000 [00:01<00:00, 1627.14it/s]warmup should be done:  71%|  | 2144/3000 [00:01<00:00, 1638.87it/s]warmup should be done:  73%|  | 2176/3000 [00:01<00:00, 1660.13it/s]warmup should be done:  71%|   | 2129/3000 [00:01<00:00, 1625.45it/s]warmup should be done:  71%|  | 2144/3000 [00:01<00:00, 1622.45it/s]warmup should be done:  71%|   | 2133/3000 [00:01<00:00, 1622.11it/s]warmup should be done:  77%|  | 2323/3000 [00:01<00:00, 1655.69it/s]warmup should be done:  76%|  | 2284/3000 [00:01<00:00, 1625.95it/s]warmup should be done:  77%|  | 2313/3000 [00:01<00:00, 1639.80it/s]warmup should be done:  77%|  | 2309/3000 [00:01<00:00, 1642.01it/s]warmup should be done:  76%|  | 2292/3000 [00:01<00:00, 1626.25it/s]warmup should be done:  78%|  | 2343/3000 [00:01<00:00, 1655.89it/s]warmup should be done:  77%|  | 2307/3000 [00:01<00:00, 1622.35it/s]warmup should be done:  77%|  | 2296/3000 [00:01<00:00, 1620.49it/s]warmup should be done:  83%| | 2489/3000 [00:01<00:00, 1653.36it/s]warmup should be done:  82%| | 2447/3000 [00:01<00:00, 1622.98it/s]warmup should be done:  83%| | 2477/3000 [00:01<00:00, 1637.26it/s]warmup should be done:  82%| | 2475/3000 [00:01<00:00, 1645.73it/s]warmup should be done:  82%| | 2457/3000 [00:01<00:00, 1631.08it/s]warmup should be done:  84%| | 2509/3000 [00:01<00:00, 1653.44it/s]warmup should be done:  82%| | 2470/3000 [00:01<00:00, 1621.05it/s]warmup should be done:  82%| | 2459/3000 [00:01<00:00, 1621.17it/s]warmup should be done:  88%| | 2655/3000 [00:01<00:00, 1648.24it/s]warmup should be done:  87%| | 2610/3000 [00:01<00:00, 1624.27it/s]warmup should be done:  88%| | 2641/3000 [00:01<00:00, 1637.47it/s]warmup should be done:  88%| | 2641/3000 [00:01<00:00, 1647.11it/s]warmup should be done:  87%| | 2623/3000 [00:01<00:00, 1638.89it/s]warmup should be done:  89%| | 2675/3000 [00:01<00:00, 1650.50it/s]warmup should be done:  88%| | 2633/3000 [00:01<00:00, 1621.90it/s]warmup should be done:  87%| | 2622/3000 [00:01<00:00, 1622.75it/s]warmup should be done:  94%|| 2820/3000 [00:01<00:00, 1641.88it/s]warmup should be done:  92%|| 2773/3000 [00:01<00:00, 1622.13it/s]warmup should be done:  94%|| 2805/3000 [00:01<00:00, 1634.25it/s]warmup should be done:  93%|| 2788/3000 [00:01<00:00, 1639.64it/s]warmup should be done:  94%|| 2806/3000 [00:01<00:00, 1639.02it/s]warmup should be done:  95%|| 2841/3000 [00:01<00:00, 1646.06it/s]warmup should be done:  93%|| 2796/3000 [00:01<00:00, 1615.00it/s]warmup should be done:  93%|| 2785/3000 [00:01<00:00, 1622.98it/s]warmup should be done: 100%|| 2985/3000 [00:01<00:00, 1640.57it/s]warmup should be done:  98%|| 2937/3000 [00:01<00:00, 1626.73it/s]warmup should be done:  99%|| 2970/3000 [00:01<00:00, 1636.25it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1655.71it/s]warmup should be done:  99%|| 2970/3000 [00:01<00:00, 1639.23it/s]warmup should be done:  98%|| 2953/3000 [00:01<00:00, 1640.24it/s]warmup should be done:  99%|| 2959/3000 [00:01<00:00, 1617.79it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1650.33it/s]warmup should be done:  98%|| 2949/3000 [00:01<00:00, 1627.79it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1640.57it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1639.03it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1629.37it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1628.23it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1623.22it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1620.15it/s]






warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   6%|         | 167/3000 [00:00<00:01, 1669.08it/s]warmup should be done:   6%|         | 167/3000 [00:00<00:01, 1667.30it/s]warmup should be done:   6%|         | 166/3000 [00:00<00:01, 1657.11it/s]warmup should be done:   5%|         | 162/3000 [00:00<00:01, 1615.22it/s]warmup should be done:   6%|         | 167/3000 [00:00<00:01, 1664.51it/s]warmup should be done:   6%|         | 169/3000 [00:00<00:01, 1684.12it/s]warmup should be done:   6%|         | 165/3000 [00:00<00:01, 1643.34it/s]warmup should be done:   6%|         | 166/3000 [00:00<00:01, 1651.75it/s]warmup should be done:  11%|         | 333/3000 [00:00<00:01, 1668.70it/s]warmup should be done:  11%|         | 336/3000 [00:00<00:01, 1676.10it/s]warmup should be done:  11%|         | 332/3000 [00:00<00:01, 1656.19it/s]warmup should be done:  11%|        | 339/3000 [00:00<00:01, 1689.28it/s]warmup should be done:  11%|         | 335/3000 [00:00<00:01, 1667.78it/s]warmup should be done:  11%|         | 332/3000 [00:00<00:01, 1644.90it/s]warmup should be done:  11%|         | 334/3000 [00:00<00:01, 1654.04it/s]warmup should be done:  11%|         | 332/3000 [00:00<00:01, 1646.35it/s]warmup should be done:  17%|        | 506/3000 [00:00<00:01, 1695.03it/s]warmup should be done:  17%|        | 500/3000 [00:00<00:01, 1665.98it/s]warmup should be done:  17%|        | 506/3000 [00:00<00:01, 1684.29it/s]warmup should be done:  17%|        | 503/3000 [00:00<00:01, 1672.51it/s]warmup should be done:  17%|        | 510/3000 [00:00<00:01, 1694.54it/s]warmup should be done:  17%|        | 497/3000 [00:00<00:01, 1646.58it/s]warmup should be done:  17%|        | 504/3000 [00:00<00:01, 1672.27it/s]warmup should be done:  17%|        | 498/3000 [00:00<00:01, 1647.73it/s]warmup should be done:  22%|       | 668/3000 [00:00<00:01, 1671.03it/s]warmup should be done:  23%|       | 680/3000 [00:00<00:01, 1710.62it/s]warmup should be done:  23%|       | 678/3000 [00:00<00:01, 1695.54it/s]warmup should be done:  22%|       | 672/3000 [00:00<00:01, 1675.79it/s]warmup should be done:  23%|       | 681/3000 [00:00<00:01, 1698.37it/s]warmup should be done:  22%|       | 663/3000 [00:00<00:01, 1651.43it/s]warmup should be done:  22%|       | 673/3000 [00:00<00:01, 1678.97it/s]warmup should be done:  22%|       | 664/3000 [00:00<00:01, 1651.93it/s]warmup should be done:  28%|       | 836/3000 [00:00<00:01, 1672.00it/s]warmup should be done:  28%|       | 853/3000 [00:00<00:01, 1715.57it/s]warmup should be done:  28%|       | 851/3000 [00:00<00:01, 1697.71it/s]warmup should be done:  28%|       | 829/3000 [00:00<00:01, 1654.41it/s]warmup should be done:  28%|       | 840/3000 [00:00<00:01, 1674.42it/s]warmup should be done:  28%|       | 843/3000 [00:00<00:01, 1683.49it/s]warmup should be done:  28%|       | 830/3000 [00:00<00:01, 1652.53it/s]warmup should be done:  28%|       | 848/3000 [00:00<00:01, 1683.25it/s]warmup should be done:  33%|      | 1004/3000 [00:00<00:01, 1671.43it/s]warmup should be done:  34%|      | 1021/3000 [00:00<00:01, 1697.82it/s]warmup should be done:  34%|      | 1025/3000 [00:00<00:01, 1711.97it/s]warmup should be done:  33%|      | 995/3000 [00:00<00:01, 1653.33it/s]warmup should be done:  34%|      | 1008/3000 [00:00<00:01, 1674.30it/s]warmup should be done:  34%|      | 1014/3000 [00:00<00:01, 1689.03it/s]warmup should be done:  33%|      | 996/3000 [00:00<00:01, 1650.20it/s]warmup should be done:  34%|      | 1018/3000 [00:00<00:01, 1687.76it/s]warmup should be done:  40%|      | 1191/3000 [00:00<00:01, 1697.85it/s]warmup should be done:  39%|      | 1173/3000 [00:00<00:01, 1675.60it/s]warmup should be done:  39%|      | 1176/3000 [00:00<00:01, 1675.06it/s]warmup should be done:  39%|      | 1161/3000 [00:00<00:01, 1653.11it/s]warmup should be done:  39%|      | 1184/3000 [00:00<00:01, 1689.69it/s]warmup should be done:  40%|      | 1189/3000 [00:00<00:01, 1692.41it/s]warmup should be done:  39%|      | 1162/3000 [00:00<00:01, 1648.27it/s]warmup should be done:  40%|      | 1197/3000 [00:00<00:01, 1692.82it/s]warmup should be done:  45%|     | 1342/3000 [00:00<00:00, 1679.59it/s]warmup should be done:  45%|     | 1362/3000 [00:00<00:00, 1700.06it/s]warmup should be done:  45%|     | 1344/3000 [00:00<00:00, 1676.44it/s]warmup should be done:  44%|     | 1327/3000 [00:00<00:01, 1653.74it/s]warmup should be done:  45%|     | 1356/3000 [00:00<00:00, 1697.86it/s]warmup should be done:  45%|     | 1361/3000 [00:00<00:00, 1698.14it/s]warmup should be done:  44%|     | 1327/3000 [00:00<00:01, 1642.80it/s]warmup should be done:  46%|     | 1368/3000 [00:00<00:00, 1695.74it/s]warmup should be done:  50%|     | 1511/3000 [00:00<00:00, 1680.73it/s]warmup should be done:  50%|     | 1512/3000 [00:00<00:00, 1676.57it/s]warmup should be done:  51%|     | 1533/3000 [00:00<00:00, 1699.99it/s]warmup should be done:  50%|     | 1494/3000 [00:00<00:00, 1657.83it/s]warmup should be done:  51%|     | 1527/3000 [00:00<00:00, 1700.63it/s]warmup should be done:  51%|     | 1532/3000 [00:00<00:00, 1699.73it/s]warmup should be done:  50%|     | 1492/3000 [00:00<00:00, 1641.55it/s]warmup should be done:  51%|    | 1538/3000 [00:00<00:00, 1696.67it/s]warmup should be done:  56%|    | 1680/3000 [00:01<00:00, 1681.62it/s]warmup should be done:  56%|    | 1680/3000 [00:01<00:00, 1676.18it/s]warmup should be done:  57%|    | 1703/3000 [00:01<00:00, 1698.18it/s]warmup should be done:  55%|    | 1660/3000 [00:01<00:00, 1658.39it/s]warmup should be done:  57%|    | 1698/3000 [00:01<00:00, 1703.34it/s]warmup should be done:  57%|    | 1703/3000 [00:01<00:00, 1702.26it/s]warmup should be done:  55%|    | 1657/3000 [00:01<00:00, 1640.58it/s]warmup should be done:  57%|    | 1711/3000 [00:01<00:00, 1704.09it/s]warmup should be done:  62%|   | 1849/3000 [00:01<00:00, 1681.23it/s]warmup should be done:  62%|   | 1874/3000 [00:01<00:00, 1700.10it/s]warmup should be done:  62%|   | 1849/3000 [00:01<00:00, 1677.58it/s]warmup should be done:  62%|   | 1870/3000 [00:01<00:00, 1706.47it/s]warmup should be done:  61%|    | 1826/3000 [00:01<00:00, 1651.36it/s]warmup should be done:  62%|   | 1874/3000 [00:01<00:00, 1696.37it/s]warmup should be done:  61%|    | 1822/3000 [00:01<00:00, 1641.28it/s]warmup should be done:  63%|   | 1884/3000 [00:01<00:00, 1709.97it/s]warmup should be done:  67%|   | 2017/3000 [00:01<00:00, 1676.82it/s]warmup should be done:  67%|   | 2018/3000 [00:01<00:00, 1678.89it/s]warmup should be done:  68%|   | 2042/3000 [00:01<00:00, 1708.23it/s]warmup should be done:  68%|   | 2045/3000 [00:01<00:00, 1695.59it/s]warmup should be done:  66%|   | 1992/3000 [00:01<00:00, 1648.05it/s]warmup should be done:  69%|   | 2057/3000 [00:01<00:00, 1714.27it/s]warmup should be done:  66%|   | 1987/3000 [00:01<00:00, 1638.92it/s]warmup should be done:  68%|   | 2044/3000 [00:01<00:00, 1667.87it/s]warmup should be done:  73%|  | 2185/3000 [00:01<00:00, 1676.36it/s]warmup should be done:  74%|  | 2213/3000 [00:01<00:00, 1708.31it/s]warmup should be done:  74%|  | 2215/3000 [00:01<00:00, 1696.28it/s]warmup should be done:  73%|  | 2186/3000 [00:01<00:00, 1674.70it/s]warmup should be done:  72%|  | 2160/3000 [00:01<00:00, 1655.51it/s]warmup should be done:  74%|  | 2229/3000 [00:01<00:00, 1715.48it/s]warmup should be done:  72%|  | 2151/3000 [00:01<00:00, 1637.96it/s]warmup should be done:  74%|  | 2211/3000 [00:01<00:00, 1652.66it/s]warmup should be done:  78%|  | 2353/3000 [00:01<00:00, 1676.22it/s]warmup should be done:  80%|  | 2385/3000 [00:01<00:00, 1709.14it/s]warmup should be done:  80%|  | 2386/3000 [00:01<00:00, 1697.55it/s]warmup should be done:  78%|  | 2354/3000 [00:01<00:00, 1671.05it/s]warmup should be done:  78%|  | 2326/3000 [00:01<00:00, 1650.44it/s]warmup should be done:  77%|  | 2316/3000 [00:01<00:00, 1638.95it/s]warmup should be done:  80%|  | 2401/3000 [00:01<00:00, 1708.31it/s]warmup should be done:  79%|  | 2377/3000 [00:01<00:00, 1645.17it/s]warmup should be done:  84%| | 2521/3000 [00:01<00:00, 1677.02it/s]warmup should be done:  85%| | 2557/3000 [00:01<00:00, 1712.05it/s]warmup should be done:  85%| | 2557/3000 [00:01<00:00, 1699.01it/s]warmup should be done:  84%| | 2522/3000 [00:01<00:00, 1669.31it/s]warmup should be done:  86%| | 2572/3000 [00:01<00:00, 1707.20it/s]warmup should be done:  83%| | 2492/3000 [00:01<00:00, 1633.92it/s]warmup should be done:  83%| | 2480/3000 [00:01<00:00, 1626.26it/s]warmup should be done:  85%| | 2542/3000 [00:01<00:00, 1626.97it/s]warmup should be done:  90%| | 2689/3000 [00:01<00:00, 1677.46it/s]warmup should be done:  91%| | 2729/3000 [00:01<00:00, 1713.91it/s]warmup should be done:  91%| | 2728/3000 [00:01<00:00, 1701.30it/s]warmup should be done:  90%| | 2689/3000 [00:01<00:00, 1668.37it/s]warmup should be done:  91%|| 2744/3000 [00:01<00:00, 1709.15it/s]warmup should be done:  88%| | 2647/3000 [00:01<00:00, 1638.17it/s]warmup should be done:  89%| | 2656/3000 [00:01<00:00, 1618.86it/s]warmup should be done:  90%| | 2705/3000 [00:01<00:00, 1612.95it/s]warmup should be done:  95%|| 2857/3000 [00:01<00:00, 1676.19it/s]warmup should be done:  97%|| 2901/3000 [00:01<00:00, 1714.42it/s]warmup should be done:  97%|| 2899/3000 [00:01<00:00, 1700.50it/s]warmup should be done:  95%|| 2856/3000 [00:01<00:00, 1665.76it/s]warmup should be done:  97%|| 2917/3000 [00:01<00:00, 1713.99it/s]warmup should be done:  94%|| 2815/3000 [00:01<00:00, 1648.99it/s]warmup should be done:  94%|| 2818/3000 [00:01<00:00, 1609.57it/s]warmup should be done:  96%|| 2867/3000 [00:01<00:00, 1602.30it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1705.36it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1700.82it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1698.16it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1675.66it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1671.53it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1656.28it/s]warmup should be done:  99%|| 2984/3000 [00:01<00:00, 1658.37it/s]warmup should be done:  99%|| 2979/3000 [00:01<00:00, 1608.77it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1645.69it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1638.90it/s]2022-12-12 02:14:39.517571: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f308b834230 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 02:14:39.517637: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 02:14:40.428661: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f308f82bf10 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 02:14:40.428727: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 02:14:40.433572: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f3087830270 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 02:14:40.433633: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 02:14:40.447494: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f308f831720 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 02:14:40.447547: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 02:14:40.461170: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f3073f94060 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 02:14:40.461234: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 02:14:40.867664: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f308f82d3c0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 02:14:40.867735: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 02:14:40.892256: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f308f830da0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 02:14:40.892327: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 02:14:40.913599: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f13fc031920 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 02:14:40.913672: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 02:14:41.779703: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 02:14:42.693735: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 02:14:42.730257: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 02:14:42.768359: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 02:14:42.802312: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 02:14:43.143296: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 02:14:43.182561: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 02:14:43.215466: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 02:14:44.687317: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 02:14:45.594898: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 02:14:45.767093: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 02:14:45.796516: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 02:14:45.814632: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 02:14:46.001731: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 02:14:46.079052: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 02:14:46.082663: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
[HCTR][02:15:13.902][ERROR][RK0][main]: replica 7 reaches 1000, calling init pre replica
[HCTR][02:15:13.902][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][02:15:13.912][ERROR][RK0][main]: coll ps creation done
[HCTR][02:15:13.912][ERROR][RK0][main]: replica 7 waits for coll ps creation barrier
[HCTR][02:15:13.962][ERROR][RK0][main]: replica 6 reaches 1000, calling init pre replica
[HCTR][02:15:13.962][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][02:15:13.971][ERROR][RK0][main]: coll ps creation done
[HCTR][02:15:13.971][ERROR][RK0][main]: replica 6 waits for coll ps creation barrier
[HCTR][02:15:14.082][ERROR][RK0][main]: replica 2 reaches 1000, calling init pre replica
[HCTR][02:15:14.083][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][02:15:14.090][ERROR][RK0][main]: replica 3 reaches 1000, calling init pre replica
[HCTR][02:15:14.090][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][02:15:14.092][ERROR][RK0][main]: coll ps creation done
[HCTR][02:15:14.092][ERROR][RK0][main]: replica 2 waits for coll ps creation barrier
[HCTR][02:15:14.098][ERROR][RK0][main]: coll ps creation done
[HCTR][02:15:14.098][ERROR][RK0][main]: replica 3 waits for coll ps creation barrier
[HCTR][02:15:14.113][ERROR][RK0][main]: replica 5 reaches 1000, calling init pre replica
[HCTR][02:15:14.113][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][02:15:14.123][ERROR][RK0][main]: coll ps creation done
[HCTR][02:15:14.123][ERROR][RK0][main]: replica 5 waits for coll ps creation barrier
[HCTR][02:15:14.152][ERROR][RK0][main]: replica 4 reaches 1000, calling init pre replica
[HCTR][02:15:14.152][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][02:15:14.160][ERROR][RK0][main]: coll ps creation done
[HCTR][02:15:14.160][ERROR][RK0][main]: replica 4 waits for coll ps creation barrier
[HCTR][02:15:14.205][ERROR][RK0][main]: replica 0 reaches 1000, calling init pre replica
[HCTR][02:15:14.206][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][02:15:14.210][ERROR][RK0][main]: coll ps creation done
[HCTR][02:15:14.210][ERROR][RK0][main]: replica 0 waits for coll ps creation barrier
[HCTR][02:15:14.219][ERROR][RK0][tid #139846945335040]: replica 1 reaches 1000, calling init pre replica
[HCTR][02:15:14.219][ERROR][RK0][tid #139846945335040]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][02:15:14.228][ERROR][RK0][tid #139846945335040]: coll ps creation done
[HCTR][02:15:14.228][ERROR][RK0][tid #139846945335040]: replica 1 waits for coll ps creation barrier
[HCTR][02:15:14.228][ERROR][RK0][main]: replica 0 preparing frequency
[HCTR][02:15:15.055][ERROR][RK0][main]: replica 0 preparing frequency done
[HCTR][02:15:15.091][ERROR][RK0][main]: replica 0 calling init per replica
[HCTR][02:15:15.091][ERROR][RK0][main]: replica 2 calling init per replica
[HCTR][02:15:15.091][ERROR][RK0][main]: replica 5 calling init per replica
[HCTR][02:15:15.091][ERROR][RK0][tid #139846945335040]: replica 1 calling init per replica
[HCTR][02:15:15.091][ERROR][RK0][main]: replica 3 calling init per replica
[HCTR][02:15:15.091][ERROR][RK0][main]: replica 7 calling init per replica
[HCTR][02:15:15.091][ERROR][RK0][main]: replica 4 calling init per replica
[HCTR][02:15:15.091][ERROR][RK0][main]: replica 6 calling init per replica
[HCTR][02:15:15.091][ERROR][RK0][main]: Calling build_v2
[HCTR][02:15:15.091][ERROR][RK0][main]: Calling build_v2
[HCTR][02:15:15.091][ERROR][RK0][main]: Calling build_v2
[HCTR][02:15:15.091][ERROR][RK0][tid #139846945335040]: Calling build_v2
[HCTR][02:15:15.091][ERROR][RK0][main]: Calling build_v2
[HCTR][02:15:15.091][ERROR][RK0][main]: Calling build_v2
[HCTR][02:15:15.091][ERROR][RK0][main]: Calling build_v2
[HCTR][02:15:15.091][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][02:15:15.091][ERROR][RK0][main]: Calling build_v2
[HCTR][02:15:15.091][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][02:15:15.091][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][02:15:15.091][ERROR][RK0][tid #139846945335040]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][02:15:15.091][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][02:15:15.091][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][02:15:15.091][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][02:15:15.091][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[2022-12-12 02:15:15. 95308: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:178[] v100x8, slow pcie
2022-12-12 02:15:15. 95353[: 2022-12-12 02:15:15E.  95389/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: :E178[ ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccv100x8, slow pcie:
2022-12-12 02:15:15196.]  95399[assigning 0 to cpu: 2022-12-12 02:15:15
E.  95437/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: :E178 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccv100x8, slow pcie:[
196] assigning 0 to cpu
[2022-12-12 02:15:15[2022-12-12 02:15:15.2022-12-12 02:15:15.[ 95450. 95494:  95494: [2022-12-12 02:15:15E: E2022-12-12 02:15:15. [E . 954962022-12-12 02:15:15/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc 95536: [.:/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:: E 95543178:2122022-12-12 02:15:15E : ] [196] . /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccEv100x8, slow pcie] build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8 95584/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: 
assigning 0 to cpu
2022-12-12 02:15:15: :178/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
.[E212] [: 956292022-12-12 02:15:15 ] v100x8, slow pcie2022-12-12 02:15:15178[: ./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccbuild asymm link desc with 8X Tesla V100-SXM2-32GB out of 8
.] 2022-12-12 02:15:15[E 95718:
 95734v100x8, slow pcie.2022-12-12 02:15:15 : 178: 
 95770[./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE] E: 2022-12-12 02:15:15[ 95822: v100x8, slow pcie E.2022-12-12 02:15:15: 178/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc  95856.E] ::/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: [ 95882 v100x8, slow pcie196213:E2022-12-12 02:15:15: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
] ] 212 .E:assigning 0 to cpuremote time is 8.68421] [/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc 95953 196

build asymm link desc with 8X Tesla V100-SXM2-32GB out of 82022-12-12 02:15:15:: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] 
.[213E:assigning 0 to cpu 960202022-12-12 02:15:15]  [196
: .[remote time is 8.68421/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 02:15:15] E 960662022-12-12 02:15:15
:.assigning 0 to cpu : .196[[ 96090
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE 96101] 2022-12-12 02:15:152022-12-12 02:15:15: : : assigning 0 to cpu..E196/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE
 96168 96171 ] [: : : /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccassigning 0 to cpu2022-12-12 02:15:15214/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccEE:
.[] :  213 962402022-12-12 02:15:15cpu time is 97.0588212/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] : .
] ::[remote time is 8.68421E 96289build asymm link desc with 8X Tesla V100-SXM2-32GB out of 82122142022-12-12 02:15:15
 : 
] ] ./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE[build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8cpu time is 97.0588 96353:[ 2022-12-12 02:15:15

: 2122022-12-12 02:15:15/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.E] .[: 96405 build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8 964202022-12-12 02:15:15212: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
: .] E:E 96464build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8 [212 : 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 02:15:15] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE:.[build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8: 214 965172022-12-12 02:15:15
213/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] : .] :cpu time is 97.0588[E 96560remote time is 8.68421213
2022-12-12 02:15:15 : 
] ./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccEremote time is 8.68421 96613[: 
: 2022-12-12 02:15:15213/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[E.] :2022-12-12 02:15:15  96660remote time is 8.68421213./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: 
]  96690:Eremote time is 8.68421: [213 
E2022-12-12 02:15:15] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc .[remote time is 8.68421:/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc 967512022-12-12 02:15:15
214:: .[] 214E 967822022-12-12 02:15:15cpu time is 97.0588]  : .
cpu time is 97.0588/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE 96820
: : 214/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE] : cpu time is 97.0588214/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
] :cpu time is 97.0588214
] cpu time is 97.0588
[2022-12-12 02:16:34.792204: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:118] solver created. now build & solve
[2022-12-12 02:16:34.832245: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:123] solver built. now solve
block 0 storage is 00010001
	access is	0	0	0	0	4	4	4	4	
block 1 storage is 00100010
	access is	1	1	1	1	5	5	5	5	
block 2 storage is 01000100
	access is	2	2	2	2	6	6	6	6	
block 3 storage is 10001000
	access is	3	3	3	3	7	7	7	7	
block 4 storage is 00000000
	access is	8	8	8	8	8	8	8	8	
[2022-12-12 02:16:34.946529: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:127] solver solved
[2022-12-12 02:16:34.946590: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:195] 0 solved master
[2022-12-12 02:16:34.946623: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 0 solved
[2022-12-12 02:16:34.946655: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 0 initing device 0
[2022-12-12 02:16:34.947223: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-12 02:16:34.947271: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 02:16:34.948194: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 02:16:34.948996: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 02:16:34.962201: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 3 solved
[2022-12-12 02:16:34.962281: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 3 initing device 3
[[2022-12-12 02:16:342022-12-12 02:16:34..962334962341: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc::202202] ] 7 solved5 solved

[[2022-12-12 02:16:342022-12-12 02:16:34..962428962430: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc::205205] ] worker 0 thread 7 initing device 7worker 0 thread 5 initing device 5

[2022-12-12 02:16:34.962721: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-12 02:16:34.962772: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[[2022-12-12 02:16:342022-12-12 02:16:34..962894962894: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::18151815] ] Building Coll Cache with ... num gpu device is 8Building Coll Cache with ... num gpu device is 8

[[2022-12-12 02:16:342022-12-12 02:16:34..962967962967: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::19801980] ] eager alloc mem 381.47 MBeager alloc mem 381.47 MB

[2022-12-12 02:16:34.964988: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 02:16:34.965103: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 02:16:34.965153: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 02:16:34.965304: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 2 solved
[2022-12-12 02:16:34.965365: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 2 initing device 2
[2022-12-12 02:16:34.965507: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 6 solved
[2022-12-12 02:16:34.965561: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 6 initing device 6
[2022-12-12 02:16:34.965795: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-12 02:16:34.965837: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 02:16:34.965987: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-12 02:16:34.966036: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 02:16:34.968082: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 02:16:34.968222: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 02:16:34.968397: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 02:16:34.968907: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 02:16:34.969449: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[[2022-12-12 02:16:34[.2022-12-12 02:16:342022-12-12 02:16:34971705..: 971692971710E: :  EE/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu [ :/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc2022-12-12 02:16:34/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc1980:.:] 202971756202eager alloc mem 381.47 MB] : ] 
4 solvedE1 solved
 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] [[eager alloc mem 381.47 MB2022-12-12 02:16:342022-12-12 02:16:34
..971825971828: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc::205205] ] worker 0 thread 4 initing device 4worker 0 thread 1 initing device 1

[[2022-12-12 02:16:342022-12-12 02:16:34..972401972401: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::18151815] ] Building Coll Cache with ... num gpu device is 8Building Coll Cache with ... num gpu device is 8

[[2022-12-12 02:16:342022-12-12 02:16:34..972471972471: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::19801980] ] eager alloc mem 381.47 MBeager alloc mem 381.47 MB

[2022-12-12 02:16:34.974541: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 02:16:34.974592: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 02:16:34.975993: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 02:16:34.976123: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 02:16:35. 26119: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.00 Bytes
[2022-12-12 02:16:35. 31361: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5
[2022-12-12 02:16:35. 31480: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 02:16:35. 32294: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 38.53 MB
[2022-12-12 02:16:35. 32870: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 02:16:35. 33904: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:16:35. 33951: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 38.15 MB
[[[2022-12-12 02:16:352022-12-12 02:16:352022-12-12 02:16:35... 55938 55938 55938: : : EEE   /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:::198019801980] ] ] eager alloc mem 5.00 Byteseager alloc mem 5.00 Byteseager alloc mem 5.00 Bytes


[[2022-12-12 02:16:352022-12-12 02:16:35.. 57692 57692: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::19801980] ] eager alloc mem 5.00 Byteseager alloc mem 5.00 Bytes

[2022-12-12 02:16:35. 61685: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5
[2022-12-12 02:16:35. 61748: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:[6382022-12-12 02:16:35] .eager release cuda mem 5 61786
: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 02:16:35. 61848[: 2022-12-12 02:16:35E.  61833/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc: :E638 [] /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu2022-12-12 02:16:35eager release cuda mem 400000000:.
1980 61893] : eager alloc mem 25.25 KBE
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5
[2022-12-12 02:16:35. 62031: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 02:16:35. 62573: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 02:16:35. 62618: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 4.77 GB
[2022-12-12 02:16:35. 62877: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 38.53 MB
[2022-12-12 02:16:35. 62969: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5
[2022-12-12 02:16:35. 63063: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 02:16:35. 63087: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5
[2022-12-12 02:16:35. 63185: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 02:16:35. 63389: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.00 Bytes
[2022-12-12 02:16:35. 63454: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 38.53 MB
[2022-12-12 02:16:35. 63572: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.00 Bytes
[2022-12-12 02:16:35. 69994: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 38.53 MB
[2022-12-12 02:16:35. 71744: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 38.53 MB
[2022-12-12 02:16:35. 72249: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 38.53 MB
[2022-12-12 02:16:35. 72963: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 02:16:35. 73192: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 02:16:35. 73442: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 02:16:35. 73692: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 02:16:35. 73735: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 02:16:35. 73989: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:16:35. 74041: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 38.15 MB
[2022-12-12 02:16:35. 74131: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5
[2022-12-12 02:16:35. 74193[: 2022-12-12 02:16:35E.  74206/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc: :E638 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cceager release cuda mem 625663[:
2022-12-12 02:16:35638.]  74241eager release cuda mem 400000000: [
E2022-12-12 02:16:35 ./hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc 74294:: 638W]  eager release cuda mem 5/hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc
:43] WORKER[0] alloc host memory 38.15 MB
[2022-12-12 02:16:35. 74373: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 02:16:35. 74453: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:16:35. 74500: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 38.15 MB
[2022-12-12 02:16:35. 74707: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:16:35. 74742: [E2022-12-12 02:16:35 ./hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc 74753:: 638W]  eager release cuda mem 625663/hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc
:43] WORKER[0] alloc host memory 38.15 MB
[2022-12-12 02:16:35. 74803: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 38.15 MB
[2022-12-12 02:16:35. 75333: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 38.53 MB
[2022-12-12 02:16:35. 75924: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 38.53 MB
[2022-12-12 02:16:35. 76819: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 02:16:35. 76938: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 02:16:35. 77830: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:16:35. 77879: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 38.15 MB
[2022-12-12 02:16:35. 77954: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:16:35. 78005: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 38.15 MB
[2022-12-12 02:16:35. 99654: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 02:16:35. 99990: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 02:16:35.100267: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 02:16:35.100311: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 4.77 GB
[2022-12-12 02:16:35.100604: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 02:16:35.100646: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 4.77 GB
[2022-12-12 02:16:35.100984: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 02:16:35.101020: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 02:16:35.101484: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 02:16:35.101592: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 02:16:35.[1016252022-12-12 02:16:35: .E101634 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccE: 638/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu] :eager release cuda mem 258551980
] eager alloc mem 4.77 GB
[2022-12-12 02:16:35.101715: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 4.77 GB
[2022-12-12 02:16:35.102088: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 02:16:35.102132: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 4.77 GB
[2022-12-12 02:16:35.103895: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[[2022-12-12 02:16:352022-12-12 02:16:35..104489104501: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::1980638] ] eager alloc mem 25.25 KBeager release cuda mem 25855

[2022-12-12 02:16:35.104604: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 4.77 GB
[2022-12-12 02:16:35.105142: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 02:16:35.105187: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 4.77 GB
[[[[[[[[2022-12-12 02:16:362022-12-12 02:16:362022-12-12 02:16:362022-12-12 02:16:362022-12-12 02:16:362022-12-12 02:16:362022-12-12 02:16:362022-12-12 02:16:36........940631940631940631940631940630940632940631940631: : : : : : : : EEEEEEEE        /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::::::::19261926192619261926192619261926] ] ] ] ] ] ] ] Device 0 init p2p of link 3Device 1 init p2p of link 7Device 6 init p2p of link 0Device 7 init p2p of link 4Device 2 init p2p of link 1Device 4 init p2p of link 5Device 5 init p2p of link 6Device 3 init p2p of link 2







[[[2022-12-12 02:16:362022-12-12 02:16:36[2022-12-12 02:16:36..[[2022-12-12 02:16:36.9412289412282022-12-12 02:16:362022-12-12 02:16:36.941227: : [[..941236: EE2022-12-12 02:16:362022-12-12 02:16:36941242941242: E  ..: : E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu941270941270EE /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::: :   /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:19801980EE/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] ]   ::1980] eager alloc mem 611.00 KBeager alloc mem 611.00 KB/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu19801980] eager alloc mem 611.00 KB

::] ] eager alloc mem 611.00 KB
19801980eager alloc mem 611.00 KBeager alloc mem 611.00 KB
] ] 

eager alloc mem 611.00 KBeager alloc mem 611.00 KB

[2022-12-12 02:16:36.942296: E[ 2022-12-12 02:16:36/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc.:942310638: ] Eeager release cuda mem 625663 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:16:36.942370: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:16:36.942403: E[[ 2022-12-12 02:16:362022-12-12 02:16:36/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc..:942413942414638: : ] [EE[eager release cuda mem 6256632022-12-12 02:16:36  2022-12-12 02:16:36
./hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc.942460::942460: 638638: E] ] E eager release cuda mem 625663eager release cuda mem 625663 /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc

/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::638638] ] eager release cuda mem 625663eager release cuda mem 625663

[2022-12-12 02:16:36.955373: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 2
[2022-12-12 02:16:36.955528: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 02:16:36.955617: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 6 init p2p of link 5
[2022-12-12 02:16:36.955768: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 02:16:36.956393: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:16:36.956629: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:16:36.960362: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 0 init p2p of link 6
[2022-12-12 02:16:36.960515: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 02:16:36.960594: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 7 init p2p of link 1
[2022-12-12 02:16:36.960742: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 02:16:36.960963: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 2 init p2p of link 3
[2022-12-12 02:16:36.961022: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 4 init p2p of link 7
[2022-12-12 02:16:36.961127: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 02:16:36.961176: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 02:16:36.961284: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:16:36.961377: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 5 init p2p of link 4
[2022-12-12 02:16:36.961513: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] [eager release cuda mem 6256632022-12-12 02:16:36
.961525: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] [Device 3 init p2p of link 02022-12-12 02:16:36
.961562: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 02:16:36.961701: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 02:16:36[.2022-12-12 02:16:36962023.: 962029E:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc :/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc638:] 638eager release cuda mem 625663] 
eager release cuda mem 625663
[2022-12-12 02:16:36.962490: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:16:36.962592: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:16:36.964722: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 3
[2022-12-12 02:16:36.964838: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 02:16:36.965026: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 6 init p2p of link 4
[2022-12-12 02:16:36.965139: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 02:16:36.965682: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:16:36.965983: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:16:36.973192: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 0 init p2p of link 1
[2022-12-12 02:16:36.973304: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 02:16:36.973891: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 7 init p2p of link 6
[2022-12-12 02:16:36.974012: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 02:16:36.974153: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:16:36.974864: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:16:36.979031: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 2 init p2p of link 0
[2022-12-12 02:16:36.979159: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 02:16:36.979271: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 4 init p2p of link 2
[2022-12-12 02:16:36.979400: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 02:16:36.979525: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 5 init p2p of link 7
[2022-12-12 02:16:36.979661: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 02:16:36.979847: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 3 init p2p of link 5
[2022-12-12 02:16:36.979996: [E2022-12-12 02:16:36 ./hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu980003:: 1980E]  eager alloc mem 611.00 KB/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc
:638] eager release cuda mem 625663
[2022-12-12 02:16:36.980181: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:16:36.980507: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:16:36.980823: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:16:36.986267: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 0
[2022-12-12 02:16:36.986382: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 02:16:36.986654: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 6 init p2p of link 7
[2022-12-12 02:16:36.986764: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 02:16:36.987241: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:16:36.987630: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:16:36.987733: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 4 init p2p of link 6
[2022-12-12 02:16:36.987856: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 02:16:36.988718: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:16:36.988875: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 0 init p2p of link 2
[2022-12-12 02:16:36.988959: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:[19262022-12-12 02:16:36] .Device 3 init p2p of link 1988990
: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 02:16:36.989101: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 02:16:36.989211: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 7 init p2p of link 5
[2022-12-12 02:16:36.989327: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 02:16:36.989871: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:16:36.989960: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:16:36.990179: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:16:36.996405: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 2 init p2p of link 4
[2022-12-12 02:16:36.996524: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 02:16:36.997100: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 5 init p2p of link 3
[2022-12-12 02:16:36.997229: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 02:16:36.997288: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:16:36.998000: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:16:37.  4338: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 40400000
[2022-12-12 02:16:37.  4673: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 40400000
[2022-12-12 02:16:37.  5022: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 10000000 / 100000000 nodes ( 10.00 %~10.00 %) | remote 30000000 / 100000000 nodes ( 30.00 %) | cpu 60000000 / 100000000 nodes ( 60.00 %) | 4.77 GB | 2.03919 secs 
[2022-12-12 02:16:37.  5240: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 10000000 / 100000000 nodes ( 10.00 %~10.00 %) | remote 30000000 / 100000000 nodes ( 30.00 %) | cpu 60000000 / 100000000 nodes ( 60.00 %) | 4.77 GB | 2.04228 secs 
[2022-12-12 02:16:37.  7562: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 40400000
[2022-12-12 02:16:37.  7926: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 10000000 / 100000000 nodes ( 10.00 %~10.00 %) | remote 30000000 / 100000000 nodes ( 30.00 %) | cpu 60000000 / 100000000 nodes ( 60.00 %) | 4.77 GB | 2.03546 secs 
[2022-12-12 02:16:37.  9557: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 40400000
[2022-12-12 02:16:37.  9921: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 10000000 / 100000000 nodes ( 10.00 %~10.00 %) | remote 30000000 / 100000000 nodes ( 30.00 %) | cpu 60000000 / 100000000 nodes ( 60.00 %) | 4.77 GB | 2.04389 secs 
[2022-12-12 02:16:37. 10167: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 40400000
[2022-12-12 02:16:37. 10545: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 10000000 / 100000000 nodes ( 10.00 %~10.00 %) | remote 30000000 / 100000000 nodes ( 30.00 %) | cpu 60000000 / 100000000 nodes ( 60.00 %) | 4.77 GB | 2.03808 secs [
2022-12-12 02:16:37. 10563: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 40400000
[2022-12-12 02:16:37.[ 109052022-12-12 02:16:37: .E 10925 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccE: 638/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu] :eager release cuda mem 404000001955
] Asymm Coll cache (policy: clique_part) | local 10000000 / 100000000 nodes ( 10.00 %~10.00 %) | remote 30000000 / 100000000 nodes ( 30.00 %) | cpu 60000000 / 100000000 nodes ( 60.00 %) | 4.77 GB | 2.06366 secs 
[2022-12-12 02:16:37. 11251: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 40400000
[2022-12-12 02:16:37. 11321: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 10000000 / 100000000 nodes ( 10.00 %~10.00 %) | remote 30000000 / 100000000 nodes ( 30.00 %) | cpu 60000000 / 100000000 nodes ( 60.00 %) | 4.77 GB | 2.04856 secs 
[2022-12-12 02:16:37. 11601: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 10000000 / 100000000 nodes ( 10.00 %~10.00 %) | remote 30000000 / 100000000 nodes ( 30.00 %) | cpu 60000000 / 100000000 nodes ( 60.00 %) | 4.77 GB | 2.04864 secs 
[HCTR][02:16:37.011][ERROR][RK0][main]: replica 3 calling init per replica done, doing barrier
[HCTR][02:16:37.011][ERROR][RK0][main]: replica 4 calling init per replica done, doing barrier
[HCTR][02:16:37.011][ERROR][RK0][main]: replica 5 calling init per replica done, doing barrier
[HCTR][02:16:37.011][ERROR][RK0][main]: replica 7 calling init per replica done, doing barrier
[HCTR][02:16:37.011][ERROR][RK0][main]: replica 6 calling init per replica done, doing barrier
[HCTR][02:16:37.011][ERROR][RK0][main]: replica 2 calling init per replica done, doing barrier
[HCTR][02:16:37.011][ERROR][RK0][main]: replica 0 calling init per replica done, doing barrier
[HCTR][02:16:37.011][ERROR][RK0][tid #139846945335040]: replica 1 calling init per replica done, doing barrier
[HCTR][02:16:37.011][ERROR][RK0][tid #139846945335040]: replica 1 calling init per replica done, doing barrier done
[HCTR][02:16:37.011][ERROR][RK0][main]: replica 7 calling init per replica done, doing barrier done
[HCTR][02:16:37.011][ERROR][RK0][main]: replica 6 calling init per replica done, doing barrier done
[HCTR][02:16:37.011][ERROR][RK0][main]: replica 4 calling init per replica done, doing barrier done
[HCTR][02:16:37.011][ERROR][RK0][main]: replica 0 calling init per replica done, doing barrier done
[HCTR][02:16:37.011][ERROR][RK0][main]: replica 3 calling init per replica done, doing barrier done
[HCTR][02:16:37.011][ERROR][RK0][main]: replica 2 calling init per replica done, doing barrier done
[HCTR][02:16:37.011][ERROR][RK0][main]: replica 5 calling init per replica done, doing barrier done
[HCTR][02:16:37.011][ERROR][RK0][tid #139846945335040]: init per replica done
[HCTR][02:16:37.011][ERROR][RK0][main]: init per replica done
[HCTR][02:16:37.011][ERROR][RK0][main]: init per replica done
[HCTR][02:16:37.011][ERROR][RK0][main]: init per replica done
[HCTR][02:16:37.011][ERROR][RK0][main]: init per replica done
[HCTR][02:16:37.011][ERROR][RK0][main]: init per replica done
[HCTR][02:16:37.011][ERROR][RK0][main]: init per replica done
[HCTR][02:16:37.014][ERROR][RK0][main]: init per replica done








