2022-12-11 19:46:16.937604: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:16.944741: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:16.956186: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:16.966426: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:16.972230: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:16.985341: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:16.991396: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:16.996408: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.050882: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.054590: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.058183: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.067003: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.069831: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.070872: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.071857: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.072933: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.074042: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.075146: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.076097: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.077051: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.078011: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.078979: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.079936: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.080868: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.082848: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.083964: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.085519: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.086933: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.087216: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.088346: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.089003: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.089917: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.090650: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.091540: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.092239: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.093046: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.093739: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.095164: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.096299: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.097360: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.098283: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 19:46:17.105354: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.106854: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.107043: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.107706: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.109414: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.109513: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.109606: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.110071: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.111846: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.112167: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.112292: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.112475: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.114779: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.114926: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.115055: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.117401: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.117739: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.117889: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.120150: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.120510: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.120681: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.122719: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.123076: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.123261: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.125264: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.125658: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.128173: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.131054: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.133511: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.134075: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.135280: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.136031: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.136370: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.145244: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.145555: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.146297: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.146568: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.147986: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.148068: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.149211: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.149625: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.151122: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.170355: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.171020: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.173647: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.184237: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.185933: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.186708: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.188705: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.188816: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.188857: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.189004: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.189443: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.191326: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.192567: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.193726: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.193767: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.193809: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.193903: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.195498: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.197202: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.199189: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.199890: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.199934: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.199989: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.200028: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.202943: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.203871: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.203946: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.203987: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.204096: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.204194: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.207642: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.208275: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.208314: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.208489: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.208554: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.212163: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.212248: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.212426: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.212470: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.215174: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.215213: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.215518: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.215761: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.218061: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.218102: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.218342: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.218785: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.220864: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.220900: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.221255: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.221856: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.224073: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.224113: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.224315: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.225165: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.226902: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.227051: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.227272: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.228245: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.230101: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.230215: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.230237: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.231181: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.233124: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.233260: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.233302: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.234089: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.236012: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.236151: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.236197: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.238390: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.238461: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.238787: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.238900: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.241725: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.242524: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.242657: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.242673: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.243716: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.245225: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.245658: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.245768: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.245809: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.247148: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.248525: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.249050: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.249090: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.249134: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.250710: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.251755: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.252566: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.252594: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.252748: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.254330: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.255187: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.255382: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.256359: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.256394: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.256526: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.258063: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.258283: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.259259: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.259412: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.260444: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.260526: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.260959: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.262697: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.262892: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.263598: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.263862: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.264982: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.265144: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.265318: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.267013: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.268118: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.268327: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.269248: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.269380: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.269540: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.271356: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.272041: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.272109: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 19:46:17.272277: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.275002: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.275286: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.275412: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.277484: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 19:46:17.277494: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 19:46:17.277496: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 19:46:17.277897: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.278031: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.278280: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.280504: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.280561: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.280778: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.282227: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.283217: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.283440: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.283445: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.285220: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.286401: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.286470: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.286503: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.286756: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.286773: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.286795: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.289415: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.291195: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.291412: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.291504: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.291617: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.291619: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.291712: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.295847: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.296017: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.296061: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.296208: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.296242: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.296333: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.300735: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.301141: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.301235: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.335948: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.336307: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.337274: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.341497: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.342057: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.342796: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.346399: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.346960: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.347596: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.352206: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.353436: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.353487: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.359420: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.359903: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.360069: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.363781: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.364154: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.365227: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.369024: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.369390: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.369697: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.373308: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.373574: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.374063: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.391083: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.392282: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.395488: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 19:46:17.404403: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.408901: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.409525: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.448552: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.449051: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.449500: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.452643: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.453294: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.458176: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 19:46:17.467343: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.538536: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.539451: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 19:46:17.544453: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.548401: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.555254: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:17.562228: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:18.401609: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:18.402249: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:18.402798: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:18.403291: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 19:46:18.403351: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-11 19:46:18.422080: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:18.422958: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:18.423488: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:18.424078: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:18.424595: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:18.425077: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-11 19:46:18.471699: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 19:46:18.471923: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 19:46:18.559848: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12340
2022-12-11 19:46:18.663955: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:18.664750: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:18.665284: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:18.665759: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 19:46:18.665813: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-11 19:46:18.684002: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:18.684873: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:18.685389: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:18.685974: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:18.686499: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:18.686975: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:1/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-11 19:46:18.701025: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:18.701637: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:18.702166: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:18.702644: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 19:46:18.702698: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-11 19:46:18.703335: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:18.703999: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:18.704340: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:18.704814: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:18.705568: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:18.705652: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 19:46:18.705697: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-11 19:46:18.706307: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:18.706781: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 19:46:18.706821: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-11 19:46:18.720771: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:18.721437: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:18.721952: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:18.722791: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:18.723334: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:18.723822: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:2/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-11 19:46:18.724452: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:18.724887: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:18.725181: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:18.725938: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:18.726158: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:18.726939: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:18.727256: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:18.728010: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:18.728212: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:18.729386: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:18.729555: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:3/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-11 19:46:18.729956: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:5/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-11 19:46:18.751524: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:18.752240: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:18.752791: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:18.753257: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 19:46:18.753310: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-11 19:46:18.755728: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:18.756334: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:18.756872: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:18.757339: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 19:46:18.757391: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-11 19:46:18.758043: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 19:46:18.758224: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 19:46:18.759896: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12341
2022-12-11 19:46:18.765397: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:18.765996: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:18.766510: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:18.766979: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 19:46:18.767026: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-11 19:46:18.771110: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:18.771949: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:18.772459: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:18.773041: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:18.773551: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:18.774025: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:7/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-11 19:46:18.774436: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 19:46:18.774613: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 19:46:18.775568: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:18.776116: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 19:46:18.776202: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:18.776259: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 19:46:18.776726: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:18.777293: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:18.777817: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:18.777833: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12345
2022-12-11 19:46:18.778282: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:6/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-11 19:46:18.780011: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12343
2022-12-11 19:46:18.785831: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:18.786460: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:18.786976: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:18.787558: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:18.788081: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:46:18.788556: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:4/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-11 19:46:18.820165: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 19:46:18.820256: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 19:46:18.820401: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 19:46:18.820474: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 19:46:18.822218: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12347
2022-12-11 19:46:18.822225: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12342
2022-12-11 19:46:18.824592: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 19:46:18.824765: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 19:46:18.826494: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12346
2022-12-11 19:46:18.834731: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 19:46:18.834894: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 19:46:18.836703: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12344
[HCTR][19:46:20.102][ERROR][RK0][main]: using mock embedding with 882774585 * 128 elements
[HCTR][19:46:20.103][ERROR][RK0][main]: using mock embedding with 882774585 * 128 elements
[HCTR][19:46:20.103][ERROR][RK0][main]: using mock embedding with 882774585 * 128 elements
[HCTR][19:46:20.103][ERROR][RK0][main]: using mock embedding with 882774585 * 128 elements
[HCTR][19:46:20.104][ERROR][RK0][main]: using mock embedding with 882774585 * 128 elements
[HCTR][19:46:20.104][ERROR][RK0][main]: using mock embedding with 882774585 * 128 elements
[HCTR][19:46:20.104][ERROR][RK0][main]: using mock embedding with 882774585 * 128 elements
[HCTR][19:46:20.104][ERROR][RK0][main]: using mock embedding with 882774585 * 128 elements
warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 1it [00:00,  2.48it/s]warmup run: 1it [00:00,  2.47it/s]warmup run: 1it [00:00,  2.43it/s]warmup run: 1it [00:00,  2.07it/s]warmup run: 64it [00:00, 166.24it/s]warmup run: 62it [00:00, 160.06it/s]warmup run: 67it [00:00, 171.18it/s]warmup run: 63it [00:00, 142.97it/s]warmup run: 117it [00:00, 264.78it/s]warmup run: 116it [00:00, 262.17it/s]warmup run: 120it [00:00, 266.33it/s]warmup run: 116it [00:00, 237.92it/s]warmup run: 170it [00:00, 338.36it/s]warmup run: 171it [00:00, 341.83it/s]warmup run: 169it [00:00, 327.44it/s]warmup run: 168it [00:00, 309.97it/s]warmup run: 222it [00:00, 388.66it/s]warmup run: 227it [00:00, 403.14it/s]warmup run: 217it [00:00, 369.35it/s]warmup run: 218it [00:00, 360.42it/s]warmup run: 271it [00:00, 417.35it/s]warmup run: 284it [00:00, 449.95it/s]warmup run: 269it [00:00, 411.54it/s]warmup run: 268it [00:00, 398.35it/s]warmup run: 341it [00:01, 483.10it/s]warmup run: 320it [00:01, 429.20it/s]warmup run: 325it [00:01, 453.52it/s]warmup run: 318it [00:01, 426.24it/s]warmup run: 396it [00:01, 501.48it/s]warmup run: 382it [00:01, 486.05it/s]warmup run: 368it [00:01, 425.81it/s]warmup run: 368it [00:01, 446.54it/s]warmup run: 451it [00:01, 511.09it/s]warmup run: 438it [00:01, 507.63it/s]warmup run: 415it [00:01, 436.90it/s]warmup run: 418it [00:01, 461.42it/s]warmup run: 505it [00:01, 513.15it/s]warmup run: 495it [00:01, 524.86it/s]warmup run: 470it [00:01, 468.65it/s]warmup run: 468it [00:01, 470.40it/s]warmup run: 1it [00:01,  1.41s/it]warmup run: 1it [00:01,  1.41s/it]warmup run: 1it [00:01,  1.41s/it]warmup run: 561it [00:01, 525.38it/s]warmup run: 553it [00:01, 538.85it/s]warmup run: 522it [00:01, 483.52it/s]warmup run: 1it [00:01,  1.48s/it]warmup run: 518it [00:01, 476.20it/s]warmup run: 94it [00:01, 86.43it/s]warmup run: 97it [00:01, 89.17it/s]warmup run: 94it [00:01, 86.09it/s]warmup run: 618it [00:01, 537.29it/s]warmup run: 610it [00:01, 547.77it/s]warmup run: 574it [00:01, 492.69it/s]warmup run: 92it [00:01, 80.63it/s]warmup run: 573it [00:01, 497.81it/s]warmup run: 188it [00:01, 185.66it/s]warmup run: 194it [00:01, 191.56it/s]warmup run: 189it [00:01, 186.21it/s]warmup run: 675it [00:01, 545.37it/s]warmup run: 666it [00:01, 550.08it/s]warmup run: 627it [00:01, 501.24it/s]warmup run: 186it [00:01, 176.37it/s]warmup run: 629it [00:01, 514.30it/s]warmup run: 285it [00:01, 297.34it/s]warmup run: 290it [00:01, 300.73it/s]warmup run: 286it [00:01, 297.15it/s]warmup run: 732it [00:01, 550.77it/s]warmup run: 722it [00:01, 548.41it/s]warmup run: 680it [00:01, 508.10it/s]warmup run: 278it [00:01, 278.18it/s]warmup run: 684it [00:01, 524.06it/s]warmup run: 381it [00:01, 409.00it/s]warmup run: 385it [00:01, 410.32it/s]warmup run: 381it [00:01, 407.04it/s]warmup run: 789it [00:01, 553.74it/s]warmup run: 779it [00:01, 552.08it/s]warmup run: 734it [00:01, 515.94it/s]warmup run: 371it [00:01, 384.77it/s]warmup run: 737it [00:01, 519.19it/s]warmup run: 476it [00:01, 513.93it/s]warmup run: 481it [00:01, 516.83it/s]warmup run: 477it [00:01, 513.52it/s]warmup run: 846it [00:01, 556.79it/s]warmup run: 835it [00:01, 553.27it/s]warmup run: 789it [00:01, 525.89it/s]warmup run: 465it [00:01, 489.50it/s]warmup run: 790it [00:02, 516.16it/s]warmup run: 573it [00:02, 611.98it/s]warmup run: 582it [00:02, 623.39it/s]warmup run: 569it [00:02, 601.02it/s]warmup run: 903it [00:02, 559.77it/s]warmup run: 892it [00:02, 556.58it/s]warmup run: 845it [00:02, 535.80it/s]warmup run: 558it [00:02, 583.04it/s]warmup run: 842it [00:02, 516.99it/s]warmup run: 670it [00:02, 695.73it/s]warmup run: 682it [00:02, 711.13it/s]warmup run: 667it [00:02, 689.43it/s]warmup run: 960it [00:02, 562.36it/s]warmup run: 949it [00:02, 559.58it/s]warmup run: 899it [00:02, 517.16it/s]warmup run: 654it [00:02, 669.43it/s]warmup run: 898it [00:02, 528.98it/s]warmup run: 764it [00:02, 756.81it/s]warmup run: 781it [00:02, 780.04it/s]warmup run: 766it [00:02, 764.58it/s]warmup run: 1017it [00:02, 562.16it/s]warmup run: 1006it [00:02, 562.31it/s]warmup run: 951it [00:02, 507.99it/s]warmup run: 753it [00:02, 747.91it/s]warmup run: 955it [00:02, 539.97it/s]warmup run: 860it [00:02, 809.24it/s]warmup run: 878it [00:02, 826.75it/s]warmup run: 867it [00:02, 827.93it/s]warmup run: 1075it [00:02, 565.29it/s]warmup run: 1063it [00:02, 551.09it/s]warmup run: 1007it [00:02, 520.74it/s]warmup run: 852it [00:02, 809.29it/s]warmup run: 1011it [00:02, 545.61it/s]warmup run: 958it [00:02, 854.11it/s]warmup run: 974it [00:02, 855.50it/s]warmup run: 967it [00:02, 873.79it/s]warmup run: 1133it [00:02, 568.82it/s]warmup run: 1119it [00:02, 541.98it/s]warmup run: 1063it [00:02, 530.49it/s]warmup run: 950it [00:02, 853.40it/s]warmup run: 1068it [00:02, 550.99it/s]warmup run: 1055it [00:02, 884.64it/s]warmup run: 1069it [00:02, 912.50it/s]warmup run: 1070it [00:02, 872.44it/s]warmup run: 1190it [00:02, 565.63it/s]warmup run: 1174it [00:02, 541.76it/s]warmup run: 1118it [00:02, 535.89it/s]warmup run: 1048it [00:02, 887.79it/s]warmup run: 1124it [00:02, 552.44it/s]warmup run: 1152it [00:02, 906.81it/s]warmup run: 1168it [00:02, 934.39it/s]warmup run: 1166it [00:02, 895.35it/s]warmup run: 1247it [00:02, 549.83it/s]warmup run: 1229it [00:02, 537.50it/s]warmup run: 1173it [00:02, 539.07it/s]warmup run: 1145it [00:02, 902.50it/s]warmup run: 1180it [00:02, 546.60it/s]warmup run: 1250it [00:02, 926.46it/s]warmup run: 1269it [00:02, 955.59it/s]warmup run: 1261it [00:02, 894.26it/s]warmup run: 1304it [00:02, 553.67it/s]warmup run: 1283it [00:02, 537.14it/s]warmup run: 1228it [00:02, 540.99it/s]warmup run: 1241it [00:02, 908.20it/s]warmup run: 1235it [00:02, 538.46it/s]warmup run: 1354it [00:02, 904.30it/s]warmup run: 1369it [00:02, 913.83it/s]warmup run: 1347it [00:02, 862.92it/s]warmup run: 1360it [00:02, 554.58it/s]warmup run: 1342it [00:02, 552.44it/s]warmup run: 1283it [00:02, 535.30it/s]warmup run: 1336it [00:02, 897.03it/s]warmup run: 1289it [00:02, 533.69it/s]warmup run: 1416it [00:02, 553.84it/s]warmup run: 1401it [00:02, 561.56it/s]warmup run: 1337it [00:02, 522.52it/s]warmup run: 1447it [00:02, 770.66it/s]warmup run: 1437it [00:03, 749.06it/s]warmup run: 1464it [00:03, 769.47it/s]warmup run: 1343it [00:03, 525.97it/s]warmup run: 1472it [00:03, 553.62it/s]warmup run: 1458it [00:03, 559.65it/s]warmup run: 1429it [00:03, 768.34it/s]warmup run: 1390it [00:03, 517.66it/s]warmup run: 1396it [00:03, 508.26it/s]warmup run: 1517it [00:03, 690.24it/s]warmup run: 1530it [00:03, 689.42it/s]warmup run: 1528it [00:03, 546.33it/s]warmup run: 1514it [00:03, 550.12it/s]warmup run: 1547it [00:03, 692.00it/s]warmup run: 1445it [00:03, 526.66it/s]warmup run: 1511it [00:03, 698.15it/s]warmup run: 1447it [00:03, 502.63it/s]warmup run: 1585it [00:03, 553.01it/s]warmup run: 1570it [00:03, 546.20it/s]warmup run: 1590it [00:03, 655.36it/s]warmup run: 1501it [00:03, 535.73it/s]warmup run: 1604it [00:03, 634.44it/s]warmup run: 1622it [00:03, 644.71it/s]warmup run: 1498it [00:03, 501.07it/s]warmup run: 1642it [00:03, 557.87it/s]warmup run: 1625it [00:03, 545.34it/s]warmup run: 1585it [00:03, 617.39it/s]warmup run: 1558it [00:03, 543.67it/s]warmup run: 1659it [00:03, 631.76it/s]warmup run: 1672it [00:03, 607.85it/s]warmup run: 1690it [00:03, 616.17it/s]warmup run: 1551it [00:03, 506.92it/s]warmup run: 1698it [00:03, 544.03it/s]warmup run: 1680it [00:03, 544.02it/s]warmup run: 1615it [00:03, 549.96it/s]warmup run: 1651it [00:03, 585.27it/s]warmup run: 1724it [00:03, 612.79it/s]warmup run: 1736it [00:03, 585.67it/s]warmup run: 1754it [00:03, 592.44it/s]warmup run: 1602it [00:03, 505.88it/s]warmup run: 1735it [00:03, 541.60it/s]warmup run: 1753it [00:03, 527.27it/s]warmup run: 1672it [00:03, 553.88it/s]warmup run: 1713it [00:03, 577.34it/s]warmup run: 1787it [00:03, 600.77it/s]warmup run: 1797it [00:03, 571.97it/s]warmup run: 1653it [00:03, 504.62it/s]warmup run: 1815it [00:03, 579.22it/s]warmup run: 1792it [00:03, 547.86it/s]warmup run: 1806it [00:03, 519.27it/s]warmup run: 1728it [00:03, 553.92it/s]warmup run: 1848it [00:03, 592.46it/s]warmup run: 1773it [00:03, 558.22it/s]warmup run: 1856it [00:03, 571.22it/s]warmup run: 1704it [00:03, 504.05it/s]warmup run: 1874it [00:03, 571.78it/s]warmup run: 1850it [00:03, 555.17it/s]warmup run: 1784it [00:03, 554.31it/s]warmup run: 1859it [00:03, 514.22it/s]warmup run: 1908it [00:03, 582.38it/s]warmup run: 1830it [00:03, 544.62it/s]warmup run: 1755it [00:03, 504.42it/s]warmup run: 1914it [00:03, 563.16it/s]warmup run: 1932it [00:03, 568.48it/s]warmup run: 1907it [00:03, 558.01it/s]warmup run: 1840it [00:03, 552.08it/s]warmup run: 1911it [00:03, 509.31it/s]warmup run: 1967it [00:03, 571.62it/s]warmup run: 1810it [00:03, 517.35it/s]warmup run: 1886it [00:03, 534.46it/s]warmup run: 1971it [00:03, 564.12it/s]warmup run: 1990it [00:03, 567.25it/s]warmup run: 1964it [00:03, 558.05it/s]warmup run: 1896it [00:03, 548.43it/s]warmup run: 1962it [00:03, 506.85it/s]warmup run: 2025it [00:04, 570.81it/s]warmup run: 1868it [00:04, 535.06it/s]warmup run: 2028it [00:04, 565.10it/s]warmup run: 1940it [00:04, 525.14it/s]warmup run: 2047it [00:04, 566.41it/s]warmup run: 2020it [00:04, 556.47it/s]warmup run: 1951it [00:04, 545.03it/s]warmup run: 2015it [00:04, 511.57it/s]warmup run: 2083it [00:04, 570.75it/s]warmup run: 1925it [00:04, 544.47it/s]warmup run: 2085it [00:04, 561.36it/s]warmup run: 1993it [00:04, 518.45it/s]warmup run: 2105it [00:04, 568.45it/s]warmup run: 2076it [00:04, 557.09it/s]warmup run: 2068it [00:04, 516.41it/s]warmup run: 2007it [00:04, 546.83it/s]warmup run: 2141it [00:04, 568.54it/s]warmup run: 1983it [00:04, 552.58it/s]warmup run: 2142it [00:04, 550.77it/s]warmup run: 2045it [00:04, 511.14it/s]warmup run: 2163it [00:04, 569.54it/s]warmup run: 2132it [00:04, 557.55it/s]warmup run: 2120it [00:04, 517.10it/s]warmup run: 2063it [00:04, 549.74it/s]warmup run: 2198it [00:04, 567.38it/s]warmup run: 2040it [00:04, 556.76it/s]warmup run: 2198it [00:04, 551.59it/s]warmup run: 2097it [00:04, 507.68it/s]warmup run: 2221it [00:04, 569.72it/s]warmup run: 2188it [00:04, 555.03it/s]warmup run: 2175it [00:04, 526.01it/s]warmup run: 2118it [00:04, 548.94it/s]warmup run: 2256it [00:04, 569.23it/s]warmup run: 2097it [00:04, 560.22it/s]warmup run: 2255it [00:04, 555.24it/s]warmup run: 2150it [00:04, 513.19it/s]warmup run: 2279it [00:04, 569.42it/s]warmup run: 2244it [00:04, 556.21it/s]warmup run: 2175it [00:04, 552.56it/s]warmup run: 2228it [00:04, 518.39it/s]warmup run: 2314it [00:04, 570.79it/s]warmup run: 2154it [00:04, 563.02it/s]warmup run: 2312it [00:04, 559.26it/s]warmup run: 2202it [00:04, 513.12it/s]warmup run: 2336it [00:04, 568.64it/s]warmup run: 2300it [00:04, 553.79it/s]warmup run: 2231it [00:04, 553.74it/s]warmup run: 2281it [00:04, 519.68it/s]warmup run: 2372it [00:04, 566.37it/s]warmup run: 2211it [00:04, 563.49it/s]warmup run: 2369it [00:04, 559.94it/s]warmup run: 2257it [00:04, 521.61it/s]warmup run: 2393it [00:04, 559.39it/s]warmup run: 2287it [00:04, 553.69it/s]warmup run: 2356it [00:04, 528.98it/s]warmup run: 2333it [00:04, 519.13it/s]warmup run: 2429it [00:04, 561.58it/s]warmup run: 2268it [00:04, 563.37it/s]warmup run: 2426it [00:04, 560.54it/s]warmup run: 2314it [00:04, 535.37it/s]warmup run: 2450it [00:04, 560.89it/s]warmup run: 2343it [00:04, 555.24it/s]warmup run: 2385it [00:04, 510.96it/s]warmup run: 2410it [00:04, 516.30it/s]warmup run: 2486it [00:04, 562.86it/s]warmup run: 2325it [00:04, 561.63it/s]warmup run: 2370it [00:04, 542.58it/s]warmup run: 2483it [00:04, 544.66it/s]warmup run: 2508it [00:04, 563.75it/s]warmup run: 2400it [00:04, 557.73it/s]warmup run: 2437it [00:04, 512.57it/s]warmup run: 2462it [00:04, 510.62it/s]warmup run: 2543it [00:04, 564.47it/s]warmup run: 2382it [00:04, 541.47it/s]warmup run: 2427it [00:04, 549.28it/s]warmup run: 2566it [00:04, 566.00it/s]warmup run: 2457it [00:05, 559.79it/s]warmup run: 2538it [00:05, 518.03it/s]warmup run: 2495it [00:05, 530.45it/s]warmup run: 2514it [00:05, 505.61it/s]warmup run: 2600it [00:05, 565.64it/s]warmup run: 2437it [00:05, 534.13it/s]warmup run: 2623it [00:05, 566.61it/s]warmup run: 2482it [00:05, 531.71it/s]warmup run: 2514it [00:05, 560.00it/s]warmup run: 2553it [00:05, 542.68it/s]warmup run: 2591it [00:05, 514.82it/s]warmup run: 2565it [00:05, 502.82it/s]warmup run: 2657it [00:05, 565.10it/s]warmup run: 2494it [00:05, 544.13it/s]warmup run: 2536it [00:05, 525.57it/s]warmup run: 2571it [00:05, 561.43it/s]warmup run: 2610it [00:05, 549.82it/s]warmup run: 2645it [00:05, 521.03it/s]warmup run: 2616it [00:05, 501.90it/s]warmup run: 2551it [00:05, 550.28it/s]warmup run: 2590it [00:05, 528.82it/s]warmup run: 2628it [00:05, 559.20it/s]warmup run: 2680it [00:05, 363.77it/s]warmup run: 2608it [00:05, 554.54it/s]warmup run: 2647it [00:05, 540.40it/s]warmup run: 2714it [00:05, 356.79it/s]warmup run: 2737it [00:05, 407.34it/s]warmup run: 2666it [00:05, 355.00it/s]warmup run: 2698it [00:05, 335.36it/s]warmup run: 2667it [00:05, 320.59it/s]warmup run: 2770it [00:05, 399.25it/s]warmup run: 2794it [00:05, 444.56it/s]warmup run: 2721it [00:05, 396.09it/s]warmup run: 2747it [00:05, 367.28it/s]warmup run: 2717it [00:05, 357.09it/s]warmup run: 2684it [00:05, 331.74it/s]warmup run: 2825it [00:05, 433.85it/s]warmup run: 2664it [00:05, 360.94it/s]warmup run: 2851it [00:05, 475.67it/s]warmup run: 2702it [00:05, 349.89it/s]warmup run: 2778it [00:05, 436.07it/s]warmup run: 2796it [00:05, 395.28it/s]warmup run: 2768it [00:05, 391.11it/s]warmup run: 2741it [00:05, 378.60it/s]warmup run: 2882it [00:05, 467.54it/s]warmup run: 2721it [00:05, 405.76it/s]warmup run: 2908it [00:05, 499.79it/s]warmup run: 2752it [00:05, 380.44it/s]warmup run: 2835it [00:05, 469.55it/s]warmup run: 2848it [00:05, 425.80it/s]warmup run: 2817it [00:05, 413.88it/s]warmup run: 2797it [00:05, 418.46it/s]warmup run: 2938it [00:05, 490.92it/s]warmup run: 2778it [00:05, 442.63it/s]warmup run: 2965it [00:05, 517.09it/s]warmup run: 2809it [00:05, 422.89it/s]warmup run: 2893it [00:05, 496.48it/s]warmup run: 2903it [00:05, 456.92it/s]warmup run: 2873it [00:05, 451.11it/s]warmup run: 2854it [00:05, 454.65it/s]warmup run: 3000it [00:05, 504.01it/s]warmup run: 2995it [00:05, 511.67it/s]warmup run: 2834it [00:05, 471.97it/s]warmup run: 3000it [00:05, 501.87it/s]warmup run: 2862it [00:05, 449.48it/s]warmup run: 2949it [00:06, 512.97it/s]warmup run: 2958it [00:06, 480.99it/s]warmup run: 2925it [00:06, 467.36it/s]warmup run: 2910it [00:06, 480.49it/s]warmup run: 2891it [00:06, 496.91it/s]warmup run: 2915it [00:06, 469.73it/s]warmup run: 3000it [00:06, 492.29it/s]warmup run: 3000it [00:06, 491.77it/s]warmup run: 2977it [00:06, 481.43it/s]warmup run: 2966it [00:06, 501.53it/s]warmup run: 2948it [00:06, 516.77it/s]warmup run: 3000it [00:06, 485.67it/s]warmup run: 2968it [00:06, 484.46it/s]warmup run: 3000it [00:06, 482.99it/s]warmup run: 3000it [00:06, 478.73it/s]warmup run: 3000it [00:06, 478.10it/s]

warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]



warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   6%|▌         | 167/3000 [00:00<00:01, 1669.75it/s]warmup should be done:   6%|▌         | 169/3000 [00:00<00:01, 1687.05it/s]warmup should be done:   6%|▌         | 171/3000 [00:00<00:01, 1705.62it/s]warmup should be done:   5%|▌         | 156/3000 [00:00<00:01, 1556.88it/s]warmup should be done:   6%|▌         | 168/3000 [00:00<00:01, 1671.92it/s]warmup should be done:   6%|▌         | 170/3000 [00:00<00:01, 1695.62it/s]warmup should be done:   6%|▌         | 168/3000 [00:00<00:01, 1674.23it/s]warmup should be done:   6%|▌         | 165/3000 [00:00<00:01, 1643.10it/s]warmup should be done:  11%|█▏        | 339/3000 [00:00<00:01, 1692.21it/s]warmup should be done:  11%|█▏        | 341/3000 [00:00<00:01, 1702.63it/s]warmup should be done:  11%|█         | 337/3000 [00:00<00:01, 1682.14it/s]warmup should be done:  10%|█         | 313/3000 [00:00<00:01, 1559.16it/s]warmup should be done:  11%|█▏        | 338/3000 [00:00<00:01, 1689.90it/s]warmup should be done:  11%|█▏        | 342/3000 [00:00<00:01, 1699.57it/s]warmup should be done:  11%|█         | 336/3000 [00:00<00:01, 1670.50it/s]warmup should be done:  11%|█         | 336/3000 [00:00<00:01, 1647.48it/s]warmup should be done:  17%|█▋        | 506/3000 [00:00<00:01, 1685.18it/s]warmup should be done:  17%|█▋        | 509/3000 [00:00<00:01, 1692.38it/s]warmup should be done:  17%|█▋        | 512/3000 [00:00<00:01, 1692.12it/s]warmup should be done:  16%|█▌        | 469/3000 [00:00<00:01, 1548.73it/s]warmup should be done:  17%|█▋        | 512/3000 [00:00<00:01, 1688.86it/s]warmup should be done:  17%|█▋        | 507/3000 [00:00<00:01, 1676.63it/s]warmup should be done:  17%|█▋        | 504/3000 [00:00<00:01, 1661.13it/s]warmup should be done:  17%|█▋        | 501/3000 [00:00<00:01, 1625.80it/s]warmup should be done:  22%|██▎       | 675/3000 [00:00<00:01, 1686.25it/s]warmup should be done:  23%|██▎       | 681/3000 [00:00<00:01, 1702.42it/s]warmup should be done:  21%|██        | 625/3000 [00:00<00:01, 1550.24it/s]warmup should be done:  23%|██▎       | 678/3000 [00:00<00:01, 1689.30it/s]warmup should be done:  22%|██▏       | 672/3000 [00:00<00:01, 1667.70it/s]warmup should be done:  23%|██▎       | 682/3000 [00:00<00:01, 1690.06it/s]warmup should be done:  23%|██▎       | 681/3000 [00:00<00:01, 1685.15it/s]warmup should be done:  22%|██▏       | 670/3000 [00:00<00:01, 1647.87it/s]warmup should be done:  28%|██▊       | 853/3000 [00:00<00:01, 1705.86it/s]warmup should be done:  28%|██▊       | 844/3000 [00:00<00:01, 1677.77it/s]warmup should be done:  28%|██▊       | 849/3000 [00:00<00:01, 1694.94it/s]warmup should be done:  28%|██▊       | 839/3000 [00:00<00:01, 1667.71it/s]warmup should be done:  26%|██▌       | 781/3000 [00:00<00:01, 1551.09it/s]warmup should be done:  28%|██▊       | 850/3000 [00:00<00:01, 1680.96it/s]warmup should be done:  28%|██▊       | 852/3000 [00:00<00:01, 1683.46it/s]warmup should be done:  28%|██▊       | 835/3000 [00:00<00:01, 1629.51it/s]warmup should be done:  34%|███▍      | 1025/3000 [00:00<00:01, 1708.66it/s]warmup should be done:  34%|███▍      | 1013/3000 [00:00<00:01, 1679.50it/s]warmup should be done:  34%|███▎      | 1006/3000 [00:00<00:01, 1667.11it/s]warmup should be done:  34%|███▍      | 1020/3000 [00:00<00:01, 1697.63it/s]warmup should be done:  31%|███       | 937/3000 [00:00<00:01, 1550.02it/s]warmup should be done:  34%|███▍      | 1019/3000 [00:00<00:01, 1681.82it/s]warmup should be done:  34%|███▍      | 1021/3000 [00:00<00:01, 1682.41it/s]warmup should be done:  33%|███▎      | 998/3000 [00:00<00:01, 1626.80it/s]warmup should be done:  40%|███▉      | 1197/3000 [00:00<00:01, 1709.16it/s]warmup should be done:  39%|███▉      | 1182/3000 [00:00<00:01, 1680.05it/s]warmup should be done:  39%|███▉      | 1173/3000 [00:00<00:01, 1667.46it/s]warmup should be done:  40%|███▉      | 1191/3000 [00:00<00:01, 1699.62it/s]warmup should be done:  36%|███▋      | 1093/3000 [00:00<00:01, 1549.27it/s]warmup should be done:  40%|███▉      | 1189/3000 [00:00<00:01, 1685.61it/s]warmup should be done:  40%|███▉      | 1190/3000 [00:00<00:01, 1682.40it/s]warmup should be done:  39%|███▊      | 1161/3000 [00:00<00:01, 1626.48it/s]warmup should be done:  45%|████▍     | 1340/3000 [00:00<00:00, 1665.38it/s]warmup should be done:  45%|████▌     | 1351/3000 [00:00<00:00, 1679.05it/s]warmup should be done:  45%|████▌     | 1362/3000 [00:00<00:00, 1700.40it/s]warmup should be done:  46%|████▌     | 1368/3000 [00:00<00:00, 1696.40it/s]warmup should be done:  42%|████▏     | 1248/3000 [00:00<00:01, 1549.18it/s]warmup should be done:  45%|████▌     | 1359/3000 [00:00<00:00, 1689.70it/s]warmup should be done:  45%|████▌     | 1359/3000 [00:00<00:00, 1681.69it/s]warmup should be done:  44%|████▍     | 1324/3000 [00:00<00:01, 1626.81it/s]warmup should be done:  51%|█████     | 1533/3000 [00:00<00:00, 1702.12it/s]warmup should be done:  50%|█████     | 1508/3000 [00:00<00:00, 1667.36it/s]warmup should be done:  51%|█████     | 1520/3000 [00:00<00:00, 1679.46it/s]warmup should be done:  51%|█████▏    | 1539/3000 [00:00<00:00, 1699.79it/s]warmup should be done:  51%|█████     | 1529/3000 [00:00<00:00, 1691.40it/s]warmup should be done:  47%|████▋     | 1403/3000 [00:00<00:01, 1547.77it/s]warmup should be done:  51%|█████     | 1528/3000 [00:00<00:00, 1682.42it/s]warmup should be done:  50%|████▉     | 1487/3000 [00:00<00:00, 1625.58it/s]warmup should be done:  56%|█████▋    | 1688/3000 [00:01<00:00, 1678.44it/s]warmup should be done:  56%|█████▌    | 1675/3000 [00:01<00:00, 1666.06it/s]warmup should be done:  57%|█████▋    | 1704/3000 [00:01<00:00, 1700.98it/s]warmup should be done:  52%|█████▏    | 1558/3000 [00:01<00:00, 1547.70it/s]warmup should be done:  57%|█████▋    | 1710/3000 [00:01<00:00, 1699.98it/s]warmup should be done:  57%|█████▋    | 1699/3000 [00:01<00:00, 1688.50it/s]warmup should be done:  57%|█████▋    | 1697/3000 [00:01<00:00, 1680.52it/s]warmup should be done:  55%|█████▌    | 1650/3000 [00:01<00:00, 1622.76it/s]warmup should be done:  63%|██████▎   | 1881/3000 [00:01<00:00, 1702.61it/s]warmup should be done:  61%|██████▏   | 1842/3000 [00:01<00:00, 1664.61it/s]warmup should be done:  57%|█████▋    | 1713/3000 [00:01<00:00, 1547.19it/s]warmup should be done:  62%|██████▏   | 1856/3000 [00:01<00:00, 1674.39it/s]warmup should be done:  62%|██████▎   | 1875/3000 [00:01<00:00, 1700.55it/s]warmup should be done:  62%|██████▏   | 1868/3000 [00:01<00:00, 1677.42it/s]warmup should be done:  62%|██████▏   | 1866/3000 [00:01<00:00, 1677.45it/s]warmup should be done:  60%|██████    | 1814/3000 [00:01<00:00, 1627.37it/s]warmup should be done:  68%|██████▊   | 2052/3000 [00:01<00:00, 1704.33it/s]warmup should be done:  67%|██████▋   | 2009/3000 [00:01<00:00, 1665.07it/s]warmup should be done:  62%|██████▏   | 1868/3000 [00:01<00:00, 1546.48it/s]warmup should be done:  68%|██████▊   | 2025/3000 [00:01<00:00, 1677.11it/s]warmup should be done:  68%|██████▊   | 2046/3000 [00:01<00:00, 1701.05it/s]warmup should be done:  68%|██████▊   | 2034/3000 [00:01<00:00, 1677.81it/s]warmup should be done:  68%|██████▊   | 2036/3000 [00:01<00:00, 1671.69it/s]warmup should be done:  66%|██████▌   | 1979/3000 [00:01<00:00, 1634.12it/s]warmup should be done:  74%|███████▍  | 2223/3000 [00:01<00:00, 1705.76it/s]warmup should be done:  73%|███████▎  | 2176/3000 [00:01<00:00, 1666.17it/s]warmup should be done:  67%|██████▋   | 2023/3000 [00:01<00:00, 1547.35it/s]warmup should be done:  74%|███████▍  | 2217/3000 [00:01<00:00, 1702.60it/s]warmup should be done:  73%|███████▎  | 2194/3000 [00:01<00:00, 1678.96it/s]warmup should be done:  73%|███████▎  | 2202/3000 [00:01<00:00, 1678.20it/s]warmup should be done:  71%|███████▏  | 2144/3000 [00:01<00:00, 1637.87it/s]warmup should be done:  73%|███████▎  | 2204/3000 [00:01<00:00, 1658.33it/s]warmup should be done:  80%|███████▉  | 2394/3000 [00:01<00:00, 1706.94it/s]warmup should be done:  78%|███████▊  | 2345/3000 [00:01<00:00, 1672.42it/s]warmup should be done:  73%|███████▎  | 2178/3000 [00:01<00:00, 1547.95it/s]warmup should be done:  80%|███████▉  | 2388/3000 [00:01<00:00, 1703.02it/s]warmup should be done:  79%|███████▊  | 2362/3000 [00:01<00:00, 1674.03it/s]warmup should be done:  79%|███████▉  | 2370/3000 [00:01<00:00, 1678.27it/s]warmup should be done:  77%|███████▋  | 2309/3000 [00:01<00:00, 1640.71it/s]warmup should be done:  79%|███████▉  | 2372/3000 [00:01<00:00, 1662.88it/s]warmup should be done:  86%|████████▌ | 2565/3000 [00:01<00:00, 1707.34it/s]warmup should be done:  84%|████████▍ | 2514/3000 [00:01<00:00, 1677.26it/s]warmup should be done:  78%|███████▊  | 2333/3000 [00:01<00:00, 1548.14it/s]warmup should be done:  85%|████████▌ | 2559/3000 [00:01<00:00, 1703.34it/s]warmup should be done:  84%|████████▍ | 2531/3000 [00:01<00:00, 1677.11it/s]warmup should be done:  85%|████████▍ | 2538/3000 [00:01<00:00, 1677.82it/s]warmup should be done:  85%|████████▍ | 2541/3000 [00:01<00:00, 1668.34it/s]warmup should be done:  82%|████████▏ | 2474/3000 [00:01<00:00, 1632.90it/s]warmup should be done:  89%|████████▉ | 2683/3000 [00:01<00:00, 1680.77it/s]warmup should be done:  91%|█████████ | 2736/3000 [00:01<00:00, 1705.47it/s]warmup should be done:  83%|████████▎ | 2488/3000 [00:01<00:00, 1548.01it/s]warmup should be done:  90%|█████████ | 2700/3000 [00:01<00:00, 1679.12it/s]warmup should be done:  91%|█████████ | 2730/3000 [00:01<00:00, 1694.55it/s]warmup should be done:  90%|█████████ | 2706/3000 [00:01<00:00, 1667.37it/s]warmup should be done:  90%|█████████ | 2711/3000 [00:01<00:00, 1675.03it/s]warmup should be done:  88%|████████▊ | 2638/3000 [00:01<00:00, 1532.70it/s]warmup should be done:  95%|█████████▌| 2853/3000 [00:01<00:00, 1684.92it/s]warmup should be done:  88%|████████▊ | 2643/3000 [00:01<00:00, 1547.44it/s]warmup should be done:  97%|█████████▋| 2907/3000 [00:01<00:00, 1696.00it/s]warmup should be done:  96%|█████████▌| 2868/3000 [00:01<00:00, 1673.37it/s]warmup should be done:  97%|█████████▋| 2902/3000 [00:01<00:00, 1699.22it/s]warmup should be done:  96%|█████████▌| 2874/3000 [00:01<00:00, 1670.16it/s]warmup should be done:  96%|█████████▌| 2879/3000 [00:01<00:00, 1672.02it/s]warmup should be done:  94%|█████████▎| 2809/3000 [00:01<00:00, 1581.39it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1700.62it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1697.55it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1679.54it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1677.89it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1677.16it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1672.96it/s]warmup should be done:  93%|█████████▎| 2798/3000 [00:01<00:00, 1547.39it/s]warmup should be done:  99%|█████████▉| 2980/3000 [00:01<00:00, 1617.62it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1621.33it/s]warmup should be done:  98%|█████████▊| 2955/3000 [00:01<00:00, 1552.46it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1549.20it/s]




warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]


warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   6%|▌         | 170/3000 [00:00<00:01, 1699.65it/s]warmup should be done:   6%|▌         | 172/3000 [00:00<00:01, 1717.59it/s]warmup should be done:   5%|▌         | 160/3000 [00:00<00:01, 1596.78it/s]warmup should be done:   6%|▌         | 176/3000 [00:00<00:01, 1754.55it/s]warmup should be done:   6%|▌         | 175/3000 [00:00<00:01, 1743.33it/s]warmup should be done:   6%|▌         | 175/3000 [00:00<00:01, 1743.54it/s]warmup should be done:   6%|▌         | 173/3000 [00:00<00:01, 1722.26it/s]warmup should be done:   6%|▌         | 173/3000 [00:00<00:01, 1720.84it/s]warmup should be done:  11%|█▏        | 341/3000 [00:00<00:01, 1701.94it/s]warmup should be done:  12%|█▏        | 352/3000 [00:00<00:01, 1755.53it/s]warmup should be done:  11%|█         | 320/3000 [00:00<00:01, 1595.20it/s]warmup should be done:  12%|█▏        | 345/3000 [00:00<00:01, 1720.67it/s]warmup should be done:  12%|█▏        | 350/3000 [00:00<00:01, 1745.18it/s]warmup should be done:  12%|█▏        | 351/3000 [00:00<00:01, 1749.13it/s]warmup should be done:  12%|█▏        | 347/3000 [00:00<00:01, 1728.09it/s]warmup should be done:  12%|█▏        | 347/3000 [00:00<00:01, 1726.86it/s]warmup should be done:  18%|█▊        | 525/3000 [00:00<00:01, 1747.13it/s]warmup should be done:  17%|█▋        | 518/3000 [00:00<00:01, 1723.81it/s]warmup should be done:  17%|█▋        | 512/3000 [00:00<00:01, 1703.39it/s]warmup should be done:  18%|█▊        | 528/3000 [00:00<00:01, 1755.30it/s]warmup should be done:  17%|█▋        | 521/3000 [00:00<00:01, 1732.42it/s]warmup should be done:  16%|█▌        | 481/3000 [00:00<00:01, 1597.55it/s]warmup should be done:  18%|█▊        | 527/3000 [00:00<00:01, 1750.98it/s]warmup should be done:  17%|█▋        | 521/3000 [00:00<00:01, 1728.72it/s]warmup should be done:  23%|██▎       | 704/3000 [00:00<00:01, 1756.52it/s]warmup should be done:  23%|██▎       | 692/3000 [00:00<00:01, 1726.98it/s]warmup should be done:  23%|██▎       | 695/3000 [00:00<00:01, 1734.43it/s]warmup should be done:  23%|██▎       | 703/3000 [00:00<00:01, 1754.25it/s]warmup should be done:  21%|██▏       | 642/3000 [00:00<00:01, 1600.98it/s]warmup should be done:  23%|██▎       | 695/3000 [00:00<00:01, 1730.74it/s]warmup should be done:  23%|██▎       | 700/3000 [00:00<00:01, 1737.03it/s]warmup should be done:  23%|██▎       | 683/3000 [00:00<00:01, 1654.14it/s]warmup should be done:  29%|██▉       | 880/3000 [00:00<00:01, 1757.40it/s]warmup should be done:  29%|██▉       | 869/3000 [00:00<00:01, 1733.72it/s]warmup should be done:  29%|██▉       | 866/3000 [00:00<00:01, 1728.70it/s]warmup should be done:  29%|██▉       | 880/3000 [00:00<00:01, 1756.39it/s]warmup should be done:  27%|██▋       | 803/3000 [00:00<00:01, 1601.54it/s]warmup should be done:  29%|██▉       | 869/3000 [00:00<00:01, 1731.54it/s]warmup should be done:  29%|██▉       | 874/3000 [00:00<00:01, 1731.34it/s]warmup should be done:  28%|██▊       | 855/3000 [00:00<00:01, 1675.64it/s]warmup should be done:  35%|███▌      | 1056/3000 [00:00<00:01, 1756.76it/s]warmup should be done:  35%|███▍      | 1039/3000 [00:00<00:01, 1728.63it/s]warmup should be done:  32%|███▏      | 964/3000 [00:00<00:01, 1601.72it/s]warmup should be done:  35%|███▍      | 1043/3000 [00:00<00:01, 1731.90it/s]warmup should be done:  35%|███▌      | 1057/3000 [00:00<00:01, 1757.82it/s]warmup should be done:  35%|███▍      | 1043/3000 [00:00<00:01, 1730.29it/s]warmup should be done:  35%|███▍      | 1048/3000 [00:00<00:01, 1720.99it/s]warmup should be done:  34%|███▍      | 1027/3000 [00:00<00:01, 1687.86it/s]warmup should be done:  38%|███▊      | 1125/3000 [00:00<00:01, 1603.20it/s]warmup should be done:  40%|████      | 1212/3000 [00:00<00:01, 1723.60it/s]warmup should be done:  41%|████      | 1217/3000 [00:00<00:01, 1730.89it/s]warmup should be done:  41%|████      | 1233/3000 [00:00<00:01, 1754.81it/s]warmup should be done:  41%|████      | 1217/3000 [00:00<00:01, 1728.84it/s]warmup should be done:  41%|████      | 1232/3000 [00:00<00:01, 1741.75it/s]warmup should be done:  41%|████      | 1221/3000 [00:00<00:01, 1712.39it/s]warmup should be done:  40%|███▉      | 1196/3000 [00:00<00:01, 1685.37it/s]warmup should be done:  46%|████▌     | 1386/3000 [00:00<00:00, 1726.99it/s]warmup should be done:  47%|████▋     | 1410/3000 [00:00<00:00, 1757.69it/s]warmup should be done:  46%|████▋     | 1392/3000 [00:00<00:00, 1734.53it/s]warmup should be done:  43%|████▎     | 1286/3000 [00:00<00:01, 1598.23it/s]warmup should be done:  46%|████▋     | 1391/3000 [00:00<00:00, 1731.60it/s]warmup should be done:  47%|████▋     | 1409/3000 [00:00<00:00, 1749.07it/s]warmup should be done:  46%|████▋     | 1393/3000 [00:00<00:00, 1713.54it/s]warmup should be done:  46%|████▌     | 1370/3000 [00:00<00:00, 1700.00it/s]warmup should be done:  52%|█████▏    | 1559/3000 [00:00<00:00, 1726.38it/s]warmup should be done:  53%|█████▎    | 1586/3000 [00:00<00:00, 1756.88it/s]warmup should be done:  52%|█████▏    | 1566/3000 [00:00<00:00, 1734.12it/s]warmup should be done:  48%|████▊     | 1447/3000 [00:00<00:00, 1601.46it/s]warmup should be done:  53%|█████▎    | 1585/3000 [00:00<00:00, 1751.87it/s]warmup should be done:  52%|█████▏    | 1565/3000 [00:00<00:00, 1717.21it/s]warmup should be done:  52%|█████▏    | 1565/3000 [00:00<00:00, 1709.42it/s]warmup should be done:  51%|█████▏    | 1541/3000 [00:00<00:00, 1702.58it/s]warmup should be done:  58%|█████▊    | 1732/3000 [00:01<00:00, 1727.07it/s]warmup should be done:  59%|█████▊    | 1762/3000 [00:01<00:00, 1757.45it/s]warmup should be done:  58%|█████▊    | 1740/3000 [00:01<00:00, 1734.06it/s]warmup should be done:  54%|█████▎    | 1608/3000 [00:01<00:00, 1603.37it/s]warmup should be done:  59%|█████▊    | 1761/3000 [00:01<00:00, 1753.37it/s]warmup should be done:  58%|█████▊    | 1739/3000 [00:01<00:00, 1722.54it/s]warmup should be done:  58%|█████▊    | 1736/3000 [00:01<00:00, 1707.01it/s]warmup should be done:  57%|█████▋    | 1713/3000 [00:01<00:00, 1705.11it/s]warmup should be done:  64%|██████▎   | 1905/3000 [00:01<00:00, 1727.44it/s]warmup should be done:  64%|██████▍   | 1914/3000 [00:01<00:00, 1733.17it/s]warmup should be done:  65%|██████▍   | 1937/3000 [00:01<00:00, 1754.82it/s]warmup should be done:  59%|█████▉    | 1769/3000 [00:01<00:00, 1602.04it/s]warmup should be done:  65%|██████▍   | 1938/3000 [00:01<00:00, 1751.31it/s]warmup should be done:  64%|██████▍   | 1913/3000 [00:01<00:00, 1727.11it/s]warmup should be done:  64%|██████▎   | 1908/3000 [00:01<00:00, 1708.64it/s]warmup should be done:  63%|██████▎   | 1885/3000 [00:01<00:00, 1709.23it/s]warmup should be done:  69%|██████▉   | 2079/3000 [00:01<00:00, 1729.43it/s]warmup should be done:  70%|███████   | 2113/3000 [00:01<00:00, 1756.18it/s]warmup should be done:  64%|██████▍   | 1930/3000 [00:01<00:00, 1602.39it/s]warmup should be done:  70%|██████▉   | 2088/3000 [00:01<00:00, 1730.69it/s]warmup should be done:  70%|███████   | 2114/3000 [00:01<00:00, 1748.45it/s]warmup should be done:  70%|██████▉   | 2088/3000 [00:01<00:00, 1731.17it/s]warmup should be done:  69%|██████▉   | 2079/3000 [00:01<00:00, 1707.00it/s]warmup should be done:  69%|██████▊   | 2057/3000 [00:01<00:00, 1712.09it/s]warmup should be done:  75%|███████▌  | 2253/3000 [00:01<00:00, 1731.35it/s]warmup should be done:  76%|███████▋  | 2289/3000 [00:01<00:00, 1756.86it/s]warmup should be done:  70%|██████▉   | 2091/3000 [00:01<00:00, 1604.23it/s]warmup should be done:  76%|███████▋  | 2289/3000 [00:01<00:00, 1747.78it/s]warmup should be done:  75%|███████▌  | 2263/3000 [00:01<00:00, 1734.08it/s]warmup should be done:  75%|███████▌  | 2262/3000 [00:01<00:00, 1714.23it/s]warmup should be done:  75%|███████▌  | 2250/3000 [00:01<00:00, 1702.28it/s]warmup should be done:  74%|███████▍  | 2229/3000 [00:01<00:00, 1683.13it/s]warmup should be done:  81%|████████  | 2427/3000 [00:01<00:00, 1727.88it/s]warmup should be done:  82%|████████▏ | 2465/3000 [00:01<00:00, 1755.42it/s]warmup should be done:  82%|████████▏ | 2464/3000 [00:01<00:00, 1745.70it/s]warmup should be done:  75%|███████▌  | 2252/3000 [00:01<00:00, 1598.64it/s]warmup should be done:  81%|████████  | 2437/3000 [00:01<00:00, 1734.46it/s]warmup should be done:  81%|████████  | 2434/3000 [00:01<00:00, 1713.92it/s]warmup should be done:  81%|████████  | 2421/3000 [00:01<00:00, 1696.74it/s]warmup should be done:  80%|████████  | 2404/3000 [00:01<00:00, 1700.23it/s]warmup should be done:  88%|████████▊ | 2641/3000 [00:01<00:00, 1755.29it/s]warmup should be done:  87%|████████▋ | 2600/3000 [00:01<00:00, 1723.36it/s]warmup should be done:  88%|████████▊ | 2639/3000 [00:01<00:00, 1742.21it/s]warmup should be done:  80%|████████  | 2412/3000 [00:01<00:00, 1595.23it/s]warmup should be done:  87%|████████▋ | 2611/3000 [00:01<00:00, 1734.70it/s]warmup should be done:  87%|████████▋ | 2606/3000 [00:01<00:00, 1713.73it/s]warmup should be done:  86%|████████▋ | 2591/3000 [00:01<00:00, 1693.18it/s]warmup should be done:  86%|████████▌ | 2579/3000 [00:01<00:00, 1714.77it/s]warmup should be done:  94%|█████████▍| 2817/3000 [00:01<00:00, 1753.08it/s]warmup should be done:  92%|█████████▏| 2773/3000 [00:01<00:00, 1719.74it/s]warmup should be done:  86%|████████▌ | 2573/3000 [00:01<00:00, 1596.71it/s]warmup should be done:  94%|█████████▍| 2814/3000 [00:01<00:00, 1741.07it/s]warmup should be done:  93%|█████████▎| 2786/3000 [00:01<00:00, 1736.70it/s]warmup should be done:  93%|█████████▎| 2778/3000 [00:01<00:00, 1715.08it/s]warmup should be done:  92%|█████████▏| 2761/3000 [00:01<00:00, 1693.55it/s]warmup should be done:  92%|█████████▏| 2754/3000 [00:01<00:00, 1724.33it/s]warmup should be done:  98%|█████████▊| 2945/3000 [00:01<00:00, 1715.84it/s]warmup should be done: 100%|█████████▉| 2993/3000 [00:01<00:00, 1744.64it/s]warmup should be done:  91%|█████████ | 2734/3000 [00:01<00:00, 1599.55it/s]warmup should be done: 100%|█████████▉| 2989/3000 [00:01<00:00, 1741.44it/s]warmup should be done:  99%|█████████▊| 2961/3000 [00:01<00:00, 1738.26it/s]warmup should be done:  98%|█████████▊| 2951/3000 [00:01<00:00, 1717.51it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1751.65it/s]warmup should be done:  98%|█████████▊| 2931/3000 [00:01<00:00, 1694.32it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1748.91it/s]warmup should be done:  98%|█████████▊| 2929/3000 [00:01<00:00, 1729.70it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1731.15it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1724.82it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1723.54it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1708.72it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1704.97it/s]warmup should be done:  96%|█████████▋| 2894/3000 [00:01<00:00, 1598.11it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1599.88it/s]
WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7fec66c6fd60>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7fec66c6e730>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7fec66bb0100>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7fec66c6db80>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7fec66bb01f0>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7fec66baf2e0>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7fec66bb0190>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7fec66bb0160>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
2022-12-11 19:47:24.551097: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fe7a282a250 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 19:47:24.551169: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 19:47:24.555344: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fe796b8a130 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 19:47:24.555385: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 19:47:24.560799: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 19:47:24.564191: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 19:47:24.815634: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fe78682e430 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 19:47:24.815701: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 19:47:24.825129: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 19:47:24.827534: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fe796f91b10 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 19:47:24.827579: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 19:47:24.837079: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 19:47:25.118273: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fe79a82b5a0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 19:47:25.118337: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 19:47:25.127893: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 19:47:25.147405: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fe79ab81500 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 19:47:25.147468: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 19:47:25.151681: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fe79eb819e0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 19:47:25.151742: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 19:47:25.153317: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fe7a282e5a0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 19:47:25.153359: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 19:47:25.156322: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 19:47:25.160055: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 19:47:25.163118: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 19:47:27.176635: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 19:47:27.189477: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 19:47:27.446174: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 19:47:27.447312: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 19:47:27.504568: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 19:47:27.505709: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 19:47:27.532003: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 19:47:27.591760: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
[HCTR][19:47:49.224][ERROR][RK0][tid #140633293432576]: replica 6 reaches 1000, calling init pre replica
[HCTR][19:47:49.224][ERROR][RK0][tid #140633293432576]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][19:47:49.233][ERROR][RK0][tid #140633293432576]: coll ps creation done
[HCTR][19:47:49.233][ERROR][RK0][tid #140633293432576]: replica 6 waits for coll ps creation barrier
[HCTR][19:47:49.244][ERROR][RK0][tid #140633217931008]: replica 7 reaches 1000, calling init pre replica
[HCTR][19:47:49.245][ERROR][RK0][tid #140633217931008]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][19:47:49.252][ERROR][RK0][main]: replica 4 reaches 1000, calling init pre replica
[HCTR][19:47:49.252][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][19:47:49.256][ERROR][RK0][main]: coll ps creation done
[HCTR][19:47:49.257][ERROR][RK0][main]: replica 4 waits for coll ps creation barrier
[HCTR][19:47:49.258][ERROR][RK0][tid #140634157455104]: replica 1 reaches 1000, calling init pre replica
[HCTR][19:47:49.258][ERROR][RK0][tid #140633217931008]: coll ps creation done
[HCTR][19:47:49.258][ERROR][RK0][tid #140634157455104]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][19:47:49.258][ERROR][RK0][tid #140633217931008]: replica 7 waits for coll ps creation barrier
[HCTR][19:47:49.263][ERROR][RK0][tid #140634157455104]: coll ps creation done
[HCTR][19:47:49.263][ERROR][RK0][tid #140634157455104]: replica 1 waits for coll ps creation barrier
[HCTR][19:47:49.309][ERROR][RK0][tid #140633285039872]: replica 3 reaches 1000, calling init pre replica
[HCTR][19:47:49.309][ERROR][RK0][tid #140633285039872]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][19:47:49.314][ERROR][RK0][tid #140633285039872]: coll ps creation done
[HCTR][19:47:49.314][ERROR][RK0][tid #140633285039872]: replica 3 waits for coll ps creation barrier
[HCTR][19:47:49.324][ERROR][RK0][tid #140633293432576]: replica 5 reaches 1000, calling init pre replica
[HCTR][19:47:49.325][ERROR][RK0][tid #140633293432576]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][19:47:49.330][ERROR][RK0][tid #140633293432576]: coll ps creation done
[HCTR][19:47:49.330][ERROR][RK0][tid #140633293432576]: replica 5 waits for coll ps creation barrier
[HCTR][19:47:49.335][ERROR][RK0][tid #140633628976896]: replica 2 reaches 1000, calling init pre replica
[HCTR][19:47:49.335][ERROR][RK0][tid #140633628976896]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][19:47:49.341][ERROR][RK0][tid #140633628976896]: coll ps creation done
[HCTR][19:47:49.341][ERROR][RK0][tid #140633628976896]: replica 2 waits for coll ps creation barrier
[HCTR][19:47:49.375][ERROR][RK0][tid #140633285039872]: replica 0 reaches 1000, calling init pre replica
[HCTR][19:47:49.375][ERROR][RK0][tid #140633285039872]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][19:47:49.380][ERROR][RK0][tid #140633285039872]: coll ps creation done
[HCTR][19:47:49.380][ERROR][RK0][tid #140633285039872]: replica 0 waits for coll ps creation barrier
[HCTR][19:47:49.380][ERROR][RK0][tid #140633285039872]: replica 0 preparing frequency
[HCTR][19:47:56.441][ERROR][RK0][tid #140633285039872]: replica 0 preparing frequency done
[HCTR][19:47:56.483][ERROR][RK0][tid #140633285039872]: replica 0 calling init per replica
[HCTR][19:47:56.483][ERROR][RK0][tid #140633293432576]: replica 5 calling init per replica
[HCTR][19:47:56.483][ERROR][RK0][main]: replica 4 calling init per replica
[HCTR][19:47:56.483][ERROR][RK0][tid #140633293432576]: replica 6 calling init per replica
[HCTR][19:47:56.483][ERROR][RK0][main]: Calling build_v2
[HCTR][19:47:56.483][ERROR][RK0][tid #140634157455104]: replica 1 calling init per replica
[HCTR][19:47:56.483][ERROR][RK0][tid #140633217931008]: replica 7 calling init per replica
[HCTR][19:47:56.483][ERROR][RK0][tid #140633628976896]: replica 2 calling init per replica
[HCTR][19:47:56.483][ERROR][RK0][tid #140633285039872]: replica 3 calling init per replica
[HCTR][19:47:56.483][ERROR][RK0][tid #140633285039872]: Calling build_v2
[HCTR][19:47:56.483][ERROR][RK0][tid #140633293432576]: Calling build_v2
[HCTR][19:47:56.483][ERROR][RK0][tid #140633293432576]: Calling build_v2
[HCTR][19:47:56.483][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][19:47:56.483][ERROR][RK0][tid #140634157455104]: Calling build_v2
[HCTR][19:47:56.483][ERROR][RK0][tid #140633217931008]: Calling build_v2
[HCTR][19:47:56.483][ERROR][RK0][tid #140633628976896]: Calling build_v2
[HCTR][19:47:56.483][ERROR][RK0][tid #140633285039872]: Calling build_v2
[HCTR][19:47:56.483][ERROR][RK0][tid #140633293432576]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][19:47:56.483][ERROR][RK0][tid #140633285039872]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][19:47:56.483][ERROR][RK0][tid #140633293432576]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][19:47:56.483][ERROR][RK0][tid #140634157455104]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][19:47:56.483][ERROR][RK0][tid #140633217931008]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][19:47:56.483][ERROR][RK0][tid #140633628976896]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][19:47:56.483][ERROR][RK0][tid #140633285039872]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[2022-12-11 19:47:56.487333: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:178] v100x8, slow pcie
[2022-12-11 19:47:56.487403: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[:196] assigning 0 to cpu
2022-12-11 19:47:56.487394: [E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-11 19:47:56[:.2022-12-11 19:47:56178487443[.487483] : : 2022-12-11 19:47:56v100x8, slow pcieEE.[
  487487/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-11 19:47:56: ::[.[E1782122022-12-11 19:47:56487531 ] ] 2022-12-11 19:47:56.: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccv100x8, slow pciebuild asymm link desc with 8X Tesla V100-SXM2-32GB out of 8.[487580E:

4875782022-12-11 19:47:56:  178: [.E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] [E[2022-12-11 19:47:56487623 :v100x8, slow pcie2022-12-11 19:47:56 .: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc178
./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc487690E2022-12-11 19:47:56:] 487706:: [ .196v100x8, slow pcie: 178E2022-12-11 19:47:56/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc487687] 
E]  .:: assigning 0 to cpu v100x8, slow pcie/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[487793178E
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
:2022-12-11 19:47:56: ]  :196.E[v100x8, slow pcie/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc213] 487886 2022-12-11 19:47:56
:] assigning 0 to cpu: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.178[remote time is 8.68421[
E:487942] 2022-12-11 19:47:56
2022-12-11 19:47:56 196: v100x8, slow pcie../hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] [E
[487996487990:assigning 0 to cpu2022-12-11 19:47:56 2022-12-11 19:47:56: : 196[
./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.EE] 2022-12-11 19:47:56488052:488065  assigning 0 to cpu.: 196: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
488100E] E:2022-12-11 19:47:56::  assigning 0 to cpu 196.212/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE[
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] 488184] : 2022-12-11 19:47:56:assigning 0 to cpu: build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8[214/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.212
E
2022-12-11 19:47:56] :488251]  .cpu time is 97.0588196: build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[488309
] E
:[2022-12-11 19:47:56: assigning 0 to cpu 2122022-12-11 19:47:56.E[
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] .488354 2022-12-11 19:47:56:build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8488373/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: .212
: :E488425] E212 [: build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8 ] [2022-12-11 19:47:56E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccbuild asymm link desc with 8X Tesla V100-SXM2-32GB out of 8.2022-12-11 19:47:56488511 ::
.[: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2132124885132022-12-11 19:47:56E[:] ] : . 2022-12-11 19:47:56213remote time is 8.68421build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8E488568/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.] 

 : :488600remote time is 8.68421/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE213[[: 
: ] 2022-12-11 19:47:562022-12-11 19:47:56E212/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[remote time is 8.68421.. ] :2022-12-11 19:47:56
488697488700: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccbuild asymm link desc with 8X Tesla V100-SXM2-32GB out of 8213.: [E:
] 488738E2022-12-11 19:47:56 213remote time is 8.68421:  ./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[] 
E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc488817:2022-12-11 19:47:56remote time is 8.68421 :[: 214.
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2132022-12-11 19:47:56E] 488867:] .[ cpu time is 97.0588: 214remote time is 8.684214889052022-12-11 19:47:56/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
E] 
: .: cpu time is 97.0588E488948214[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
 : ] 2022-12-11 19:47:56:/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccEcpu time is 97.0588.213: 
489020] 214/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: remote time is 8.68421] :E
cpu time is 97.0588214 
] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cccpu time is 97.0588:
[2142022-12-11 19:47:56] .cpu time is 97.0588489134
: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:214] cpu time is 97.0588
[2022-12-11 19:49:40.258606: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:118] solver created. now build & solve
[2022-12-11 19:49:40.609114: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:123] solver built. now solve
[2022-12-11 19:49:40.609208: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:919] num_cached_nodes = 35310982
[2022-12-11 19:49:42. 70754: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:127] solver solved
[2022-12-11 19:49:42. 70864: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:195] 0 solved master
[2022-12-11 19:49:42. 70908: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 0 solved
[2022-12-11 19:49:42. 70950: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 0 initing device 0
[2022-12-11 19:49:42. 71431: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 3.29 GB
[2022-12-11 19:49:42. 76129: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 3.29 GB
[2022-12-11 19:49:42. 80185: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 3.29 GB
[2022-12-11 19:49:42.204247: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 4 solved
[2022-12-11 19:49:42.204327: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 4 initing device 4
[2022-12-11 19:49:42.204707: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 3.29 GB
[2022-12-11 19:49:42.204759: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 5 solved
[2022-12-11 19:49:42.204834: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 5 initing device 5
[2022-12-11 19:49:42.205211: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 3.29 GB
[2022-12-11 19:49:42.206774: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 3 solved
[2022-12-11 19:49:42.206837: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 3 initing device 3
[2022-12-11 19:49:42.207210: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 3.29 GB
[2022-12-11 19:49:42.208533: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 1 solved
[2022-12-11 19:49:42.208589: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 1 initing device 1
[2022-12-11 19:49:42.[2089472022-12-11 19:49:42: .E208939 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE: 1980/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc] :eager alloc mem 3.29 GB202
] 6 solved
[2022-12-11 19:49:42.209050: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 6 initing device 6
[2022-12-11 19:49:42.209406: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 3.29 GB
[2022-12-11 19:49:42.210005: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 2 solved[
2022-12-11 19:49:42.210039: [E2022-12-11 19:49:42 ./hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc210073:: 202E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc] :7 solved205
] worker 0 thread 2 initing device 2
[2022-12-11 19:49:42.210146: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 7 initing device 7
[2022-12-11 19:49:42.210470: E [/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu2022-12-11 19:49:42:.1980210485] : eager alloc mem 3.29 GBE
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 3.29 GB
[2022-12-11 19:49:42.231380: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 3.29 GB
[2022-12-11 19:49:42.231802: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 3.29 GB
[2022-12-11 19:49:42.231937: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 3.29 GB
[2022-12-11 19:49:42.235692: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 3.29 GB
[2022-12-11 19:49:42.235755: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 3.29 GB
[2022-12-11 19:49:42.235856: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 3.29 GB
[2022-12-11 19:49:42.235951: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 3.29 GB
[2022-12-11 19:49:42.260561: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 3.29 GB
[2022-12-11 19:49:42.260974: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 3.29 GB
[2022-12-11 19:49:42.261064: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 3.29 GB
[2022-12-11 19:49:42.264827: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 3.29 GB
[2022-12-11 19:49:42.264902: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 3.29 GB
[2022-12-11 19:49:42.265040: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 3.29 GB
[2022-12-11 19:49:42.265132: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 3.29 GB
[2022-12-11 19:49:42.753121: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 2.00 Bytes
[2022-12-11 19:49:42.753545: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1024.00 Bytes
[2022-12-11 19:49:42.753595: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1683] using empty feat=27
[2022-12-11 19:49:42.777954: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-11 19:49:42.778087: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 2
[2022-12-11 19:49:42.778139: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 3531098340
[2022-12-11 19:49:42.782474: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 138.07 MB
[2022-12-11 19:49:42.783307: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:49:42.790671: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:49:42.791205: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 19:49:42.796974: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 19:49:42.797030: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 16.84 GB
[[[2022-12-11 19:49:432022-12-11 19:49:432022-12-11 19:49:43... 30736 30757 30757: : : EEE   /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:::198019801980] ] ] eager alloc mem 2.00 Byteseager alloc mem 2.00 Byteseager alloc mem 2.00 Bytes


[2022-12-11 19:49:43. 31176[: [2022-12-11 19:49:43E2022-12-11 19:49:43. . 31183/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu 31185: :: E1980E ]  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cueager alloc mem 1024.00 Bytes/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:
:19801980] ] eager alloc mem 1024.00 Byteseager alloc mem 1024.00 Bytes

[2022-12-11 19:49:43. 31272: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1683] [using empty feat=27[2022-12-11 19:49:43
2022-12-11 19:49:43.. 31291 31293: : WW  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::16831683] ] using empty feat=27using empty feat=27

[2022-12-11 19:49:43. 49091: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-11 19:49:43. 49161: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638[] 2022-12-11 19:49:43eager release cuda mem 2.
 49166: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-11 19:49:43. 49216: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] [eager release cuda mem 35310983402022-12-11 19:49:43
. 49237: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 2
[[2022-12-11 19:49:432022-12-11 19:49:43.. 49283 49270: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::638638] ] eager release cuda mem 3531098340eager release cuda mem 1024

[2022-12-11 19:49:43. 49358: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 2
[2022-12-11 19:49:43. 49400: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 3531098340
[2022-12-11 19:49:43. 53804: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 138.07 MB
[2022-12-11 19:49:43. 54820: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 2.00 Bytes
[[[2022-12-11 19:49:432022-12-11 19:49:432022-12-11 19:49:43... 54903 54906 54905: : : EEE   /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:::198019801980] ] ] eager alloc mem 2.00 Byteseager alloc mem 2.00 Byteseager alloc mem 2.00 Bytes


[2022-12-11 19:49:43. 55200: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1024.00 Bytes
[2022-12-11 19:49:43. 55255: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1683] using empty feat=27
[2022-12-11 19:49:43.[ 55315[2022-12-11 19:49:43: 2022-12-11 19:49:43.E. 55320  55323: /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu: E:E 1980 /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu] /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:eager alloc mem 1024.00 Bytes:1980
1980] ] eager alloc mem 1024.00 Byteseager alloc mem 1024.00 Bytes

[2022-12-11 19:49:43. 55421: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:[[16832022-12-11 19:49:432022-12-11 19:49:43] ..using empty feat=27 55435 55435
: : WW  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::16831683] ] using empty feat=27using empty feat=27

[2022-12-11 19:49:43. 57817: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 138.07 MB
[2022-12-11 19:49:43. 61981: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 138.07 MB
[2022-12-11 19:49:43. 64131: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:49:43. 64459: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:49:43. 64650: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:49:43. 71626: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:49:43. 71899: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:49:43. 71958: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 19:49:43. 72013: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:49:43. 72286: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 19:49:43. 72435: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 19:49:43. 77670: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 19:49:43. 77715: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 16.84 GB
[2022-12-11 19:49:43. 77990: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 19:49:43. 78033: E [/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu2022-12-11 19:49:43:.1980]  78028eager alloc mem 16.84 GB: 
E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-11 19:49:43. 78128: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 2
[2022-12-11 19:49:43. 78153: E[ 2022-12-11 19:49:43/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc[.:2022-12-11 19:49:43 78171638.: ]  78168Eeager release cuda mem 25855:  
E/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc :/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc638:] 638eager release cuda mem 3531098340] [
eager release cuda mem 1024[2022-12-11 19:49:43
2022-12-11 19:49:43.. 78261 78256: : E[E 2022-12-11 19:49:43 /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu./hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc: 78313[:1980: 2022-12-11 19:49:43638] E.] eager alloc mem 16.84 GB  78340eager release cuda mem 1024
/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc: 
:E638 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cceager release cuda mem 2:
638] [eager release cuda mem 10242022-12-11 19:49:43
. 78431: E [/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc2022-12-11 19:49:43:.638 78449[] : 2022-12-11 19:49:43eager release cuda mem 2E.
  78460/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc: :E638 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cceager release cuda mem 3531098340[:
2022-12-11 19:49:43638.]  78496eager release cuda mem 2: 
E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 3531098340
[2022-12-11 19:49:43. 78549: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 3531098340
[2022-12-11 19:49:43.279669: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 138.07 MB
[2022-12-11 19:49:43.283656: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 138.07 MB
[2022-12-11 19:49:43.287754: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 138.07 MB
[2022-12-11 19:49:43.291672: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 138.07 MB
[2022-12-11 19:49:43.294778: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] [eager alloc mem 5.26 MB2022-12-11 19:49:43
.294801: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:49:43.294845: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:49:43.294928: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:49:43.302597: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:49:43.302699: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:49:43.302742: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:49:43.302786: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:49:43.302977: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 19:49:43.303359: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 19:49:43.303427: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 19:49:43.303493: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 19:49:43.308676: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 19:49:43.308718: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 16.84 GB
[2022-12-11 19:49:43.309069: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 19:49:43.309112: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 16.84 GB
[2022-12-11 19:49:43.309144: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 19:49:43.309188[: 2022-12-11 19:49:43E. 309189/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu: :E1980 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cceager alloc mem 16.84 GB:
638] eager release cuda mem 25855
[2022-12-11 19:49:43.309254: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 16.84 GB
[[[[[[[[2022-12-11 19:49:462022-12-11 19:49:462022-12-11 19:49:462022-12-11 19:49:462022-12-11 19:49:462022-12-11 19:49:462022-12-11 19:49:462022-12-11 19:49:46........485366485366485366485406485367485373485373485393: : : : : : : : EEEEEEEE        /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::::::::19801980198019801980198019801980] ] eager alloc mem 5.26 MB] ] ] ] ] ] eager alloc mem 5.26 MB
eager alloc mem 5.26 MBeager alloc mem 5.26 MBeager alloc mem 5.26 MBeager alloc mem 5.26 MBeager alloc mem 5.26 MBeager alloc mem 5.26 MB






[2022-12-11 19:49:46.494856: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:49:46.494931: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:49:46.494964: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[[2022-12-11 19:49:462022-12-11 19:49:46..495072495078: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::638638] ] eager release cuda mem 5518079[
eager release cuda mem 55180792022-12-11 19:49:46
.495151: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079[
2022-12-11 19:49:46.495196: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:49:46.495232: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:49:46.495443: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:49:46.495913: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:49:46.496371: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:49:46.496710: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:49:46.497088: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:49:46.497142: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:49:46.497433: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:49:46.497500: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:49:46.504836: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:49:46.505150: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:49:46.505244: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:49:46.505428: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:49:46.505607: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:49:46.505703: E [/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc2022-12-11 19:49:46:.638505713[] : [2022-12-11 19:49:46eager release cuda mem 5518079E2022-12-11 19:49:46[.
 .2022-12-11 19:49:46505756/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc505768.: :: 505797E638E:  ]  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cceager release cuda mem 5518079/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu :
:/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc6381980:] ] 638eager release cuda mem 5518079eager alloc mem 5.26 MB] 

eager release cuda mem 5518079
[2022-12-11 19:49:46.505995: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:49:46.506797: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:49:46.507141: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:49:46.507288: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:49:46.507603: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:49:46.507661: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:49:46.511740: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:49:46.512056: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:49:46.514210: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:49:46.514461: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:49:46.514515: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:49:46.514982: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:49:46.515118: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:49:46.515282: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:49:46.515364: E [/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc2022-12-11 19:49:46:.638515396] : eager release cuda mem 5518079[E
2022-12-11 19:49:46 ./hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc515437:: 638E]  eager release cuda mem 5518079/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc
:638] eager release cuda mem 5518079
[2022-12-11 19:49:46.515620: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:49:46.516237: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:49:46.516517: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:49:46.516572: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:49:46.516823: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:49:46.518408: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:49:46.518709: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:49:46.521027: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:49:46.521334: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:49:46.521597: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:49:46.521948: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:49:46.523362: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:49:46.523878: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:49:46.523956: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:49:46.524097[: 2022-12-11 19:49:46E.524123 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccE: 638[/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc] 2022-12-11 19:49:46:.eager release cuda mem 5518079638524171
] : eager release cuda mem 5518079E
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:49:46.524632: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:49:46.525077: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:49:46.525195: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980[] 2022-12-11 19:49:46.eager alloc mem 5.26 MB525240
: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:49:46.525305: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:49:46.526439: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:49:46.527741: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:49:46.528057: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:49:46.528317: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:49:46.528664: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:49:46.530638: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:49:46.530941: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:49:46.532250: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:49:46.532552: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:49:46.532652: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:49:46.532884: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:49:46.532926: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:49:46.533086: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:49:46.533303: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:49:46.533582: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:49:46.533970: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:49:46.534285: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:49:46.534476: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:49:46.535034: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:49:46.535508: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:49:46.535675: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:49:46.537318: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:49:46.537613: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:49:46.539277: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:49:46.539592: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:49:46.540994: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:49:46.541132: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:49:46.541324: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:49:46.541417: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:49:46.541486: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:49:46.541588: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:49:46.542217: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB[
2022-12-11 19:49:46.542237: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:49:46.542271: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:49:46.542409: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 144775028
[2022-12-11 19:49:46.543272: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 144775028
[2022-12-11 19:49:46.543335: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 144775028
[2022-12-11 19:49:46.543569: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 35310982 / 882774585 nodes ( 4.00 %~4.00 %) | remote 0 / 882774585 nodes ( 0.00 %) | cpu 847463603 / 882774585 nodes ( 96.00 %) | 16.84 GB | 4.3331 secs 
[2022-12-11 19:49:46.543968: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:49:46.544044: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 35310982 / 882774585 nodes ( 4.00 %~4.00 %) | remote 0 / 882774585 nodes ( 0.00 %) | cpu 847463603 / 882774585 nodes ( 96.00 %) | 16.84 GB | 4.33684 secs 
[2022-12-11 19:49:46.544271: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 35310982 / 882774585 nodes ( 4.00 %~4.00 %) | remote 0 / 882774585 nodes ( 0.00 %) | cpu 847463603 / 882774585 nodes ( 96.00 %) | 16.84 GB | 4.33533 secs 
[2022-12-11 19:49:46.544446: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 144775028
[2022-12-11 19:49:46.544843: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 35310982 / 882774585 nodes ( 4.00 %~4.00 %) | remote 0 / 882774585 nodes ( 0.00 %) | cpu 847463603 / 882774585 nodes ( 96.00 %) | 16.84 GB | 4.33964 secs 
[2022-12-11 19:49:46.545967: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:49:46.546222: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 144775028
[2022-12-11 19:49:46.546623: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 35310982 / 882774585 nodes ( 4.00 %~4.00 %) | remote 0 / 882774585 nodes ( 0.00 %) | cpu 847463603 / 882774585 nodes ( 96.00 %) | 16.84 GB | 4.34193 secs 
[2022-12-11 19:49:46.548806: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:49:46.548949: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:49:46.549073: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 144775028
[2022-12-11 19:49:46.549372: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 144775028
[[2022-12-11 19:49:462022-12-11 19:49:46..549599549607: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::6381793] ] eager release cuda mem 5518079Collaborative GPU cache (policy: rep_cache) | local 35310982 / 882774585 nodes ( 4.00 %~4.00 %) | remote 0 / 882774585 nodes ( 0.00 %) | cpu 847463603 / 882774585 nodes ( 96.00 %) | 16.84 GB | 4.34021 secs 

[2022-12-11 19:49:46.549958: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 35310982 / 882774585 nodes ( 4.00 %~4.00 %) | remote 0 / 882774585 nodes ( 0.00 %) | cpu 847463603 / 882774585 nodes ( 96.00 %) | 16.84 GB | 4.47853 secs 
[2022-12-11 19:49:46.550126: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 144775028
[2022-12-11 19:49:46.550518: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 35310982 / 882774585 nodes ( 4.00 %~4.00 %) | remote 0 / 882774585 nodes ( 0.00 %) | cpu 847463603 / 882774585 nodes ( 96.00 %) | 16.84 GB | 4.34004 secs 
[HCTR][19:49:46.550][ERROR][RK0][tid #140633217931008]: replica 7 calling init per replica done, doing barrier
[HCTR][19:49:46.550][ERROR][RK0][tid #140633285039872]: replica 0 calling init per replica done, doing barrier
[HCTR][19:49:46.550][ERROR][RK0][tid #140633293432576]: replica 6 calling init per replica done, doing barrier
[HCTR][19:49:46.550][ERROR][RK0][tid #140634157455104]: replica 1 calling init per replica done, doing barrier
[HCTR][19:49:46.550][ERROR][RK0][tid #140633628976896]: replica 2 calling init per replica done, doing barrier
[HCTR][19:49:46.550][ERROR][RK0][main]: replica 4 calling init per replica done, doing barrier
[HCTR][19:49:46.550][ERROR][RK0][tid #140633285039872]: replica 3 calling init per replica done, doing barrier
[HCTR][19:49:46.550][ERROR][RK0][tid #140633293432576]: replica 5 calling init per replica done, doing barrier
[HCTR][19:49:46.550][ERROR][RK0][tid #140633293432576]: replica 5 calling init per replica done, doing barrier done
[HCTR][19:49:46.550][ERROR][RK0][tid #140633628976896]: replica 2 calling init per replica done, doing barrier done
[HCTR][19:49:46.550][ERROR][RK0][main]: replica 4 calling init per replica done, doing barrier done
[HCTR][19:49:46.550][ERROR][RK0][tid #140633285039872]: replica 0 calling init per replica done, doing barrier done
[HCTR][19:49:46.550][ERROR][RK0][tid #140633217931008]: replica 7 calling init per replica done, doing barrier done
[HCTR][19:49:46.550][ERROR][RK0][tid #140634157455104]: replica 1 calling init per replica done, doing barrier done
[HCTR][19:49:46.550][ERROR][RK0][tid #140633285039872]: replica 3 calling init per replica done, doing barrier done
[HCTR][19:49:46.550][ERROR][RK0][main]: init per replica done
[HCTR][19:49:46.550][ERROR][RK0][tid #140633217931008]: init per replica done
[HCTR][19:49:46.550][ERROR][RK0][tid #140633293432576]: replica 6 calling init per replica done, doing barrier done
[HCTR][19:49:46.550][ERROR][RK0][tid #140633293432576]: init per replica done
[HCTR][19:49:46.550][ERROR][RK0][tid #140633628976896]: init per replica done
[HCTR][19:49:46.550][ERROR][RK0][tid #140634157455104]: init per replica done
[HCTR][19:49:46.550][ERROR][RK0][tid #140633285039872]: init per replica done
[HCTR][19:49:46.550][ERROR][RK0][tid #140633293432576]: init per replica done
[HCTR][19:49:46.569][ERROR][RK0][tid #140633285039872]: init per replica done
