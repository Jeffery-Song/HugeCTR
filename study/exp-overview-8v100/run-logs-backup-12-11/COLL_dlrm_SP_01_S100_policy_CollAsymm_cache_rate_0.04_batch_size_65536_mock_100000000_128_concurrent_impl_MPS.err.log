2022-12-12 00:09:33.324905: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.331370: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.339612: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.344123: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.349225: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.362018: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.367783: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.380399: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.432182: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.440485: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.443642: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.444671: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.445613: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.446635: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.453133: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.454792: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.455952: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.456039: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.457922: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.457952: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.459531: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.459649: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.461120: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.461334: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.462616: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.463090: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.464035: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.464869: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.465442: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.466715: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.467094: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.468405: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.469639: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.470666: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.471661: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.472623: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.473647: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.474618: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.475577: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.476521: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.480982: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.481711: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 00:09:33.482196: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.483687: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.485141: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.485233: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.486871: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.487051: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.488559: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.488805: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.490019: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.490452: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.491424: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.491428: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.492034: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.493445: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.493840: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.494872: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.495489: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.497035: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.501574: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.504233: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.504430: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.507086: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.507321: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.507842: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.509621: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.509963: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.510671: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.512229: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.512562: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.513343: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.514460: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.514724: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.515064: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.516121: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.516920: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.517731: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.519315: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.519552: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.520698: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.521529: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.521788: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.522560: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.522996: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.524298: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.524863: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.524992: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.538130: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.542495: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.560345: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.561697: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.562178: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.562256: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.563647: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.563732: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.565619: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.565808: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.566411: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.567411: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.567664: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.569932: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.570216: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.570974: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.572418: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.572461: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.574004: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.574217: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.575112: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.575678: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.575909: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.577547: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.577596: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.579280: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.579401: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.579496: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.581515: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.581606: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.583294: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.583355: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.584805: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.584991: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.586095: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.586335: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.587496: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.587908: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.588990: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.589126: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.590478: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.590804: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.591804: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.591893: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.593283: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.593607: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.594739: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.594878: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.596324: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.596602: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.597604: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.597766: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.599216: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.599685: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.600801: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.600846: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.602193: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.603406: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.603439: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.603550: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.604752: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.606137: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.606353: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.606368: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.607751: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.608496: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.608919: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.609411: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.609415: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.610959: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.611852: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.612952: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.613290: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.614418: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.614461: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.615259: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.616332: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.616812: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.617903: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.617958: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.618613: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.618802: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.620028: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.620409: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.621679: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.622454: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.622471: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.622605: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.623860: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.624432: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.625640: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.626387: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.626650: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.626681: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.627075: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.628220: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.628680: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.629992: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.630905: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.631278: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.631619: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.631975: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.633373: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.634188: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.635024: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.635334: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.635553: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.635859: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.637099: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.637175: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 00:09:33.638265: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.638824: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.639013: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.639154: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.639857: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.640873: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.642113: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.642955: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.643079: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.643349: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.643745: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.646364: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.646592: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.647001: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.647121: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.647349: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.649156: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 00:09:33.649425: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 00:09:33.649774: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.650220: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.650486: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.650716: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.650919: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.653359: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.653793: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.654274: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.654390: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.656461: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.656747: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.657138: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.658479: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 00:09:33.658935: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.658939: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.659219: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.659706: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.660168: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.662808: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.662899: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.663160: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.663876: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.664194: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.666865: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.666978: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.667213: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.667858: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.674019: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.674848: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.677049: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.677419: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.678642: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.679532: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.681472: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.682024: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.683434: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.684361: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.715084: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.715506: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.716976: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.720341: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.720652: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.722248: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.725798: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.726720: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.726884: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.731110: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.732001: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.732060: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.737775: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.738910: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.739792: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.742897: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.743792: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.747571: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 00:09:33.747964: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.748694: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.757180: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.780359: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.781711: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.788190: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.793774: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.794287: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.796651: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 00:09:33.799996: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.806985: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.864288: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.864611: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 00:09:33.869911: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.874252: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.903055: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:33.909818: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:34.750098: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:34.750720: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:34.751504: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:34.751981: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 00:09:34.752033: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-12 00:09:34.770207: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:34.771491: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:34.772009: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:34.772942: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:34.773650: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:34.774121: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-12 00:09:34.820609: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 00:09:34.820823: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 00:09:34.871817: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12340
2022-12-12 00:09:35.022103: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:35.022736: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:35.023889: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:35.024574: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 00:09:35.024631: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-12 00:09:35.041060: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:35.041721: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:35.042402: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:35.042511: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:35.043427: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 00:09:35.043476: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-12 00:09:35.043668: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:35.044288: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:35.044863: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:35.045591: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:35.046059: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:1/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-12 00:09:35.058080: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:35.058914: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:35.059472: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:35.059947: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 00:09:35.060000: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-12 00:09:35.061448: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:35.062039: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:35.062566: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:35.063421: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:35.064153: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:35.064654: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:2/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-12 00:09:35.070480: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:35.071059: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:35.071612: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:35.072088: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 00:09:35.072133: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-12 00:09:35.077852: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:35.078901: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:35.079459: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:35.080031: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:35.080549: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:35.081015: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:4/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-12 00:09:35.090793: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:35.091571: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:35.092087: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:35.092668: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:35.093421: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:35.093891: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:3/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-12 00:09:35.120380: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:35.121021: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:35.121594: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:35.122078: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 00:09:35.122131: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-12 00:09:35.123097: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:35.123736: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:35.124273: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:35.124702: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:35.124907: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 00:09:35.124977: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-12 00:09:35.125635: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:35.126057: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 00:09:35.126182: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:35.126245: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 00:09:35.126662: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 00:09:35.126705: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-12 00:09:35.128247: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12344
2022-12-12 00:09:35.139798: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 00:09:35.139992: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 00:09:35.140336: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:35.140987: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:35.141500: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:35.141794: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12343
2022-12-12 00:09:35.142030: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:35.142097: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:35.143278: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:35.143309: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:35.144396: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:35.144412: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:5/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-12 00:09:35.144453: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:35.145437: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:35.145495: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:35.145836: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 00:09:35.145982: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 00:09:35.146375: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:35.146451: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:35.147279: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:6/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-12 00:09:35.147464: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:35.147838: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12341
2022-12-12 00:09:35.147975: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:09:35.148443: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:7/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-12 00:09:35.164619: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 00:09:35.164828: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 00:09:35.165933: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12342
2022-12-12 00:09:35.190251: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 00:09:35.190479: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 00:09:35.191542: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12345
2022-12-12 00:09:35.193917: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 00:09:35.194111: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 00:09:35.195147: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12346
2022-12-12 00:09:35.195423: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 00:09:35.195577: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 00:09:35.196446: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12347
[HCTR][00:09:36.465][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][00:09:36.465][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][00:09:36.465][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][00:09:36.465][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][00:09:36.466][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][00:09:36.466][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][00:09:36.466][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][00:09:36.466][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 1it [00:01,  1.55s/it]warmup run: 97it [00:01, 81.42it/s]warmup run: 1it [00:01,  1.52s/it]warmup run: 1it [00:01,  1.50s/it]warmup run: 194it [00:01, 176.72it/s]warmup run: 99it [00:01, 84.50it/s]warmup run: 1it [00:01,  1.51s/it]warmup run: 99it [00:01, 85.53it/s]warmup run: 1it [00:01,  1.51s/it]warmup run: 1it [00:01,  1.52s/it]warmup run: 292it [00:01, 283.26it/s]warmup run: 196it [00:01, 180.82it/s]warmup run: 1it [00:01,  1.48s/it]warmup run: 95it [00:01, 81.61it/s]warmup run: 200it [00:01, 187.22it/s]warmup run: 99it [00:01, 85.49it/s]warmup run: 1it [00:01,  1.47s/it]warmup run: 96it [00:01, 81.99it/s]warmup run: 388it [00:01, 390.90it/s]warmup run: 296it [00:01, 290.83it/s]warmup run: 98it [00:01, 86.16it/s]warmup run: 192it [00:01, 178.91it/s]warmup run: 301it [00:01, 298.72it/s]warmup run: 198it [00:01, 184.90it/s]warmup run: 95it [00:01, 83.64it/s]warmup run: 193it [00:01, 178.72it/s]warmup run: 485it [00:02, 497.85it/s]warmup run: 397it [00:01, 406.46it/s]warmup run: 191it [00:01, 180.56it/s]warmup run: 289it [00:01, 285.68it/s]warmup run: 402it [00:01, 413.83it/s]warmup run: 296it [00:01, 292.85it/s]warmup run: 190it [00:01, 180.66it/s]warmup run: 291it [00:01, 286.50it/s]warmup run: 582it [00:02, 596.09it/s]warmup run: 497it [00:02, 517.20it/s]warmup run: 288it [00:01, 289.29it/s]warmup run: 384it [00:01, 392.88it/s]warmup run: 503it [00:02, 525.25it/s]warmup run: 392it [00:01, 400.66it/s]warmup run: 286it [00:01, 288.04it/s]warmup run: 390it [00:01, 399.66it/s]warmup run: 677it [00:02, 675.95it/s]warmup run: 598it [00:02, 620.36it/s]warmup run: 383it [00:01, 397.69it/s]warmup run: 477it [00:02, 493.08it/s]warmup run: 606it [00:02, 631.19it/s]warmup run: 487it [00:02, 503.37it/s]warmup run: 382it [00:01, 398.54it/s]warmup run: 489it [00:02, 509.38it/s]warmup run: 772it [00:02, 743.55it/s]warmup run: 694it [00:02, 694.41it/s]warmup run: 475it [00:01, 496.06it/s]warmup run: 568it [00:02, 580.34it/s]warmup run: 709it [00:02, 722.16it/s]warmup run: 582it [00:02, 597.81it/s]warmup run: 477it [00:01, 502.83it/s]warmup run: 589it [00:02, 612.38it/s]warmup run: 870it [00:02, 804.26it/s]warmup run: 790it [00:02, 760.00it/s]warmup run: 570it [00:02, 592.15it/s]warmup run: 666it [00:02, 672.33it/s]warmup run: 811it [00:02, 795.80it/s]warmup run: 678it [00:02, 681.06it/s]warmup run: 573it [00:02, 599.73it/s]warmup run: 689it [00:02, 700.73it/s]warmup run: 970it [00:02, 855.13it/s]warmup run: 890it [00:02, 821.86it/s]warmup run: 666it [00:02, 676.41it/s]warmup run: 764it [00:02, 748.46it/s]warmup run: 913it [00:02, 853.77it/s]warmup run: 670it [00:02, 684.87it/s]warmup run: 789it [00:02, 774.76it/s]warmup run: 1071it [00:02, 896.91it/s]warmup run: 772it [00:02, 711.44it/s]warmup run: 990it [00:02, 867.75it/s]warmup run: 765it [00:02, 753.36it/s]warmup run: 863it [00:02, 810.53it/s]warmup run: 1015it [00:02, 897.09it/s]warmup run: 766it [00:02, 752.37it/s]warmup run: 890it [00:02, 835.23it/s]warmup run: 1169it [00:02, 878.34it/s]warmup run: 1091it [00:02, 905.94it/s]warmup run: 861it [00:02, 718.62it/s]warmup run: 862it [00:02, 809.65it/s]warmup run: 961it [00:02, 856.06it/s]warmup run: 1117it [00:02, 931.17it/s]warmup run: 863it [00:02, 807.58it/s]warmup run: 991it [00:02, 881.73it/s]warmup run: 1192it [00:02, 933.98it/s]warmup run: 954it [00:02, 771.64it/s]warmup run: 1263it [00:02, 872.75it/s]warmup run: 959it [00:02, 851.77it/s]warmup run: 1059it [00:02, 890.53it/s]warmup run: 1220it [00:02, 958.92it/s]warmup run: 958it [00:02, 845.06it/s]warmup run: 1091it [00:02, 913.75it/s]warmup run: 1294it [00:02, 956.78it/s]warmup run: 1051it [00:02, 823.43it/s]warmup run: 1363it [00:02, 906.18it/s]warmup run: 1057it [00:02, 885.69it/s]warmup run: 1157it [00:02, 914.34it/s]warmup run: 1323it [00:02, 977.50it/s]warmup run: 1055it [00:02, 878.08it/s]warmup run: 1191it [00:02, 937.01it/s]warmup run: 1395it [00:02, 971.86it/s]warmup run: 1143it [00:02, 848.81it/s]warmup run: 1462it [00:03, 929.43it/s]warmup run: 1155it [00:02, 911.15it/s]warmup run: 1257it [00:02, 936.53it/s]warmup run: 1425it [00:02, 987.85it/s]warmup run: 1151it [00:02, 901.41it/s]warmup run: 1292it [00:02, 956.64it/s]warmup run: 1496it [00:03, 979.79it/s]warmup run: 1239it [00:02, 878.54it/s]warmup run: 1561it [00:03, 944.04it/s]warmup run: 1255it [00:02, 934.93it/s]warmup run: 1356it [00:02, 950.12it/s]warmup run: 1527it [00:03, 989.56it/s]warmup run: 1249it [00:02, 922.11it/s]warmup run: 1392it [00:02, 968.37it/s]warmup run: 1596it [00:03, 981.00it/s]warmup run: 1336it [00:02, 902.84it/s]warmup run: 1657it [00:03, 940.56it/s]warmup run: 1356it [00:02, 954.31it/s]warmup run: 1454it [00:03, 957.38it/s]warmup run: 1346it [00:02, 933.75it/s]warmup run: 1628it [00:03, 990.83it/s]warmup run: 1493it [00:03, 979.35it/s]warmup run: 1696it [00:03, 983.20it/s]warmup run: 1433it [00:03, 920.29it/s]warmup run: 1754it [00:03, 946.30it/s]warmup run: 1458it [00:02, 972.37it/s]warmup run: 1552it [00:03, 959.28it/s]warmup run: 1443it [00:02, 943.60it/s]warmup run: 1729it [00:03, 994.15it/s]warmup run: 1594it [00:03, 988.37it/s]warmup run: 1796it [00:03, 985.23it/s]warmup run: 1529it [00:03, 931.80it/s]warmup run: 1850it [00:03, 949.58it/s]warmup run: 1559it [00:03, 982.17it/s]warmup run: 1650it [00:03, 965.01it/s]warmup run: 1542it [00:03, 955.98it/s]warmup run: 1830it [00:03, 995.67it/s]warmup run: 1697it [00:03, 998.95it/s]warmup run: 1896it [00:03, 986.24it/s]warmup run: 1626it [00:03, 942.61it/s]warmup run: 1948it [00:03, 955.64it/s]warmup run: 1661it [00:03, 990.77it/s]warmup run: 1748it [00:03, 965.63it/s]warmup run: 1642it [00:03, 967.18it/s]warmup run: 1931it [00:03, 996.73it/s]warmup run: 1799it [00:03, 1004.86it/s]warmup run: 1996it [00:03, 985.74it/s]warmup run: 1724it [00:03, 951.89it/s]warmup run: 2053it [00:03, 982.75it/s]warmup run: 1762it [00:03, 995.73it/s]warmup run: 1742it [00:03, 975.46it/s]warmup run: 2036it [00:03, 1011.09it/s]warmup run: 1846it [00:03, 943.77it/s]warmup run: 1901it [00:03, 1009.04it/s]warmup run: 2107it [00:03, 1021.56it/s]warmup run: 1822it [00:03, 957.77it/s]warmup run: 2169it [00:03, 1035.29it/s]warmup run: 1863it [00:03, 997.67it/s]warmup run: 1841it [00:03, 975.64it/s]warmup run: 2153it [00:03, 1057.65it/s]warmup run: 1945it [00:03, 955.53it/s]warmup run: 2003it [00:03, 1009.95it/s]warmup run: 2223it [00:03, 1061.50it/s]warmup run: 1919it [00:03, 961.39it/s]warmup run: 2285it [00:03, 1072.34it/s]warmup run: 1964it [00:03, 999.87it/s]warmup run: 1941it [00:03, 981.33it/s]warmup run: 2270it [00:03, 1090.98it/s]warmup run: 2051it [00:03, 983.97it/s]warmup run: 2123it [00:03, 1064.39it/s]warmup run: 2338it [00:03, 1087.34it/s]warmup run: 2020it [00:03, 974.51it/s]warmup run: 2400it [00:03, 1093.70it/s]warmup run: 2075it [00:03, 1031.03it/s]warmup run: 2049it [00:03, 1010.31it/s]warmup run: 2169it [00:03, 1041.03it/s]warmup run: 2380it [00:03, 1076.17it/s]warmup run: 2242it [00:03, 1101.81it/s]warmup run: 2453it [00:03, 1105.30it/s]warmup run: 2139it [00:03, 1038.13it/s]warmup run: 2516it [00:04, 1111.27it/s]warmup run: 2193it [00:03, 1073.64it/s]warmup run: 2169it [00:03, 1064.41it/s]warmup run: 2287it [00:03, 1081.77it/s]warmup run: 2362it [00:03, 1130.87it/s]warmup run: 2488it [00:03, 1032.97it/s]warmup run: 2569it [00:04, 1119.67it/s]warmup run: 2258it [00:03, 1083.15it/s]warmup run: 2631it [00:04, 1122.10it/s]warmup run: 2311it [00:03, 1104.61it/s]warmup run: 2288it [00:03, 1101.01it/s]warmup run: 2406it [00:03, 1111.72it/s]warmup run: 2482it [00:03, 1150.71it/s]warmup run: 2608it [00:04, 1080.72it/s]warmup run: 2377it [00:03, 1114.79it/s]warmup run: 2682it [00:04, 1109.03it/s]warmup run: 2747it [00:04, 1133.08it/s]warmup run: 2430it [00:03, 1129.24it/s]warmup run: 2407it [00:03, 1127.52it/s]warmup run: 2522it [00:04, 1125.64it/s]warmup run: 2602it [00:04, 1164.06it/s]warmup run: 2727it [00:04, 1111.71it/s]warmup run: 2496it [00:04, 1137.12it/s]warmup run: 2797it [00:04, 1120.46it/s]warmup run: 2863it [00:04, 1140.90it/s]warmup run: 2549it [00:03, 1146.00it/s]warmup run: 2526it [00:03, 1145.98it/s]warmup run: 2638it [00:04, 1135.21it/s]warmup run: 2721it [00:04, 1170.71it/s]warmup run: 2847it [00:04, 1136.42it/s]warmup run: 2615it [00:04, 1151.77it/s]warmup run: 2910it [00:04, 1116.23it/s]warmup run: 2980it [00:04, 1147.03it/s]warmup run: 2667it [00:04, 1154.55it/s]warmup run: 3000it [00:04, 663.60it/s] warmup run: 2644it [00:04, 1153.49it/s]warmup run: 2756it [00:04, 1146.75it/s]warmup run: 2839it [00:04, 1171.08it/s]warmup run: 2966it [00:04, 1151.18it/s]warmup run: 3000it [00:04, 675.95it/s] warmup run: 2732it [00:04, 1154.66it/s]warmup run: 2784it [00:04, 1157.84it/s]warmup run: 3000it [00:04, 684.97it/s] warmup run: 2761it [00:04, 1157.58it/s]warmup run: 2874it [00:04, 1154.83it/s]warmup run: 2958it [00:04, 1174.17it/s]warmup run: 2848it [00:04, 1154.24it/s]warmup run: 2902it [00:04, 1162.96it/s]warmup run: 3000it [00:04, 685.31it/s] warmup run: 2879it [00:04, 1163.77it/s]warmup run: 2992it [00:04, 1161.23it/s]warmup run: 3000it [00:04, 673.79it/s] warmup run: 2964it [00:04, 1154.24it/s]warmup run: 3000it [00:04, 684.79it/s] warmup run: 3000it [00:04, 667.02it/s] warmup run: 2998it [00:04, 1169.29it/s]warmup run: 3000it [00:04, 682.12it/s] 


warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]


warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   5%|         | 163/3000 [00:00<00:01, 1628.30it/s]warmup should be done:   6%|         | 165/3000 [00:00<00:01, 1647.96it/s]warmup should be done:   5%|         | 158/3000 [00:00<00:01, 1578.87it/s]warmup should be done:   5%|         | 159/3000 [00:00<00:01, 1585.23it/s]warmup should be done:   5%|         | 164/3000 [00:00<00:01, 1633.46it/s]warmup should be done:   5%|         | 154/3000 [00:00<00:01, 1532.31it/s]warmup should be done:   5%|         | 157/3000 [00:00<00:01, 1563.23it/s]warmup should be done:   5%|         | 137/3000 [00:00<00:02, 1363.11it/s]warmup should be done:  10%|         | 315/3000 [00:00<00:01, 1572.81it/s]warmup should be done:  11%|         | 318/3000 [00:00<00:01, 1588.07it/s]warmup should be done:  11%|         | 328/3000 [00:00<00:01, 1635.38it/s]warmup should be done:  11%|         | 331/3000 [00:00<00:01, 1649.84it/s]warmup should be done:  11%|         | 319/3000 [00:00<00:01, 1590.61it/s]warmup should be done:  11%|         | 327/3000 [00:00<00:01, 1629.54it/s]warmup should be done:  11%|         | 318/3000 [00:00<00:01, 1590.31it/s]warmup should be done:   9%|         | 274/3000 [00:00<00:02, 1200.60it/s]warmup should be done:  16%|        | 492/3000 [00:00<00:01, 1636.09it/s]warmup should be done:  16%|        | 482/3000 [00:00<00:01, 1610.28it/s]warmup should be done:  16%|        | 473/3000 [00:00<00:01, 1570.49it/s]warmup should be done:  17%|        | 496/3000 [00:00<00:01, 1644.73it/s]warmup should be done:  16%|        | 477/3000 [00:00<00:01, 1583.17it/s]warmup should be done:  16%|        | 479/3000 [00:00<00:01, 1588.24it/s]warmup should be done:  16%|        | 490/3000 [00:00<00:01, 1620.70it/s]warmup should be done:  15%|        | 436/3000 [00:00<00:01, 1375.96it/s]warmup should be done:  22%|       | 656/3000 [00:00<00:01, 1634.08it/s]warmup should be done:  22%|       | 647/3000 [00:00<00:01, 1622.97it/s]warmup should be done:  21%|       | 639/3000 [00:00<00:01, 1590.23it/s]warmup should be done:  22%|       | 661/3000 [00:00<00:01, 1641.44it/s]warmup should be done:  21%|        | 636/3000 [00:00<00:01, 1579.01it/s]warmup should be done:  21%|        | 631/3000 [00:00<00:01, 1565.03it/s]warmup should be done:  22%|       | 653/3000 [00:00<00:01, 1616.07it/s]warmup should be done:  20%|        | 596/3000 [00:00<00:01, 1458.84it/s]warmup should be done:  27%|       | 812/3000 [00:00<00:01, 1631.02it/s]warmup should be done:  27%|       | 799/3000 [00:00<00:01, 1591.72it/s]warmup should be done:  27%|       | 820/3000 [00:00<00:01, 1630.54it/s]warmup should be done:  26%|       | 794/3000 [00:00<00:01, 1577.02it/s]warmup should be done:  26%|       | 788/3000 [00:00<00:01, 1563.51it/s]warmup should be done:  28%|       | 826/3000 [00:00<00:01, 1634.67it/s]warmup should be done:  27%|       | 815/3000 [00:00<00:01, 1612.58it/s]warmup should be done:  25%|       | 758/3000 [00:00<00:01, 1513.41it/s]warmup should be done:  33%|      | 976/3000 [00:00<00:01, 1631.88it/s]warmup should be done:  32%|      | 959/3000 [00:00<00:01, 1591.28it/s]warmup should be done:  32%|      | 952/3000 [00:00<00:01, 1572.91it/s]warmup should be done:  33%|      | 984/3000 [00:00<00:01, 1625.03it/s]warmup should be done:  32%|      | 945/3000 [00:00<00:01, 1559.13it/s]warmup should be done:  33%|      | 990/3000 [00:00<00:01, 1627.71it/s]warmup should be done:  33%|      | 977/3000 [00:00<00:01, 1605.67it/s]warmup should be done:  31%|       | 917/3000 [00:00<00:01, 1537.03it/s]warmup should be done:  38%|      | 1140/3000 [00:00<00:01, 1634.40it/s]warmup should be done:  37%|      | 1119/3000 [00:00<00:01, 1588.03it/s]warmup should be done:  38%|      | 1147/3000 [00:00<00:01, 1625.55it/s]warmup should be done:  37%|      | 1110/3000 [00:00<00:01, 1566.31it/s]warmup should be done:  38%|      | 1153/3000 [00:00<00:01, 1625.72it/s]warmup should be done:  38%|      | 1138/3000 [00:00<00:01, 1606.02it/s]warmup should be done:  37%|      | 1101/3000 [00:00<00:01, 1552.48it/s]warmup should be done:  36%|      | 1075/3000 [00:00<00:01, 1548.70it/s]warmup should be done:  43%|     | 1304/3000 [00:00<00:01, 1635.03it/s]warmup should be done:  43%|     | 1278/3000 [00:00<00:01, 1588.38it/s]warmup should be done:  44%|     | 1310/3000 [00:00<00:01, 1626.10it/s]warmup should be done:  42%|     | 1267/3000 [00:00<00:01, 1565.02it/s]warmup should be done:  44%|     | 1316/3000 [00:00<00:01, 1625.52it/s]warmup should be done:  43%|     | 1299/3000 [00:00<00:01, 1605.22it/s]warmup should be done:  42%|     | 1257/3000 [00:00<00:01, 1552.41it/s]warmup should be done:  41%|      | 1235/3000 [00:00<00:01, 1561.98it/s]warmup should be done:  49%|     | 1468/3000 [00:00<00:00, 1635.08it/s]warmup should be done:  48%|     | 1437/3000 [00:00<00:00, 1588.63it/s]warmup should be done:  49%|     | 1473/3000 [00:00<00:00, 1624.70it/s]warmup should be done:  47%|     | 1424/3000 [00:00<00:01, 1563.89it/s]warmup should be done:  49%|     | 1460/3000 [00:00<00:00, 1598.72it/s]warmup should be done:  49%|     | 1479/3000 [00:00<00:00, 1617.75it/s]warmup should be done:  47%|     | 1413/3000 [00:00<00:01, 1547.23it/s]warmup should be done:  46%|     | 1394/3000 [00:00<00:01, 1568.31it/s]warmup should be done:  54%|    | 1633/3000 [00:01<00:00, 1637.02it/s]warmup should be done:  53%|    | 1599/3000 [00:01<00:00, 1595.44it/s]warmup should be done:  55%|    | 1636/3000 [00:01<00:00, 1625.21it/s]warmup should be done:  53%|    | 1581/3000 [00:01<00:00, 1564.06it/s]warmup should be done:  52%|    | 1568/3000 [00:01<00:00, 1546.93it/s]warmup should be done:  54%|    | 1621/3000 [00:01<00:00, 1600.52it/s]warmup should be done:  55%|    | 1641/3000 [00:01<00:00, 1611.99it/s]warmup should be done:  52%|    | 1555/3000 [00:01<00:00, 1578.63it/s]warmup should be done:  60%|    | 1797/3000 [00:01<00:00, 1636.02it/s]warmup should be done:  59%|    | 1760/3000 [00:01<00:00, 1598.17it/s]warmup should be done:  60%|    | 1800/3000 [00:01<00:00, 1626.90it/s]warmup should be done:  58%|    | 1738/3000 [00:01<00:00, 1563.98it/s]warmup should be done:  57%|    | 1723/3000 [00:01<00:00, 1545.87it/s]warmup should be done:  59%|    | 1784/3000 [00:01<00:00, 1606.99it/s]warmup should be done:  60%|    | 1804/3000 [00:01<00:00, 1616.26it/s]warmup should be done:  57%|    | 1715/3000 [00:01<00:00, 1584.49it/s]warmup should be done:  65%|   | 1961/3000 [00:01<00:00, 1629.79it/s]warmup should be done:  64%|   | 1921/3000 [00:01<00:00, 1600.07it/s]warmup should be done:  65%|   | 1964/3000 [00:01<00:00, 1629.26it/s]warmup should be done:  63%|   | 1896/3000 [00:01<00:00, 1565.85it/s]warmup should be done:  63%|   | 1879/3000 [00:01<00:00, 1548.10it/s]warmup should be done:  65%|   | 1948/3000 [00:01<00:00, 1615.01it/s]warmup should be done:  66%|   | 1966/3000 [00:01<00:00, 1600.82it/s]warmup should be done:  62%|   | 1875/3000 [00:01<00:00, 1586.50it/s]warmup should be done:  69%|   | 2082/3000 [00:01<00:00, 1601.35it/s]warmup should be done:  71%|   | 2128/3000 [00:01<00:00, 1630.53it/s]warmup should be done:  71%|   | 2124/3000 [00:01<00:00, 1622.10it/s]warmup should be done:  68%|   | 2055/3000 [00:01<00:00, 1570.80it/s]warmup should be done:  70%|   | 2112/3000 [00:01<00:00, 1621.11it/s]warmup should be done:  68%|   | 2035/3000 [00:01<00:00, 1549.03it/s]warmup should be done:  71%|   | 2127/3000 [00:01<00:00, 1595.00it/s]warmup should be done:  68%|   | 2036/3000 [00:01<00:00, 1591.26it/s]warmup should be done:  75%|  | 2243/3000 [00:01<00:00, 1595.33it/s]warmup should be done:  74%|  | 2215/3000 [00:01<00:00, 1579.21it/s]warmup should be done:  76%|  | 2292/3000 [00:01<00:00, 1620.28it/s]warmup should be done:  76%|  | 2275/3000 [00:01<00:00, 1622.11it/s]warmup should be done:  76%|  | 2287/3000 [00:01<00:00, 1610.19it/s]warmup should be done:  73%|  | 2191/3000 [00:01<00:00, 1549.49it/s]warmup should be done:  76%|  | 2289/3000 [00:01<00:00, 1600.35it/s]warmup should be done:  73%|  | 2197/3000 [00:01<00:00, 1594.43it/s]warmup should be done:  79%|  | 2373/3000 [00:01<00:00, 1578.24it/s]warmup should be done:  82%| | 2455/3000 [00:01<00:00, 1620.34it/s]warmup should be done:  80%|  | 2403/3000 [00:01<00:00, 1584.31it/s]warmup should be done:  81%| | 2438/3000 [00:01<00:00, 1622.07it/s]warmup should be done:  78%|  | 2347/3000 [00:01<00:00, 1551.37it/s]warmup should be done:  82%| | 2449/3000 [00:01<00:00, 1609.32it/s]warmup should be done:  82%| | 2453/3000 [00:01<00:00, 1609.66it/s]warmup should be done:  79%|  | 2357/3000 [00:01<00:00, 1594.75it/s]warmup should be done:  84%| | 2532/3000 [00:01<00:00, 1581.44it/s]warmup should be done:  87%| | 2619/3000 [00:01<00:00, 1625.75it/s]warmup should be done:  83%| | 2503/3000 [00:01<00:00, 1553.61it/s]warmup should be done:  87%| | 2613/3000 [00:01<00:00, 1617.76it/s]warmup should be done:  87%| | 2602/3000 [00:01<00:00, 1624.64it/s]warmup should be done:  85%| | 2562/3000 [00:01<00:00, 1577.53it/s]warmup should be done:  87%| | 2617/3000 [00:01<00:00, 1616.43it/s]warmup should be done:  84%| | 2518/3000 [00:01<00:00, 1596.58it/s]warmup should be done:  90%| | 2691/3000 [00:01<00:00, 1583.03it/s]warmup should be done:  93%|| 2783/3000 [00:01<00:00, 1629.44it/s]warmup should be done:  89%| | 2661/3000 [00:01<00:00, 1560.81it/s]warmup should be done:  92%|| 2765/3000 [00:01<00:00, 1625.92it/s]warmup should be done:  93%|| 2777/3000 [00:01<00:00, 1623.39it/s]warmup should be done:  91%| | 2720/3000 [00:01<00:00, 1570.15it/s]warmup should be done:  93%|| 2780/3000 [00:01<00:00, 1619.59it/s]warmup should be done:  89%| | 2679/3000 [00:01<00:00, 1599.96it/s]warmup should be done:  95%|| 2851/3000 [00:01<00:00, 1587.62it/s]warmup should be done:  98%|| 2948/3000 [00:01<00:00, 1635.57it/s]warmup should be done:  98%|| 2942/3000 [00:01<00:00, 1631.08it/s]warmup should be done:  94%|| 2820/3000 [00:01<00:00, 1567.57it/s]warmup should be done:  98%|| 2931/3000 [00:01<00:00, 1633.25it/s]warmup should be done:  96%|| 2881/3000 [00:01<00:00, 1581.53it/s]warmup should be done:  98%|| 2945/3000 [00:01<00:00, 1627.70it/s]warmup should be done:  95%|| 2841/3000 [00:01<00:00, 1605.03it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1629.07it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1624.50it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1621.31it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1617.87it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1589.93it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1577.38it/s]warmup should be done:  99%|| 2977/3000 [00:01<00:00, 1565.15it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1556.67it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1555.53it/s]






warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   6%|         | 167/3000 [00:00<00:01, 1669.09it/s]warmup should be done:   5%|         | 163/3000 [00:00<00:01, 1627.48it/s]warmup should be done:   5%|         | 160/3000 [00:00<00:01, 1596.78it/s]warmup should be done:   5%|         | 161/3000 [00:00<00:01, 1604.73it/s]warmup should be done:   5%|         | 163/3000 [00:00<00:01, 1623.13it/s]warmup should be done:   6%|         | 165/3000 [00:00<00:01, 1641.95it/s]warmup should be done:   6%|         | 168/3000 [00:00<00:01, 1669.94it/s]warmup should be done:   6%|         | 168/3000 [00:00<00:01, 1670.69it/s]warmup should be done:  11%|         | 322/3000 [00:00<00:01, 1610.01it/s]warmup should be done:  11%|         | 324/3000 [00:00<00:01, 1617.50it/s]warmup should be done:  11%|         | 327/3000 [00:00<00:01, 1630.90it/s]warmup should be done:  11%|         | 333/3000 [00:00<00:01, 1662.50it/s]warmup should be done:  11%|         | 336/3000 [00:00<00:01, 1675.62it/s]warmup should be done:  11%|         | 336/3000 [00:00<00:01, 1674.17it/s]warmup should be done:  11%|         | 336/3000 [00:00<00:01, 1672.94it/s]warmup should be done:  11%|         | 328/3000 [00:00<00:01, 1633.31it/s]warmup should be done:  16%|        | 490/3000 [00:00<00:01, 1637.80it/s]warmup should be done:  16%|        | 489/3000 [00:00<00:01, 1629.16it/s]warmup should be done:  16%|        | 492/3000 [00:00<00:01, 1636.12it/s]warmup should be done:  17%|        | 505/3000 [00:00<00:01, 1678.10it/s]warmup should be done:  16%|        | 492/3000 [00:00<00:01, 1635.71it/s]warmup should be done:  17%|        | 505/3000 [00:00<00:01, 1678.26it/s]warmup should be done:  17%|        | 502/3000 [00:00<00:01, 1671.03it/s]warmup should be done:  17%|        | 504/3000 [00:00<00:01, 1671.45it/s]warmup should be done:  22%|       | 656/3000 [00:00<00:01, 1637.44it/s]warmup should be done:  22%|       | 655/3000 [00:00<00:01, 1640.78it/s]warmup should be done:  22%|       | 673/3000 [00:00<00:01, 1677.13it/s]warmup should be done:  22%|       | 673/3000 [00:00<00:01, 1677.07it/s]warmup should be done:  22%|       | 670/3000 [00:00<00:01, 1670.40it/s]warmup should be done:  22%|       | 656/3000 [00:00<00:01, 1633.03it/s]warmup should be done:  22%|       | 672/3000 [00:00<00:01, 1671.25it/s]warmup should be done:  22%|       | 652/3000 [00:00<00:01, 1614.22it/s]warmup should be done:  28%|       | 841/3000 [00:00<00:01, 1676.62it/s]warmup should be done:  28%|       | 842/3000 [00:00<00:01, 1680.13it/s]warmup should be done:  28%|       | 838/3000 [00:00<00:01, 1671.87it/s]warmup should be done:  27%|       | 820/3000 [00:00<00:01, 1633.61it/s]warmup should be done:  28%|       | 841/3000 [00:00<00:01, 1674.99it/s]warmup should be done:  27%|       | 820/3000 [00:00<00:01, 1626.47it/s]warmup should be done:  27%|       | 814/3000 [00:00<00:01, 1600.50it/s]warmup should be done:  27%|       | 820/3000 [00:00<00:01, 1577.71it/s]warmup should be done:  34%|      | 1009/3000 [00:00<00:01, 1675.43it/s]warmup should be done:  34%|      | 1007/3000 [00:00<00:01, 1674.66it/s]warmup should be done:  34%|      | 1009/3000 [00:00<00:01, 1671.54it/s]warmup should be done:  33%|      | 984/3000 [00:00<00:01, 1626.03it/s]warmup should be done:  33%|      | 987/3000 [00:00<00:01, 1640.69it/s]warmup should be done:  34%|      | 1011/3000 [00:00<00:01, 1668.97it/s]warmup should be done:  32%|      | 975/3000 [00:00<00:01, 1599.41it/s]warmup should be done:  33%|      | 982/3000 [00:00<00:01, 1589.24it/s]warmup should be done:  39%|      | 1177/3000 [00:00<00:01, 1669.32it/s]warmup should be done:  39%|      | 1175/3000 [00:00<00:01, 1668.37it/s]warmup should be done:  38%|      | 1154/3000 [00:00<00:01, 1650.09it/s]warmup should be done:  38%|      | 1148/3000 [00:00<00:01, 1628.22it/s]warmup should be done:  39%|      | 1179/3000 [00:00<00:01, 1671.31it/s]warmup should be done:  39%|      | 1177/3000 [00:00<00:01, 1655.38it/s]warmup should be done:  38%|      | 1135/3000 [00:00<00:01, 1598.60it/s]warmup should be done:  38%|      | 1146/3000 [00:00<00:01, 1605.30it/s]warmup should be done:  45%|     | 1344/3000 [00:00<00:00, 1666.46it/s]warmup should be done:  44%|     | 1311/3000 [00:00<00:01, 1627.70it/s]warmup should be done:  44%|     | 1320/3000 [00:00<00:01, 1650.86it/s]warmup should be done:  45%|     | 1349/3000 [00:00<00:00, 1677.84it/s]warmup should be done:  45%|     | 1343/3000 [00:00<00:01, 1647.81it/s]warmup should be done:  43%|     | 1295/3000 [00:00<00:01, 1589.32it/s]warmup should be done:  45%|     | 1342/3000 [00:00<00:01, 1627.79it/s]warmup should be done:  44%|     | 1309/3000 [00:00<00:01, 1612.72it/s]warmup should be done:  50%|     | 1511/3000 [00:00<00:00, 1664.42it/s]warmup should be done:  51%|     | 1518/3000 [00:00<00:00, 1681.05it/s]warmup should be done:  49%|     | 1474/3000 [00:00<00:00, 1625.69it/s]warmup should be done:  50%|     | 1486/3000 [00:00<00:00, 1648.27it/s]warmup should be done:  49%|     | 1456/3000 [00:00<00:00, 1594.54it/s]warmup should be done:  50%|     | 1508/3000 [00:00<00:00, 1643.91it/s]warmup should be done:  49%|     | 1472/3000 [00:00<00:00, 1616.65it/s]warmup should be done:  50%|     | 1505/3000 [00:00<00:00, 1625.87it/s]warmup should be done:  56%|    | 1678/3000 [00:01<00:00, 1663.27it/s]warmup should be done:  56%|    | 1687/3000 [00:01<00:00, 1682.53it/s]warmup should be done:  55%|    | 1640/3000 [00:01<00:00, 1633.65it/s]warmup should be done:  55%|    | 1654/3000 [00:01<00:00, 1656.69it/s]warmup should be done:  56%|    | 1674/3000 [00:01<00:00, 1647.94it/s]warmup should be done:  55%|    | 1638/3000 [00:01<00:00, 1628.31it/s]warmup should be done:  56%|    | 1674/3000 [00:01<00:00, 1643.83it/s]warmup should be done:  54%|    | 1616/3000 [00:01<00:00, 1409.09it/s]warmup should be done:  60%|    | 1807/3000 [00:01<00:00, 1643.06it/s]warmup should be done:  62%|   | 1845/3000 [00:01<00:00, 1661.51it/s]warmup should be done:  61%|    | 1822/3000 [00:01<00:00, 1662.27it/s]warmup should be done:  62%|   | 1856/3000 [00:01<00:00, 1667.11it/s]warmup should be done:  61%|   | 1841/3000 [00:01<00:00, 1652.22it/s]warmup should be done:  61%|   | 1843/3000 [00:01<00:00, 1657.24it/s]warmup should be done:  60%|    | 1803/3000 [00:01<00:00, 1632.26it/s]warmup should be done:  59%|    | 1767/3000 [00:01<00:00, 1435.27it/s]warmup should be done:  66%|   | 1973/3000 [00:01<00:00, 1647.05it/s]warmup should be done:  67%|   | 2012/3000 [00:01<00:00, 1659.53it/s]warmup should be done:  66%|   | 1989/3000 [00:01<00:00, 1656.89it/s]warmup should be done:  68%|   | 2025/3000 [00:01<00:00, 1672.47it/s]warmup should be done:  67%|   | 2007/3000 [00:01<00:00, 1653.01it/s]warmup should be done:  67%|   | 2012/3000 [00:01<00:00, 1664.63it/s]warmup should be done:  66%|   | 1967/3000 [00:01<00:00, 1633.00it/s]warmup should be done:  64%|   | 1925/3000 [00:01<00:00, 1476.41it/s]warmup should be done:  71%|  | 2140/3000 [00:01<00:00, 1651.80it/s]warmup should be done:  73%|  | 2178/3000 [00:01<00:00, 1657.66it/s]warmup should be done:  73%|  | 2194/3000 [00:01<00:00, 1675.72it/s]warmup should be done:  72%|  | 2173/3000 [00:01<00:00, 1652.72it/s]warmup should be done:  73%|  | 2180/3000 [00:01<00:00, 1668.72it/s]warmup should be done:  71%|   | 2132/3000 [00:01<00:00, 1637.19it/s]warmup should be done:  72%|  | 2155/3000 [00:01<00:00, 1626.50it/s]warmup should be done:  69%|   | 2084/3000 [00:01<00:00, 1508.51it/s]warmup should be done:  77%|  | 2307/3000 [00:01<00:00, 1656.34it/s]warmup should be done:  78%|  | 2345/3000 [00:01<00:00, 1659.01it/s]warmup should be done:  79%|  | 2363/3000 [00:01<00:00, 1679.42it/s]warmup should be done:  78%|  | 2339/3000 [00:01<00:00, 1654.72it/s]warmup should be done:  78%|  | 2347/3000 [00:01<00:00, 1667.28it/s]warmup should be done:  77%|  | 2298/3000 [00:01<00:00, 1641.98it/s]warmup should be done:  77%|  | 2320/3000 [00:01<00:00, 1632.14it/s]warmup should be done:  75%|  | 2243/3000 [00:01<00:00, 1531.07it/s]warmup should be done:  82%| | 2474/3000 [00:01<00:00, 1659.55it/s]warmup should be done:  84%| | 2512/3000 [00:01<00:00, 1661.42it/s]warmup should be done:  84%| | 2532/3000 [00:01<00:00, 1682.45it/s]warmup should be done:  84%| | 2506/3000 [00:01<00:00, 1656.68it/s]warmup should be done:  84%| | 2514/3000 [00:01<00:00, 1667.16it/s]warmup should be done:  82%| | 2463/3000 [00:01<00:00, 1643.64it/s]warmup should be done:  83%| | 2485/3000 [00:01<00:00, 1635.28it/s]warmup should be done:  80%|  | 2401/3000 [00:01<00:00, 1543.73it/s]warmup should be done:  88%| | 2641/3000 [00:01<00:00, 1659.90it/s]warmup should be done:  89%| | 2680/3000 [00:01<00:00, 1666.05it/s]warmup should be done:  90%| | 2701/3000 [00:01<00:00, 1682.45it/s]warmup should be done:  89%| | 2672/3000 [00:01<00:00, 1656.76it/s]warmup should be done:  88%| | 2628/3000 [00:01<00:00, 1643.55it/s]warmup should be done:  88%| | 2649/3000 [00:01<00:00, 1636.00it/s]warmup should be done:  89%| | 2681/3000 [00:01<00:00, 1663.18it/s]warmup should be done:  85%| | 2560/3000 [00:01<00:00, 1556.30it/s]warmup should be done:  94%|| 2808/3000 [00:01<00:00, 1661.77it/s]warmup should be done:  95%|| 2848/3000 [00:01<00:00, 1668.73it/s]warmup should be done:  96%|| 2870/3000 [00:01<00:00, 1680.74it/s]warmup should be done:  95%|| 2838/3000 [00:01<00:00, 1656.94it/s]warmup should be done:  94%|| 2814/3000 [00:01<00:00, 1639.44it/s]warmup should be done:  93%|| 2794/3000 [00:01<00:00, 1646.25it/s]warmup should be done:  95%|| 2848/3000 [00:01<00:00, 1659.77it/s]warmup should be done:  91%| | 2720/3000 [00:01<00:00, 1566.56it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1677.70it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1667.70it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1659.23it/s]warmup should be done:  99%|| 2976/3000 [00:01<00:00, 1664.90it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1658.06it/s]warmup should be done:  99%|| 2960/3000 [00:01<00:00, 1648.57it/s]warmup should be done:  99%|| 2978/3000 [00:01<00:00, 1621.90it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1645.79it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1637.11it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1630.10it/s]warmup should be done:  96%|| 2883/3000 [00:01<00:00, 1582.97it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1556.57it/s]2022-12-12 00:11:11.166596: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7ef1f802e830 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 00:11:11.166658: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 00:11:12.129771: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f0de7831320 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 00:11:12.129842: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 00:11:12.134193: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7ef1a402a790 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 00:11:12.134241: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 00:11:12.134841: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7ef1f402ffb0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 00:11:12.134894: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 00:11:12.142917: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f0def796180 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 00:11:12.142962: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 00:11:12.545128: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7ef1b40298c0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 00:11:12.545164: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f0deb8313a0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 00:11:12.545207: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 00:11:12.545218: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 00:11:12.586503: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f0de78338a0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 00:11:12.586591: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 00:11:13.418617: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 00:11:14.394400: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 00:11:14.421877: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 00:11:14.440414: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 00:11:14.466660: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 00:11:14.795732: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 00:11:14.840692: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 00:11:14.880062: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 00:11:16.318897: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 00:11:17.251708: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 00:11:17.337298: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 00:11:17.362810: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 00:11:17.364954: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 00:11:17.615556: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 00:11:17.760145: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 00:11:17.762150: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
[HCTR][00:11:53.158][ERROR][RK0][main]: replica 0 reaches 1000, calling init pre replica
[HCTR][00:11:53.158][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][00:11:53.169][ERROR][RK0][main]: coll ps creation done
[HCTR][00:11:53.169][ERROR][RK0][main]: replica 0 waits for coll ps creation barrier
[HCTR][00:11:53.191][ERROR][RK0][tid #139698274023168]: replica 1 reaches 1000, calling init pre replica
[HCTR][00:11:53.191][ERROR][RK0][tid #139698274023168]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][00:11:53.196][ERROR][RK0][tid #139698274023168]: coll ps creation done
[HCTR][00:11:53.196][ERROR][RK0][tid #139698274023168]: replica 1 waits for coll ps creation barrier
[HCTR][00:11:53.471][ERROR][RK0][tid #139698265630464]: replica 4 reaches 1000, calling init pre replica
[HCTR][00:11:53.472][ERROR][RK0][tid #139698265630464]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][00:11:53.477][ERROR][RK0][tid #139698265630464]: coll ps creation done
[HCTR][00:11:53.477][ERROR][RK0][tid #139698265630464]: replica 4 waits for coll ps creation barrier
[HCTR][00:11:53.540][ERROR][RK0][main]: replica 6 reaches 1000, calling init pre replica
[HCTR][00:11:53.540][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][00:11:53.545][ERROR][RK0][main]: coll ps creation done
[HCTR][00:11:53.545][ERROR][RK0][main]: replica 6 waits for coll ps creation barrier
[HCTR][00:11:53.550][ERROR][RK0][tid #139698131412736]: replica 3 reaches 1000, calling init pre replica
[HCTR][00:11:53.550][ERROR][RK0][tid #139698131412736]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][00:11:53.555][ERROR][RK0][tid #139698131412736]: coll ps creation done
[HCTR][00:11:53.555][ERROR][RK0][tid #139698131412736]: replica 3 waits for coll ps creation barrier
[HCTR][00:11:53.577][ERROR][RK0][tid #139698139805440]: replica 5 reaches 1000, calling init pre replica
[HCTR][00:11:53.577][ERROR][RK0][tid #139698139805440]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][00:11:53.586][ERROR][RK0][tid #139698139805440]: coll ps creation done
[HCTR][00:11:53.586][ERROR][RK0][tid #139698139805440]: replica 5 waits for coll ps creation barrier
[HCTR][00:11:53.604][ERROR][RK0][main]: replica 7 reaches 1000, calling init pre replica
[HCTR][00:11:53.604][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][00:11:53.610][ERROR][RK0][main]: coll ps creation done
[HCTR][00:11:53.610][ERROR][RK0][main]: replica 7 waits for coll ps creation barrier
[HCTR][00:11:53.622][ERROR][RK0][main]: replica 2 reaches 1000, calling init pre replica
[HCTR][00:11:53.623][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][00:11:53.627][ERROR][RK0][main]: coll ps creation done
[HCTR][00:11:53.627][ERROR][RK0][main]: replica 2 waits for coll ps creation barrier
[HCTR][00:11:53.627][ERROR][RK0][main]: replica 0 preparing frequency
[HCTR][00:11:54.687][ERROR][RK0][main]: replica 0 preparing frequency done
[HCTR][00:11:54.719][ERROR][RK0][main]: replica 0 calling init per replica
[HCTR][00:11:54.719][ERROR][RK0][tid #139698265630464]: replica 4 calling init per replica
[HCTR][00:11:54.719][ERROR][RK0][tid #139698139805440]: replica 5 calling init per replica
[HCTR][00:11:54.719][ERROR][RK0][main]: replica 7 calling init per replica
[HCTR][00:11:54.719][ERROR][RK0][tid #139698274023168]: replica 1 calling init per replica
[HCTR][00:11:54.719][ERROR][RK0][main]: replica 2 calling init per replica
[HCTR][00:11:54.719][ERROR][RK0][tid #139698131412736]: replica 3 calling init per replica
[HCTR][00:11:54.719][ERROR][RK0][main]: replica 6 calling init per replica
[HCTR][00:11:54.719][ERROR][RK0][main]: Calling build_v2
[HCTR][00:11:54.719][ERROR][RK0][tid #139698265630464]: Calling build_v2
[HCTR][00:11:54.719][ERROR][RK0][tid #139698139805440]: Calling build_v2
[HCTR][00:11:54.719][ERROR][RK0][main]: Calling build_v2
[HCTR][00:11:54.719][ERROR][RK0][tid #139698274023168]: Calling build_v2
[HCTR][00:11:54.719][ERROR][RK0][main]: Calling build_v2
[HCTR][00:11:54.719][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][00:11:54.719][ERROR][RK0][tid #139698131412736]: Calling build_v2
[HCTR][00:11:54.719][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][00:11:54.719][ERROR][RK0][tid #139698265630464]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][00:11:54.719][ERROR][RK0][main]: Calling build_v2
[HCTR][00:11:54.719][ERROR][RK0][tid #139698139805440]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][00:11:54.719][ERROR][RK0][tid #139698274023168]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][00:11:54.719][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][00:11:54.719][ERROR][RK0][tid #139698131412736]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][00:11:54.719][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[[[[[2022-12-12 00:11:542022-12-12 00:11:54[.2022-12-12 00:11:54.2022-12-12 00:11:542022-12-12 00:11:54719548.719558[[..: 719548: 2022-12-12 00:11:547195627195682022-12-12 00:11:54E: 2022-12-12 00:11:54E.: : . E. 719578EE719585/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc 719585/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc:   : :/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc: :E/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.ccE136:E136 :: ] 136 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc136136/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.ccusing concurrent impl MPS] /hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.ccusing concurrent impl MPS:] ] :
using concurrent impl MPS:
136using concurrent impl MPSusing concurrent impl MPS136
136] 

] ] using concurrent impl MPSusing concurrent impl MPSusing concurrent impl MPS


[2022-12-12 00:11:54.723783: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:178] v100x8, slow pcie
[2022-12-12 00:11:54.723823: E [/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 00:11:54:.196723829] : assigning 8 to cpuE
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:178] v100x8, slow pcie
[2022-12-12 00:11:54.[7238772022-12-12 00:11:54: .E723876 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE[: 2022-12-12 00:11:54196/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.] :723902assigning 8 to cpu178: 
[] E2022-12-12 00:11:54v100x8, slow pcie .
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc723923:: 212E[[]  2022-12-12 00:11:542022-12-12 00:11:54build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc..
:[7239547239541782022-12-12 00:11:54: : ] [.EEv100x8, slow pcie2022-12-12 00:11:54723969  
.: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc723997E[:[::  2022-12-12 00:11:541962022-12-12 00:11:54212E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.] .]  :724017assigning 8 to cpu724023build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc178: 
: 
[:] EE2022-12-12 00:11:54213v100x8, slow pcie [ [.] 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 00:11:54/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[2022-12-12 00:11:54724072remote time is 8.68421[:.:2022-12-12 00:11:54.: 
2022-12-12 00:11:54178724103196.724116E.] : [] 724141:  724135v100x8, slow pcieE2022-12-12 00:11:54assigning 8 to cpu: E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: 
 .
E :E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc724181[ /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc178 :: 2022-12-12 00:11:54/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc213E.:178[v100x8, slow pcie:]  724242196] 2022-12-12 00:11:54
212remote time is 8.68421/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: ] v100x8, slow pcie.] 
:[Eassigning 8 to cpu
724287build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8214[2022-12-12 00:11:54[ 
: 
] 2022-12-12 00:11:54.2022-12-12 00:11:54/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccEcpu time is 97.0588.724344[.: 
724383[: 2022-12-12 00:11:54724367196/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: 2022-12-12 00:11:54E.: ] :E. 724416Eassigning 8 to cpu212 724438/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:  
] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: :E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccbuild asymm link desc with 8X Tesla V100-SXM2-32GB out of 8:E196 :
[196 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2142022-12-12 00:11:54] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[assigning 8 to cpu:] .assigning 8 to cpu:2022-12-12 00:11:54
213cpu time is 97.0588724565
212.] 
: ] 724593remote time is 8.68421Ebuild asymm link desc with 8X Tesla V100-SXM2-32GB out of 8: 
 
E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc :[[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc212[2022-12-12 00:11:542022-12-12 00:11:54:] [2022-12-12 00:11:54..213build asymm link desc with 8X Tesla V100-SXM2-32GB out of 82022-12-12 00:11:54.724685724685] 
.724694: : remote time is 8.68421724704: [EE
: E2022-12-12 00:11:54  E [./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 00:11:54724772::/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:.: 212214:213724807E] ] 212] :  build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8cpu time is 97.0588] remote time is 8.68421E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc

build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8
 :
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc213[[:] 2022-12-12 00:11:54[2022-12-12 00:11:54214remote time is 8.68421.2022-12-12 00:11:54.] 
724950.724950cpu time is 97.0588: 724966[2022-12-12 00:11:54: 
E: .E E725008 /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc : /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE:214: 213] 213/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] cpu time is 97.0588] :remote time is 8.68421
remote time is 8.68421214

] cpu time is 97.0588
[[2022-12-12 00:11:542022-12-12 00:11:54..725176725179: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc::214214] ] cpu time is 97.0588cpu time is 97.0588

[2022-12-12 00:13:12.258676: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:118] solver created. now build & solve
[2022-12-12 00:13:12.298890: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 381.47 MB
[2022-12-12 00:13:12.298971: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 381.47 MB
[2022-12-12 00:13:12.300006: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:74] mapping nid to rank...
[2022-12-12 00:13:12.378315: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:91] counting slots...
[2022-12-12 00:13:12.765171: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:105] Final num slot is 49
[2022-12-12 00:13:12.765256: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:109] counting blocks...
[2022-12-12 00:13:19.647732: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:118] Final num block is 1024
[2022-12-12 00:13:19.647826: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:123] counting freq and density...
[2022-12-12 00:13:21.374919: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:154] averaging freq and density...
[2022-12-12 00:13:21.375012: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:155] 1024
[2022-12-12 00:13:21.378039: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:123] solver built. now solve
[2022-12-12 00:13:21.378096: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:354] constructing optimal solver, device=8, stream=1
1024 blocks, 8 devices
[2022-12-12 00:13:21.693071: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:527] Add Var...
[2022-12-12 00:13:21.721635: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:545] Capacity...
[2022-12-12 00:13:21.723051: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:548] Connect CPU...
[2022-12-12 00:13:21.743572: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:550] Connect Access To Storage...
[2022-12-12 00:13:22.269444: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:554] Time...
[2022-12-12 00:13:45.643951: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:569] Coll Cache init block placement array
[2022-12-12 00:13:45.652070: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:645] Coll Cache init block placement array done
[2022-12-12 00:13:45.654612: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:647] Coll Cache model reset done
[2022-12-12 00:13:45.697906: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:127] solver solved
[2022-12-12 00:13:45.698003: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:195] 0 solved master
[2022-12-12 00:13:45.698036: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 0 solved
[2022-12-12 00:13:45.698068: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 0 initing device 0
[2022-12-12 00:13:45.698622: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-12 00:13:45.698674: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 00:13:45.699582: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 00:13:45.700246: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 00:13:45.713396: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 1 solved
[2022-12-12 00:13:45.713475: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 1 initing device 1
[2022-12-12 00:13:45.713798: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 6 solved
[2022-12-12 00:13:45.713859: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 6 initing device 6
[2022-12-12 00:13:45.[7138922022-12-12 00:13:45: .E713932 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.ccE: 202[/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu] 2022-12-12 00:13:45:7 solved.1815
713973] : Building Coll Cache with ... num gpu device is 8E[
 2022-12-12 00:13:45/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc.:714041[202: 2022-12-12 00:13:45] E.4 solved 714073[
/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc: [2022-12-12 00:13:45:E2022-12-12 00:13:45.205 .714087] /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu714139: worker 0 thread 7 initing device 7:: E
1980E ]  /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cceager alloc mem 381.47 MB/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:
:202205] ] worker 0 thread 4 initing device 43 solved

[2022-12-12 00:13:45.714286: [E2022-12-12 00:13:45 ./hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc714291:: 205E] worker 0 thread 3 initing device 3 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-12 00:13:45.714376: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 00:13:45.714488: E[ 2022-12-12 00:13:45/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc.:714506202: ] E5 solved 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 2 solved[
2022-12-12 00:13:45.714579: [E2022-12-12 00:13:45 ./hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc714596:: 205E]  worker 0 thread 5 initing device 5/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc
:205] worker 0 thread 2 initing device 2
[2022-12-12 00:13:45.714672: E[ 2022-12-12 00:13:45/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu.:7146821815: ] EBuilding Coll Cache with ... num gpu device is 8 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-12 00:13:45.[7147562022-12-12 00:13:45[: .2022-12-12 00:13:45E714762. : 714766/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE: : E1980/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu ] :/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cueager alloc mem 381.47 MB1980:
] 1815eager alloc mem 381.47 MB] 
Building Coll Cache with ... num gpu device is 8
[2022-12-12 00:13:45.714908: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 00:13:45.715043: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu[:2022-12-12 00:13:451815.] 715060Building Coll Cache with ... num gpu device is 8: 
E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-12 00:13:45.715113: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu[:2022-12-12 00:13:451980.] 715141eager alloc mem 381.47 MB: 
E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 00:13:45.718448: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 00:13:45.718918: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 00:13:45.719447: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 00:13:45.719492: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 00:13:45.719558: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 00:13:45.719616: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 00:13:45.719684: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 00:13:45.722838: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 00:13:45.723211: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 00:13:45.723280: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 00:13:45.723774: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 00:13:45.723844: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 00:13:45.723899: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 00:13:45.723962: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 00:13:45.776961: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1024.00 Bytes
[2022-12-12 00:13:45.782236: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-12 00:13:45.782372: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 00:13:45.783185: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 15.64 MB
[2022-12-12 00:13:45.783759: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:13:45.784762: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:13:45.784808: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 15.24 MB
[2022-12-12 00:13:45.787835: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 00:13:45.788594: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 00:13:45.788639: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1.91 GB
[[[[[[[2022-12-12 00:13:452022-12-12 00:13:45.2022-12-12 00:13:452022-12-12 00:13:452022-12-12 00:13:452022-12-12 00:13:452022-12-12 00:13:45.804209.....804209: 804209804209804210804209804240: E: : : : : E EEEEE /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu     /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980:::::1980] 19801980198019801980] eager alloc mem 1024.00 Bytes] ] ] ] ] eager alloc mem 1024.00 Bytes
eager alloc mem 1024.00 Byteseager alloc mem 1024.00 Byteseager alloc mem 1024.00 Byteseager alloc mem 1024.00 Byteseager alloc mem 1024.00 Bytes





[2022-12-12 00:13:45.811046: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-12 00:13:45.811095: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] [eager release cuda mem 10242022-12-12 00:13:45
.811141: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 00:13:45.811190: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc[:2022-12-12 00:13:45638.] 811191eager release cuda mem 400000000: 
E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-12 00:13:45.811268: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638[] 2022-12-12 00:13:45eager release cuda mem 1024.
811303: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[[2022-12-12 00:13:452022-12-12 00:13:45..811356811347: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::638638] ] eager release cuda mem 400000000eager release cuda mem 1024

[[2022-12-12 00:13:452022-12-12 00:13:45..811439811453: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::638638] ] eager release cuda mem 1024eager release cuda mem 400000000
[
2022-12-12 00:13:45.811518: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638[] 2022-12-12 00:13:45eager release cuda mem 1024.
811572: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 00:13:45.811623: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 00:13:45.812177: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 15.64 MB
[2022-12-12 00:13:45.812710: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 15.64 MB
[2022-12-12 00:13:45.818247: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 15.64 MB
[2022-12-12 00:13:45.818859: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 15.64 MB
[2022-12-12 00:13:45.819606: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 15.64 MB
[2022-12-12 00:13:45.820182: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 15.64 MB
[2022-12-12 00:13:45.820757: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 15.64 MB
[2022-12-12 00:13:45.821366: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:13:45.821447: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:13:45.821772: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:13:45.821880: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:13:45.822055: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:13:45.822099: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:13:45.822135: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:13:45.822350: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:13:45.822394: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 15.25 MB
[2022-12-12 00:13:45.822415: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:13:45.822458: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 15.21 MB
[2022-12-12 00:13:45.822753: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:13:45.822801: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 15.22 MB
[2022-12-12 00:13:45.822860: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:13:45.822905: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 15.15 MB
[2022-12-12 00:13:45.823023: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:13:45.823069: W[ 2022-12-12 00:13:45[/hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc.2022-12-12 00:13:45:823076.43: 823083: ] EEWORKER[0] alloc host memory 15.25 MB  
/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::638638] ] eager release cuda mem 625663eager release cuda mem 625663

[[2022-12-12 00:13:452022-12-12 00:13:45..823204823207: : WW  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc::4343] ] WORKER[0] alloc host memory 15.26 MBWORKER[0] alloc host memory 15.24 MB

[2022-12-12 00:13:45.833913: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 00:13:45.834120: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 00:13:45.834219: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 00:13:45.834561: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 00:13:45.834605: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1.91 GB
[2022-12-12 00:13:45.834622: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 00:13:45.834720: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 00:13:45.834766: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1.91 GB
[2022-12-12 00:13:45.[8348022022-12-12 00:13:45: .E834821 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE: 1980/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc] :eager alloc mem 25.25 KB638
] eager release cuda mem 25855
[2022-12-12 00:13:45.834853: E[ 2022-12-12 00:13:45/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu.:8348841980: ] Eeager alloc mem 25.25 KB 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu[:2022-12-12 00:13:451980.] 834918eager alloc mem 1.91 GB: 
E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 00:13:45.835232: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 00:13:45.835276: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1.90 GB
[2022-12-12 00:13:45.835426: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 00:13:45.835473: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1.91 GB
[2022-12-12 00:13:45.835494: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 00:13:45.835540: E[ 2022-12-12 00:13:45/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu.:8355461980: ] Eeager alloc mem 1.91 GB 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 00:13:45.835605: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1.91 GB
[[[[[[[[2022-12-12 00:13:462022-12-12 00:13:462022-12-12 00:13:462022-12-12 00:13:462022-12-12 00:13:462022-12-12 00:13:462022-12-12 00:13:462022-12-12 00:13:46........440545440545440545440545440545440545440546440545: : : : : : : : EEEEEEEE        /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::::::::19261926192619261926192619261926] ] ] ] ] ] ] ] Device 7 init p2p of link 4Device 0 init p2p of link 3Device 2 init p2p of link 1Device 3 init p2p of link 2Device 1 init p2p of link 7Device 4 init p2p of link 5Device 6 init p2p of link 0Device 5 init p2p of link 6







[[[2022-12-12 00:13:46[[[[2022-12-12 00:13:462022-12-12 00:13:46[.2022-12-12 00:13:462022-12-12 00:13:462022-12-12 00:13:462022-12-12 00:13:46..2022-12-12 00:13:46441080....441080441082.: 441079441079441086441088: : 441099E: : : : EE:  EEEE  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu    /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:::1980::::198019801980] 1980198019801980] ] ] eager alloc mem 611.00 KB] ] ] ] eager alloc mem 611.00 KBeager alloc mem 611.00 KBeager alloc mem 611.00 KB
eager alloc mem 611.00 KBeager alloc mem 611.00 KBeager alloc mem 611.00 KBeager alloc mem 611.00 KB






[2022-12-12 00:13:46.442198[: 2022-12-12 00:13:46E.[ [[[442204[[2022-12-12 00:13:46/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc2022-12-12 00:13:462022-12-12 00:13:462022-12-12 00:13:46: 2022-12-12 00:13:462022-12-12 00:13:46.:...E..442214638442214442217442219 442218442218: ] : : : /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc: : Eeager release cuda mem 625663EEE:EE
    638  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc] /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::::eager release cuda mem 625663::638638638638] 
638638] ] ] eager release cuda mem 625663] ] eager release cuda mem 625663eager release cuda mem 625663eager release cuda mem 625663eager release cuda mem 625663
eager release cuda mem 625663




[2022-12-12 00:13:46.455342: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 6 init p2p of link 5
[2022-12-12 00:13:46.455498: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:13:46.456329: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:13:46.456412: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 4 init p2p of link 7
[2022-12-12 00:13:46.456487: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 3 init p2p of link 0
[2022-12-12 00:13:46.456558: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:13:46.456618[: 2022-12-12 00:13:46E. 456639/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu: :E1926 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuDevice 5 init p2p of link 4:
1980] eager alloc mem 611.00 KB
[2022-12-12 00:13:46.456803: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:13:46.456851: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 0 init p2p of link 6[
2022-12-12 00:13:46.456880: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 2
[2022-12-12 00:13:46.457019: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB[
2022-12-12 00:13:46.457043: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:13:46.457299: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 7 init p2p of link 1
[2022-12-12 00:13:46.457379: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:13:46.457449: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:13:46.457491: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:13:46.457603: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:13:46.457718: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 2 init p2p of link 3
[2022-12-12 00:13:46.457816: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:13:46.[4578672022-12-12 00:13:46: .E457871 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE: 1980/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc] :eager alloc mem 611.00 KB638
] eager release cuda mem 625663
[2022-12-12 00:13:46.458231: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:13:46.458715: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:13:46.467090: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 6 init p2p of link 4
[2022-12-12 00:13:46.467215: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:13:46.468015: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:13:46.469845: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 4 init p2p of link 2
[2022-12-12 00:13:46.469957: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:13:46.[4707392022-12-12 00:13:46: .E470760 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE: 1926/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc] :[Device 3 init p2p of link 56382022-12-12 00:13:46
] .eager release cuda mem 625663470804
: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 0 init p2p of link 1
[[2022-12-12 00:13:462022-12-12 00:13:46..470905470919: : EE[  2022-12-12 00:13:46/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu.::47095219261980: ] ] EDevice 5 init p2p of link 7eager alloc mem 611.00 KB 

/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:13:46.471085: E [/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu2022-12-12 00:13:46:.1926471111] : Device 7 init p2p of link 6E
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:13:46.471244: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:13:46.471448: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 3
[2022-12-12 00:13:46.471564: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:13:46.471786: E[ 2022-12-12 00:13:46/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu.:471810[1926: 2022-12-12 00:13:46] E.Device 2 init p2p of link 0 471823
/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc: :E638 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cceager release cuda mem 625663:
638] eager release cuda mem 625663
[2022-12-12 00:13:46.471936: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980[] 2022-12-12 00:13:46eager alloc mem 611.00 KB.
471951: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:13:46.472055: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:13:46.472356: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:13:46.472753: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:13:46.480626: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 6 init p2p of link 7
[2022-12-12 00:13:46.480748: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:13:46.481545: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:13:46.483794: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 4 init p2p of link 6
[2022-12-12 00:13:46.483905: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:13:46.484696: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:13:46.485831: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 3 init p2p of link 1
[2022-12-12 00:13:46.485949: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:13:46.486744: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:13:46.487814: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 0 init p2p of link 2
[2022-12-12 00:13:46.487933: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:13:46.488111: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 5 init p2p of link 3
[2022-12-12 00:13:46.488227: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:13:46.488297: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 7 init p2p of link 5
[2022-12-12 00:13:46.488413: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:13:46.488738: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:13:46.488916: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 0
[2022-12-12 00:13:46.489007: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:13:46.489030: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:13:46.489205: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:13:46.489584: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 2 init p2p of link 4
[2022-12-12 00:13:46.489704: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:13:46.489827: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:13:46.490481: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:13:46.496994: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 16399996
[2022-12-12 00:13:46.499500: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 16399996
[2022-12-12 00:13:46.500164: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 3999186 / 100000000 nodes ( 4.00 %~4.00 %) | remote 11964970 / 100000000 nodes ( 11.96 %) | cpu 84035844 / 100000000 nodes ( 84.04 %) | 1.91 GB | 0.785793 secs 
[2022-12-12 00:13:46.500838: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 16399996
[2022-12-12 00:13:46.501897: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 3986987 / 100000000 nodes ( 3.99 %~4.00 %) | remote 11977169 / 100000000 nodes ( 11.98 %) | cpu 84035844 / 100000000 nodes ( 84.04 %) | 1.91 GB | 0.787142 secs 
[2022-12-12 00:13:46.502433: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 16399996
[2022-12-12 00:13:46.503270: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 16399996
[2022-12-12 00:13:46.503498: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 3972412 / 100000000 nodes ( 3.97 %~4.00 %) | remote 11991744 / 100000000 nodes ( 11.99 %) | cpu 84035844 / 100000000 nodes ( 84.04 %) | 1.90 GB | 0.788598 secs 
[2022-12-12 00:13:46.503732: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 16399996
[2022-12-12 00:13:46.503772: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 16399996
[2022-12-12 00:13:46.503804: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 3994652 / 100000000 nodes ( 3.99 %~4.00 %) | remote 11969504 / 100000000 nodes ( 11.97 %) | cpu 84035844 / 100000000 nodes ( 84.04 %) | 1.91 GB | 0.788699 secs 
[2022-12-12 00:13:46.504084: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 16399996
[2022-12-12 00:13:46.504496: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 3995536 / 100000000 nodes ( 4.00 %~4.00 %) | remote 11968620 / 100000000 nodes ( 11.97 %) | cpu 84035844 / 100000000 nodes ( 84.04 %) | 1.91 GB | 0.805833 secs 
[2022-12-12 00:13:46.506712: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 8.55 GB
[2022-12-12 00:13:46.506830: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 3997534 / 100000000 nodes ( 4.00 %~4.00 %) | remote 11966622 / 100000000 nodes ( 11.97 %) | cpu 84035844 / 100000000 nodes ( 84.04 %) | 1.91 GB | 0.791709 secs 
[2022-12-12 00:13:46.506916: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 3990159 / 100000000 nodes ( 3.99 %~4.00 %) | remote 11973997 / 100000000 nodes ( 11.97 %) | cpu 84035844 / 100000000 nodes ( 84.04 %) | 1.91 GB | 0.792172 secs 
[2022-12-12 00:13:46.507921: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 3998674 / 100000000 nodes ( 4.00 %~4.00 %) | remote 11965482 / 100000000 nodes ( 11.97 %) | cpu 84035844 / 100000000 nodes ( 84.04 %) | 1.91 GB | 0.793858 secs 
[2022-12-12 00:13:47.808573: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 8.81 GB
[2022-12-12 00:13:47.809373: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 8.81 GB
[2022-12-12 00:13:47.832404: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 8.81 GB
[2022-12-12 00:13:49.521910: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 9.07 GB
[2022-12-12 00:13:49.522387: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 9.07 GB
[2022-12-12 00:13:49.524890: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 9.07 GB
[2022-12-12 00:13:50.804118: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 9.29 GB
[2022-12-12 00:13:50.804497: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 9.29 GB
[2022-12-12 00:13:50.806116: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 9.29 GB
[2022-12-12 00:13:52. 55894: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 9.50 GB
[2022-12-12 00:13:52. 56084: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 9.50 GB
[2022-12-12 00:13:52. 56427: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 9.50 GB
[2022-12-12 00:13:53.421886: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 9.96 GB
[2022-12-12 00:13:53.422059: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 9.96 GB
[2022-12-12 00:13:53.422414: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 9.96 GB
[2022-12-12 00:13:54.561162: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 10.16 GB
[2022-12-12 00:13:54.561332: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 10.16 GB
[HCTR][00:13:54.784][ERROR][RK0][main]: replica 2 calling init per replica done, doing barrier
[HCTR][00:13:54.784][ERROR][RK0][tid #139698131412736]: replica 3 calling init per replica done, doing barrier
[HCTR][00:13:54.784][ERROR][RK0][main]: replica 7 calling init per replica done, doing barrier
[HCTR][00:13:54.784][ERROR][RK0][main]: replica 0 calling init per replica done, doing barrier
[HCTR][00:13:54.784][ERROR][RK0][tid #139698139805440]: replica 5 calling init per replica done, doing barrier
[HCTR][00:13:54.784][ERROR][RK0][tid #139698265630464]: replica 4 calling init per replica done, doing barrier
[HCTR][00:13:54.784][ERROR][RK0][tid #139698274023168]: replica 1 calling init per replica done, doing barrier
[HCTR][00:13:54.784][ERROR][RK0][main]: replica 6 calling init per replica done, doing barrier
[HCTR][00:13:54.784][ERROR][RK0][tid #139698274023168]: replica 1 calling init per replica done, doing barrier done
[HCTR][00:13:54.784][ERROR][RK0][tid #139698131412736]: replica 3 calling init per replica done, doing barrier done
[HCTR][00:13:54.784][ERROR][RK0][main]: replica 2 calling init per replica done, doing barrier done
[HCTR][00:13:54.784][ERROR][RK0][tid #139698139805440]: replica 5 calling init per replica done, doing barrier done
[HCTR][00:13:54.784][ERROR][RK0][main]: replica 6 calling init per replica done, doing barrier done
[HCTR][00:13:54.784][ERROR][RK0][main]: replica 7 calling init per replica done, doing barrier done
[HCTR][00:13:54.784][ERROR][RK0][tid #139698265630464]: replica 4 calling init per replica done, doing barrier done
[HCTR][00:13:54.784][ERROR][RK0][main]: replica 0 calling init per replica done, doing barrier done
[HCTR][00:13:54.784][ERROR][RK0][tid #139698274023168]: init per replica done
[HCTR][00:13:54.784][ERROR][RK0][main]: init per replica done
[HCTR][00:13:54.784][ERROR][RK0][tid #139698131412736]: init per replica done
[HCTR][00:13:54.784][ERROR][RK0][tid #139698139805440]: init per replica done
[HCTR][00:13:54.784][ERROR][RK0][main]: init per replica done
[HCTR][00:13:54.784][ERROR][RK0][main]: init per replica done
[HCTR][00:13:54.784][ERROR][RK0][tid #139698265630464]: init per replica done
[HCTR][00:13:54.787][ERROR][RK0][main]: init per replica done
[HCTR][00:13:54.790][ERROR][RK0][tid #139698139805440]: 5 allocated 3276800 at 0x7efb07520000
[HCTR][00:13:54.790][ERROR][RK0][tid #139698139805440]: 5 allocated 6553600 at 0x7f0fd8e00000
[HCTR][00:13:54.790][ERROR][RK0][tid #139698139805440]: 5 allocated 3276800 at 0x7f0fd9440000
[HCTR][00:13:54.790][ERROR][RK0][tid #139698139805440]: 5 allocated 6553600 at 0x7f0fd9760000
[HCTR][00:13:54.790][ERROR][RK0][main]: 1 allocated 3276800 at 0x7efb03520000
[HCTR][00:13:54.790][ERROR][RK0][main]: 1 allocated 6553600 at 0x7f0fd8e00000
[HCTR][00:13:54.790][ERROR][RK0][main]: 1 allocated 3276800 at 0x7f0fd9440000
[HCTR][00:13:54.790][ERROR][RK0][main]: 1 allocated 6553600 at 0x7f0fd9760000
[HCTR][00:13:54.790][ERROR][RK0][main]: 6 allocated 3276800 at 0x7efb03520000
[HCTR][00:13:54.790][ERROR][RK0][main]: 6 allocated 6553600 at 0x7f0fdae00000
[HCTR][00:13:54.790][ERROR][RK0][main]: 6 allocated 3276800 at 0x7f0fdb440000
[HCTR][00:13:54.790][ERROR][RK0][main]: 4 allocated 3276800 at 0x7efb03520000
[HCTR][00:13:54.790][ERROR][RK0][main]: 6 allocated 6553600 at 0x7f0fdb760000
[HCTR][00:13:54.790][ERROR][RK0][main]: 4 allocated 6553600 at 0x7f0fd8e00000
[HCTR][00:13:54.790][ERROR][RK0][main]: 2 allocated 3276800 at 0x7efb03520000
[HCTR][00:13:54.790][ERROR][RK0][main]: 4 allocated 3276800 at 0x7f0fd9440000
[HCTR][00:13:54.790][ERROR][RK0][main]: 2 allocated 6553600 at 0x7f0fd8e00000
[HCTR][00:13:54.790][ERROR][RK0][main]: 4 allocated 6553600 at 0x7f0fd9760000
[HCTR][00:13:54.790][ERROR][RK0][main]: 2 allocated 3276800 at 0x7f0fd9440000
[HCTR][00:13:54.790][ERROR][RK0][main]: 2 allocated 6553600 at 0x7f0fd9760000
[HCTR][00:13:54.790][ERROR][RK0][main]: 3 allocated 3276800 at 0x7efb03520000
[HCTR][00:13:54.791][ERROR][RK0][main]: 3 allocated 6553600 at 0x7f0fd8e00000
[HCTR][00:13:54.791][ERROR][RK0][main]: 3 allocated 3276800 at 0x7f0fd9440000
[HCTR][00:13:54.791][ERROR][RK0][main]: 3 allocated 6553600 at 0x7f0fd9760000
[HCTR][00:13:54.791][ERROR][RK0][tid #139698139805440]: 7 allocated 3276800 at 0x7efb07520000
[HCTR][00:13:54.791][ERROR][RK0][tid #139698139805440]: 7 allocated 6553600 at 0x7f0fd8e00000
[HCTR][00:13:54.791][ERROR][RK0][tid #139698139805440]: 7 allocated 3276800 at 0x7f0fd9440000
[HCTR][00:13:54.791][ERROR][RK0][tid #139698139805440]: 7 allocated 6553600 at 0x7f0fd9760000
[HCTR][00:13:54.794][ERROR][RK0][tid #139698274023168]: 0 allocated 3276800 at 0x7f0fdaf20000
[HCTR][00:13:54.794][ERROR][RK0][tid #139698274023168]: 0 allocated 6553600 at 0x7f0fdb400000
[HCTR][00:13:54.794][ERROR][RK0][tid #139698274023168]: 0 allocated 3276800 at 0x7f0fdc10e800
[HCTR][00:13:54.794][ERROR][RK0][tid #139698274023168]: 0 allocated 6553600 at 0x7f0fdc42e800








