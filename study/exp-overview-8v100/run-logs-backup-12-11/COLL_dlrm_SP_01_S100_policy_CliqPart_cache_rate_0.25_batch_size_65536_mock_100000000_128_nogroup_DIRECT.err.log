2022-12-12 06:08:12.075474: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.083241: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.088680: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.096842: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.102141: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.115002: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.121404: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.126099: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.186112: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.195315: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.199303: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.200169: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.201048: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.202350: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.204184: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.204962: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.205281: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.206582: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.206617: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.208253: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.208304: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.209691: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.209985: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.211058: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.211432: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.212891: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.213215: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.214444: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.214880: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.215830: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.216537: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.217129: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.219168: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.220594: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.221510: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.222467: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.223454: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.224342: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.225332: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.226322: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.231931: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 06:08:12.232123: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.233249: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.234295: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.235355: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.236894: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.238397: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.238769: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.239915: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.240431: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.241835: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.242013: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.242185: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.244157: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.244299: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.245955: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.245982: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.247746: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.249401: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.251422: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.254891: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.257410: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.257461: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.260251: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.260374: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.260546: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.263033: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.263193: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.263485: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.263675: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.263955: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.266399: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.266535: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.266970: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.267047: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.267611: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.269481: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.269677: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.270019: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.270020: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.284183: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.287195: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.287216: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.287415: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.287443: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.288321: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.289133: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.290640: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.290823: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.290907: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.291151: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.292282: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.293137: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.298566: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.328423: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.328995: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.329124: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.330307: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.330788: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.331580: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.332808: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.334074: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.335139: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.335274: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.335754: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.337501: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.340335: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.340497: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.340566: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.341480: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.343459: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.343780: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.344605: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.346322: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.346510: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.347389: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.349163: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.349309: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.350366: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.351540: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.351908: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.352657: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.354003: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.354180: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.355303: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.356259: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.356642: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.357725: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.358803: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.359316: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.360437: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.361348: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.361785: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.363717: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.363807: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.364169: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.366134: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.366261: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.366644: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.368315: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.368679: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.369055: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.370945: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.371189: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.371464: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.373301: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.374014: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.374400: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.374468: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.375852: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.376601: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.377314: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.377558: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.377829: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.378880: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.380015: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.380610: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.380937: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.381360: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.382226: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.382687: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.383651: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.384297: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.384690: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.385136: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.386676: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.387582: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.388141: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.388629: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.389158: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.390476: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.390499: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.390804: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 06:08:12.391274: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.391653: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.392457: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.392955: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.394748: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.394752: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.395544: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.396142: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.397126: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.397414: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.398652: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.398728: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.399934: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.400496: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.401112: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.401331: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.402542: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.402673: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.403829: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 06:08:12.404166: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.404718: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.405341: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.406922: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.407140: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.408558: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.408927: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 06:08:12.408943: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.409443: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.410756: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.410868: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.411697: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.412853: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.414178: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.414236: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.414411: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.416589: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.417861: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.418685: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.419880: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.419942: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.420416: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.421640: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.423030: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.423918: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.425237: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.425326: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.425755: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.426983: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.429021: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.430049: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.431108: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.431216: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.432302: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.433666: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.435830: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.435943: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.436936: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.438060: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.440202: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.440415: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.442455: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.442850: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.444648: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.444910: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.476856: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.477822: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.478819: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.479083: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.482475: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.483338: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.484209: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.485250: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.487509: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.488226: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.488899: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.498662: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.503059: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.503881: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.505011: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.506013: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.509364: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.510210: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.510934: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.511706: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.513956: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.542719: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.543991: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.544741: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.546757: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.556973: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.557457: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.559882: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.573522: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.574191: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.574762: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 06:08:12.575111: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.580065: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.580701: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.582564: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 06:08:12.584242: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.592437: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.643074: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.644801: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.644881: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.647560: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 06:08:12.648659: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.651230: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.651274: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.657464: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.657932: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 06:08:12.662917: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.667801: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.668494: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.673849: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:12.679239: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:13.563515: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:13.564639: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:13.565954: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:13.567308: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 06:08:13.567369: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-12 06:08:13.585345: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:13.586648: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:13.588047: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:13.589296: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:13.590609: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:13.591770: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-12 06:08:13.636948: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 06:08:13.637148: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 06:08:13.684460: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12340
2022-12-12 06:08:13.844928: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:13.845735: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:13.846272: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:13.846741: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 06:08:13.846792: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-12 06:08:13.860247: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:13.860868: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:13.861404: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:13.861872: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 06:08:13.861944: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-12 06:08:13.864007: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:13.864628: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:13.865141: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:13.865814: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:13.866343: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:13.866811: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:1/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-12 06:08:13.878733: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:13.879846: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:13.881035: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:13.882275: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:13.883454: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:13.884370: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:2/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-12 06:08:13.898299: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:13.899536: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:13.900834: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:13.901820: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 06:08:13.901874: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-12 06:08:13.920645: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:13.923017: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:13.924475: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:13.925210: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:13.925732: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:13.926213: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:3/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-12 06:08:13.941024: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 06:08:13.941229: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 06:08:13.942976: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12341
2022-12-12 06:08:13.958405: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:13.959034: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:13.959577: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:13.960060: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 06:08:13.960111: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-12 06:08:13.962329: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:13.962921: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:13.963476: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:13.963948: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 06:08:13.964002: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-12 06:08:13.967935: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:13.968554: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:13.969091: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:13.969560: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 06:08:13.969611: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-12 06:08:13.972519: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 06:08:13.972688: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 06:08:13.974567: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12343
2022-12-12 06:08:13.975874: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 06:08:13.976019: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 06:08:13.977809: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12342
2022-12-12 06:08:13.977921: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:13.978926: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:13.979053: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:13.980989: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:13.981289: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:13.981437: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:13.983982: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 06:08:13.984038: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-12 06:08:13.984550: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:13.984814: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:13.986343: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:13.986369: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:13.986479: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:13.988330: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:13.988594: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:13.988636: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:13.990224: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:4/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-12 06:08:13.990803: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:13.990906: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:13.992441: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:13.992473: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:7/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-12 06:08:13.993520: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:13.994745: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:5/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-12 06:08:14.001095: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:14.002123: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:14.003144: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:14.004240: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:14.005273: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:08:14.006270: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:6/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-12 06:08:14.035662: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 06:08:14.035866: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 06:08:14.037726: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12344
2022-12-12 06:08:14.039415: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 06:08:14.039601: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 06:08:14.040505: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 06:08:14.040656: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 06:08:14.041481: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12347
2022-12-12 06:08:14.042468: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12345
2022-12-12 06:08:14.051054: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 06:08:14.051222: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 06:08:14.052938: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12346
[HCTR][06:08:15.304][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][06:08:15.304][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][06:08:15.305][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][06:08:15.307][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][06:08:15.312][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][06:08:15.316][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][06:08:15.316][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][06:08:15.316][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 1it [00:01,  1.55s/it]warmup run: 1it [00:01,  1.57s/it]warmup run: 1it [00:01,  1.56s/it]warmup run: 1it [00:01,  1.57s/it]warmup run: 1it [00:01,  1.55s/it]warmup run: 94it [00:01, 78.91it/s]warmup run: 1it [00:01,  1.51s/it]warmup run: 95it [00:01, 79.09it/s]warmup run: 101it [00:01, 84.39it/s]warmup run: 99it [00:01, 82.25it/s]warmup run: 1it [00:01,  1.64s/it]warmup run: 1it [00:01,  1.63s/it]warmup run: 100it [00:01, 84.01it/s]warmup run: 192it [00:01, 175.53it/s]warmup run: 97it [00:01, 83.59it/s]warmup run: 189it [00:01, 170.82it/s]warmup run: 201it [00:01, 182.17it/s]warmup run: 197it [00:01, 177.72it/s]warmup run: 97it [00:01, 77.41it/s]warmup run: 100it [00:01, 80.50it/s]warmup run: 197it [00:01, 179.18it/s]warmup run: 290it [00:01, 282.44it/s]warmup run: 283it [00:01, 272.40it/s]warmup run: 196it [00:01, 183.05it/s]warmup run: 302it [00:01, 291.62it/s]warmup run: 286it [00:01, 271.64it/s]warmup run: 193it [00:01, 167.95it/s]warmup run: 201it [00:01, 176.46it/s]warmup run: 295it [00:01, 285.48it/s]warmup run: 388it [00:01, 393.53it/s]warmup run: 379it [00:01, 381.33it/s]warmup run: 295it [00:01, 292.26it/s]warmup run: 403it [00:01, 405.43it/s]warmup run: 370it [00:01, 361.40it/s]warmup run: 290it [00:01, 270.18it/s]warmup run: 302it [00:01, 283.47it/s]warmup run: 395it [00:01, 399.07it/s]warmup run: 487it [00:02, 503.24it/s]warmup run: 395it [00:01, 406.94it/s]warmup run: 473it [00:02, 484.10it/s]warmup run: 505it [00:02, 518.16it/s]warmup run: 470it [00:02, 478.65it/s]warmup run: 387it [00:02, 377.64it/s]warmup run: 403it [00:02, 395.94it/s]warmup run: 495it [00:02, 510.00it/s]warmup run: 587it [00:02, 606.57it/s]warmup run: 573it [00:02, 590.85it/s]warmup run: 495it [00:02, 517.56it/s]warmup run: 605it [00:02, 618.58it/s]warmup run: 571it [00:02, 588.85it/s]warmup run: 483it [00:02, 481.96it/s]warmup run: 504it [00:02, 506.67it/s]warmup run: 594it [00:02, 610.34it/s]warmup run: 689it [00:02, 700.38it/s]warmup run: 673it [00:02, 683.54it/s]warmup run: 597it [00:02, 622.77it/s]warmup run: 702it [00:02, 697.41it/s]warmup run: 672it [00:02, 683.86it/s]warmup run: 580it [00:02, 581.64it/s]warmup run: 607it [00:02, 614.07it/s]warmup run: 693it [00:02, 696.74it/s]warmup run: 788it [00:02, 771.35it/s]warmup run: 698it [00:02, 711.93it/s]warmup run: 768it [00:02, 742.31it/s]warmup run: 772it [00:02, 761.01it/s]warmup run: 799it [00:02, 759.93it/s]warmup run: 677it [00:02, 668.96it/s]warmup run: 709it [00:02, 704.74it/s]warmup run: 792it [00:02, 767.93it/s]warmup run: 887it [00:02, 826.91it/s]warmup run: 799it [00:02, 784.87it/s]warmup run: 867it [00:02, 804.86it/s]warmup run: 873it [00:02, 825.52it/s]warmup run: 895it [00:02, 808.23it/s]warmup run: 772it [00:02, 737.27it/s]warmup run: 809it [00:02, 775.56it/s]warmup run: 891it [00:02, 824.31it/s]warmup run: 986it [00:02, 868.74it/s]warmup run: 901it [00:02, 846.12it/s]warmup run: 965it [00:02, 850.11it/s]warmup run: 974it [00:02, 873.27it/s]warmup run: 991it [00:02, 847.89it/s]warmup run: 868it [00:02, 793.83it/s]warmup run: 990it [00:02, 867.89it/s]warmup run: 908it [00:02, 769.60it/s]warmup run: 1086it [00:02, 904.54it/s]warmup run: 1003it [00:02, 891.02it/s]warmup run: 1061it [00:02, 880.12it/s]warmup run: 1075it [00:02, 910.54it/s]warmup run: 1088it [00:02, 880.08it/s]warmup run: 963it [00:02, 835.47it/s]warmup run: 1090it [00:02, 902.84it/s]warmup run: 1001it [00:02, 810.08it/s]warmup run: 1186it [00:02, 930.83it/s]warmup run: 1106it [00:02, 927.36it/s]warmup run: 1163it [00:02, 918.37it/s]warmup run: 1176it [00:02, 938.74it/s]warmup run: 1188it [00:02, 913.29it/s]warmup run: 1058it [00:02, 863.03it/s]warmup run: 1190it [00:02, 928.12it/s]warmup run: 1095it [00:02, 842.56it/s]warmup run: 1286it [00:02, 949.18it/s]warmup run: 1208it [00:02, 952.69it/s]warmup run: 1266it [00:02, 948.96it/s]warmup run: 1277it [00:02, 958.16it/s]warmup run: 1288it [00:02, 936.59it/s]warmup run: 1153it [00:02, 883.27it/s]warmup run: 1289it [00:02, 944.68it/s]warmup run: 1190it [00:02, 870.38it/s]warmup run: 1386it [00:02, 962.94it/s]warmup run: 1311it [00:02, 973.34it/s]warmup run: 1369it [00:02, 970.83it/s]warmup run: 1378it [00:02, 971.80it/s]warmup run: 1390it [00:02, 958.25it/s]warmup run: 1249it [00:02, 904.11it/s]warmup run: 1388it [00:02, 953.81it/s]warmup run: 1287it [00:02, 896.31it/s]warmup run: 1486it [00:03, 972.00it/s]warmup run: 1415it [00:02, 991.22it/s]warmup run: 1472it [00:03, 988.14it/s]warmup run: 1480it [00:03, 984.57it/s]warmup run: 1489it [00:03, 965.76it/s]warmup run: 1344it [00:03, 917.37it/s]warmup run: 1487it [00:03, 960.87it/s]warmup run: 1381it [00:03, 905.19it/s]warmup run: 1586it [00:03, 977.89it/s]warmup run: 1518it [00:03, 1002.34it/s]warmup run: 1573it [00:03, 994.06it/s]warmup run: 1589it [00:03, 974.38it/s]warmup run: 1581it [00:03, 974.34it/s]warmup run: 1441it [00:03, 932.75it/s]warmup run: 1586it [00:03, 967.64it/s]warmup run: 1479it [00:03, 926.26it/s]warmup run: 1688it [00:03, 988.86it/s]warmup run: 1621it [00:03, 1008.16it/s]warmup run: 1675it [00:03, 999.03it/s]warmup run: 1689it [00:03, 981.60it/s]warmup run: 1680it [00:03, 972.31it/s]warmup run: 1541it [00:03, 949.65it/s]warmup run: 1685it [00:03, 968.60it/s]warmup run: 1575it [00:03, 934.80it/s]warmup run: 1790it [00:03, 997.44it/s]warmup run: 1724it [00:03, 1013.31it/s]warmup run: 1776it [00:03, 994.55it/s]warmup run: 1790it [00:03, 988.40it/s]warmup run: 1782it [00:03, 983.84it/s]warmup run: 1640it [00:03, 959.99it/s]warmup run: 1783it [00:03, 966.90it/s]warmup run: 1671it [00:03, 935.32it/s]warmup run: 1892it [00:03, 1002.03it/s]warmup run: 1827it [00:03, 1014.01it/s]warmup run: 1877it [00:03, 989.58it/s]warmup run: 1890it [00:03, 983.82it/s]warmup run: 1884it [00:03, 994.31it/s]warmup run: 1738it [00:03, 960.84it/s]warmup run: 1881it [00:03, 960.69it/s]warmup run: 1766it [00:03, 937.12it/s]warmup run: 1994it [00:03, 1005.68it/s]warmup run: 1930it [00:03, 1008.75it/s]warmup run: 1978it [00:03, 993.56it/s]warmup run: 1985it [00:03, 996.51it/s]warmup run: 1989it [00:03, 969.99it/s]warmup run: 1838it [00:03, 970.99it/s]warmup run: 1978it [00:03, 945.48it/s]warmup run: 1864it [00:03, 946.86it/s]warmup run: 2114it [00:03, 1062.31it/s]warmup run: 2035it [00:03, 1018.82it/s]warmup run: 2094it [00:03, 1041.06it/s]warmup run: 2100it [00:03, 1040.39it/s]warmup run: 2105it [00:03, 1024.03it/s]warmup run: 1939it [00:03, 982.13it/s]warmup run: 2084it [00:03, 976.61it/s]warmup run: 1965it [00:03, 964.99it/s]warmup run: 2236it [00:03, 1107.49it/s]warmup run: 2153it [00:03, 1064.06it/s]warmup run: 2214it [00:03, 1086.90it/s]warmup run: 2219it [00:03, 1083.58it/s]warmup run: 2225it [00:03, 1073.50it/s]warmup run: 2047it [00:03, 1010.39it/s]warmup run: 2195it [00:03, 1015.45it/s]warmup run: 2078it [00:03, 1013.85it/s]warmup run: 2357it [00:03, 1138.04it/s]warmup run: 2270it [00:03, 1095.32it/s]warmup run: 2334it [00:03, 1119.90it/s]warmup run: 2337it [00:03, 1112.20it/s]warmup run: 2345it [00:03, 1109.18it/s]warmup run: 2167it [00:03, 1064.42it/s]warmup run: 2306it [00:03, 1042.95it/s]warmup run: 2199it [00:03, 1070.25it/s]warmup run: 2478it [00:03, 1157.89it/s]warmup run: 2387it [00:03, 1115.16it/s]warmup run: 2454it [00:03, 1143.33it/s]warmup run: 2456it [00:03, 1133.72it/s]warmup run: 2464it [00:03, 1132.23it/s]warmup run: 2287it [00:03, 1102.66it/s]warmup run: 2417it [00:03, 1062.40it/s]warmup run: 2320it [00:03, 1109.98it/s]warmup run: 2600it [00:04, 1173.75it/s]warmup run: 2505it [00:03, 1131.96it/s]warmup run: 2574it [00:04, 1159.74it/s]warmup run: 2575it [00:04, 1149.04it/s]warmup run: 2583it [00:04, 1148.87it/s]warmup run: 2407it [00:04, 1129.53it/s]warmup run: 2529it [00:04, 1077.76it/s]warmup run: 2441it [00:04, 1138.05it/s]warmup run: 2721it [00:04, 1181.69it/s]warmup run: 2624it [00:04, 1146.71it/s]warmup run: 2693it [00:04, 1167.80it/s]warmup run: 2693it [00:04, 1157.20it/s]warmup run: 2703it [00:04, 1162.44it/s]warmup run: 2527it [00:04, 1148.25it/s]warmup run: 2640it [00:04, 1084.91it/s]warmup run: 2562it [00:04, 1158.64it/s]warmup run: 2843it [00:04, 1191.39it/s]warmup run: 2741it [00:04, 1153.25it/s]warmup run: 2813it [00:04, 1176.96it/s]warmup run: 2812it [00:04, 1166.38it/s]warmup run: 2822it [00:04, 1168.11it/s]warmup run: 2646it [00:04, 1158.56it/s]warmup run: 2751it [00:04, 1092.25it/s]warmup run: 2682it [00:04, 1169.38it/s]warmup run: 2965it [00:04, 1198.63it/s]warmup run: 2860it [00:04, 1161.44it/s]warmup run: 2933it [00:04, 1183.39it/s]warmup run: 3000it [00:04, 681.27it/s] warmup run: 2930it [00:04, 1169.82it/s]warmup run: 2942it [00:04, 1175.08it/s]warmup run: 2766it [00:04, 1168.04it/s]warmup run: 3000it [00:04, 674.06it/s] warmup run: 2862it [00:04, 1096.13it/s]warmup run: 2802it [00:04, 1176.85it/s]warmup run: 3000it [00:04, 675.23it/s] warmup run: 3000it [00:04, 673.30it/s] warmup run: 2979it [00:04, 1168.73it/s]warmup run: 3000it [00:04, 689.14it/s] warmup run: 2885it [00:04, 1171.80it/s]warmup run: 2974it [00:04, 1101.59it/s]warmup run: 2922it [00:04, 1182.47it/s]warmup run: 3000it [00:04, 665.26it/s] warmup run: 3000it [00:04, 657.69it/s] warmup run: 3000it [00:04, 660.02it/s] 
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]




warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   5%|▌         | 164/3000 [00:00<00:01, 1636.76it/s]warmup should be done:   5%|▌         | 162/3000 [00:00<00:01, 1619.56it/s]warmup should be done:   6%|▌         | 165/3000 [00:00<00:01, 1645.38it/s]warmup should be done:   5%|▌         | 154/3000 [00:00<00:01, 1535.34it/s]warmup should be done:   5%|▌         | 162/3000 [00:00<00:01, 1613.84it/s]warmup should be done:   5%|▌         | 161/3000 [00:00<00:01, 1603.46it/s]warmup should be done:   5%|▌         | 163/3000 [00:00<00:01, 1622.18it/s]warmup should be done:   5%|▌         | 163/3000 [00:00<00:01, 1620.93it/s]warmup should be done:  11%|█         | 329/3000 [00:00<00:01, 1642.96it/s]warmup should be done:  11%|█         | 326/3000 [00:00<00:01, 1627.48it/s]warmup should be done:  11%|█         | 331/3000 [00:00<00:01, 1649.60it/s]warmup should be done:  11%|█         | 327/3000 [00:00<00:01, 1631.37it/s]warmup should be done:  11%|█         | 328/3000 [00:00<00:01, 1636.02it/s]warmup should be done:  10%|█         | 308/3000 [00:00<00:01, 1531.45it/s]warmup should be done:  11%|█         | 324/3000 [00:00<00:01, 1613.69it/s]warmup should be done:  11%|█         | 324/3000 [00:00<00:01, 1605.29it/s]warmup should be done:  16%|█▋        | 489/3000 [00:00<00:01, 1627.67it/s]warmup should be done:  16%|█▋        | 494/3000 [00:00<00:01, 1638.95it/s]warmup should be done:  17%|█▋        | 496/3000 [00:00<00:01, 1645.85it/s]warmup should be done:  16%|█▋        | 492/3000 [00:00<00:01, 1634.31it/s]warmup should be done:  16%|█▋        | 491/3000 [00:00<00:01, 1628.47it/s]warmup should be done:  16%|█▌        | 486/3000 [00:00<00:01, 1607.04it/s]warmup should be done:  16%|█▌        | 485/3000 [00:00<00:01, 1597.73it/s]warmup should be done:  15%|█▌        | 462/3000 [00:00<00:01, 1497.74it/s]warmup should be done:  22%|██▏       | 652/3000 [00:00<00:01, 1626.76it/s]warmup should be done:  22%|██▏       | 656/3000 [00:00<00:01, 1636.46it/s]warmup should be done:  22%|██▏       | 658/3000 [00:00<00:01, 1636.09it/s]warmup should be done:  22%|██▏       | 661/3000 [00:00<00:01, 1643.83it/s]warmup should be done:  22%|██▏       | 654/3000 [00:00<00:01, 1626.39it/s]warmup should be done:  22%|██▏       | 647/3000 [00:00<00:01, 1608.08it/s]warmup should be done:  22%|██▏       | 645/3000 [00:00<00:01, 1592.07it/s]warmup should be done:  20%|██        | 615/3000 [00:00<00:01, 1508.15it/s]warmup should be done:  27%|██▋       | 821/3000 [00:00<00:01, 1638.71it/s]warmup should be done:  27%|██▋       | 822/3000 [00:00<00:01, 1634.07it/s]warmup should be done:  27%|██▋       | 815/3000 [00:00<00:01, 1620.67it/s]warmup should be done:  27%|██▋       | 817/3000 [00:00<00:01, 1624.51it/s]warmup should be done:  28%|██▊       | 826/3000 [00:00<00:01, 1640.49it/s]warmup should be done:  27%|██▋       | 809/3000 [00:00<00:01, 1609.48it/s]warmup should be done:  26%|██▌       | 769/3000 [00:00<00:01, 1517.07it/s]warmup should be done:  27%|██▋       | 805/3000 [00:00<00:01, 1556.10it/s]warmup should be done:  33%|███▎      | 985/3000 [00:00<00:01, 1628.68it/s]warmup should be done:  33%|███▎      | 986/3000 [00:00<00:01, 1628.59it/s]warmup should be done:  32%|███▏      | 970/3000 [00:00<00:01, 1603.84it/s]warmup should be done:  33%|███▎      | 980/3000 [00:00<00:01, 1619.00it/s]warmup should be done:  33%|███▎      | 978/3000 [00:00<00:01, 1612.58it/s]warmup should be done:  33%|███▎      | 991/3000 [00:00<00:01, 1632.69it/s]warmup should be done:  31%|███       | 928/3000 [00:00<00:01, 1538.79it/s]warmup should be done:  32%|███▏      | 961/3000 [00:00<00:01, 1537.06it/s]warmup should be done:  38%|███▊      | 1149/3000 [00:00<00:01, 1627.92it/s]warmup should be done:  38%|███▊      | 1142/3000 [00:00<00:01, 1618.09it/s]warmup should be done:  38%|███▊      | 1140/3000 [00:00<00:01, 1612.53it/s]warmup should be done:  38%|███▊      | 1131/3000 [00:00<00:01, 1601.42it/s]warmup should be done:  38%|███▊      | 1148/3000 [00:00<00:01, 1618.38it/s]warmup should be done:  38%|███▊      | 1155/3000 [00:00<00:01, 1629.99it/s]warmup should be done:  36%|███▌      | 1086/3000 [00:00<00:01, 1551.55it/s]warmup should be done:  37%|███▋      | 1118/3000 [00:00<00:01, 1547.09it/s]warmup should be done:  44%|████▍     | 1313/3000 [00:00<00:01, 1628.81it/s]warmup should be done:  44%|████▎     | 1305/3000 [00:00<00:01, 1620.12it/s]warmup should be done:  43%|████▎     | 1302/3000 [00:00<00:01, 1613.92it/s]warmup should be done:  43%|████▎     | 1292/3000 [00:00<00:01, 1599.96it/s]warmup should be done:  44%|████▍     | 1319/3000 [00:00<00:01, 1630.94it/s]warmup should be done:  44%|████▎     | 1310/3000 [00:00<00:01, 1614.43it/s]warmup should be done:  42%|████▏     | 1248/3000 [00:00<00:01, 1571.10it/s]warmup should be done:  43%|████▎     | 1276/3000 [00:00<00:01, 1556.93it/s]warmup should be done:  49%|████▉     | 1468/3000 [00:00<00:00, 1621.98it/s]warmup should be done:  49%|████▉     | 1465/3000 [00:00<00:00, 1618.74it/s]warmup should be done:  49%|████▉     | 1476/3000 [00:00<00:00, 1625.91it/s]warmup should be done:  48%|████▊     | 1453/3000 [00:00<00:00, 1602.27it/s]warmup should be done:  49%|████▉     | 1483/3000 [00:00<00:00, 1628.93it/s]warmup should be done:  49%|████▉     | 1472/3000 [00:00<00:00, 1612.40it/s]warmup should be done:  47%|████▋     | 1410/3000 [00:00<00:01, 1585.13it/s]warmup should be done:  48%|████▊     | 1433/3000 [00:00<00:01, 1558.01it/s]warmup should be done:  55%|█████▍    | 1640/3000 [00:01<00:00, 1628.02it/s]warmup should be done:  54%|█████▍    | 1631/3000 [00:01<00:00, 1629.12it/s]warmup should be done:  54%|█████▍    | 1631/3000 [00:01<00:00, 1618.60it/s]warmup should be done:  55%|█████▍    | 1646/3000 [00:01<00:00, 1627.39it/s]warmup should be done:  54%|█████▍    | 1614/3000 [00:01<00:00, 1595.19it/s]warmup should be done:  54%|█████▍    | 1634/3000 [00:01<00:00, 1611.67it/s]warmup should be done:  52%|█████▏    | 1572/3000 [00:01<00:00, 1593.27it/s]warmup should be done:  53%|█████▎    | 1592/3000 [00:01<00:00, 1564.93it/s]warmup should be done:  60%|██████    | 1804/3000 [00:01<00:00, 1630.23it/s]warmup should be done:  60%|█████▉    | 1797/3000 [00:01<00:00, 1636.62it/s]warmup should be done:  60%|█████▉    | 1793/3000 [00:01<00:00, 1613.31it/s]warmup should be done:  59%|█████▉    | 1777/3000 [00:01<00:00, 1605.32it/s]warmup should be done:  60%|██████    | 1810/3000 [00:01<00:00, 1628.98it/s]warmup should be done:  60%|█████▉    | 1796/3000 [00:01<00:00, 1613.55it/s]warmup should be done:  58%|█████▊    | 1736/3000 [00:01<00:00, 1606.37it/s]warmup should be done:  58%|█████▊    | 1751/3000 [00:01<00:00, 1570.44it/s]warmup should be done:  66%|██████▌   | 1968/3000 [00:01<00:00, 1631.28it/s]warmup should be done:  65%|██████▌   | 1963/3000 [00:01<00:00, 1641.37it/s]warmup should be done:  66%|██████▌   | 1974/3000 [00:01<00:00, 1630.70it/s]warmup should be done:  65%|██████▌   | 1955/3000 [00:01<00:00, 1610.05it/s]warmup should be done:  65%|██████▍   | 1939/3000 [00:01<00:00, 1607.56it/s]warmup should be done:  65%|██████▌   | 1958/3000 [00:01<00:00, 1612.52it/s]warmup should be done:  63%|██████▎   | 1900/3000 [00:01<00:00, 1614.29it/s]warmup should be done:  64%|██████▎   | 1909/3000 [00:01<00:00, 1572.55it/s]warmup should be done:  71%|███████   | 2132/3000 [00:01<00:00, 1632.44it/s]warmup should be done:  71%|███████   | 2129/3000 [00:01<00:00, 1646.09it/s]warmup should be done:  71%|███████▏  | 2138/3000 [00:01<00:00, 1632.39it/s]warmup should be done:  70%|███████   | 2101/3000 [00:01<00:00, 1608.83it/s]warmup should be done:  71%|███████   | 2117/3000 [00:01<00:00, 1608.24it/s]warmup should be done:  71%|███████   | 2120/3000 [00:01<00:00, 1611.48it/s]warmup should be done:  69%|██████▉   | 2064/3000 [00:01<00:00, 1620.92it/s]warmup should be done:  69%|██████▉   | 2067/3000 [00:01<00:00, 1574.62it/s]warmup should be done:  76%|███████▋  | 2295/3000 [00:01<00:00, 1647.67it/s]warmup should be done:  77%|███████▋  | 2296/3000 [00:01<00:00, 1630.93it/s]warmup should be done:  77%|███████▋  | 2302/3000 [00:01<00:00, 1631.30it/s]warmup should be done:  75%|███████▌  | 2262/3000 [00:01<00:00, 1607.52it/s]warmup should be done:  76%|███████▌  | 2278/3000 [00:01<00:00, 1604.07it/s]warmup should be done:  74%|███████▍  | 2228/3000 [00:01<00:00, 1626.20it/s]warmup should be done:  76%|███████▌  | 2282/3000 [00:01<00:00, 1603.35it/s]warmup should be done:  74%|███████▍  | 2225/3000 [00:01<00:00, 1575.85it/s]warmup should be done:  82%|████████▏ | 2461/3000 [00:01<00:00, 1650.42it/s]warmup should be done:  82%|████████▏ | 2460/3000 [00:01<00:00, 1632.53it/s]warmup should be done:  82%|████████▏ | 2466/3000 [00:01<00:00, 1632.82it/s]warmup should be done:  81%|████████  | 2423/3000 [00:01<00:00, 1605.95it/s]warmup should be done:  80%|███████▉  | 2392/3000 [00:01<00:00, 1628.45it/s]warmup should be done:  81%|████████▏ | 2439/3000 [00:01<00:00, 1601.26it/s]warmup should be done:  81%|████████▏ | 2443/3000 [00:01<00:00, 1600.40it/s]warmup should be done:  79%|███████▉  | 2383/3000 [00:01<00:00, 1574.96it/s]warmup should be done:  87%|████████▋ | 2624/3000 [00:01<00:00, 1634.07it/s]warmup should be done:  88%|████████▊ | 2628/3000 [00:01<00:00, 1653.41it/s]warmup should be done:  88%|████████▊ | 2630/3000 [00:01<00:00, 1634.25it/s]warmup should be done:  86%|████████▌ | 2586/3000 [00:01<00:00, 1611.08it/s]warmup should be done:  85%|████████▌ | 2555/3000 [00:01<00:00, 1624.24it/s]warmup should be done:  87%|████████▋ | 2600/3000 [00:01<00:00, 1599.29it/s]warmup should be done:  87%|████████▋ | 2604/3000 [00:01<00:00, 1599.15it/s]warmup should be done:  85%|████████▍ | 2542/3000 [00:01<00:00, 1577.10it/s]warmup should be done:  93%|█████████▎| 2788/3000 [00:01<00:00, 1633.96it/s]warmup should be done:  93%|█████████▎| 2794/3000 [00:01<00:00, 1654.44it/s]warmup should be done:  93%|█████████▎| 2794/3000 [00:01<00:00, 1635.50it/s]warmup should be done:  92%|█████████▏| 2750/3000 [00:01<00:00, 1616.78it/s]warmup should be done:  91%|█████████ | 2718/3000 [00:01<00:00, 1624.66it/s]warmup should be done:  92%|█████████▏| 2760/3000 [00:01<00:00, 1596.25it/s]warmup should be done:  92%|█████████▏| 2764/3000 [00:01<00:00, 1598.58it/s]warmup should be done:  90%|█████████ | 2701/3000 [00:01<00:00, 1578.30it/s]warmup should be done:  99%|█████████▊| 2962/3000 [00:01<00:00, 1659.92it/s]warmup should be done:  98%|█████████▊| 2953/3000 [00:01<00:00, 1635.54it/s]warmup should be done:  99%|█████████▊| 2960/3000 [00:01<00:00, 1640.50it/s]warmup should be done:  96%|█████████▌| 2882/3000 [00:01<00:00, 1627.21it/s]warmup should be done:  97%|█████████▋| 2921/3000 [00:01<00:00, 1598.27it/s]warmup should be done:  98%|█████████▊| 2926/3000 [00:01<00:00, 1603.29it/s]warmup should be done:  97%|█████████▋| 2912/3000 [00:01<00:00, 1600.54it/s]warmup should be done:  95%|█████████▌| 2860/3000 [00:01<00:00, 1581.02it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1638.14it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1634.98it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1619.78it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1612.58it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1610.65it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1604.17it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1591.67it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1573.37it/s]




warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]

warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   5%|▌         | 161/3000 [00:00<00:01, 1608.52it/s]warmup should be done:   6%|▌         | 166/3000 [00:00<00:01, 1656.20it/s]warmup should be done:   6%|▌         | 166/3000 [00:00<00:01, 1658.16it/s]warmup should be done:   5%|▌         | 163/3000 [00:00<00:01, 1626.07it/s]warmup should be done:   6%|▌         | 167/3000 [00:00<00:01, 1664.71it/s]warmup should be done:   6%|▌         | 165/3000 [00:00<00:01, 1645.00it/s]warmup should be done:   5%|▌         | 164/3000 [00:00<00:01, 1630.11it/s]warmup should be done:   4%|▎         | 111/3000 [00:00<00:02, 1104.53it/s]warmup should be done:  11%|█         | 326/3000 [00:00<00:01, 1632.21it/s]warmup should be done:  11%|█         | 334/3000 [00:00<00:01, 1666.41it/s]warmup should be done:  11%|█         | 327/3000 [00:00<00:01, 1632.07it/s]warmup should be done:   9%|▊         | 261/3000 [00:00<00:02, 1336.08it/s]warmup should be done:  11%|█         | 334/3000 [00:00<00:01, 1665.85it/s]warmup should be done:  11%|█         | 330/3000 [00:00<00:01, 1642.78it/s]warmup should be done:  11%|█         | 332/3000 [00:00<00:01, 1651.20it/s]warmup should be done:  11%|█         | 328/3000 [00:00<00:01, 1626.64it/s]warmup should be done:  16%|█▋        | 494/3000 [00:00<00:01, 1653.74it/s]warmup should be done:  16%|█▋        | 491/3000 [00:00<00:01, 1633.88it/s]warmup should be done:  16%|█▋        | 495/3000 [00:00<00:01, 1645.88it/s]warmup should be done:  17%|█▋        | 501/3000 [00:00<00:01, 1663.88it/s]warmup should be done:  14%|█▍        | 431/3000 [00:00<00:01, 1498.78it/s]warmup should be done:  17%|█▋        | 502/3000 [00:00<00:01, 1668.02it/s]warmup should be done:  17%|█▋        | 498/3000 [00:00<00:01, 1650.41it/s]warmup should be done:  16%|█▋        | 491/3000 [00:00<00:01, 1626.55it/s]warmup should be done:  22%|██▏       | 663/3000 [00:00<00:01, 1665.87it/s]warmup should be done:  22%|██▏       | 661/3000 [00:00<00:01, 1648.13it/s]warmup should be done:  20%|██        | 601/3000 [00:00<00:01, 1574.69it/s]warmup should be done:  22%|██▏       | 670/3000 [00:00<00:01, 1669.19it/s]warmup should be done:  22%|██▏       | 655/3000 [00:00<00:01, 1631.33it/s]warmup should be done:  22%|██▏       | 668/3000 [00:00<00:01, 1660.54it/s]warmup should be done:  22%|██▏       | 655/3000 [00:00<00:01, 1628.64it/s]warmup should be done:  22%|██▏       | 664/3000 [00:00<00:01, 1646.43it/s]warmup should be done:  28%|██▊       | 832/3000 [00:00<00:01, 1674.43it/s]warmup should be done:  28%|██▊       | 826/3000 [00:00<00:01, 1648.63it/s]warmup should be done:  26%|██▌       | 770/3000 [00:00<00:01, 1615.25it/s]warmup should be done:  27%|██▋       | 818/3000 [00:00<00:01, 1628.62it/s]warmup should be done:  28%|██▊       | 835/3000 [00:00<00:01, 1662.67it/s]warmup should be done:  27%|██▋       | 819/3000 [00:00<00:01, 1632.89it/s]warmup should be done:  28%|██▊       | 837/3000 [00:00<00:01, 1665.04it/s]warmup should be done:  28%|██▊       | 829/3000 [00:00<00:01, 1645.19it/s]warmup should be done:  33%|███▎      | 1001/3000 [00:00<00:01, 1677.61it/s]warmup should be done:  33%|███▎      | 991/3000 [00:00<00:01, 1646.32it/s]warmup should be done:  33%|███▎      | 1003/3000 [00:00<00:01, 1667.75it/s]warmup should be done:  31%|███▏      | 940/3000 [00:00<00:01, 1641.04it/s]warmup should be done:  33%|███▎      | 984/3000 [00:00<00:01, 1635.98it/s]warmup should be done:  33%|███▎      | 983/3000 [00:00<00:01, 1631.47it/s]warmup should be done:  33%|███▎      | 996/3000 [00:00<00:01, 1650.35it/s]warmup should be done:  33%|███▎      | 1004/3000 [00:00<00:01, 1614.09it/s]warmup should be done:  39%|███▊      | 1156/3000 [00:00<00:01, 1645.35it/s]warmup should be done:  37%|███▋      | 1110/3000 [00:00<00:01, 1659.51it/s]warmup should be done:  39%|███▉      | 1171/3000 [00:00<00:01, 1668.73it/s]warmup should be done:  38%|███▊      | 1151/3000 [00:00<00:01, 1644.55it/s]warmup should be done:  38%|███▊      | 1148/3000 [00:00<00:01, 1635.64it/s]warmup should be done:  39%|███▉      | 1163/3000 [00:00<00:01, 1654.42it/s]warmup should be done:  39%|███▉      | 1169/3000 [00:00<00:01, 1659.04it/s]warmup should be done:  39%|███▉      | 1166/3000 [00:00<00:01, 1614.62it/s]warmup should be done:  43%|████▎     | 1279/3000 [00:00<00:01, 1667.93it/s]warmup should be done:  45%|████▍     | 1338/3000 [00:00<00:00, 1668.96it/s]warmup should be done:  44%|████▍     | 1321/3000 [00:00<00:01, 1642.74it/s]warmup should be done:  44%|████▍     | 1317/3000 [00:00<00:01, 1648.22it/s]warmup should be done:  44%|████▎     | 1312/3000 [00:00<00:01, 1633.12it/s]warmup should be done:  45%|████▍     | 1338/3000 [00:00<00:00, 1667.33it/s]warmup should be done:  44%|████▍     | 1329/3000 [00:00<00:01, 1652.38it/s]warmup should be done:  44%|████▍     | 1328/3000 [00:00<00:01, 1608.29it/s]warmup should be done:  50%|█████     | 1506/3000 [00:00<00:00, 1671.19it/s]warmup should be done:  48%|████▊     | 1449/3000 [00:00<00:00, 1676.12it/s]warmup should be done:  50%|████▉     | 1486/3000 [00:00<00:00, 1644.74it/s]warmup should be done:  49%|████▉     | 1484/3000 [00:00<00:00, 1653.17it/s]warmup should be done:  49%|████▉     | 1477/3000 [00:00<00:00, 1635.55it/s]warmup should be done:  50%|█████     | 1507/3000 [00:00<00:00, 1673.72it/s]warmup should be done:  50%|████▉     | 1496/3000 [00:00<00:00, 1655.32it/s]warmup should be done:  50%|████▉     | 1490/3000 [00:00<00:00, 1609.54it/s]warmup should be done:  56%|█████▌    | 1674/3000 [00:01<00:00, 1673.28it/s]warmup should be done:  55%|█████▌    | 1652/3000 [00:01<00:00, 1648.93it/s]warmup should be done:  54%|█████▍    | 1619/3000 [00:01<00:00, 1680.97it/s]warmup should be done:  55%|█████▌    | 1651/3000 [00:01<00:00, 1656.20it/s]warmup should be done:  55%|█████▍    | 1642/3000 [00:01<00:00, 1639.13it/s]warmup should be done:  56%|█████▌    | 1677/3000 [00:01<00:00, 1679.03it/s]warmup should be done:  55%|█████▌    | 1663/3000 [00:01<00:00, 1657.87it/s]warmup should be done:  55%|█████▌    | 1652/3000 [00:01<00:00, 1610.64it/s]warmup should be done:  61%|██████    | 1817/3000 [00:01<00:00, 1648.23it/s]warmup should be done:  61%|██████▏   | 1843/3000 [00:01<00:00, 1675.34it/s]warmup should be done:  60%|█████▉    | 1789/3000 [00:01<00:00, 1683.81it/s]warmup should be done:  61%|██████    | 1818/3000 [00:01<00:00, 1657.59it/s]warmup should be done:  60%|██████    | 1807/3000 [00:01<00:00, 1639.42it/s]warmup should be done:  62%|██████▏   | 1847/3000 [00:01<00:00, 1682.89it/s]warmup should be done:  61%|██████    | 1830/3000 [00:01<00:00, 1659.66it/s]warmup should be done:  60%|██████    | 1814/3000 [00:01<00:00, 1611.73it/s]warmup should be done:  66%|██████▌   | 1982/3000 [00:01<00:00, 1646.73it/s]warmup should be done:  67%|██████▋   | 2011/3000 [00:01<00:00, 1675.35it/s]warmup should be done:  65%|██████▌   | 1958/3000 [00:01<00:00, 1683.52it/s]warmup should be done:  66%|██████▌   | 1984/3000 [00:01<00:00, 1657.36it/s]warmup should be done:  66%|██████▌   | 1971/3000 [00:01<00:00, 1638.21it/s]warmup should be done:  67%|██████▋   | 2016/3000 [00:01<00:00, 1684.85it/s]warmup should be done:  67%|██████▋   | 1996/3000 [00:01<00:00, 1658.66it/s]warmup should be done:  66%|██████▌   | 1976/3000 [00:01<00:00, 1611.00it/s]warmup should be done:  72%|███████▏  | 2147/3000 [00:01<00:00, 1647.38it/s]warmup should be done:  73%|███████▎  | 2179/3000 [00:01<00:00, 1674.95it/s]warmup should be done:  72%|███████▏  | 2151/3000 [00:01<00:00, 1658.50it/s]warmup should be done:  71%|███████   | 2127/3000 [00:01<00:00, 1680.97it/s]warmup should be done:  71%|███████   | 2136/3000 [00:01<00:00, 1639.04it/s]warmup should be done:  73%|███████▎  | 2185/3000 [00:01<00:00, 1685.33it/s]warmup should be done:  72%|███████▏  | 2163/3000 [00:01<00:00, 1659.65it/s]warmup should be done:  71%|███████▏  | 2138/3000 [00:01<00:00, 1610.40it/s]warmup should be done:  77%|███████▋  | 2313/3000 [00:01<00:00, 1649.77it/s]warmup should be done:  78%|███████▊  | 2347/3000 [00:01<00:00, 1676.04it/s]warmup should be done:  77%|███████▋  | 2318/3000 [00:01<00:00, 1661.08it/s]warmup should be done:  77%|███████▋  | 2301/3000 [00:01<00:00, 1641.69it/s]warmup should be done:  77%|███████▋  | 2296/3000 [00:01<00:00, 1679.56it/s]warmup should be done:  78%|███████▊  | 2355/3000 [00:01<00:00, 1687.21it/s]warmup should be done:  78%|███████▊  | 2330/3000 [00:01<00:00, 1661.57it/s]warmup should be done:  77%|███████▋  | 2300/3000 [00:01<00:00, 1611.57it/s]warmup should be done:  83%|████████▎ | 2478/3000 [00:01<00:00, 1649.73it/s]warmup should be done:  83%|████████▎ | 2485/3000 [00:01<00:00, 1662.15it/s]warmup should be done:  82%|████████▏ | 2464/3000 [00:01<00:00, 1677.72it/s]warmup should be done:  82%|████████▏ | 2466/3000 [00:01<00:00, 1640.54it/s]warmup should be done:  84%|████████▍ | 2525/3000 [00:01<00:00, 1689.21it/s]warmup should be done:  84%|████████▍ | 2515/3000 [00:01<00:00, 1662.86it/s]warmup should be done:  83%|████████▎ | 2497/3000 [00:01<00:00, 1649.14it/s]warmup should be done:  82%|████████▏ | 2462/3000 [00:01<00:00, 1611.87it/s]warmup should be done:  88%|████████▊ | 2643/3000 [00:01<00:00, 1648.25it/s]warmup should be done:  88%|████████▊ | 2652/3000 [00:01<00:00, 1660.32it/s]warmup should be done:  88%|████████▊ | 2631/3000 [00:01<00:00, 1640.59it/s]warmup should be done:  90%|████████▉ | 2695/3000 [00:01<00:00, 1689.89it/s]warmup should be done:  88%|████████▊ | 2632/3000 [00:01<00:00, 1673.51it/s]warmup should be done:  89%|████████▉ | 2682/3000 [00:01<00:00, 1659.92it/s]warmup should be done:  89%|████████▊ | 2662/3000 [00:01<00:00, 1644.37it/s]warmup should be done:  87%|████████▋ | 2624/3000 [00:01<00:00, 1609.72it/s]warmup should be done:  94%|█████████▎| 2808/3000 [00:01<00:00, 1648.13it/s]warmup should be done:  94%|█████████▍| 2819/3000 [00:01<00:00, 1660.24it/s]warmup should be done:  93%|█████████▎| 2796/3000 [00:01<00:00, 1642.63it/s]warmup should be done:  95%|█████████▌| 2864/3000 [00:01<00:00, 1688.65it/s]warmup should be done:  93%|█████████▎| 2800/3000 [00:01<00:00, 1672.51it/s]warmup should be done:  95%|█████████▍| 2849/3000 [00:01<00:00, 1661.11it/s]warmup should be done:  94%|█████████▍| 2828/3000 [00:01<00:00, 1647.40it/s]warmup should be done:  93%|█████████▎| 2785/3000 [00:01<00:00, 1605.12it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1677.46it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1667.86it/s]warmup should be done:  99%|█████████▉| 2974/3000 [00:01<00:00, 1649.82it/s]warmup should be done: 100%|█████████▉| 2986/3000 [00:01<00:00, 1662.30it/s]warmup should be done:  99%|█████████▊| 2961/3000 [00:01<00:00, 1644.14it/s]warmup should be done:  99%|█████████▉| 2968/3000 [00:01<00:00, 1673.89it/s]warmup should be done: 100%|█████████▉| 2995/3000 [00:01<00:00, 1653.94it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1653.04it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1651.78it/s]warmup should be done:  98%|█████████▊| 2951/3000 [00:01<00:00, 1618.94it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1647.26it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1641.28it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1637.51it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1621.54it/s]2022-12-12 06:09:45.781515: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fe94b833090 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 06:09:45.781567: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 06:09:48.008265: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 06:09:51.161869: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fcac002d160 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 06:09:51.161937: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 06:09:51.196154: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fcaa0030290 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 06:09:51.196219: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 06:09:51.552569: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fe94b82c7c0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 06:09:51.552634: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 06:09:51.556181: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x1d645780 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 06:09:51.556241: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 06:09:51.621010: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fcac80288a0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 06:09:51.621068: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 06:09:51.634996: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fe94f830340 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 06:09:51.635054: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 06:09:51.639087: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fcb0c02d1c0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 06:09:51.639161: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 06:09:51.680342: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 06:09:53.420488: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 06:09:53.428403: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 06:09:53.779406: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 06:09:53.872814: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 06:09:53.874821: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 06:09:53.916407: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 06:09:53.945950: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 06:09:56.290968: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 06:09:56.383619: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 06:09:56.709269: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 06:09:56.754637: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 06:09:56.798862: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 06:09:56.799676: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 06:09:56.818753: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
[HCTR][06:10:20.557][ERROR][RK0][tid #140640239216384]: replica 3 reaches 1000, calling init pre replica
[HCTR][06:10:20.557][ERROR][RK0][tid #140640239216384]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][06:10:20.563][ERROR][RK0][tid #140640239216384]: coll ps creation done
[HCTR][06:10:20.563][ERROR][RK0][tid #140640239216384]: replica 3 waits for coll ps creation barrier
[HCTR][06:10:20.568][ERROR][RK0][tid #140640113391360]: replica 7 reaches 1000, calling init pre replica
[HCTR][06:10:20.568][ERROR][RK0][tid #140640113391360]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][06:10:20.572][ERROR][RK0][tid #140640113391360]: replica 6 reaches 1000, calling init pre replica
[HCTR][06:10:20.572][ERROR][RK0][tid #140640113391360]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][06:10:20.577][ERROR][RK0][tid #140640113391360]: coll ps creation done
[HCTR][06:10:20.577][ERROR][RK0][tid #140640113391360]: replica 7 waits for coll ps creation barrier
[HCTR][06:10:20.578][ERROR][RK0][tid #140640113391360]: coll ps creation done
[HCTR][06:10:20.578][ERROR][RK0][tid #140640113391360]: replica 6 waits for coll ps creation barrier
[HCTR][06:10:20.583][ERROR][RK0][main]: replica 5 reaches 1000, calling init pre replica
[HCTR][06:10:20.583][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][06:10:20.588][ERROR][RK0][main]: coll ps creation done
[HCTR][06:10:20.588][ERROR][RK0][main]: replica 5 waits for coll ps creation barrier
[HCTR][06:10:20.588][ERROR][RK0][main]: replica 4 reaches 1000, calling init pre replica
[HCTR][06:10:20.588][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][06:10:20.588][ERROR][RK0][tid #140640104998656]: replica 1 reaches 1000, calling init pre replica
[HCTR][06:10:20.589][ERROR][RK0][tid #140640104998656]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][06:10:20.593][ERROR][RK0][main]: coll ps creation done
[HCTR][06:10:20.593][ERROR][RK0][main]: replica 4 waits for coll ps creation barrier
[HCTR][06:10:20.596][ERROR][RK0][tid #140640104998656]: coll ps creation done
[HCTR][06:10:20.596][ERROR][RK0][tid #140640104998656]: replica 1 waits for coll ps creation barrier
[HCTR][06:10:20.629][ERROR][RK0][main]: replica 0 reaches 1000, calling init pre replica
[HCTR][06:10:20.629][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][06:10:20.636][ERROR][RK0][main]: coll ps creation done
[HCTR][06:10:20.636][ERROR][RK0][main]: replica 0 waits for coll ps creation barrier
[HCTR][06:10:20.643][ERROR][RK0][main]: replica 2 reaches 1000, calling init pre replica
[HCTR][06:10:20.643][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][06:10:20.651][ERROR][RK0][main]: coll ps creation done
[HCTR][06:10:20.651][ERROR][RK0][main]: replica 2 waits for coll ps creation barrier
[HCTR][06:10:20.651][ERROR][RK0][main]: replica 0 preparing frequency
[HCTR][06:10:21.621][ERROR][RK0][main]: replica 0 preparing frequency done
[HCTR][06:10:21.666][ERROR][RK0][main]: replica 0 calling init per replica
[HCTR][06:10:21.666][ERROR][RK0][tid #140640113391360]: replica 7 calling init per replica
[HCTR][06:10:21.666][ERROR][RK0][main]: replica 2 calling init per replica
[HCTR][06:10:21.666][ERROR][RK0][main]: replica 5 calling init per replica
[HCTR][06:10:21.666][ERROR][RK0][main]: replica 4 calling init per replica
[HCTR][06:10:21.666][ERROR][RK0][tid #140640104998656]: replica 1 calling init per replica
[HCTR][06:10:21.666][ERROR][RK0][tid #140640113391360]: replica 6 calling init per replica
[HCTR][06:10:21.666][ERROR][RK0][tid #140640239216384]: replica 3 calling init per replica
[HCTR][06:10:21.666][ERROR][RK0][tid #140640113391360]: Calling build_v2
[HCTR][06:10:21.666][ERROR][RK0][main]: Calling build_v2
[HCTR][06:10:21.666][ERROR][RK0][tid #140640113391360]: Calling build_v2
[HCTR][06:10:21.666][ERROR][RK0][main]: Calling build_v2
[HCTR][06:10:21.666][ERROR][RK0][main]: Calling build_v2
[HCTR][06:10:21.666][ERROR][RK0][main]: Calling build_v2
[HCTR][06:10:21.666][ERROR][RK0][tid #140640104998656]: Calling build_v2
[HCTR][06:10:21.666][ERROR][RK0][tid #140640239216384]: Calling build_v2
[HCTR][06:10:21.666][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][06:10:21.666][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][06:10:21.666][ERROR][RK0][tid #140640113391360]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][06:10:21.666][ERROR][RK0][tid #140640113391360]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][06:10:21.666][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][06:10:21.666][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][06:10:21.666][ERROR][RK0][tid #140640104998656]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][06:10:21.666][ERROR][RK0][tid #140640239216384]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[2022-12-12 06:10:21.670942: [E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:178] 2022-12-12 06:10:21v100x8, slow pcie.
670990[: E [/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 06:10:212022-12-12 06:10:21:..178671033671057[] : : v100x8, slow pcieEE 2022-12-12 06:10:21
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:[[671077:1962022-12-12 06:10:21: 178] 2022-12-12 06:10:21.E] assigning 0 to cpu[.671150 v100x8, slow pcie
671136: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 06:10:21
[: E:.E[ 178[6711822022-12-12 06:10:21 2022-12-12 06:10:21/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] 2022-12-12 06:10:21: .[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:.:v100x8, slow pcie.E6712291786712512022-12-12 06:10:21196
671257 : [] : .] : /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE2022-12-12 06:10:21v100x8, slow pcieE671272assigning 0 to cpuE: .
 : 
 178/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc671363/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] :: : 2022-12-12 06:10:21:v100x8, slow pcie178E196/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.212
[]  ] :671428] 2022-12-12 06:10:21v100x8, slow pcie/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[assigning 0 to cpu178: build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8.
:2022-12-12 06:10:21
] E
671474196.[v100x8, slow pcie : ] 6715002022-12-12 06:10:21
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE[assigning 0 to cpu: .[: [2022-12-12 06:10:21
E6715432022-12-12 06:10:21196/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 06:10:21.[ : .] :.6715702022-12-12 06:10:21/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE671577assigning 0 to cpu212671586: .: : 
] : E671637196/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccEbuild asymm link desc with 8X Tesla V100-SXM2-32GB out of 8E : ] : 
[ /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccEassigning 0 to cpu196/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 06:10:21/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: 
[] :.:213/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 06:10:21assigning 0 to cpu212671722196] :.
] : ] [remote time is 8.68421212671756build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8Eassigning 0 to cpu2022-12-12 06:10:21
] : 
 
.build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8E[[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc671817
[ 2022-12-12 06:10:212022-12-12 06:10:21:: 2022-12-12 06:10:21/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.[[.212E.:6718732022-12-12 06:10:212022-12-12 06:10:21671873]  671887213: ..: build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: ] E671916671919E
:Eremote time is 8.68421 : :  212 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[EE/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:2022-12-12 06:10:21[  :build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8:214.2022-12-12 06:10:21/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc212
213] 672036.::] ] cpu time is 97.0588[: 672056213212build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8remote time is 8.68421
2022-12-12 06:10:21E: ] ] 

. Eremote time is 8.68421build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8672123[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[ 

: 2022-12-12 06:10:21:2022-12-12 06:10:21/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE[.213[.: 2022-12-12 06:10:21672189] 2022-12-12 06:10:21672191214/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.: remote time is 8.68421.: ] :672219E
672224Ecpu time is 97.0588213:  :  
[2022-12-12 06:10:21] E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.remote time is 8.68421 : :672299
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc214/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc213: :] :[] E214cpu time is 97.05882132022-12-12 06:10:21remote time is 8.68421 ] 
] .
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cccpu time is 97.0588remote time is 8.68421672364:
[
: 2142022-12-12 06:10:21E] [. cpu time is 97.05882022-12-12 06:10:21672420/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
.: :672443E214:  ] E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cccpu time is 97.0588 :
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc214:] 214cpu time is 97.0588] 
cpu time is 97.0588
[2022-12-12 06:11:40.404353: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:118] solver created. now build & solve
[2022-12-12 06:11:40.444479: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:123] solver built. now solve
block 0 storage is 00010001
	access is	0	0	0	0	4	4	4	4	
block 1 storage is 00100010
	access is	1	1	1	1	5	5	5	5	
block 2 storage is 01000100
	access is	2	2	2	2	6	6	6	6	
block 3 storage is 10001000
	access is	3	3	3	3	7	7	7	7	
block 4 storage is 00000000
	access is	8	8	8	8	8	8	8	8	
[2022-12-12 06:11:40.583747: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:127] solver solved
[2022-12-12 06:11:40.583809: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:195] 0 solved master
[2022-12-12 06:11:40.639769: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 0 solved
[2022-12-12 06:11:40.639804: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 0 initing device 0
[2022-12-12 06:11:40.640466: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-12 06:11:40.640518: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 06:11:40.641480: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 06:11:40.642325: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 06:11:40.655058: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 2 solved
[2022-12-12 06:11:40.655138: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 2 initing device 2
[[2022-12-12 06:11:402022-12-12 06:11:40..655355655356: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc::202202] ] 5 solved7 solved

[[2022-12-12 06:11:402022-12-12 06:11:40..655445655447: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc::205205] ] worker 0 thread 5 initing device 5worker 0 thread 7 initing device 7

[2022-12-12 06:11:40.655577: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-12 06:11:40.655629: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[[2022-12-12 06:11:402022-12-12 06:11:40..655905655906: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::18151815] ] Building Coll Cache with ... num gpu device is 8Building Coll Cache with ... num gpu device is 8

[[2022-12-12 06:11:402022-12-12 06:11:40..655975655975: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::19801980] ] eager alloc mem 381.47 MBeager alloc mem 381.47 MB

[2022-12-12 06:11:40.657226: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 6 solved
[2022-12-12 06:11:40.657283: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:[2052022-12-12 06:11:40] .worker 0 thread 6 initing device 6657282
: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 3 solved
[2022-12-12 06:11:40.657360: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 3 initing device 3
[2022-12-12 06:11:40.657720: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-12 06:11:40[.2022-12-12 06:11:40657768.: [657765E2022-12-12 06:11:40:  .E/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu657780 :: /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu1980E:]  1980eager alloc mem 381.47 MB/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu] 
:[eager alloc mem 381.47 MB18152022-12-12 06:11:40
] .Building Coll Cache with ... num gpu device is 8657866
: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980[] 2022-12-12 06:11:40eager alloc mem 381.47 MB.
[6579212022-12-12 06:11:40: .E657940 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE: 1980/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu] :eager alloc mem 381.47 MB1980
] eager alloc mem 381.47 MB
[2022-12-12 06:11:40.659371: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 4 solved
[[2022-12-12 06:11:402022-12-12 06:11:40..659412659427: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc::202205] ] 1 solvedworker 0 thread 4 initing device 4

[2022-12-12 06:11:40.659530: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 1 initing device 1
[2022-12-12 06:11:40.659918: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-12 06:11:40.659947: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:[18152022-12-12 06:11:40] .Building Coll Cache with ... num gpu device is 8659967
: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 06:11:40.660018: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 06:11:40.662115: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 06:11:40.662191: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 06:11:40.662265: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 06:11:40.662321: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 06:11:40.662553: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 06:11:40.664407: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 06:11:40.664484: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 06:11:40.666612: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 06:11:40.666903: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 06:11:40.667452: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 06:11:40.667510: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 06:11:40.717562: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.00 Bytes
[2022-12-12 06:11:40.722845: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5
[2022-12-12 06:11:40.722962: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 06:11:40.723782: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 95.75 MB
[2022-12-12 06:11:40.724409: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:11:40.725513: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:11:40.725560: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 95.37 MB
[[[[2022-12-12 06:11:402022-12-12 06:11:402022-12-12 06:11:402022-12-12 06:11:40....752351752351752351752351: : : : EEEE    /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::::1980198019801980] ] ] ] eager alloc mem 5.00 Byteseager alloc mem 5.00 Bytes
eager alloc mem 5.00 Byteseager alloc mem 5.00 Bytes


[2022-12-12 06:11:40.753526: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.00 Bytes
[2022-12-12 06:11:40.755368: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.00 Bytes
[2022-12-12 06:11:40.755535: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.00 Bytes
[2022-12-12 06:11:40.758431: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5
[2022-12-12 06:11:40.758515[: 2022-12-12 06:11:40E. 758508/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc: :E638 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cceager release cuda mem 400000000:
638] eager release cuda mem 5
[2022-12-12 06:11:40[.2022-12-12 06:11:40758602.: 758594E:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc :/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc638:] 638eager release cuda mem 400000000] 
eager release cuda mem 5
[[2022-12-12 06:11:402022-12-12 06:11:40..758691758678: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::638638] ] eager release cuda mem 400000000eager release cuda mem 5

[2022-12-12 06:11:40.758777: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 06:11:40.758861: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5
[2022-12-12 06:11:40.758933: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 06:11:40.760511: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 95.75 MB
[2022-12-12 06:11:40.760802: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5
[2022-12-12 06:11:40.760907: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 06:11:40.761111: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 95.75 MB
[2022-12-12 06:11:40.761222: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5
[2022-12-12 06:11:40.761307: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 06:11:40.761620: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 95.75 MB
[2022-12-12 06:11:40.762322: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 95.75 MB
[2022-12-12 06:11:40.762976: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 95.75 MB
[2022-12-12 06:11:40.764878: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 95.75 MB
[2022-12-12 06:11:40.765458: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 95.75 MB
[2022-12-12 06:11:40.765841: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:11:40.765939: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:11:40.766147: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:11:40.766362: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:11:40.766409: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:11:40.766804: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:11:40.766848: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:11:40.766923: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:11:40.766971: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 95.37 MB
[2022-12-12 06:11:40.767002: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:11:40.767046: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 95.37 MB
[2022-12-12 06:11:40.767224: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:11:40.767279: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 95.37 MB
[2022-12-12 06:11:40.767426: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:11:40.767473: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:[432022-12-12 06:11:40] .WORKER[0] alloc host memory 95.37 MB767484
: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:11:40.767539: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 95.37 MB
[2022-12-12 06:11:40.767880: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663[
2022-12-12 06:11:40.767906: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:11:40.767936: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] [WORKER[0] alloc host memory 95.37 MB2022-12-12 06:11:40
.767956: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 95.37 MB
[2022-12-12 06:11:40.789132: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 06:11:40.789774: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 06:11:40.789818: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 11.93 GB
[2022-12-12 06:11:40.827744: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 06:11:40.827893: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 06:11:40.828035: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 06:11:40.828375: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 06:11:40.828418: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 11.93 GB
[2022-12-12 06:11:40.828501: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 06:11:40.828543: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 11.93 GB
[2022-12-12 06:11:40.828637: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 06:11:40.828680: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 11.93 GB
[2022-12-12 06:11:40.829257: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 06:11:40.829860: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 06:11:40.829900: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 11.93 GB
[2022-12-12 06:11:40.830287: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 06:11:40.830889: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 06:11:40.830928: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 11.93 GB
[2022-12-12 06:11:40.831090: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 06:11:40.831635: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 06:11:40.831711: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 06:11:40.831754: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 11.93 GB
[2022-12-12 06:11:40.832232: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 06:11:40.832282: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 11.93 GB
[[[[[[[[2022-12-12 06:11:452022-12-12 06:11:452022-12-12 06:11:452022-12-12 06:11:452022-12-12 06:11:452022-12-12 06:11:452022-12-12 06:11:452022-12-12 06:11:45........196494196493196494196495196495196493196494196495: : : : : : : : EEEEEEEE        /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::::::::19261926192619261926192619261926] ] ] ] ] ] ] ] Device 5 init p2p of link 6Device 4 init p2p of link 5Device 7 init p2p of link 4Device 6 init p2p of link 0Device 0 init p2p of link 3Device 1 init p2p of link 7Device 3 init p2p of link 2Device 2 init p2p of link 1







[[[[2022-12-12 06:11:452022-12-12 06:11:45[[2022-12-12 06:11:45[2022-12-12 06:11:45..2022-12-12 06:11:452022-12-12 06:11:45.2022-12-12 06:11:45.197124197124[..197124.197131: : 2022-12-12 06:11:45197132197131: 197138: EE.: : E: E  197168EE E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:  E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:19801980:/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980:1980] ] 1980:1980] 1980] eager alloc mem 611.00 KBeager alloc mem 611.00 KB] 1980] eager alloc mem 611.00 KB] eager alloc mem 611.00 KB

eager alloc mem 611.00 KB] eager alloc mem 611.00 KB
eager alloc mem 611.00 KB

eager alloc mem 611.00 KB


[2022-12-12 06:11:45.198231: E[ 2022-12-12 06:11:45/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc.:198242638: ] Eeager release cuda mem 625663 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:11:45[.2022-12-12 06:11:45198366.: 198372E:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc[ :2022-12-12 06:11:45[/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc638.2022-12-12 06:11:45[:] 198394[.2022-12-12 06:11:45638eager release cuda mem 625663: 2022-12-12 06:11:45198401.] 
E.: 198414eager release cuda mem 625663 198431E: 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:  E:E/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc 638 :/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc] /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc638:eager release cuda mem 625663:] 638
638eager release cuda mem 625663] ] 
eager release cuda mem 625663eager release cuda mem 625663

[2022-12-12 06:11:45.212967: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 6 init p2p of link 5
[2022-12-12 06:11:45.213124: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:11:45.213198: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 2
[2022-12-12 06:11:45.213360: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:11:45.214074: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:11:45.214299: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:11:45.222249: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 7 init p2p of link 1
[2022-12-12 06:11:45.222399: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:11:45.222660: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 3 init p2p of link 0
[2022-12-12 06:11:45.222829: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:11:45.222846: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 0 init p2p of link 6
[2022-12-12 06:11:45.223000: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:11:45.223035: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 5 init p2p of link 4
[2022-12-12 06:11:45.223172: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:11:45.223208: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:11:45.223373: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 4 init p2p of link 7
[2022-12-12 06:11:45.223566: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:11:45.223758: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663[
2022-12-12 06:11:45.223780: [E2022-12-12 06:11:45 ./hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc223779:: 638E]  eager release cuda mem 625663/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu
:1926] Device 2 init p2p of link 3
[2022-12-12 06:11:45.223973: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:11:45.224193: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:11:45.224522: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:11:45.224946: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:11:45.227439: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 6 init p2p of link 4
[2022-12-12 06:11:45.227561: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:11:45.227754: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 3
[2022-12-12 06:11:45.227876: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:11:45.228488: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:11:45.228800: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:11:45.235618: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 7 init p2p of link 6
[2022-12-12 06:11:45.235738: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:11:45.236385: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 0 init p2p of link 1
[2022-12-12 06:11:45.236507: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:11:45.236668: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:11:45.237474: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:11:45.244733: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 3 init p2p of link 5
[2022-12-12 06:11:45.244877: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:11:45.245411: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 5 init p2p of link 7
[2022-12-12 06:11:45.245532: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[[2022-12-12 06:11:452022-12-12 06:11:45..245653245670: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::1926638] ] Device 4 init p2p of link 2eager release cuda mem 625663

[2022-12-12 06:11:45.245832: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:11:45.246293: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 2 init p2p of link 0
[2022-12-12 06:11:45.[2464512022-12-12 06:11:45: .E246455 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE: 1980/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc] :eager alloc mem 611.00 KB638
] eager release cuda mem 625663
[2022-12-12 06:11:45.246612: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:11:45.247457: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:11:45.252260: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 3 init p2p of link 1
[2022-12-12 06:11:45.252388: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:11:45.253334: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:11:45.254626: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 4 init p2p of link 6
[2022-12-12 06:11:45.254749: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:11:45.255695: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:11:45.257365: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 6 init p2p of link 7
[2022-12-12 06:11:45.257481: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:11:45.257709: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 0
[2022-12-12 06:11:45.257833: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:11:45.258425: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:11:45.258793: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:11:45.260022: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 7 init p2p of link 5
[2022-12-12 06:11:45.260138: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:11:45.260337: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 0 init p2p of link 2
[2022-12-12 06:11:45.260462: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:11:45.261074: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:11:45.261403: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:11:45.271580: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 5 init p2p of link 3
[2022-12-12 06:11:45.271696: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:11:45.272472: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:11:45.273159: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 2 init p2p of link 4
[2022-12-12 06:11:45.273310: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:11:45.274091: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:11:45.275337: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 100400000
[2022-12-12 06:11:45.275899: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 100400000
[2022-12-12 06:11:45.276171: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 100400000
[2022-12-12 06:11:45.276728: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 25000000 / 100000000 nodes ( 25.00 %~25.00 %) | remote 75000000 / 100000000 nodes ( 75.00 %) | cpu 0 / 100000000 nodes ( 0.00 %) | 11.93 GB | 4.6188 secs 
[2022-12-12 06:11:45.277223: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 25000000 / 100000000 nodes ( 25.00 %~25.00 %) | remote 75000000 / 100000000 nodes ( 75.00 %) | cpu 0 / 100000000 nodes ( 0.00 %) | 11.93 GB | 4.62126 secs 
[2022-12-12 06:11:45.277413: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 25000000 / 100000000 nodes ( 25.00 %~25.00 %) | remote 75000000 / 100000000 nodes ( 75.00 %) | cpu 0 / 100000000 nodes ( 0.00 %) | 11.93 GB | 4.61746 secs 
[2022-12-12 06:11:45.278121: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 100400000
[2022-12-12 06:11:45.278544: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 25000000 / 100000000 nodes ( 25.00 %~25.00 %) | remote 75000000 / 100000000 nodes ( 75.00 %) | cpu 0 / 100000000 nodes ( 0.00 %) | 11.93 GB | 4.62293 secs 
[2022-12-12 06:11:45.282145: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 100400000
[2022-12-12 06:11:45.282575: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 25000000 / 100000000 nodes ( 25.00 %~25.00 %) | remote 75000000 / 100000000 nodes ( 75.00 %) | cpu 0 / 100000000 nodes ( 0.00 %) | 11.93 GB | 4.62482 secs 
[2022-12-12 06:11:45.283104: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 100400000
[2022-12-12 06:11:45.283540: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 25000000 / 100000000 nodes ( 25.00 %~25.00 %) | remote 75000000 / 100000000 nodes ( 75.00 %) | cpu 0 / 100000000 nodes ( 0.00 %) | 11.93 GB | 4.62353 secs 
[2022-12-12 06:11:45.284970: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 100400000
[2022-12-12 06:11:45.285397: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 25000000 / 100000000 nodes ( 25.00 %~25.00 %) | remote 75000000 / 100000000 nodes ( 75.00 %) | cpu 0 / 100000000 nodes ( 0.00 %) | 11.93 GB | 4.62944 secs 
[2022-12-12 06:11:45.285467: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 100400000
[2022-12-12 06:11:45.285888: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 25000000 / 100000000 nodes ( 25.00 %~25.00 %) | remote 75000000 / 100000000 nodes ( 75.00 %) | cpu 0 / 100000000 nodes ( 0.00 %) | 11.93 GB | 4.64538 secs 
[HCTR][06:11:45.286][ERROR][RK0][main]: replica 5 calling init per replica done, doing barrier
[HCTR][06:11:45.286][ERROR][RK0][main]: replica 2 calling init per replica done, doing barrier
[HCTR][06:11:45.286][ERROR][RK0][tid #140640239216384]: replica 3 calling init per replica done, doing barrier
[HCTR][06:11:45.286][ERROR][RK0][tid #140640104998656]: replica 1 calling init per replica done, doing barrier
[HCTR][06:11:45.286][ERROR][RK0][main]: replica 0 calling init per replica done, doing barrier
[HCTR][06:11:45.286][ERROR][RK0][tid #140640113391360]: replica 6 calling init per replica done, doing barrier
[HCTR][06:11:45.286][ERROR][RK0][tid #140640113391360]: replica 7 calling init per replica done, doing barrier
[HCTR][06:11:45.286][ERROR][RK0][main]: replica 4 calling init per replica done, doing barrier
[HCTR][06:11:45.286][ERROR][RK0][main]: replica 4 calling init per replica done, doing barrier done
[HCTR][06:11:45.286][ERROR][RK0][tid #140640239216384]: replica 3 calling init per replica done, doing barrier done
[HCTR][06:11:45.286][ERROR][RK0][tid #140640113391360]: replica 7 calling init per replica done, doing barrier done
[HCTR][06:11:45.286][ERROR][RK0][main]: replica 5 calling init per replica done, doing barrier done
[HCTR][06:11:45.286][ERROR][RK0][main]: replica 2 calling init per replica done, doing barrier done
[HCTR][06:11:45.286][ERROR][RK0][tid #140640104998656]: replica 1 calling init per replica done, doing barrier done
[HCTR][06:11:45.286][ERROR][RK0][tid #140640113391360]: replica 6 calling init per replica done, doing barrier done
[HCTR][06:11:45.286][ERROR][RK0][main]: replica 0 calling init per replica done, doing barrier done
[HCTR][06:11:45.286][ERROR][RK0][main]: init per replica done
[HCTR][06:11:45.286][ERROR][RK0][tid #140640239216384]: init per replica done
[HCTR][06:11:45.286][ERROR][RK0][tid #140640113391360]: init per replica done
[HCTR][06:11:45.286][ERROR][RK0][tid #140640113391360]: init per replica done
[HCTR][06:11:45.286][ERROR][RK0][main]: init per replica done
[HCTR][06:11:45.286][ERROR][RK0][main]: init per replica done
[HCTR][06:11:45.286][ERROR][RK0][tid #140640104998656]: init per replica done
[HCTR][06:11:45.288][ERROR][RK0][main]: init per replica done








