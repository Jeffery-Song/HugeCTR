2022-12-11 21:41:24.823234: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:24.837549: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:24.844540: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:24.849056: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:24.855371: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:24.866503: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:24.874528: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:24.885380: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:24.936049: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:24.940378: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:24.943402: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:24.944526: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:24.945562: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:24.946623: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:24.947946: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:24.949605: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:24.950496: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:24.950701: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:24.952246: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:24.952256: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:24.953710: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:24.953911: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:24.955281: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:24.955537: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:24.957242: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:24.957302: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:24.958862: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:24.958969: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:24.960563: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:24.960841: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:24.962021: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:24.962652: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:24.964403: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:24.965460: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:24.966489: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:24.967479: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:24.968420: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:24.969403: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:24.970451: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:24.971512: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:24.975479: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:24.976507: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:24.976849: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 21:41:24.977464: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:24.978421: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:24.979438: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:24.980413: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:24.981586: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:24.983435: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:24.984964: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:24.986113: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:24.986565: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:24.987456: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:24.988188: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:24.989033: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:24.989750: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:24.990581: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:24.992113: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:24.994096: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:24.996460: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:24.997256: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:24.999258: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:25.001252: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:25.003746: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:25.004644: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:25.006255: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:25.007458: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:25.007578: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:25.007967: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:25.008864: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:25.010388: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:25.010661: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:25.010943: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:25.010949: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:25.013364: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:25.015360: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:25.015667: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:25.015885: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:25.016133: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:25.017008: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:25.018700: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:25.018983: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:25.019427: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:25.019578: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:25.028597: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:25.041915: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:25.054649: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:25.054712: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:25.055503: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:25.055520: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:25.056390: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:25.056704: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:25.058672: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:25.058712: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:25.060218: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:25.060255: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:25.060881: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:25.061134: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:25.063653: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:25.063946: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:25.064226: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:25.065400: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:25.065476: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:25.066788: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:25.067110: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:25.068842: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:25.069883: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:25.071321: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:25.071440: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:25.071520: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:25.071700: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:25.073098: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:25.074604: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:25.075908: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:25.076090: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:25.076138: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:25.077244: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:25.078340: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:25.079731: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:25.079770: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:25.080870: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:25.081518: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:25.082691: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:25.082868: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:25.083721: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:25.084246: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:25.085550: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:25.085641: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:25.086481: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:25.087246: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:25.088338: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:25.088477: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:25.089497: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:25.090075: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:25.091946: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:25.092481: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:25.093526: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:25.094146: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:25.095372: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:25.095418: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:25.096483: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:25.097081: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:25.098582: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:25.098670: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:25.100081: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:25.100268: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:25.101301: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:25.101409: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:25.102827: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:25.103440: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:25.104076: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:25.104159: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:25.105874: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:25.106308: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:25.106795: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:25.106971: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:25.108743: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:25.109493: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:25.111184: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:25.111904: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:25.112489: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:25.112734: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:25.113881: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:25.114139: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:25.115022: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:25.115653: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:25.116135: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:25.117304: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:25.117751: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:25.118391: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:25.119424: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:25.119847: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:25.120217: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:25.120808: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:25.121019: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:25.121922: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:25.123087: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:25.123395: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:25.124092: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:25.124245: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:25.124891: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:25.125205: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:25.126682: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:25.129460: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:25.130378: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:25.130596: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:25.131157: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:25.131398: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:25.132205: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:25.133734: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:25.133734: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 21:41:25.134620: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:25.134837: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:25.135380: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:25.135633: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:25.136574: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:25.137966: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:25.138665: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:25.138785: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:25.139510: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:25.139912: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:25.140921: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:25.142258: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:25.142926: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:25.143120: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:25.143670: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:25.143738: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:25.146728: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:25.147785: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:25.148273: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:25.148473: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:25.148519: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 21:41:25.148548: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:25.148689: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 21:41:25.150958: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:25.152081: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:25.152405: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:25.152442: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:25.152490: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:25.155051: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:25.155675: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:25.156449: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:25.158106: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:25.158281: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:25.158451: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:25.158782: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:25.159470: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:25.159892: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 21:41:25.162052: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:25.162194: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:25.162440: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:25.162561: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:25.163314: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:25.167509: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:25.167739: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:25.168097: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:25.168108: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:25.168829: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:25.169245: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:25.171570: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:25.172399: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:25.173441: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:25.173875: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:25.176130: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:25.176781: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:25.177857: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:25.178212: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:25.180548: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:25.209701: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:25.212082: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:25.213914: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:25.214517: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:25.217192: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:25.219095: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:25.220670: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:25.223020: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:25.224957: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:25.225017: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:25.228048: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:25.236300: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:25.236312: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:25.239487: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:25.241851: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:25.241899: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:25.243663: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:25.247234: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:25.247312: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:25.278414: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:25.280320: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:25.280416: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:25.282563: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:25.286024: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:25.286085: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:25.293217: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:25.294007: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 21:41:25.294207: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:25.297726: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:25.300568: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 21:41:25.303764: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:25.309678: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:25.360546: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:25.360702: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:25.362003: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 21:41:25.371999: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:25.391770: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:25.392008: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:25.400268: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:25.407058: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:26.262607: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:26.263280: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:26.263817: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:26.264275: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 21:41:26.264336: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-11 21:41:26.281558: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:26.282430: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:26.283160: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:26.283749: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:26.284259: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:26.284889: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-11 21:41:26.327851: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 21:41:26.328049: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 21:41:26.376396: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12340
2022-12-11 21:41:26.547098: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:26.548359: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:26.548896: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:26.549355: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 21:41:26.549415: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-11 21:41:26.566464: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:26.567208: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:26.567734: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:26.568305: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:26.569016: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:26.569492: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:1/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-11 21:41:26.587759: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:26.588586: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:26.589112: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:26.589701: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 21:41:26.589764: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-11 21:41:26.594116: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:26.594839: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:26.595383: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:26.595851: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 21:41:26.595907: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-11 21:41:26.608031: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:26.608662: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:26.609162: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:26.609986: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:26.610802: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:26.611316: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:2/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-11 21:41:26.612485: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:26.613293: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:26.613813: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:26.614537: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:26.615264: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:26.615742: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:5/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-11 21:41:26.633583: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:26.634207: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:26.634742: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:26.635224: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 21:41:26.635285: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-11 21:41:26.648361: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 21:41:26.648573: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 21:41:26.650355: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12341
2022-12-11 21:41:26.652113: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:26.652827: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:26.653398: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:26.653991: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:26.654525: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:26.654995: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:3/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-11 21:41:26.655711: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:26.656321: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:26.656896: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:26.656952: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:26.657910: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 21:41:26.657973: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-11 21:41:26.658044: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 21:41:26.658096: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:26.658195: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 21:41:26.658831: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:26.659339: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 21:41:26.659397: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-11 21:41:26.660013: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12345
2022-12-11 21:41:26.674763: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:26.675437: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:26.675978: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:26.676543: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:26.676721: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:26.677679: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:26.677886: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:26.678578: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:26.678849: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:7/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-11 21:41:26.679330: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:26.679871: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:26.680346: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:6/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-11 21:41:26.688262: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:26.689848: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 21:41:26.690019: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 21:41:26.691913: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12342
2022-12-11 21:41:26.692486: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:26.693028: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:26.693493: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 21:41:26.693555: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-11 21:41:26.698140: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 21:41:26.698307: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 21:41:26.700103: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12343
2022-12-11 21:41:26.710661: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:26.711314: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:26.711833: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:26.712403: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:26.712924: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:41:26.713387: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:4/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-11 21:41:26.721264: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 21:41:26.721455: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 21:41:26.722747: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 21:41:26.722906: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 21:41:26.723238: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12347
2022-12-11 21:41:26.724716: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12346
2022-12-11 21:41:26.754643: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 21:41:26.754833: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 21:41:26.756655: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12344
[HCTR][21:41:28.037][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][21:41:28.038][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][21:41:28.038][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][21:41:28.038][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][21:41:28.038][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][21:41:28.038][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][21:41:28.038][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][21:41:28.038][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 1it [00:01,  1.56s/it]warmup run: 97it [00:01, 81.33it/s]warmup run: 1it [00:01,  1.53s/it]warmup run: 195it [00:01, 177.71it/s]warmup run: 1it [00:01,  1.51s/it]warmup run: 98it [00:01, 83.22it/s]warmup run: 1it [00:01,  1.51s/it]warmup run: 294it [00:01, 285.28it/s]warmup run: 1it [00:01,  1.50s/it]warmup run: 1it [00:01,  1.51s/it]warmup run: 1it [00:01,  1.49s/it]warmup run: 101it [00:01, 87.07it/s]warmup run: 193it [00:01, 177.24it/s]warmup run: 101it [00:01, 87.12it/s]warmup run: 1it [00:01,  1.49s/it]warmup run: 393it [00:01, 396.93it/s]warmup run: 101it [00:01, 87.70it/s]warmup run: 89it [00:01, 76.86it/s]warmup run: 96it [00:01, 83.49it/s]warmup run: 202it [00:01, 188.45it/s]warmup run: 286it [00:01, 277.93it/s]warmup run: 199it [00:01, 185.15it/s]warmup run: 95it [00:01, 82.77it/s]warmup run: 488it [00:02, 499.35it/s]warmup run: 188it [00:01, 177.38it/s]warmup run: 202it [00:01, 189.56it/s]warmup run: 191it [00:01, 179.48it/s]warmup run: 300it [00:01, 295.82it/s]warmup run: 377it [00:01, 378.81it/s]warmup run: 299it [00:01, 295.68it/s]warmup run: 193it [00:01, 182.33it/s]warmup run: 585it [00:02, 596.59it/s]warmup run: 303it [00:01, 301.38it/s]warmup run: 286it [00:01, 286.21it/s]warmup run: 288it [00:01, 287.40it/s]warmup run: 399it [00:01, 407.84it/s]warmup run: 467it [00:02, 475.70it/s]warmup run: 400it [00:01, 411.14it/s]warmup run: 291it [00:01, 291.28it/s]warmup run: 684it [00:02, 686.09it/s]warmup run: 403it [00:01, 415.18it/s]warmup run: 386it [00:01, 402.13it/s]warmup run: 386it [00:01, 399.98it/s]warmup run: 497it [00:02, 515.10it/s]warmup run: 559it [00:02, 568.18it/s]warmup run: 501it [00:02, 523.02it/s]warmup run: 389it [00:01, 403.71it/s]warmup run: 783it [00:02, 759.57it/s]warmup run: 504it [00:01, 526.90it/s]warmup run: 485it [00:02, 512.13it/s]warmup run: 484it [00:01, 508.68it/s]warmup run: 597it [00:02, 617.16it/s]warmup run: 652it [00:02, 650.21it/s]warmup run: 602it [00:02, 625.65it/s]warmup run: 488it [00:01, 513.97it/s]warmup run: 881it [00:02, 816.54it/s]warmup run: 605it [00:02, 628.44it/s]warmup run: 585it [00:02, 614.54it/s]warmup run: 583it [00:02, 609.77it/s]warmup run: 697it [00:02, 705.59it/s]warmup run: 745it [00:02, 718.63it/s]warmup run: 702it [00:02, 711.51it/s]warmup run: 586it [00:02, 612.85it/s]warmup run: 979it [00:02, 859.16it/s]warmup run: 685it [00:02, 703.44it/s]warmup run: 705it [00:02, 713.19it/s]warmup run: 682it [00:02, 697.16it/s]warmup run: 798it [00:02, 779.78it/s]warmup run: 838it [00:02, 772.27it/s]warmup run: 802it [00:02, 781.67it/s]warmup run: 685it [00:02, 700.57it/s]warmup run: 1076it [00:02, 886.12it/s]warmup run: 805it [00:02, 784.26it/s]warmup run: 785it [00:02, 775.86it/s]warmup run: 781it [00:02, 769.01it/s]warmup run: 898it [00:02, 836.48it/s]warmup run: 930it [00:02, 811.27it/s]warmup run: 900it [00:02, 782.02it/s]warmup run: 784it [00:02, 772.03it/s]warmup run: 1173it [00:02, 902.85it/s]warmup run: 905it [00:02, 839.11it/s]warmup run: 885it [00:02, 832.46it/s]warmup run: 879it [00:02, 823.22it/s]warmup run: 997it [00:02, 877.28it/s]warmup run: 1022it [00:02, 841.19it/s]warmup run: 993it [00:02, 820.40it/s]warmup run: 881it [00:02, 818.79it/s]warmup run: 1269it [00:02, 918.33it/s]warmup run: 1005it [00:02, 882.63it/s]warmup run: 984it [00:02, 872.78it/s]warmup run: 976it [00:02, 862.92it/s]warmup run: 1098it [00:02, 912.27it/s]warmup run: 1116it [00:02, 866.83it/s]warmup run: 1093it [00:02, 868.86it/s]warmup run: 977it [00:02, 853.80it/s]warmup run: 1370it [00:02, 942.43it/s]warmup run: 1105it [00:02, 914.93it/s]warmup run: 1082it [00:02, 900.10it/s]warmup run: 1074it [00:02, 895.04it/s]warmup run: 1198it [00:02, 934.50it/s]warmup run: 1209it [00:02, 883.66it/s]warmup run: 1192it [00:02, 900.22it/s]warmup run: 1075it [00:02, 886.87it/s]warmup run: 1472it [00:03, 964.89it/s]warmup run: 1205it [00:02, 937.07it/s]warmup run: 1180it [00:02, 922.66it/s]warmup run: 1176it [00:02, 928.78it/s]warmup run: 1297it [00:02, 950.46it/s]warmup run: 1303it [00:02, 898.86it/s]warmup run: 1290it [00:02, 921.03it/s]warmup run: 1174it [00:02, 913.72it/s]warmup run: 1571it [00:03, 971.04it/s]warmup run: 1279it [00:02, 941.00it/s]warmup run: 1279it [00:02, 956.61it/s]warmup run: 1305it [00:02, 936.93it/s]warmup run: 1396it [00:02, 957.97it/s]warmup run: 1398it [00:03, 911.05it/s]warmup run: 1391it [00:02, 944.31it/s]warmup run: 1275it [00:02, 939.96it/s]warmup run: 1671it [00:03, 977.00it/s]warmup run: 1378it [00:02, 954.66it/s]warmup run: 1383it [00:02, 978.78it/s]warmup run: 1403it [00:02, 935.34it/s]warmup run: 1495it [00:03, 961.44it/s]warmup run: 1491it [00:03, 916.61it/s]warmup run: 1492it [00:03, 962.30it/s]warmup run: 1373it [00:02, 949.85it/s]warmup run: 1771it [00:03, 982.43it/s]warmup run: 1478it [00:03, 966.77it/s]warmup run: 1484it [00:03, 983.95it/s]warmup run: 1500it [00:03, 934.06it/s]warmup run: 1595it [00:03, 971.77it/s]warmup run: 1585it [00:03, 922.83it/s]warmup run: 1593it [00:03, 974.91it/s]warmup run: 1474it [00:03, 965.14it/s]warmup run: 1871it [00:03, 987.43it/s]warmup run: 1578it [00:03, 975.65it/s]warmup run: 1585it [00:03, 982.84it/s]warmup run: 1596it [00:03, 936.22it/s]warmup run: 1699it [00:03, 990.49it/s]warmup run: 1679it [00:03, 926.59it/s]warmup run: 1694it [00:03, 983.34it/s]warmup run: 1575it [00:03, 977.05it/s]warmup run: 1971it [00:03, 989.50it/s]warmup run: 1679it [00:03, 983.61it/s]warmup run: 1685it [00:03, 981.48it/s]warmup run: 1693it [00:03, 946.00it/s]warmup run: 1802it [00:03, 999.97it/s]warmup run: 1773it [00:03, 928.19it/s]warmup run: 1795it [00:03, 988.76it/s]warmup run: 1676it [00:03, 985.96it/s]warmup run: 2085it [00:03, 1032.64it/s]warmup run: 1781it [00:03, 992.60it/s]warmup run: 1785it [00:03, 977.12it/s]warmup run: 1789it [00:03, 949.63it/s]warmup run: 1904it [00:03, 1004.82it/s]warmup run: 1869it [00:03, 936.25it/s]warmup run: 1896it [00:03, 993.41it/s]warmup run: 1777it [00:03, 990.74it/s]warmup run: 2205it [00:03, 1082.49it/s]warmup run: 1884it [00:03, 1002.29it/s]warmup run: 1884it [00:03, 972.90it/s]warmup run: 1888it [00:03, 959.33it/s]warmup run: 2006it [00:03, 1009.15it/s]warmup run: 1969it [00:03, 954.73it/s]warmup run: 1996it [00:03, 994.78it/s]warmup run: 1878it [00:03, 996.24it/s]warmup run: 2325it [00:03, 1117.17it/s]warmup run: 1988it [00:03, 1011.03it/s]warmup run: 1982it [00:03, 972.68it/s]warmup run: 1991it [00:03, 978.44it/s]warmup run: 2126it [00:03, 1064.25it/s]warmup run: 2080it [00:03, 1000.92it/s]warmup run: 2113it [00:03, 1045.85it/s]warmup run: 1979it [00:03, 997.06it/s]warmup run: 2445it [00:03, 1140.82it/s]warmup run: 2106it [00:03, 1059.62it/s]warmup run: 2097it [00:03, 1022.69it/s]warmup run: 2110it [00:03, 1040.04it/s]warmup run: 2246it [00:03, 1103.74it/s]warmup run: 2197it [00:03, 1049.45it/s]warmup run: 2231it [00:03, 1084.64it/s]warmup run: 2094it [00:03, 1042.32it/s]warmup run: 2567it [00:04, 1163.98it/s]warmup run: 2228it [00:03, 1104.90it/s]warmup run: 2216it [00:03, 1071.90it/s]warmup run: 2232it [00:03, 1090.93it/s]warmup run: 2366it [00:03, 1132.20it/s]warmup run: 2314it [00:03, 1083.52it/s]warmup run: 2350it [00:03, 1114.38it/s]warmup run: 2214it [00:03, 1086.83it/s]warmup run: 2688it [00:04, 1177.41it/s]warmup run: 2350it [00:03, 1138.56it/s]warmup run: 2336it [00:03, 1109.24it/s]warmup run: 2353it [00:03, 1125.90it/s]warmup run: 2486it [00:03, 1151.13it/s]warmup run: 2431it [00:04, 1107.66it/s]warmup run: 2469it [00:03, 1135.30it/s]warmup run: 2334it [00:03, 1117.87it/s]warmup run: 2810it [00:04, 1190.10it/s]warmup run: 2472it [00:03, 1160.38it/s]warmup run: 2456it [00:03, 1135.81it/s]warmup run: 2606it [00:04, 1164.81it/s]warmup run: 2475it [00:03, 1151.33it/s]warmup run: 2547it [00:04, 1123.06it/s]warmup run: 2588it [00:04, 1150.81it/s]warmup run: 2454it [00:03, 1139.64it/s]warmup run: 2932it [00:04, 1198.94it/s]warmup run: 2593it [00:04, 1174.01it/s]warmup run: 2576it [00:04, 1154.88it/s]warmup run: 2725it [00:04, 1171.97it/s]warmup run: 2596it [00:04, 1168.26it/s]warmup run: 2662it [00:04, 1130.70it/s]warmup run: 3000it [00:04, 676.74it/s] warmup run: 2706it [00:04, 1158.92it/s]warmup run: 2574it [00:04, 1154.83it/s]warmup run: 2712it [00:04, 1177.72it/s]warmup run: 2696it [00:04, 1165.65it/s]warmup run: 2846it [00:04, 1181.44it/s]warmup run: 2713it [00:04, 1147.33it/s]warmup run: 2778it [00:04, 1138.95it/s]warmup run: 2826it [00:04, 1171.15it/s]warmup run: 2693it [00:04, 1163.15it/s]warmup run: 2832it [00:04, 1183.72it/s]warmup run: 2815it [00:04, 1170.82it/s]warmup run: 2968it [00:04, 1192.16it/s]warmup run: 2834it [00:04, 1165.43it/s]warmup run: 2895it [00:04, 1145.88it/s]warmup run: 3000it [00:04, 688.49it/s] warmup run: 2945it [00:04, 1176.73it/s]warmup run: 2813it [00:04, 1173.61it/s]warmup run: 2952it [00:04, 1186.02it/s]warmup run: 2935it [00:04, 1178.02it/s]warmup run: 3000it [00:04, 682.05it/s] warmup run: 2955it [00:04, 1175.83it/s]warmup run: 3000it [00:04, 659.90it/s] warmup run: 3000it [00:04, 686.94it/s] warmup run: 3000it [00:04, 685.45it/s] warmup run: 3000it [00:04, 684.87it/s] warmup run: 2933it [00:04, 1180.41it/s]warmup run: 3000it [00:04, 686.29it/s] 

warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]

warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]


warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   6%|▌         | 166/3000 [00:00<00:01, 1659.30it/s]warmup should be done:   6%|▌         | 166/3000 [00:00<00:01, 1658.34it/s]warmup should be done:   5%|▌         | 164/3000 [00:00<00:01, 1634.70it/s]warmup should be done:   5%|▌         | 163/3000 [00:00<00:01, 1623.86it/s]warmup should be done:   5%|▌         | 160/3000 [00:00<00:01, 1592.90it/s]warmup should be done:   5%|▌         | 162/3000 [00:00<00:01, 1610.45it/s]warmup should be done:   6%|▌         | 166/3000 [00:00<00:01, 1649.97it/s]warmup should be done:   5%|▌         | 164/3000 [00:00<00:01, 1630.11it/s]warmup should be done:  11%|█         | 333/3000 [00:00<00:01, 1659.99it/s]warmup should be done:  11%|█         | 327/3000 [00:00<00:01, 1631.46it/s]warmup should be done:  11%|█         | 321/3000 [00:00<00:01, 1601.32it/s]warmup should be done:  11%|█         | 334/3000 [00:00<00:01, 1666.30it/s]warmup should be done:  11%|█         | 331/3000 [00:00<00:01, 1650.36it/s]warmup should be done:  11%|█         | 326/3000 [00:00<00:01, 1627.37it/s]warmup should be done:  11%|█         | 330/3000 [00:00<00:01, 1645.51it/s]warmup should be done:  11%|█         | 333/3000 [00:00<00:01, 1659.03it/s]warmup should be done:  17%|█▋        | 499/3000 [00:00<00:01, 1659.07it/s]warmup should be done:  17%|█▋        | 501/3000 [00:00<00:01, 1663.30it/s]warmup should be done:  16%|█▋        | 489/3000 [00:00<00:01, 1623.96it/s]warmup should be done:  17%|█▋        | 496/3000 [00:00<00:01, 1648.11it/s]warmup should be done:  16%|█▌        | 482/3000 [00:00<00:01, 1595.24it/s]warmup should be done:  16%|█▋        | 491/3000 [00:00<00:01, 1624.52it/s]warmup should be done:  17%|█▋        | 497/3000 [00:00<00:01, 1634.81it/s]warmup should be done:  17%|█▋        | 499/3000 [00:00<00:01, 1634.80it/s]warmup should be done:  22%|██▏       | 665/3000 [00:00<00:01, 1657.01it/s]warmup should be done:  22%|██▏       | 668/3000 [00:00<00:01, 1662.39it/s]warmup should be done:  22%|██▏       | 661/3000 [00:00<00:01, 1645.06it/s]warmup should be done:  22%|██▏       | 652/3000 [00:00<00:01, 1617.56it/s]warmup should be done:  21%|██▏       | 642/3000 [00:00<00:01, 1590.34it/s]warmup should be done:  22%|██▏       | 654/3000 [00:00<00:01, 1616.68it/s]warmup should be done:  22%|██▏       | 661/3000 [00:00<00:01, 1630.45it/s]warmup should be done:  22%|██▏       | 663/3000 [00:00<00:01, 1618.71it/s]warmup should be done:  28%|██▊       | 831/3000 [00:00<00:01, 1655.24it/s]warmup should be done:  28%|██▊       | 826/3000 [00:00<00:01, 1645.40it/s]warmup should be done:  28%|██▊       | 835/3000 [00:00<00:01, 1660.04it/s]warmup should be done:  27%|██▋       | 814/3000 [00:00<00:01, 1617.35it/s]warmup should be done:  27%|██▋       | 802/3000 [00:00<00:01, 1587.10it/s]warmup should be done:  27%|██▋       | 816/3000 [00:00<00:01, 1611.10it/s]warmup should be done:  28%|██▊       | 825/3000 [00:00<00:01, 1627.20it/s]warmup should be done:  28%|██▊       | 825/3000 [00:00<00:01, 1613.49it/s]warmup should be done:  33%|███▎      | 997/3000 [00:00<00:01, 1652.91it/s]warmup should be done:  33%|███▎      | 991/3000 [00:00<00:01, 1641.01it/s]warmup should be done:  33%|███▎      | 976/3000 [00:00<00:01, 1611.00it/s]warmup should be done:  33%|███▎      | 1002/3000 [00:00<00:01, 1652.82it/s]warmup should be done:  32%|███▏      | 961/3000 [00:00<00:01, 1580.08it/s]warmup should be done:  33%|███▎      | 988/3000 [00:00<00:01, 1623.36it/s]warmup should be done:  33%|███▎      | 978/3000 [00:00<00:01, 1600.32it/s]warmup should be done:  33%|███▎      | 988/3000 [00:00<00:01, 1617.42it/s]warmup should be done:  39%|███▉      | 1163/3000 [00:00<00:01, 1648.52it/s]warmup should be done:  39%|███▊      | 1156/3000 [00:00<00:01, 1641.46it/s]warmup should be done:  38%|███▊      | 1138/3000 [00:00<00:01, 1610.64it/s]warmup should be done:  39%|███▉      | 1168/3000 [00:00<00:01, 1650.14it/s]warmup should be done:  38%|███▊      | 1153/3000 [00:00<00:01, 1630.99it/s]warmup should be done:  38%|███▊      | 1151/3000 [00:00<00:01, 1621.45it/s]warmup should be done:  38%|███▊      | 1139/3000 [00:00<00:01, 1596.39it/s]warmup should be done:  37%|███▋      | 1120/3000 [00:00<00:01, 1566.44it/s]warmup should be done:  44%|████▍     | 1321/3000 [00:00<00:01, 1643.73it/s]warmup should be done:  44%|████▍     | 1329/3000 [00:00<00:01, 1650.01it/s]warmup should be done:  43%|████▎     | 1300/3000 [00:00<00:01, 1612.53it/s]warmup should be done:  44%|████▍     | 1318/3000 [00:00<00:01, 1635.87it/s]warmup should be done:  44%|████▍     | 1315/3000 [00:00<00:01, 1625.06it/s]warmup should be done:  43%|████▎     | 1299/3000 [00:00<00:01, 1595.72it/s]warmup should be done:  43%|████▎     | 1278/3000 [00:00<00:01, 1569.75it/s]warmup should be done:  44%|████▍     | 1334/3000 [00:00<00:01, 1621.46it/s]warmup should be done:  50%|████▉     | 1486/3000 [00:00<00:00, 1644.29it/s]warmup should be done:  50%|████▉     | 1495/3000 [00:00<00:00, 1646.97it/s]warmup should be done:  49%|████▊     | 1462/3000 [00:00<00:00, 1609.24it/s]warmup should be done:  49%|████▉     | 1482/3000 [00:00<00:00, 1634.20it/s]warmup should be done:  49%|████▉     | 1480/3000 [00:00<00:00, 1629.76it/s]warmup should be done:  49%|████▊     | 1459/3000 [00:00<00:00, 1596.10it/s]warmup should be done:  48%|████▊     | 1436/3000 [00:00<00:00, 1570.42it/s]warmup should be done:  50%|████▉     | 1497/3000 [00:00<00:00, 1620.34it/s]warmup should be done:  55%|█████▌    | 1651/3000 [00:01<00:00, 1644.40it/s]warmup should be done:  55%|█████▌    | 1660/3000 [00:01<00:00, 1645.37it/s]warmup should be done:  54%|█████▍    | 1624/3000 [00:01<00:00, 1611.80it/s]warmup should be done:  55%|█████▍    | 1647/3000 [00:01<00:00, 1638.77it/s]warmup should be done:  55%|█████▍    | 1644/3000 [00:01<00:00, 1631.96it/s]warmup should be done:  54%|█████▍    | 1619/3000 [00:01<00:00, 1596.93it/s]warmup should be done:  53%|█████▎    | 1594/3000 [00:01<00:00, 1572.06it/s]warmup should be done:  55%|█████▌    | 1660/3000 [00:01<00:00, 1615.99it/s]warmup should be done:  61%|██████    | 1816/3000 [00:01<00:00, 1644.36it/s]warmup should be done:  60%|█████▉    | 1786/3000 [00:01<00:00, 1611.79it/s]warmup should be done:  61%|██████    | 1825/3000 [00:01<00:00, 1637.18it/s]warmup should be done:  60%|██████    | 1813/3000 [00:01<00:00, 1642.44it/s]warmup should be done:  59%|█████▉    | 1779/3000 [00:01<00:00, 1597.10it/s]warmup should be done:  58%|█████▊    | 1752/3000 [00:01<00:00, 1567.56it/s]warmup should be done:  60%|██████    | 1808/3000 [00:01<00:00, 1621.17it/s]warmup should be done:  61%|██████    | 1822/3000 [00:01<00:00, 1608.02it/s]warmup should be done:  66%|██████▌   | 1981/3000 [00:01<00:00, 1643.98it/s]warmup should be done:  65%|██████▍   | 1948/3000 [00:01<00:00, 1610.06it/s]warmup should be done:  66%|██████▌   | 1978/3000 [00:01<00:00, 1641.93it/s]warmup should be done:  66%|██████▋   | 1989/3000 [00:01<00:00, 1631.94it/s]warmup should be done:  65%|██████▍   | 1939/3000 [00:01<00:00, 1597.10it/s]warmup should be done:  64%|██████▎   | 1910/3000 [00:01<00:00, 1570.28it/s]warmup should be done:  66%|██████▌   | 1972/3000 [00:01<00:00, 1625.33it/s]warmup should be done:  66%|██████▌   | 1983/3000 [00:01<00:00, 1601.03it/s]warmup should be done:  72%|███████▏  | 2146/3000 [00:01<00:00, 1643.64it/s]warmup should be done:  70%|███████   | 2110/3000 [00:01<00:00, 1608.19it/s]warmup should be done:  71%|███████▏  | 2143/3000 [00:01<00:00, 1641.22it/s]warmup should be done:  70%|██████▉   | 2099/3000 [00:01<00:00, 1596.50it/s]warmup should be done:  72%|███████▏  | 2153/3000 [00:01<00:00, 1623.86it/s]warmup should be done:  69%|██████▉   | 2068/3000 [00:01<00:00, 1571.01it/s]warmup should be done:  71%|███████   | 2135/3000 [00:01<00:00, 1623.63it/s]warmup should be done:  71%|███████▏  | 2144/3000 [00:01<00:00, 1593.14it/s]warmup should be done:  77%|███████▋  | 2311/3000 [00:01<00:00, 1641.63it/s]warmup should be done:  77%|███████▋  | 2308/3000 [00:01<00:00, 1642.00it/s]warmup should be done:  75%|███████▌  | 2259/3000 [00:01<00:00, 1593.12it/s]warmup should be done:  76%|███████▌  | 2271/3000 [00:01<00:00, 1598.31it/s]warmup should be done:  74%|███████▍  | 2226/3000 [00:01<00:00, 1571.89it/s]warmup should be done:  77%|███████▋  | 2316/3000 [00:01<00:00, 1618.12it/s]warmup should be done:  77%|███████▋  | 2298/3000 [00:01<00:00, 1624.44it/s]warmup should be done:  77%|███████▋  | 2304/3000 [00:01<00:00, 1588.84it/s]warmup should be done:  83%|████████▎ | 2476/3000 [00:01<00:00, 1643.42it/s]warmup should be done:  82%|████████▏ | 2473/3000 [00:01<00:00, 1642.87it/s]warmup should be done:  81%|████████  | 2419/3000 [00:01<00:00, 1594.92it/s]warmup should be done:  81%|████████  | 2431/3000 [00:01<00:00, 1595.33it/s]warmup should be done:  82%|████████▏ | 2462/3000 [00:01<00:00, 1628.84it/s]warmup should be done:  79%|███████▉  | 2384/3000 [00:01<00:00, 1570.00it/s]warmup should be done:  83%|████████▎ | 2478/3000 [00:01<00:00, 1612.42it/s]warmup should be done:  82%|████████▏ | 2463/3000 [00:01<00:00, 1587.93it/s]warmup should be done:  88%|████████▊ | 2642/3000 [00:01<00:00, 1645.44it/s]warmup should be done:  88%|████████▊ | 2638/3000 [00:01<00:00, 1641.82it/s]warmup should be done:  86%|████████▌ | 2579/3000 [00:01<00:00, 1595.42it/s]warmup should be done:  86%|████████▋ | 2591/3000 [00:01<00:00, 1591.26it/s]warmup should be done:  88%|████████▊ | 2626/3000 [00:01<00:00, 1631.39it/s]warmup should be done:  85%|████████▍ | 2542/3000 [00:01<00:00, 1571.33it/s]warmup should be done:  88%|████████▊ | 2640/3000 [00:01<00:00, 1611.02it/s]warmup should be done:  87%|████████▋ | 2622/3000 [00:01<00:00, 1587.29it/s]warmup should be done:  94%|█████████▎| 2808/3000 [00:01<00:00, 1647.17it/s]warmup should be done:  93%|█████████▎| 2803/3000 [00:01<00:00, 1642.88it/s]warmup should be done:  91%|█████████▏| 2739/3000 [00:01<00:00, 1594.55it/s]warmup should be done:  93%|█████████▎| 2790/3000 [00:01<00:00, 1632.29it/s]warmup should be done:  92%|█████████▏| 2751/3000 [00:01<00:00, 1589.70it/s]warmup should be done:  90%|█████████ | 2700/3000 [00:01<00:00, 1571.57it/s]warmup should be done:  93%|█████████▎| 2803/3000 [00:01<00:00, 1615.62it/s]warmup should be done:  93%|█████████▎| 2781/3000 [00:01<00:00, 1587.03it/s]warmup should be done:  99%|█████████▉| 2975/3000 [00:01<00:00, 1653.07it/s]warmup should be done:  99%|█████████▉| 2969/3000 [00:01<00:00, 1646.22it/s]warmup should be done:  97%|█████████▋| 2901/3000 [00:01<00:00, 1599.28it/s]warmup should be done:  97%|█████████▋| 2912/3000 [00:01<00:00, 1595.09it/s]warmup should be done:  99%|█████████▊| 2956/3000 [00:01<00:00, 1638.62it/s]warmup should be done:  95%|█████████▌| 2859/3000 [00:01<00:00, 1575.57it/s]warmup should be done:  99%|█████████▉| 2968/3000 [00:01<00:00, 1624.02it/s]warmup should be done:  98%|█████████▊| 2942/3000 [00:01<00:00, 1592.55it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1645.36it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1638.96it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1634.77it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1629.51it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1614.37it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1606.03it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1600.90it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1576.26it/s]





warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   6%|▌         | 167/3000 [00:00<00:01, 1668.57it/s]warmup should be done:   5%|▌         | 163/3000 [00:00<00:01, 1629.40it/s]warmup should be done:   6%|▌         | 169/3000 [00:00<00:01, 1689.26it/s]warmup should be done:   6%|▌         | 170/3000 [00:00<00:01, 1694.55it/s]warmup should be done:   5%|▌         | 162/3000 [00:00<00:01, 1614.50it/s]warmup should be done:   6%|▌         | 168/3000 [00:00<00:01, 1672.86it/s]warmup should be done:   6%|▌         | 165/3000 [00:00<00:01, 1640.16it/s]warmup should be done:   6%|▌         | 166/3000 [00:00<00:01, 1650.15it/s]warmup should be done:  11%|█         | 337/3000 [00:00<00:01, 1683.93it/s]warmup should be done:  11%|█▏        | 338/3000 [00:00<00:01, 1684.13it/s]warmup should be done:  11%|█▏        | 340/3000 [00:00<00:01, 1692.62it/s]warmup should be done:  11%|█         | 325/3000 [00:00<00:01, 1618.29it/s]warmup should be done:  11%|█         | 332/3000 [00:00<00:01, 1650.90it/s]warmup should be done:  11%|█         | 326/3000 [00:00<00:01, 1614.70it/s]warmup should be done:  11%|█         | 330/3000 [00:00<00:01, 1623.82it/s]warmup should be done:  11%|█         | 336/3000 [00:00<00:01, 1578.38it/s]warmup should be done:  17%|█▋        | 508/3000 [00:00<00:01, 1693.22it/s]warmup should be done:  16%|█▋        | 488/3000 [00:00<00:01, 1622.50it/s]warmup should be done:  17%|█▋        | 510/3000 [00:00<00:01, 1694.12it/s]warmup should be done:  17%|█▋        | 507/3000 [00:00<00:01, 1684.03it/s]warmup should be done:  16%|█▋        | 488/3000 [00:00<00:01, 1602.27it/s]warmup should be done:  17%|█▋        | 498/3000 [00:00<00:01, 1635.15it/s]warmup should be done:  16%|█▋        | 493/3000 [00:00<00:01, 1615.98it/s]warmup should be done:  17%|█▋        | 504/3000 [00:00<00:01, 1622.12it/s]warmup should be done:  23%|██▎       | 678/3000 [00:00<00:01, 1693.57it/s]warmup should be done:  22%|██▏       | 651/3000 [00:00<00:01, 1622.80it/s]warmup should be done:  23%|██▎       | 681/3000 [00:00<00:01, 1697.13it/s]warmup should be done:  23%|██▎       | 676/3000 [00:00<00:01, 1681.69it/s]warmup should be done:  22%|██▏       | 662/3000 [00:00<00:01, 1633.90it/s]warmup should be done:  22%|██▏       | 656/3000 [00:00<00:01, 1617.56it/s]warmup should be done:  22%|██▏       | 649/3000 [00:00<00:01, 1590.29it/s]warmup should be done:  22%|██▏       | 673/3000 [00:00<00:01, 1647.50it/s]warmup should be done:  28%|██▊       | 851/3000 [00:00<00:01, 1697.92it/s]warmup should be done:  28%|██▊       | 848/3000 [00:00<00:01, 1691.01it/s]warmup should be done:  28%|██▊       | 845/3000 [00:00<00:01, 1680.88it/s]warmup should be done:  27%|██▋       | 814/3000 [00:00<00:01, 1619.84it/s]warmup should be done:  28%|██▊       | 826/3000 [00:00<00:01, 1635.55it/s]warmup should be done:  27%|██▋       | 823/3000 [00:00<00:01, 1633.36it/s]warmup should be done:  27%|██▋       | 812/3000 [00:00<00:01, 1602.68it/s]warmup should be done:  28%|██▊       | 841/3000 [00:00<00:01, 1658.07it/s]warmup should be done:  34%|███▍      | 1021/3000 [00:00<00:01, 1696.63it/s]warmup should be done:  34%|███▍      | 1018/3000 [00:00<00:01, 1692.55it/s]warmup should be done:  34%|███▍      | 1016/3000 [00:00<00:01, 1688.27it/s]warmup should be done:  33%|███▎      | 978/3000 [00:00<00:01, 1623.37it/s]warmup should be done:  33%|███▎      | 992/3000 [00:00<00:01, 1640.57it/s]warmup should be done:  33%|███▎      | 991/3000 [00:00<00:01, 1647.20it/s]warmup should be done:  33%|███▎      | 977/3000 [00:00<00:01, 1615.31it/s]warmup should be done:  34%|███▎      | 1010/3000 [00:00<00:01, 1667.36it/s]warmup should be done:  40%|███▉      | 1191/3000 [00:00<00:01, 1693.07it/s]warmup should be done:  40%|███▉      | 1186/3000 [00:00<00:01, 1691.61it/s]warmup should be done:  40%|███▉      | 1188/3000 [00:00<00:01, 1690.05it/s]warmup should be done:  38%|███▊      | 1142/3000 [00:00<00:01, 1626.62it/s]warmup should be done:  39%|███▊      | 1157/3000 [00:00<00:01, 1642.98it/s]warmup should be done:  39%|███▊      | 1158/3000 [00:00<00:01, 1653.61it/s]warmup should be done:  38%|███▊      | 1142/3000 [00:00<00:01, 1623.68it/s]warmup should be done:  39%|███▉      | 1179/3000 [00:00<00:01, 1672.86it/s]warmup should be done:  45%|████▌     | 1361/3000 [00:00<00:00, 1694.68it/s]warmup should be done:  45%|████▌     | 1357/3000 [00:00<00:00, 1695.56it/s]warmup should be done:  45%|████▌     | 1358/3000 [00:00<00:00, 1691.28it/s]warmup should be done:  44%|████▎     | 1305/3000 [00:00<00:01, 1622.09it/s]warmup should be done:  44%|████▍     | 1322/3000 [00:00<00:01, 1644.23it/s]warmup should be done:  44%|████▍     | 1325/3000 [00:00<00:01, 1656.21it/s]warmup should be done:  44%|████▎     | 1305/3000 [00:00<00:01, 1623.55it/s]warmup should be done:  45%|████▍     | 1348/3000 [00:00<00:00, 1677.51it/s]warmup should be done:  51%|█████     | 1527/3000 [00:00<00:00, 1696.77it/s]warmup should be done:  51%|█████     | 1531/3000 [00:00<00:00, 1694.51it/s]warmup should be done:  51%|█████     | 1528/3000 [00:00<00:00, 1690.93it/s]warmup should be done:  49%|████▉     | 1468/3000 [00:00<00:00, 1618.29it/s]warmup should be done:  50%|████▉     | 1489/3000 [00:00<00:00, 1649.51it/s]warmup should be done:  50%|████▉     | 1493/3000 [00:00<00:00, 1663.03it/s]warmup should be done:  49%|████▉     | 1470/3000 [00:00<00:00, 1628.97it/s]warmup should be done:  51%|█████     | 1517/3000 [00:00<00:00, 1680.30it/s]warmup should be done:  57%|█████▋    | 1698/3000 [00:01<00:00, 1698.41it/s]warmup should be done:  57%|█████▋    | 1698/3000 [00:01<00:00, 1691.23it/s]warmup should be done:  57%|█████▋    | 1701/3000 [00:01<00:00, 1684.72it/s]warmup should be done:  54%|█████▍    | 1634/3000 [00:01<00:00, 1629.31it/s]warmup should be done:  55%|█████▌    | 1656/3000 [00:01<00:00, 1653.85it/s]warmup should be done:  55%|█████▌    | 1662/3000 [00:01<00:00, 1668.51it/s]warmup should be done:  55%|█████▍    | 1635/3000 [00:01<00:00, 1633.38it/s]warmup should be done:  56%|█████▌    | 1686/3000 [00:01<00:00, 1674.47it/s]warmup should be done:  62%|██████▏   | 1869/3000 [00:01<00:00, 1700.07it/s]warmup should be done:  62%|██████▏   | 1868/3000 [00:01<00:00, 1692.63it/s]warmup should be done:  60%|█████▉    | 1799/3000 [00:01<00:00, 1634.77it/s]warmup should be done:  62%|██████▏   | 1870/3000 [00:01<00:00, 1679.53it/s]warmup should be done:  61%|██████    | 1823/3000 [00:01<00:00, 1656.14it/s]warmup should be done:  61%|██████    | 1831/3000 [00:01<00:00, 1672.51it/s]warmup should be done:  60%|█████▉    | 1799/3000 [00:01<00:00, 1634.31it/s]warmup should be done:  62%|██████▏   | 1854/3000 [00:01<00:00, 1673.19it/s]warmup should be done:  68%|██████▊   | 2040/3000 [00:01<00:00, 1700.74it/s]warmup should be done:  68%|██████▊   | 2038/3000 [00:01<00:00, 1693.47it/s]warmup should be done:  66%|██████▌   | 1965/3000 [00:01<00:00, 1640.36it/s]warmup should be done:  68%|██████▊   | 2038/3000 [00:01<00:00, 1676.21it/s]warmup should be done:  67%|██████▋   | 1999/3000 [00:01<00:00, 1673.83it/s]warmup should be done:  65%|██████▌   | 1963/3000 [00:01<00:00, 1633.94it/s]warmup should be done:  67%|██████▋   | 2023/3000 [00:01<00:00, 1677.27it/s]warmup should be done:  66%|██████▋   | 1989/3000 [00:01<00:00, 1558.66it/s]warmup should be done:  74%|███████▎  | 2211/3000 [00:01<00:00, 1699.03it/s]warmup should be done:  74%|███████▎  | 2208/3000 [00:01<00:00, 1692.55it/s]warmup should be done:  71%|███████   | 2131/3000 [00:01<00:00, 1645.01it/s]warmup should be done:  72%|███████▏  | 2167/3000 [00:01<00:00, 1673.69it/s]warmup should be done:  74%|███████▎  | 2206/3000 [00:01<00:00, 1671.67it/s]warmup should be done:  71%|███████   | 2128/3000 [00:01<00:00, 1636.55it/s]warmup should be done:  73%|███████▎  | 2192/3000 [00:01<00:00, 1678.78it/s]warmup should be done:  72%|███████▏  | 2147/3000 [00:01<00:00, 1440.10it/s]warmup should be done:  79%|███████▉  | 2378/3000 [00:01<00:00, 1692.41it/s]warmup should be done:  79%|███████▉  | 2382/3000 [00:01<00:00, 1699.30it/s]warmup should be done:  77%|███████▋  | 2296/3000 [00:01<00:00, 1644.56it/s]warmup should be done:  78%|███████▊  | 2336/3000 [00:01<00:00, 1676.83it/s]warmup should be done:  79%|███████▉  | 2374/3000 [00:01<00:00, 1669.89it/s]warmup should be done:  76%|███████▋  | 2293/3000 [00:01<00:00, 1638.34it/s]warmup should be done:  79%|███████▊  | 2362/3000 [00:01<00:00, 1682.89it/s]warmup should be done:  77%|███████▋  | 2316/3000 [00:01<00:00, 1507.87it/s]warmup should be done:  85%|████████▌ | 2553/3000 [00:01<00:00, 1699.67it/s]warmup should be done:  85%|████████▍ | 2548/3000 [00:01<00:00, 1691.61it/s]warmup should be done:  82%|████████▏ | 2461/3000 [00:01<00:00, 1639.39it/s]warmup should be done:  84%|████████▎ | 2505/3000 [00:01<00:00, 1679.26it/s]warmup should be done:  85%|████████▍ | 2542/3000 [00:01<00:00, 1670.77it/s]warmup should be done:  82%|████████▏ | 2457/3000 [00:01<00:00, 1637.65it/s]warmup should be done:  84%|████████▍ | 2531/3000 [00:01<00:00, 1684.83it/s]warmup should be done:  83%|████████▎ | 2485/3000 [00:01<00:00, 1558.83it/s]warmup should be done:  91%|█████████ | 2718/3000 [00:01<00:00, 1693.34it/s]warmup should be done:  91%|█████████ | 2724/3000 [00:01<00:00, 1700.77it/s]warmup should be done:  89%|████████▉ | 2674/3000 [00:01<00:00, 1681.12it/s]warmup should be done:  88%|████████▊ | 2625/3000 [00:01<00:00, 1632.84it/s]warmup should be done:  90%|█████████ | 2710/3000 [00:01<00:00, 1671.18it/s]warmup should be done:  87%|████████▋ | 2621/3000 [00:01<00:00, 1635.82it/s]warmup should be done:  90%|█████████ | 2700/3000 [00:01<00:00, 1685.28it/s]warmup should be done:  88%|████████▊ | 2653/3000 [00:01<00:00, 1593.54it/s]warmup should be done:  96%|█████████▋| 2888/3000 [00:01<00:00, 1692.30it/s]warmup should be done:  95%|█████████▍| 2844/3000 [00:01<00:00, 1685.84it/s]warmup should be done:  96%|█████████▋| 2895/3000 [00:01<00:00, 1684.04it/s]warmup should be done:  93%|█████████▎| 2789/3000 [00:01<00:00, 1630.96it/s]warmup should be done:  96%|█████████▌| 2878/3000 [00:01<00:00, 1668.46it/s]warmup should be done:  93%|█████████▎| 2786/3000 [00:01<00:00, 1638.43it/s]warmup should be done:  96%|█████████▌| 2869/3000 [00:01<00:00, 1684.22it/s]warmup should be done:  94%|█████████▍| 2822/3000 [00:01<00:00, 1620.45it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1692.29it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1691.36it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1681.01it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1671.88it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1665.61it/s]warmup should be done:  98%|█████████▊| 2953/3000 [00:01<00:00, 1631.49it/s]warmup should be done:  98%|█████████▊| 2951/3000 [00:01<00:00, 1641.38it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1629.79it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1628.63it/s]warmup should be done: 100%|█████████▉| 2992/3000 [00:01<00:00, 1641.39it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1609.69it/s]2022-12-11 21:43:03.893602: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f71b802f200 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 21:43:03.893668: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 21:43:04.968562: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f8d6b8309d0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 21:43:04.968628: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 21:43:04.972328: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f8d5ff92100 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 21:43:04.972395: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 21:43:05.181581: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f71500289c0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 21:43:05.181652: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 21:43:05.376536: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f71bc0300f0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 21:43:05.376607: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 21:43:05.389128: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f7204028a30 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 21:43:05.389202: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 21:43:05.395388: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f8d6f830500 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 21:43:05.395446: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 21:43:05.422878: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f71c402d1c0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 21:43:05.422955: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 21:43:06.110006: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 21:43:07.222657: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 21:43:07.317671: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 21:43:07.444722: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 21:43:07.661837: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 21:43:07.689721: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 21:43:07.707153: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 21:43:07.778279: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 21:43:08.950423: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 21:43:10.106432: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 21:43:10.225594: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 21:43:10.320281: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 21:43:10.545615: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 21:43:10.548468: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 21:43:10.608818: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 21:43:10.611376: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
[HCTR][21:43:49.724][ERROR][RK0][main]: replica 2 reaches 1000, calling init pre replica
[HCTR][21:43:49.724][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][21:43:49.724][ERROR][RK0][main]: replica 1 reaches 1000, calling init pre replica
[HCTR][21:43:49.724][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][21:43:49.730][ERROR][RK0][main]: coll ps creation done
[HCTR][21:43:49.730][ERROR][RK0][main]: replica 2 waits for coll ps creation barrier
[HCTR][21:43:49.730][ERROR][RK0][main]: coll ps creation done
[HCTR][21:43:49.730][ERROR][RK0][main]: replica 1 waits for coll ps creation barrier
[HCTR][21:43:49.741][ERROR][RK0][tid #140246134003456]: replica 0 reaches 1000, calling init pre replica
[HCTR][21:43:49.741][ERROR][RK0][tid #140246134003456]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][21:43:49.742][ERROR][RK0][main]: replica 3 reaches 1000, calling init pre replica
[HCTR][21:43:49.742][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][21:43:49.749][ERROR][RK0][tid #140246134003456]: coll ps creation done
[HCTR][21:43:49.749][ERROR][RK0][tid #140246134003456]: replica 0 waits for coll ps creation barrier
[HCTR][21:43:49.750][ERROR][RK0][main]: coll ps creation done
[HCTR][21:43:49.750][ERROR][RK0][main]: replica 3 waits for coll ps creation barrier
[HCTR][21:43:50.016][ERROR][RK0][main]: replica 4 reaches 1000, calling init pre replica
[HCTR][21:43:50.016][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][21:43:50.021][ERROR][RK0][main]: coll ps creation done
[HCTR][21:43:50.021][ERROR][RK0][main]: replica 4 waits for coll ps creation barrier
[HCTR][21:43:50.067][ERROR][RK0][tid #140245513271040]: replica 5 reaches 1000, calling init pre replica
[HCTR][21:43:50.067][ERROR][RK0][tid #140245513271040]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][21:43:50.072][ERROR][RK0][tid #140245513271040]: coll ps creation done
[HCTR][21:43:50.072][ERROR][RK0][tid #140245513271040]: replica 5 waits for coll ps creation barrier
[HCTR][21:43:50.109][ERROR][RK0][tid #140245513271040]: replica 7 reaches 1000, calling init pre replica
[HCTR][21:43:50.109][ERROR][RK0][tid #140245513271040]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][21:43:50.117][ERROR][RK0][tid #140245513271040]: coll ps creation done
[HCTR][21:43:50.117][ERROR][RK0][tid #140245513271040]: replica 7 waits for coll ps creation barrier
[HCTR][21:43:50.124][ERROR][RK0][main]: replica 6 reaches 1000, calling init pre replica
[HCTR][21:43:50.124][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][21:43:50.129][ERROR][RK0][main]: coll ps creation done
[HCTR][21:43:50.129][ERROR][RK0][main]: replica 6 waits for coll ps creation barrier
[HCTR][21:43:50.129][ERROR][RK0][tid #140246134003456]: replica 0 preparing frequency
[HCTR][21:43:50.987][ERROR][RK0][tid #140246134003456]: replica 0 preparing frequency done
[HCTR][21:43:51.025][ERROR][RK0][tid #140246134003456]: replica 0 calling init per replica
[HCTR][21:43:51.025][ERROR][RK0][tid #140245513271040]: replica 7 calling init per replica
[HCTR][21:43:51.025][ERROR][RK0][main]: replica 2 calling init per replica
[HCTR][21:43:51.025][ERROR][RK0][main]: replica 3 calling init per replica
[HCTR][21:43:51.025][ERROR][RK0][main]: Calling build_v2
[HCTR][21:43:51.025][ERROR][RK0][main]: replica 4 calling init per replica
[HCTR][21:43:51.025][ERROR][RK0][tid #140245513271040]: replica 5 calling init per replica
[HCTR][21:43:51.025][ERROR][RK0][main]: replica 6 calling init per replica
[HCTR][21:43:51.025][ERROR][RK0][main]: replica 1 calling init per replica
[HCTR][21:43:51.025][ERROR][RK0][tid #140246134003456]: Calling build_v2
[HCTR][21:43:51.025][ERROR][RK0][tid #140245513271040]: Calling build_v2
[HCTR][21:43:51.025][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][21:43:51.025][ERROR][RK0][main]: Calling build_v2
[HCTR][21:43:51.025][ERROR][RK0][main]: Calling build_v2
[HCTR][21:43:51.025][ERROR][RK0][tid #140245513271040]: Calling build_v2
[HCTR][21:43:51.025][ERROR][RK0][main]: Calling build_v2
[HCTR][21:43:51.025][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][21:43:51.025][ERROR][RK0][main]: Calling build_v2
[HCTR][21:43:51.025][ERROR][RK0][tid #140246134003456]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][21:43:51.025][ERROR][RK0][tid #140245513271040]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][21:43:51.025][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][21:43:51.025][ERROR][RK0][tid #140245513271040]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][21:43:51.025][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][21:43:51.025][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[[[2022-12-11 21:43:51.2022-12-11 21:43:512022-12-11 21:43:51 25457.[[[[[.:  254492022-12-11 21:43:512022-12-11 21:43:512022-12-11 21:43:51 25453E2022-12-11 21:43:512022-12-11 21:43:51: ...:  ..E 25479 25479 25477E/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc 25489 25475 : : :  :: : /hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.ccEEE/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc136EE:   :]   136/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc136using concurrent impl MPS/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc] :::] 
::using concurrent impl MPS136136136using concurrent impl MPS136136
] ] ] 
] ] using concurrent impl MPSusing concurrent impl MPSusing concurrent impl MPSusing concurrent impl MPSusing concurrent impl MPS




[2022-12-11 21:43:51. 29455: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:178] v100x8, slow pcie
[2022-12-11 21:43:51. 29493: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:196] assigning 8 to cpu
[2022-12-11 21:43:51. 29542: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:212] build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8
[2022-12-11 21:43:51. 29584: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:213] remote time is 8.68421
[2022-12-11 21:43:51. 29612: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:214] cpu time is 97.0588
[2022-12-11 21:43:51. 29734: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:178] v100x8, slow pcie
[2022-12-11 21:43:51. 29771: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[:2022-12-11 21:43:51196.]  29779assigning 8 to cpu: 
E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:178] v100x8, slow pcie
[2022-12-11 21:43:51. 29825: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:196[] 2022-12-11 21:43:51assigning 8 to cpu.
 29831[: 2022-12-11 21:43:51E.  29860: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE: [178/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-11 21:43:51[] :.2022-12-11 21:43:51v100x8, slow pcie212 29881.
] [:  29896build asymm link desc with 8X Tesla V100-SXM2-32GB out of 82022-12-11 21:43:51E[: 
. 2022-12-11 21:43:51E 29926/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[. [: :2022-12-11 21:43:51 29947/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-11 21:43:51E178.[: :E. ]  299732022-12-11 21:43:51212  29987/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccv100x8, slow pcie: .] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: :
E 30013build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8:E178 : [
196 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE2022-12-11 21:43:51] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccv100x8, slow pcie[: .:assigning 8 to cpu
2022-12-11 21:43:51178/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc 30101213
.] [:: ]  30132v100x8, slow pcie2022-12-11 21:43:51178Eremote time is 8.68421: 
.]  
E[ 30181v100x8, slow pcie/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[ 2022-12-11 21:43:51[: 
:2022-12-11 21:43:51/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.2022-12-11 21:43:51 30225E196.[:.:  ]  302382022-12-11 21:43:51213 30248E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccassigning 8 to cpu: .] :  :
E 30279remote time is 8.68421E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc196 : 
 :] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc212[assigning 8 to cpu: :[] 2022-12-11 21:43:51
196/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2142022-12-11 21:43:51build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8.] :] .
 30385assigning 8 to cpu196cpu time is 97.0588 30397: 
[] 
[: E2022-12-11 21:43:51assigning 8 to cpu2022-12-11 21:43:51E .
. /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc 30490 30495/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[:: : :2022-12-11 21:43:51214EE212.] [  ]  30561cpu time is 97.05882022-12-11 21:43:51/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccbuild asymm link desc with 8X Tesla V100-SXM2-32GB out of 8: 
.::
E 30606212213 : ] ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[Ebuild asymm link desc with 8X Tesla V100-SXM2-32GB out of 8remote time is 8.68421:2022-12-11 21:43:51 

212./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc]  30704:build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8[: 212[
2022-12-11 21:43:51E] 2022-12-11 21:43:51. build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8[. 30755/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
2022-12-11 21:43:51 30755: : :.[EE213 307882022-12-11 21:43:51  ] : ./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccremote time is 8.68421E 30826::
 : 213214/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE[] ] : 2022-12-11 21:43:51remote time is 8.68421cpu time is 97.0588213/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.

] : 30905[remote time is 8.68421213: 2022-12-11 21:43:51
] E.remote time is 8.68421  30960[
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: 2022-12-11 21:43:51:E.[214  309812022-12-11 21:43:51] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: .cpu time is 97.0588:E 30996
214 : ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccEcpu time is 97.0588: 
214/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] :cpu time is 97.0588214
] cpu time is 97.0588
[2022-12-11 21:45:07.800813: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:118] solver created. now build & solve
[2022-12-11 21:45:07.840968: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:123] solver built. now solve
[2022-12-11 21:45:07.841032: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:919] num_cached_nodes = 1999999
[2022-12-11 21:45:07.976492: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:127] solver solved
[2022-12-11 21:45:07.976586: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:195] 0 solved master
[2022-12-11 21:45:07.976618: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 0 solved
[2022-12-11 21:45:07.976649: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 0 initing device 0
[2022-12-11 21:45:07.977057: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 21:45:07.977927: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 21:45:07.978583: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 21:45:07.991840: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 3 solved
[2022-12-11 21:45:07.991905: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 3 initing device 3
[2022-12-11 21:45:07.992037: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 4 solved
[[2022-12-11 21:45:072022-12-11 21:45:07[..2022-12-11 21:45:07992092992111.: : 992106EE:   E/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc ::/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc202205:] ] 202worker 0 thread 4 initing device 4] 5 solved
1 solved

[[2022-12-11 21:45:072022-12-11 21:45:07..992198992200: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc::205205] ] [worker 0 thread 5 initing device 5worker 0 thread 1 initing device 12022-12-11 21:45:07[

.2022-12-11 21:45:07992225.: 992237E:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc :/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc202:] 2027 solved] 
6 solved
[[[2022-12-11 21:45:072022-12-11 21:45:072022-12-11 21:45:07...992312992311992318: : : EEE   /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:::2051980205] ] ] worker 0 thread 7 initing device 7eager alloc mem 381.47 MBworker 0 thread 6 initing device 6


[2022-12-11 21:45:07.992460: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 2 solved
[2022-12-11 21:45:07.992513: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 2 initing device 2
[2022-12-11 21:45:07.992559: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 21:45:07.992625[: 2022-12-11 21:45:07E. 992633/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu: :E1980 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cueager alloc mem 381.47 MB:
1980] eager alloc mem 381.47 MB
[2022-12-11 21:45:07.992734: E[ 2022-12-11 21:45:07/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu.:9927451980: ] Eeager alloc mem 381.47 MB 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 21:45:07.992891: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 21:45:07.997118: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 21:45:07.997262: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 21:45:07.997315: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 21:45:07.997381: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 21:45:07.997426: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 21:45:07.997481: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 21:45:07.997975: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 21:45:08.  1470: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 21:45:08.  1590: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 21:45:08.  1636: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 21:45:08.  1688: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 21:45:08.  1788: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 21:45:08.  1838: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 21:45:08.  1902: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 21:45:08. 56159: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 2.00 Bytes
[2022-12-11 21:45:08. 56547: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1024.00 Bytes
[2022-12-11 21:45:08. 62472: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-11 21:45:08. 62570: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 2
[2022-12-11 21:45:08. 62617: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-11 21:45:08. 63571: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 8.01 MB
[2022-12-11 21:45:08. 64325: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:45:08. 65311: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:45:08. 65401: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 21:45:08. 66080: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 21:45:08. 66123: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 976.56 MB
[[[[[[[2022-12-11 21:45:082022-12-11 21:45:082022-12-11 21:45:082022-12-11 21:45:082022-12-11 21:45:082022-12-11 21:45:082022-12-11 21:45:08....... 88116 88115 88115 88115 88115 88115 88115: : : : : : : EEEEEEE       /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:::::::1980198019801980198019801980] ] ] ] ] ] ] eager alloc mem 2.00 Byteseager alloc mem 2.00 Bytes
eager alloc mem 2.00 Byteseager alloc mem 2.00 Byteseager alloc mem 2.00 Byteseager alloc mem 2.00 Byteseager alloc mem 2.00 Bytes





[[[2022-12-11 21:45:082022-12-11 21:45:08[2022-12-11 21:45:08[[..2022-12-11 21:45:08.2022-12-11 21:45:082022-12-11 21:45:08 88613 88615. 88618..: :  88620:  88619[ 88621EE: E: 2022-12-11 21:45:08:   E E.E/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu  88647 ::/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu: /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu19801980:1980:E:] ] 1980] 1980 1980eager alloc mem 1024.00 Byteseager alloc mem 1024.00 Bytes] eager alloc mem 1024.00 Bytes] /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu] 

eager alloc mem 1024.00 Bytes
eager alloc mem 1024.00 Bytes:eager alloc mem 1024.00 Bytes

1980
] eager alloc mem 1024.00 Bytes
[2022-12-11 21:45:08.105084: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[[2022-12-11 21:45:082022-12-11 21:45:08..105150105172: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::638638] ] eager release cuda mem 1024eager release cuda mem 2

[[2022-12-11 21:45:082022-12-11 21:45:08..105243105245: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::638638] ] eager release cuda mem 2eager release cuda mem 400000000

[2022-12-11 21:45:08.105277: E [/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc2022-12-11 21:45:08:.638105309] : eager release cuda mem 1024E
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[[2022-12-11 21:45:082022-12-11 21:45:08..105367105357: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::638638] ] eager release cuda mem 2eager release cuda mem 1024

[2022-12-11 21:45:08[.2022-12-11 21:45:08105440.: 105443E:  E[/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc 2022-12-11 21:45:08:/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc.638:105446] 638: eager release cuda mem 400000000] E
eager release cuda mem 2 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-11 21:45:08.105525: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc[:2022-12-11 21:45:08638.] 105538[eager release cuda mem 400000000: 2022-12-11 21:45:08
E. 105537/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc: :E638 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cceager release cuda mem 2:
638] eager release cuda mem 1024
[2022-12-11 21:45:08.105615[: 2022-12-11 21:45:08E. 105621/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc: :E638 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cceager release cuda mem 400000000:
638] eager release cuda mem 2
[[2022-12-11 21:45:082022-12-11 21:45:08..105682105665: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::638638] ] eager release cuda mem 400000000eager release cuda mem 1024

[2022-12-11 21:45:08.105763: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 2
[2022-12-11 21:45:08.105811: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-11 21:45:08.106449: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 8.01 MB
[2022-12-11 21:45:08.107204: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 8.01 MB
[2022-12-11 21:45:08.112636: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 8.01 MB
[2022-12-11 21:45:08.113303: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 8.01 MB
[2022-12-11 21:45:08.113874: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 8.01 MB
[2022-12-11 21:45:08.114424: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 8.01 MB
[2022-12-11 21:45:08.115048: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 8.01 MB
[2022-12-11 21:45:08.120770: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:45:08.120946: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:45:08.121376: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:45:08.121473: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:45:08.121573: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:45:08.121619: E [/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu2022-12-11 21:45:08:.1980121641] : eager alloc mem 611.00 KBE
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:45:08.121738: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 21:45:08.121749: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:45:08.121841: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:45:08.121923: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 21:45:08.122292: E[ 2022-12-11 21:45:08/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc.:122302638: ] Eeager release cuda mem 25855 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:45:08.122346: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 976.56 MB[
2022-12-11 21:45:08.122366: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:45:08.122396: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 21:45:08.122448: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 21:45:08.122475: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 21:45:08[.2022-12-11 21:45:08122509.: 122516E:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc :/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu638:] 1980eager release cuda mem 625663] 
eager alloc mem 976.56 MB
[2022-12-11 21:45:08.122579: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:45:08.122613: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 21:45:08.122665: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 21:45:08.122747: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:45:08.122850: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 21:45:08.123074: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 21:45:08.123117: E[ 2022-12-11 21:45:08/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu.:1231221980: ] Eeager alloc mem 976.56 MB 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 21:45:08.123191: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 976.56 MB
[2022-12-11 21:45:08.123289: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 21:45:08.123332[: 2022-12-11 21:45:08E. 123336/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu: :E1980 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cceager alloc mem 976.56 MB:
638] eager release cuda mem 25855
[2022-12-11 21:45:08.123396: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 976.56 MB
[2022-12-11 21:45:08.123527: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 21:45:08.123572: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 976.56 MB
[[[[[[[[2022-12-11 21:45:082022-12-11 21:45:082022-12-11 21:45:082022-12-11 21:45:082022-12-11 21:45:082022-12-11 21:45:082022-12-11 21:45:082022-12-11 21:45:08........311375311375311382311383311376311375311392311394: : : : : : : : EEEEEEEE        /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::::::::19801980198019801980198019801980] ] ] ] ] ] ] ] eager alloc mem 611.00 KBeager alloc mem 611.00 KBeager alloc mem 611.00 KBeager alloc mem 611.00 KBeager alloc mem 611.00 KBeager alloc mem 611.00 KBeager alloc mem 611.00 KBeager alloc mem 611.00 KB







[2022-12-11 21:45:08.312410[: 2022-12-11 21:45:08E. [312419/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc2022-12-11 21:45:08[: :.2022-12-11 21:45:08[E638312428.[2022-12-11 21:45:08 ] [: 312435[2022-12-11 21:45:08./hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cceager release cuda mem 6256632022-12-11 21:45:08E: 2022-12-11 21:45:08.312444:
. E.312450: 638312456/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc 312461: E] : :/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc: E eager release cuda mem 625663E638:E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc
 ] 638 /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:[/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cceager release cuda mem 625663] /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:6382022-12-11 21:45:08:
eager release cuda mem 625663:638] .638
638] eager release cuda mem 625663312554] ] eager release cuda mem 625663[
: eager release cuda mem 625663eager release cuda mem 625663
2022-12-11 21:45:08E

. 312605[/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu: 2022-12-11 21:45:08:E.1980 312636[] /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu: 2022-12-11 21:45:08eager alloc mem 611.00 KB:E[.
1980[ 2022-12-11 21:45:08312651] 2022-12-11 21:45:08/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu[.[: eager alloc mem 611.00 KB.:2022-12-11 21:45:083126652022-12-11 21:45:08E
3126781980.: . : ] 312684E312684/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuEeager alloc mem 611.00 KB:  : : 
E/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE1980/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu : ] :/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu1980/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cueager alloc mem 611.00 KB1980:] :
] 1980eager alloc mem 611.00 KB1980eager alloc mem 611.00 KB] 
] 
eager alloc mem 611.00 KBeager alloc mem 611.00 KB

[2022-12-11 21:45:08.313402: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:45:08.313447: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:45:08.313475: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:45:08.313503: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc[:2022-12-11 21:45:08638.] 313519eager release cuda mem 625663: 
E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] [eager alloc mem 611.00 KB2022-12-11 21:45:08
.313545: [E2022-12-11 21:45:08[ [.2022-12-11 21:45:08[/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc[2022-12-11 21:45:08313560.2022-12-11 21:45:08:2022-12-11 21:45:08.: 313566.638.313566E: 313570] 313583:  E: eager release cuda mem 625663: E/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc E
E :/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc638:/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:] 638::638eager release cuda mem 625663] 6381980] 
eager release cuda mem 625663] ] eager release cuda mem 625663[
eager release cuda mem 625663eager alloc mem 611.00 KB
2022-12-11 21:45:08

.313684: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:45:08.313728: E [/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu2022-12-11 21:45:08[[:.2022-12-11 21:45:082022-12-11 21:45:081980313740..] : 313744313750eager alloc mem 611.00 KBE: : 
 EE/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu  :/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu1980::] 19801980eager alloc mem 611.00 KB] ] 
eager alloc mem 611.00 KBeager alloc mem 611.00 KB

[2022-12-11 21:45:08.314227: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:45:08.314283: E [/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc2022-12-11 21:45:08:.638314296] : eager release cuda mem 625663E
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:45:08.314361: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:45:08.314451: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:45:08.314479: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:45:08.314520: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[[2022-12-11 21:45:082022-12-11 21:45:08..314549314549: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::1980[638] [2022-12-11 21:45:08] eager alloc mem 611.00 KB[2022-12-11 21:45:08.eager release cuda mem 625663
2022-12-11 21:45:08.314583
.314585: 314592: E: E E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638:638] 638] eager release cuda mem 625663[] eager release cuda mem 625663
2022-12-11 21:45:08eager release cuda mem 625663
.
314661: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:45:08[.2022-12-11 21:45:08[314717.2022-12-11 21:45:08: 314721.E: 314726 E: /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu E:/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu 1980:/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu] 1980:eager alloc mem 611.00 KB] 1980
eager alloc mem 611.00 KB] 
eager alloc mem 611.00 KB
[2022-12-11 21:45:08.315064: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:45:08.315107: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663[
2022-12-11 21:45:08.315143: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:45:08.315193: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:45:08.315285: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:45:08.315354: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:[19802022-12-11 21:45:08] .eager alloc mem 611.00 KB315367
: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:45:08.315444: E [/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu2022-12-11 21:45:08:.1980315454] : eager alloc mem 611.00 KBE
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:45:08.315534: [E2022-12-11 21:45:08[ .[2022-12-11 21:45:08/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu3155412022-12-11 21:45:08.:: .3155431980E315550: ]  : Eeager alloc mem 611.00 KB/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccE 
: /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc638/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:] :638eager release cuda mem 625663638] 
] eager release cuda mem 625663eager release cuda mem 625663

[2022-12-11 21:45:08.315683: E[ 2022-12-11 21:45:08[/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu.2022-12-11 21:45:08:315690.1980: 315697] E: eager alloc mem 611.00 KB E
/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu :/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu1980:] 1980eager alloc mem 611.00 KB] 
eager alloc mem 611.00 KB
[2022-12-11 21:45:08.315899: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:45:08.315945: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] [eager release cuda mem 6256632022-12-11 21:45:08
.315965: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:45:08.316014: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:45:08.316127: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:45:08.316196: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:45:08.316234: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:45:08.316301: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:45:08.316356: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:45:08.316423: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:45:08.316490: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:45:08.316510: E[ 2022-12-11 21:45:08/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc.:316525638: ] Eeager release cuda mem 625663 
[/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc2022-12-11 21:45:08:.638316558] : eager release cuda mem 625663E
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:45:08.316601: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB[
2022-12-11 21:45:08.316632: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:45:08.316720: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:45:08.316758: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:45:08.316789: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:45:08.316823: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:45:08.316960: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:45:08.317024: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:45:08.317084: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:45:08.317150: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:45:08.317200: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:45:08.317266: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:45:08.317350: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:45:08.317387: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] [eager release cuda mem 625663[2022-12-11 21:45:08
2022-12-11 21:45:08..317416317416: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::1980638] ] eager alloc mem 611.00 KBeager release cuda mem 625663

[2022-12-11 21:45:08.317470: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:45:08.317515: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980[] 2022-12-11 21:45:08eager alloc mem 611.00 KB.
317532: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:45:08[.2022-12-11 21:45:08317569.: 317575E:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc :/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc638:] 638eager release cuda mem 625663] 
eager release cuda mem 8399996
[2022-12-11 21:45:08.317627: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 8399996
[2022-12-11 21:45:08.317794: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:45:08.317830: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 8399996
[2022-12-11 21:45:08.317927: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:45:08.317963: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 8399996
[2022-12-11 21:45:08.318043: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:45:08.318084: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 8399996
[2022-12-11 21:45:08.318224: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:45:08.318248: E[ 2022-12-11 21:45:08/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc.:318261638: ] Eeager release cuda mem 625663 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 8399996
[[2022-12-11 21:45:082022-12-11 21:45:08..318301318302: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::638638] ] eager release cuda mem 8399996eager release cuda mem 625663

[2022-12-11 21:45:08.318364: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 8399996
[2022-12-11 21:45:08.321208: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 1999999 / 100000000 nodes ( 2.00 %~2.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 98000001 / 100000000 nodes ( 98.00 %) | 976.56 MB | 0.328653 secs 
[2022-12-11 21:45:08.321285: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 1999999 / 100000000 nodes ( 2.00 %~2.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 98000001 / 100000000 nodes ( 98.00 %) | 976.56 MB | 0.328981 secs 
[2022-12-11 21:45:08.321363: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 1999999 / 100000000 nodes ( 2.00 %~2.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 98000001 / 100000000 nodes ( 98.00 %) | 976.56 MB | 0.328624 secs 
[2022-12-11 21:45:08.321449: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 1999999 / 100000000 nodes ( 2.00 %~2.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 98000001 / 100000000 nodes ( 98.00 %) | 976.56 MB | 0.32883 secs 
[2022-12-11 21:45:08.321526: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 1999999 / 100000000 nodes ( 2.00 %~2.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 98000001 / 100000000 nodes ( 98.00 %) | 976.56 MB | 0.328799 secs 
[2022-12-11 21:45:08.321613: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 1999999 / 100000000 nodes ( 2.00 %~2.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 98000001 / 100000000 nodes ( 98.00 %) | 976.56 MB | 0.328986 secs 
[2022-12-11 21:45:08.322039: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 1999999 / 100000000 nodes ( 2.00 %~2.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 98000001 / 100000000 nodes ( 98.00 %) | 976.56 MB | 0.344988 secs 
[2022-12-11 21:45:08.322143: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 1999999 / 100000000 nodes ( 2.00 %~2.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 98000001 / 100000000 nodes ( 98.00 %) | 976.56 MB | 0.329258 secs 
[2022-12-11 21:45:08.322574: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 7.59 GB
[2022-12-11 21:45:10.231841: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 7.86 GB
[2022-12-11 21:45:10.232179: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 7.86 GB
[2022-12-11 21:45:10.233175: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 7.86 GB
[2022-12-11 21:45:12. 25520: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 8.12 GB
[2022-12-11 21:45:12. 28124: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 8.12 GB
[2022-12-11 21:45:12. 31844: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 8.12 GB
[2022-12-11 21:45:13.786318: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 8.33 GB
[2022-12-11 21:45:13.786815: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 8.33 GB
[2022-12-11 21:45:13.787436: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 8.33 GB
[2022-12-11 21:45:15.929660: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 8.55 GB
[2022-12-11 21:45:15.930910: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 8.55 GB
[2022-12-11 21:45:15.932378: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 8.55 GB
[2022-12-11 21:45:18.181469: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 9.01 GB
[2022-12-11 21:45:18.181940: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 9.01 GB
[2022-12-11 21:45:18.184851: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 9.01 GB
[2022-12-11 21:45:19.975180: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 9.20 GB
[2022-12-11 21:45:19.975350: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 9.20 GB
[HCTR][21:45:20.012][ERROR][RK0][tid #140245513271040]: replica 5 calling init per replica done, doing barrier
[HCTR][21:45:20.012][ERROR][RK0][main]: replica 6 calling init per replica done, doing barrier
[HCTR][21:45:20.012][ERROR][RK0][tid #140245513271040]: replica 7 calling init per replica done, doing barrier
[HCTR][21:45:20.012][ERROR][RK0][main]: replica 4 calling init per replica done, doing barrier
[HCTR][21:45:20.012][ERROR][RK0][tid #140246134003456]: replica 0 calling init per replica done, doing barrier
[HCTR][21:45:20.012][ERROR][RK0][main]: replica 1 calling init per replica done, doing barrier
[HCTR][21:45:20.012][ERROR][RK0][main]: replica 3 calling init per replica done, doing barrier
[HCTR][21:45:20.012][ERROR][RK0][main]: replica 2 calling init per replica done, doing barrier
[HCTR][21:45:20.012][ERROR][RK0][main]: replica 1 calling init per replica done, doing barrier done
[HCTR][21:45:20.012][ERROR][RK0][main]: replica 6 calling init per replica done, doing barrier done
[HCTR][21:45:20.012][ERROR][RK0][tid #140245513271040]: replica 5 calling init per replica done, doing barrier done
[HCTR][21:45:20.012][ERROR][RK0][main]: replica 2 calling init per replica done, doing barrier done
[HCTR][21:45:20.012][ERROR][RK0][tid #140245513271040]: replica 7 calling init per replica done, doing barrier done
[HCTR][21:45:20.012][ERROR][RK0][main]: replica 4 calling init per replica done, doing barrier done
[HCTR][21:45:20.012][ERROR][RK0][main]: replica 3 calling init per replica done, doing barrier done
[HCTR][21:45:20.012][ERROR][RK0][tid #140246134003456]: replica 0 calling init per replica done, doing barrier done
[HCTR][21:45:20.012][ERROR][RK0][main]: init per replica done
[HCTR][21:45:20.012][ERROR][RK0][main]: init per replica done
[HCTR][21:45:20.012][ERROR][RK0][tid #140245513271040]: init per replica done
[HCTR][21:45:20.012][ERROR][RK0][main]: init per replica done
[HCTR][21:45:20.012][ERROR][RK0][tid #140245513271040]: init per replica done
[HCTR][21:45:20.012][ERROR][RK0][main]: init per replica done
[HCTR][21:45:20.012][ERROR][RK0][main]: init per replica done
[HCTR][21:45:20.015][ERROR][RK0][tid #140246134003456]: init per replica done
[HCTR][21:45:20.018][ERROR][RK0][main]: 3 allocated 3276800 at 0x7f8f59f20000
[HCTR][21:45:20.019][ERROR][RK0][main]: 3 allocated 6553600 at 0x7f8f5a400000
[HCTR][21:45:20.019][ERROR][RK0][main]: 3 allocated 3276800 at 0x7f8f5aa40000
[HCTR][21:45:20.019][ERROR][RK0][main]: 3 allocated 6553600 at 0x7f8f5ad60000
[HCTR][21:45:20.019][ERROR][RK0][main]: 7 allocated 3276800 at 0x7f8f5bf20000
[HCTR][21:45:20.019][ERROR][RK0][main]: 7 allocated 6553600 at 0x7f8f5c400000
[HCTR][21:45:20.019][ERROR][RK0][main]: 7 allocated 3276800 at 0x7f8f5ca40000
[HCTR][21:45:20.019][ERROR][RK0][main]: 7 allocated 6553600 at 0x7f8f5cd60000
[HCTR][21:45:20.019][ERROR][RK0][tid #140245446162176]: 4 allocated 3276800 at 0x7f8f5bf20000
[HCTR][21:45:20.019][ERROR][RK0][tid #140245446162176]: 4 allocated 6553600 at 0x7f8f5c400000
[HCTR][21:45:20.019][ERROR][RK0][tid #140245446162176]: 4 allocated 3276800 at 0x7f8f5ca40000
[HCTR][21:45:20.019][ERROR][RK0][tid #140245446162176]: 4 allocated 6553600 at 0x7f8f5cd60000
[HCTR][21:45:20.019][ERROR][RK0][tid #140245513271040]: 6 allocated 3276800 at 0x7f8f59f20000
[HCTR][21:45:20.019][ERROR][RK0][tid #140245513271040]: 6 allocated 6553600 at 0x7f8f5a400000
[HCTR][21:45:20.019][ERROR][RK0][tid #140245513271040]: 6 allocated 3276800 at 0x7f8f5aa40000
[HCTR][21:45:20.019][ERROR][RK0][tid #140245513271040]: 6 allocated 6553600 at 0x7f8f5ad60000
[HCTR][21:45:20.019][ERROR][RK0][tid #140245630703360]: 2 allocated 3276800 at 0x7f8f57f20000
[HCTR][21:45:20.019][ERROR][RK0][tid #140245974640384]: 1 allocated 3276800 at 0x7f8f5bf20000
[HCTR][21:45:20.019][ERROR][RK0][tid #140245630703360]: 2 allocated 6553600 at 0x7f8f58400000
[HCTR][21:45:20.019][ERROR][RK0][tid #140245974640384]: 1 allocated 6553600 at 0x7f8f5c400000
[HCTR][21:45:20.019][ERROR][RK0][tid #140245630703360]: 2 allocated 3276800 at 0x7f8f58a40000
[HCTR][21:45:20.019][ERROR][RK0][tid #140245974640384]: 1 allocated 3276800 at 0x7f8f5ca40000
[HCTR][21:45:20.019][ERROR][RK0][tid #140245630703360]: 2 allocated 6553600 at 0x7f8f58d60000
[HCTR][21:45:20.019][ERROR][RK0][tid #140245974640384]: 1 allocated 6553600 at 0x7f8f5cd60000
[HCTR][21:45:20.019][ERROR][RK0][main]: 5 allocated 3276800 at 0x7f8f59f20000
[HCTR][21:45:20.019][ERROR][RK0][main]: 5 allocated 6553600 at 0x7f8f5a400000
[HCTR][21:45:20.019][ERROR][RK0][main]: 5 allocated 3276800 at 0x7f8f5aa40000
[HCTR][21:45:20.019][ERROR][RK0][main]: 5 allocated 6553600 at 0x7f8f5ad60000
[HCTR][21:45:20.021][ERROR][RK0][main]: 0 allocated 3276800 at 0x7f8f5c520000
[HCTR][21:45:20.021][ERROR][RK0][main]: 0 allocated 6553600 at 0x7f8f5ca00000
[HCTR][21:45:20.021][ERROR][RK0][main]: 0 allocated 3276800 at 0x7f8f5d70e800
[HCTR][21:45:20.021][ERROR][RK0][main]: 0 allocated 6553600 at 0x7f8f5da2e800








