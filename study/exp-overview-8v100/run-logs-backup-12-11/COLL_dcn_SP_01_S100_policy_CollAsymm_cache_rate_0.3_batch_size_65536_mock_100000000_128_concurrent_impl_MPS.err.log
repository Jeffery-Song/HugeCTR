2022-12-12 08:17:01.257968: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.265042: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.272895: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.277360: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.282267: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.294462: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.303218: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.316316: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.366562: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.374403: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.377490: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.378533: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.379542: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.380530: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.381593: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.382891: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.384794: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.385860: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.385976: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.387547: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.387571: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.389225: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.389288: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.390683: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.390887: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.392066: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.392512: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.393733: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.394081: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.395519: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.395961: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.397510: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.397760: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.399252: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.400179: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.401118: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.402160: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.403226: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.404238: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.405186: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.410442: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 08:17:01.414462: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.415600: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.416243: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.417732: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.418485: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.419696: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.419709: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.419733: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.420516: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.422494: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.422607: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.422686: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.423303: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.425399: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.425788: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.425842: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.426354: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.429044: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.429090: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.429638: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.430359: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.432384: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.432430: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.432945: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.433884: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.435623: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.435724: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.437329: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.438954: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.440344: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.441599: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.442129: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.442908: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.443160: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.445674: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.445850: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.446084: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.446167: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.448430: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.448737: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.448844: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.448872: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.451206: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.451417: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.451633: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.451817: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.453889: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.453922: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.454226: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.457237: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.457450: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.457677: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.459221: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.459655: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.459827: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.461211: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.462005: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.480556: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.482222: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.488087: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.495681: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.497325: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.497489: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.499112: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.499252: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.499712: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.500137: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.501570: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.501670: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.503479: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.503563: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.503764: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.506207: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.506252: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.508391: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.508530: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.508676: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.510194: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.510329: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.511630: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.511759: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.511926: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.513464: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.514219: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.515360: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.515444: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.515587: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.517392: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.517769: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.519115: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.519215: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.519321: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.521038: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.521151: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.522550: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.522630: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.522775: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.524577: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.524625: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.529627: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.529708: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.529754: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.531828: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.532495: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.532716: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.532806: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.532949: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.535067: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.536115: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.536239: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.536322: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.536624: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.538396: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.539528: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.539596: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.539680: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.539870: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.541848: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.543025: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.543036: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.543143: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.543241: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.543842: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.545306: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.546744: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.546800: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.547002: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.547048: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.547922: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.548294: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.549384: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.551117: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.551181: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.551520: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.551566: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.552590: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.552961: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.554555: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.556188: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.556242: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.556464: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.556506: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.557731: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.558229: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.559334: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.561355: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.562290: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.562350: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.562614: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.562947: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.563080: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.565450: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.566664: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.566701: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.566792: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.567404: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.567574: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.569509: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.569839: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 08:17:01.570811: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.570894: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.570967: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.571344: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.571560: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.573840: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.574848: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.574935: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.574962: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.575491: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.575735: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.577741: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.578663: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.578872: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.579010: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.579042: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.579565: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.579937: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.582636: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.583707: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.583811: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.583867: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.583948: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.584375: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.589065: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.591603: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.592727: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.592785: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.592820: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.592901: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.593299: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.593713: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.596161: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.597488: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.597572: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.597655: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.597930: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.598327: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.600911: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.602273: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.602589: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.605695: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.605820: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 08:17:01.605915: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.606034: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 08:17:01.606195: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 08:17:01.607981: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 08:17:01.607993: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.608265: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.610865: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.611076: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.613500: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.614022: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.615185: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.615436: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.615536: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.616953: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.617025: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.617601: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.619781: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.619880: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.620093: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.622126: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.622134: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.622445: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.624854: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.625089: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.625145: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.626677: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.626904: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.627338: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.631568: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.631964: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.667479: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.668161: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.673383: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.673751: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.707956: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.708405: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.715185: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.717199: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 08:17:01.724167: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 08:17:01.725800: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.730258: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.732770: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.734735: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.736932: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:01.757291: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:02.735109: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:02.735764: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:02.736402: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:02.736874: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 08:17:02.736936: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-12 08:17:02.755490: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:02.756376: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:02.756894: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:02.757499: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:02.758145: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:02.758627: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-12 08:17:02.805570: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 08:17:02.805775: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 08:17:02.843784: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12340
2022-12-12 08:17:02.987047: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:02.987696: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:02.988334: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:02.988937: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 08:17:02.988995: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-12 08:17:03.006955: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:03.007998: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:03.008914: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:03.009533: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:03.010478: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:03.011154: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:1/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-12 08:17:03.043670: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:03.044297: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:03.045124: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:03.045598: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 08:17:03.045653: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-12 08:17:03.046603: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:03.047303: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:03.047841: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:03.048310: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 08:17:03.048364: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-12 08:17:03.063962: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:03.064609: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:03.065130: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:03.065706: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:03.066241: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:03.066258: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:03.066758: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:03.067479: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:2/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-12 08:17:03.067772: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:03.067917: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:03.068248: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:03.069471: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:03.069656: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:03.070107: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:03.071159: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:03.071288: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:03.071385: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 08:17:03.071444: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-12 08:17:03.072389: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:03.072409: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 08:17:03.072461: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-12 08:17:03.073042: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:5/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-12 08:17:03.084319: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:03.084924: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:03.085477: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:03.085944: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 08:17:03.085999: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-12 08:17:03.090311: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:03.090906: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:03.091449: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:03.091491: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:03.092673: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:03.092771: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:03.093618: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 08:17:03.093670: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:03.093758: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 08:17:03.093793: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:03.094626: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:6/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-12 08:17:03.094862: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:03.095406: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:03.096102: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:4/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-12 08:17:03.096430: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12341
2022-12-12 08:17:03.104528: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:03.105183: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:03.105699: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:03.106289: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:03.106802: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:03.107287: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:7/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-12 08:17:03.119368: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 08:17:03.119552: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 08:17:03.125348: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12345
2022-12-12 08:17:03.138718: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:03.139399: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:03.139721: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 08:17:03.139816: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 08:17:03.139920: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 08:17:03.139959: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 08:17:03.140005: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:03.140515: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 08:17:03.140575: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-12 08:17:03.141751: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12346
2022-12-12 08:17:03.141989: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 08:17:03.142104: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12342
2022-12-12 08:17:03.142145: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 08:17:03.143993: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12344
2022-12-12 08:17:03.153730: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 08:17:03.153892: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 08:17:03.155797: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12347
2022-12-12 08:17:03.158638: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:03.159292: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:03.159797: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:03.160373: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:03.160884: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:17:03.161365: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:3/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-12 08:17:03.205730: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 08:17:03.205930: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 08:17:03.207639: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12343
[HCTR][08:17:04.474][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][08:17:04.474][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][08:17:04.474][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][08:17:04.475][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][08:17:04.475][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][08:17:04.475][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][08:17:04.518][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][08:17:04.518][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 1it [00:01,  1.56s/it]warmup run: 1it [00:01,  1.53s/it]warmup run: 87it [00:01, 72.86it/s]warmup run: 1it [00:01,  1.52s/it]warmup run: 97it [00:01, 82.31it/s]warmup run: 180it [00:01, 164.63it/s]warmup run: 1it [00:01,  1.51s/it]warmup run: 1it [00:01,  1.50s/it]warmup run: 101it [00:01, 86.45it/s]warmup run: 192it [00:01, 176.24it/s]warmup run: 275it [00:01, 268.82it/s]warmup run: 103it [00:01, 88.98it/s]warmup run: 96it [00:01, 82.69it/s]warmup run: 201it [00:01, 186.06it/s]warmup run: 1it [00:01,  1.48s/it]warmup run: 1it [00:01,  1.50s/it]warmup run: 287it [00:01, 279.85it/s]warmup run: 372it [00:01, 380.38it/s]warmup run: 191it [00:01, 177.83it/s]warmup run: 205it [00:01, 191.32it/s]warmup run: 302it [00:01, 296.89it/s]warmup run: 1it [00:01,  1.51s/it]warmup run: 99it [00:01, 85.98it/s]warmup run: 101it [00:01, 88.37it/s]warmup run: 382it [00:01, 386.94it/s]warmup run: 473it [00:02, 497.05it/s]warmup run: 286it [00:01, 282.40it/s]warmup run: 307it [00:01, 303.76it/s]warmup run: 402it [00:01, 410.31it/s]warmup run: 99it [00:01, 85.23it/s]warmup run: 197it [00:01, 184.90it/s]warmup run: 201it [00:01, 189.75it/s]warmup run: 475it [00:02, 487.59it/s]warmup run: 578it [00:02, 612.38it/s]warmup run: 382it [00:01, 392.09it/s]warmup run: 406it [00:01, 414.51it/s]warmup run: 498it [00:02, 513.18it/s]warmup run: 178it [00:01, 162.58it/s]warmup run: 292it [00:01, 288.94it/s]warmup run: 301it [00:01, 300.80it/s]warmup run: 566it [00:02, 575.06it/s]warmup run: 682it [00:02, 709.28it/s]warmup run: 478it [00:02, 498.53it/s]warmup run: 500it [00:02, 512.83it/s]warmup run: 595it [00:02, 609.27it/s]warmup run: 262it [00:01, 254.39it/s]warmup run: 387it [00:01, 396.55it/s]warmup run: 402it [00:01, 416.37it/s]warmup run: 655it [00:02, 636.67it/s]warmup run: 784it [00:02, 785.66it/s]warmup run: 575it [00:02, 598.06it/s]warmup run: 596it [00:02, 607.11it/s]warmup run: 692it [00:02, 691.99it/s]warmup run: 358it [00:01, 368.85it/s]warmup run: 483it [00:01, 502.65it/s]warmup run: 503it [00:01, 527.96it/s]warmup run: 742it [00:02, 686.95it/s]warmup run: 886it [00:02, 846.17it/s]warmup run: 674it [00:02, 687.99it/s]warmup run: 693it [00:02, 690.05it/s]warmup run: 790it [00:02, 761.70it/s]warmup run: 457it [00:02, 485.12it/s]warmup run: 580it [00:02, 601.28it/s]warmup run: 605it [00:02, 632.26it/s]warmup run: 837it [00:02, 754.08it/s]warmup run: 987it [00:02, 890.42it/s]warmup run: 773it [00:02, 761.74it/s]warmup run: 789it [00:02, 756.44it/s]warmup run: 886it [00:02, 812.09it/s]warmup run: 557it [00:02, 593.81it/s]warmup run: 676it [00:02, 684.21it/s]warmup run: 708it [00:02, 724.14it/s]warmup run: 933it [00:02, 807.97it/s]warmup run: 1089it [00:02, 925.09it/s]warmup run: 872it [00:02, 819.83it/s]warmup run: 884it [00:02, 801.66it/s]warmup run: 982it [00:02, 851.14it/s]warmup run: 655it [00:02, 682.32it/s]warmup run: 772it [00:02, 751.04it/s]warmup run: 811it [00:02, 798.42it/s]warmup run: 1031it [00:02, 853.70it/s]warmup run: 1192it [00:02, 953.15it/s]warmup run: 971it [00:02, 864.66it/s]warmup run: 1078it [00:02, 876.88it/s]warmup run: 979it [00:02, 830.77it/s]warmup run: 750it [00:02, 748.48it/s]warmup run: 867it [00:02, 801.13it/s]warmup run: 912it [00:02, 853.55it/s]warmup run: 1128it [00:02, 886.34it/s]warmup run: 1294it [00:02, 947.99it/s]warmup run: 1068it [00:02, 889.53it/s]warmup run: 1174it [00:02, 900.34it/s]warmup run: 1076it [00:02, 868.12it/s]warmup run: 845it [00:02, 800.60it/s]warmup run: 1014it [00:02, 898.62it/s]warmup run: 961it [00:02, 832.36it/s]warmup run: 1226it [00:02, 911.21it/s]warmup run: 1393it [00:02, 957.76it/s]warmup run: 1165it [00:02, 905.37it/s]warmup run: 1272it [00:02, 921.78it/s]warmup run: 1178it [00:02, 908.67it/s]warmup run: 942it [00:02, 845.48it/s]warmup run: 1116it [00:02, 930.29it/s]warmup run: 1055it [00:02, 857.88it/s]warmup run: 1325it [00:02, 933.30it/s]warmup run: 1493it [00:03, 969.08it/s]warmup run: 1262it [00:02, 922.63it/s]warmup run: 1372it [00:02, 942.71it/s]warmup run: 1277it [00:02, 929.88it/s]warmup run: 1042it [00:02, 887.59it/s]warmup run: 1218it [00:02, 955.06it/s]warmup run: 1155it [00:02, 898.11it/s]warmup run: 1425it [00:03, 950.37it/s]warmup run: 1592it [00:03, 968.57it/s]warmup run: 1360it [00:02, 937.70it/s]warmup run: 1475it [00:03, 967.38it/s]warmup run: 1378it [00:02, 951.87it/s]warmup run: 1144it [00:02, 923.96it/s]warmup run: 1321it [00:02, 974.15it/s]warmup run: 1253it [00:02, 919.53it/s]warmup run: 1525it [00:03, 964.47it/s]warmup run: 1691it [00:03, 970.84it/s]warmup run: 1458it [00:03, 948.63it/s]warmup run: 1578it [00:03, 983.38it/s]warmup run: 1479it [00:03, 967.07it/s]warmup run: 1245it [00:02, 947.36it/s]warmup run: 1424it [00:02, 987.76it/s]warmup run: 1351it [00:02, 934.11it/s]warmup run: 1624it [00:03, 971.33it/s]warmup run: 1790it [00:03, 970.30it/s]warmup run: 1556it [00:03, 955.86it/s]warmup run: 1680it [00:03, 994.16it/s]warmup run: 1580it [00:03, 976.90it/s]warmup run: 1346it [00:02, 961.11it/s]warmup run: 1526it [00:02, 993.62it/s]warmup run: 1448it [00:03, 930.97it/s]warmup run: 1724it [00:03, 977.92it/s]warmup run: 1888it [00:03, 968.63it/s]warmup run: 1653it [00:03, 958.01it/s]warmup run: 1782it [00:03, 1001.13it/s]warmup run: 1680it [00:03, 973.03it/s]warmup run: 1450it [00:03, 982.06it/s]warmup run: 1629it [00:03, 1001.60it/s]warmup run: 1543it [00:03, 925.60it/s]warmup run: 1823it [00:03, 978.02it/s]warmup run: 1986it [00:03, 967.97it/s]warmup run: 1750it [00:03, 960.83it/s]warmup run: 1883it [00:03, 983.29it/s] warmup run: 1552it [00:03, 992.20it/s]warmup run: 1779it [00:03, 965.80it/s]warmup run: 1731it [00:03, 1004.96it/s]warmup run: 1637it [00:03, 923.34it/s]warmup run: 1922it [00:03, 981.26it/s]warmup run: 2101it [00:03, 1020.74it/s]warmup run: 1848it [00:03, 965.00it/s]warmup run: 1655it [00:03, 1002.30it/s]warmup run: 1982it [00:03, 968.81it/s]warmup run: 1877it [00:03, 960.63it/s]warmup run: 1833it [00:03, 1008.42it/s]warmup run: 1731it [00:03, 923.27it/s]warmup run: 2026it [00:03, 996.56it/s]warmup run: 2221it [00:03, 1072.48it/s]warmup run: 1946it [00:03, 968.68it/s]warmup run: 1757it [00:03, 1003.15it/s]warmup run: 2098it [00:03, 1022.50it/s]warmup run: 1974it [00:03, 952.49it/s]warmup run: 1936it [00:03, 1012.52it/s]warmup run: 1824it [00:03, 923.18it/s]warmup run: 2147it [00:03, 1059.51it/s]warmup run: 2341it [00:03, 1109.71it/s]warmup run: 2052it [00:03, 995.23it/s]warmup run: 2218it [00:03, 1073.30it/s]warmup run: 1859it [00:03, 1000.76it/s]warmup run: 2084it [00:03, 995.01it/s]warmup run: 2045it [00:03, 1035.18it/s]warmup run: 1917it [00:03, 924.20it/s]warmup run: 2268it [00:03, 1102.37it/s]warmup run: 2460it [00:03, 1131.09it/s]warmup run: 2169it [00:03, 1047.40it/s]warmup run: 2338it [00:03, 1108.25it/s]warmup run: 1960it [00:03, 1001.51it/s]warmup run: 2199it [00:03, 1040.12it/s]warmup run: 2167it [00:03, 1089.26it/s]warmup run: 2011it [00:03, 928.57it/s]warmup run: 2388it [00:03, 1131.40it/s]warmup run: 2580it [00:04, 1150.29it/s]warmup run: 2287it [00:03, 1084.46it/s]warmup run: 2458it [00:03, 1134.83it/s]warmup run: 2074it [00:03, 1040.44it/s]warmup run: 2316it [00:03, 1076.43it/s]warmup run: 2288it [00:03, 1125.03it/s]warmup run: 2128it [00:03, 999.00it/s]warmup run: 2508it [00:04, 1151.66it/s]warmup run: 2700it [00:04, 1162.32it/s]warmup run: 2405it [00:03, 1112.17it/s]warmup run: 2579it [00:04, 1155.53it/s]warmup run: 2195it [00:03, 1089.32it/s]warmup run: 2432it [00:03, 1098.96it/s]warmup run: 2410it [00:03, 1153.37it/s]warmup run: 2245it [00:03, 1048.14it/s]warmup run: 2629it [00:04, 1167.38it/s]warmup run: 2818it [00:04, 1166.53it/s]warmup run: 2523it [00:04, 1132.17it/s]warmup run: 2700it [00:04, 1170.77it/s]warmup run: 2316it [00:03, 1124.65it/s]warmup run: 2549it [00:04, 1118.99it/s]warmup run: 2533it [00:03, 1173.90it/s]warmup run: 2362it [00:03, 1084.04it/s]warmup run: 2750it [00:04, 1178.86it/s]warmup run: 2938it [00:04, 1175.68it/s]warmup run: 2642it [00:04, 1148.96it/s]warmup run: 2819it [00:04, 1176.33it/s]warmup run: 2438it [00:03, 1151.27it/s]warmup run: 2666it [00:04, 1131.46it/s]warmup run: 3000it [00:04, 675.33it/s] warmup run: 2655it [00:03, 1186.90it/s]warmup run: 2478it [00:04, 1106.64it/s]warmup run: 2868it [00:04, 1176.94it/s]warmup run: 2760it [00:04, 1156.29it/s]warmup run: 2941it [00:04, 1188.72it/s]warmup run: 2560it [00:04, 1170.45it/s]warmup run: 2783it [00:04, 1140.20it/s]warmup run: 2776it [00:04, 1192.72it/s]warmup run: 2594it [00:04, 1121.88it/s]warmup run: 3000it [00:04, 681.68it/s] warmup run: 2986it [00:04, 1153.12it/s]warmup run: 3000it [00:04, 668.04it/s] warmup run: 2879it [00:04, 1165.56it/s]warmup run: 2682it [00:04, 1182.89it/s]warmup run: 2901it [00:04, 1151.40it/s]warmup run: 2897it [00:04, 1197.25it/s]warmup run: 2710it [00:04, 1131.40it/s]warmup run: 2998it [00:04, 1172.85it/s]warmup run: 3000it [00:04, 676.36it/s] warmup run: 3000it [00:04, 677.03it/s] warmup run: 2802it [00:04, 1187.35it/s]warmup run: 3000it [00:04, 699.23it/s] warmup run: 2824it [00:04, 1129.46it/s]warmup run: 2924it [00:04, 1194.68it/s]warmup run: 2941it [00:04, 1141.07it/s]warmup run: 3000it [00:04, 682.28it/s] warmup run: 3000it [00:04, 668.72it/s] 
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]



warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]


warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   5%|         | 160/3000 [00:00<00:01, 1596.48it/s]warmup should be done:   5%|         | 160/3000 [00:00<00:01, 1599.47it/s]warmup should be done:   6%|         | 166/3000 [00:00<00:01, 1657.04it/s]warmup should be done:   5%|         | 148/3000 [00:00<00:01, 1479.35it/s]warmup should be done:   5%|         | 153/3000 [00:00<00:01, 1524.44it/s]warmup should be done:   6%|         | 167/3000 [00:00<00:01, 1663.85it/s]warmup should be done:   5%|         | 162/3000 [00:00<00:01, 1615.04it/s]warmup should be done:   5%|         | 151/3000 [00:00<00:01, 1503.08it/s]warmup should be done:  11%|         | 321/3000 [00:00<00:01, 1604.39it/s]warmup should be done:  11%|         | 333/3000 [00:00<00:01, 1664.36it/s]warmup should be done:  11%|         | 334/3000 [00:00<00:01, 1664.71it/s]warmup should be done:  10%|         | 310/3000 [00:00<00:01, 1558.52it/s]warmup should be done:  11%|         | 326/3000 [00:00<00:01, 1625.59it/s]warmup should be done:  10%|         | 309/3000 [00:00<00:01, 1540.08it/s]warmup should be done:  10%|         | 303/3000 [00:00<00:01, 1508.48it/s]warmup should be done:  11%|         | 320/3000 [00:00<00:01, 1575.07it/s]warmup should be done:  17%|        | 500/3000 [00:00<00:01, 1665.75it/s]warmup should be done:  16%|        | 482/3000 [00:00<00:01, 1602.99it/s]warmup should be done:  16%|        | 476/3000 [00:00<00:01, 1600.59it/s]warmup should be done:  17%|        | 501/3000 [00:00<00:01, 1661.82it/s]warmup should be done:  16%|        | 490/3000 [00:00<00:01, 1628.04it/s]warmup should be done:  16%|        | 466/3000 [00:00<00:01, 1550.76it/s]warmup should be done:  15%|        | 454/3000 [00:00<00:01, 1506.90it/s]warmup should be done:  16%|        | 479/3000 [00:00<00:01, 1580.22it/s]warmup should be done:  22%|       | 667/3000 [00:00<00:01, 1665.41it/s]warmup should be done:  22%|       | 653/3000 [00:00<00:01, 1626.89it/s]warmup should be done:  21%|       | 643/3000 [00:00<00:01, 1624.73it/s]warmup should be done:  21%|       | 643/3000 [00:00<00:01, 1599.59it/s]warmup should be done:  21%|        | 623/3000 [00:00<00:01, 1554.98it/s]warmup should be done:  20%|        | 605/3000 [00:00<00:01, 1499.61it/s]warmup should be done:  21%|       | 638/3000 [00:00<00:01, 1580.92it/s]warmup should be done:  22%|       | 668/3000 [00:00<00:01, 1495.16it/s]warmup should be done:  28%|       | 834/3000 [00:00<00:01, 1665.05it/s]warmup should be done:  27%|       | 816/3000 [00:00<00:01, 1627.93it/s]warmup should be done:  27%|       | 810/3000 [00:00<00:01, 1639.49it/s]warmup should be done:  27%|       | 803/3000 [00:00<00:01, 1596.52it/s]warmup should be done:  26%|       | 781/3000 [00:00<00:01, 1560.70it/s]warmup should be done:  25%|       | 756/3000 [00:00<00:01, 1502.24it/s]warmup should be done:  27%|       | 797/3000 [00:00<00:01, 1578.85it/s]warmup should be done:  27%|       | 823/3000 [00:00<00:01, 1512.87it/s]warmup should be done:  33%|      | 1001/3000 [00:00<00:01, 1664.58it/s]warmup should be done:  33%|      | 977/3000 [00:00<00:01, 1647.18it/s]warmup should be done:  33%|      | 979/3000 [00:00<00:01, 1623.88it/s]warmup should be done:  31%|      | 939/3000 [00:00<00:01, 1565.03it/s]warmup should be done:  32%|      | 963/3000 [00:00<00:01, 1590.77it/s]warmup should be done:  30%|       | 907/3000 [00:00<00:01, 1500.80it/s]warmup should be done:  32%|      | 955/3000 [00:00<00:01, 1576.48it/s]warmup should be done:  33%|      | 987/3000 [00:00<00:01, 1552.28it/s]warmup should be done:  39%|      | 1168/3000 [00:00<00:01, 1660.08it/s]warmup should be done:  38%|      | 1142/3000 [00:00<00:01, 1623.84it/s]warmup should be done:  38%|      | 1143/3000 [00:00<00:01, 1648.64it/s]warmup should be done:  37%|      | 1097/3000 [00:00<00:01, 1568.28it/s]warmup should be done:  35%|      | 1058/3000 [00:00<00:01, 1498.24it/s]warmup should be done:  37%|      | 1123/3000 [00:00<00:01, 1585.13it/s]warmup should be done:  37%|      | 1113/3000 [00:00<00:01, 1571.62it/s]warmup should be done:  38%|      | 1151/3000 [00:00<00:01, 1579.27it/s]warmup should be done:  44%|     | 1306/3000 [00:00<00:01, 1628.75it/s]warmup should be done:  44%|     | 1309/3000 [00:00<00:01, 1652.12it/s]warmup should be done:  44%|     | 1335/3000 [00:00<00:01, 1660.58it/s]warmup should be done:  42%|     | 1257/3000 [00:00<00:01, 1576.07it/s]warmup should be done:  43%|     | 1282/3000 [00:00<00:01, 1584.76it/s]warmup should be done:  40%|      | 1208/3000 [00:00<00:01, 1493.75it/s]warmup should be done:  42%|     | 1271/3000 [00:00<00:01, 1571.92it/s]warmup should be done:  44%|     | 1316/3000 [00:00<00:01, 1598.85it/s]warmup should be done:  49%|     | 1471/3000 [00:00<00:00, 1634.29it/s]warmup should be done:  47%|     | 1415/3000 [00:00<00:01, 1576.77it/s]warmup should be done:  50%|     | 1502/3000 [00:00<00:00, 1659.22it/s]warmup should be done:  49%|     | 1475/3000 [00:00<00:00, 1646.91it/s]warmup should be done:  48%|     | 1441/3000 [00:00<00:00, 1584.06it/s]warmup should be done:  45%|     | 1358/3000 [00:00<00:01, 1494.80it/s]warmup should be done:  48%|     | 1429/3000 [00:00<00:00, 1571.54it/s]warmup should be done:  49%|     | 1481/3000 [00:00<00:00, 1612.86it/s]warmup should be done:  55%|    | 1636/3000 [00:01<00:00, 1637.18it/s]warmup should be done:  56%|    | 1668/3000 [00:01<00:00, 1652.95it/s]warmup should be done:  53%|    | 1600/3000 [00:01<00:00, 1585.26it/s]warmup should be done:  52%|    | 1573/3000 [00:01<00:00, 1563.60it/s]warmup should be done:  50%|     | 1508/3000 [00:01<00:00, 1493.84it/s]warmup should be done:  53%|    | 1587/3000 [00:01<00:00, 1573.30it/s]warmup should be done:  55%|    | 1640/3000 [00:01<00:00, 1624.52it/s]warmup should be done:  55%|    | 1646/3000 [00:01<00:00, 1623.30it/s]warmup should be done:  60%|    | 1801/3000 [00:01<00:00, 1638.79it/s]warmup should be done:  59%|    | 1759/3000 [00:01<00:00, 1584.37it/s]warmup should be done:  61%|    | 1834/3000 [00:01<00:00, 1646.89it/s]warmup should be done:  58%|    | 1730/3000 [00:01<00:00, 1563.42it/s]warmup should be done:  55%|    | 1658/3000 [00:01<00:00, 1493.25it/s]warmup should be done:  58%|    | 1745/3000 [00:01<00:00, 1573.79it/s]warmup should be done:  60%|    | 1803/3000 [00:01<00:00, 1625.63it/s]warmup should be done:  60%|    | 1811/3000 [00:01<00:00, 1629.91it/s]warmup should be done:  66%|   | 1965/3000 [00:01<00:00, 1638.71it/s]warmup should be done:  64%|   | 1918/3000 [00:01<00:00, 1582.81it/s]warmup should be done:  63%|   | 1903/3000 [00:01<00:00, 1574.04it/s]warmup should be done:  63%|   | 1888/3000 [00:01<00:00, 1565.43it/s]warmup should be done:  67%|   | 1999/3000 [00:01<00:00, 1641.91it/s]warmup should be done:  60%|    | 1808/3000 [00:01<00:00, 1491.90it/s]warmup should be done:  66%|   | 1967/3000 [00:01<00:00, 1629.65it/s]warmup should be done:  66%|   | 1976/3000 [00:01<00:00, 1635.78it/s]warmup should be done:  71%|   | 2129/3000 [00:01<00:00, 1632.28it/s]warmup should be done:  69%|   | 2077/3000 [00:01<00:00, 1584.28it/s]warmup should be done:  69%|   | 2061/3000 [00:01<00:00, 1574.56it/s]warmup should be done:  65%|   | 1958/3000 [00:01<00:00, 1491.36it/s]warmup should be done:  72%|  | 2164/3000 [00:01<00:00, 1633.97it/s]warmup should be done:  68%|   | 2045/3000 [00:01<00:00, 1554.00it/s]warmup should be done:  71%|   | 2130/3000 [00:01<00:00, 1611.42it/s]warmup should be done:  71%|  | 2141/3000 [00:01<00:00, 1639.47it/s]warmup should be done:  76%|  | 2293/3000 [00:01<00:00, 1628.39it/s]warmup should be done:  75%|  | 2236/3000 [00:01<00:00, 1585.37it/s]warmup should be done:  74%|  | 2219/3000 [00:01<00:00, 1574.89it/s]warmup should be done:  70%|   | 2108/3000 [00:01<00:00, 1490.49it/s]warmup should be done:  73%|  | 2201/3000 [00:01<00:00, 1551.91it/s]warmup should be done:  78%|  | 2328/3000 [00:01<00:00, 1628.52it/s]warmup should be done:  76%|  | 2292/3000 [00:01<00:00, 1606.72it/s]warmup should be done:  77%|  | 2306/3000 [00:01<00:00, 1641.61it/s]warmup should be done:  80%|  | 2395/3000 [00:01<00:00, 1583.00it/s]warmup should be done:  82%| | 2456/3000 [00:01<00:00, 1617.73it/s]warmup should be done:  79%|  | 2377/3000 [00:01<00:00, 1572.14it/s]warmup should be done:  75%|  | 2258/3000 [00:01<00:00, 1489.25it/s]warmup should be done:  79%|  | 2362/3000 [00:01<00:00, 1566.87it/s]warmup should be done:  83%| | 2491/3000 [00:01<00:00, 1620.20it/s]warmup should be done:  82%| | 2456/3000 [00:01<00:00, 1613.96it/s]warmup should be done:  82%| | 2471/3000 [00:01<00:00, 1641.06it/s]warmup should be done:  85%| | 2557/3000 [00:01<00:00, 1591.46it/s]warmup should be done:  84%| | 2535/3000 [00:01<00:00, 1574.42it/s]warmup should be done:  87%| | 2618/3000 [00:01<00:00, 1614.70it/s]warmup should be done:  80%|  | 2408/3000 [00:01<00:00, 1491.14it/s]warmup should be done:  84%| | 2525/3000 [00:01<00:00, 1585.40it/s]warmup should be done:  88%| | 2654/3000 [00:01<00:00, 1618.09it/s]warmup should be done:  87%| | 2621/3000 [00:01<00:00, 1624.40it/s]warmup should be done:  88%| | 2636/3000 [00:01<00:00, 1643.35it/s]warmup should be done:  91%| | 2719/3000 [00:01<00:00, 1598.86it/s]warmup should be done:  90%| | 2696/3000 [00:01<00:00, 1582.09it/s]warmup should be done:  93%|| 2780/3000 [00:01<00:00, 1612.93it/s]warmup should be done:  85%| | 2559/3000 [00:01<00:00, 1495.15it/s]warmup should be done:  90%| | 2689/3000 [00:01<00:00, 1599.18it/s]warmup should be done:  94%|| 2817/3000 [00:01<00:00, 1618.84it/s]warmup should be done:  93%|| 2786/3000 [00:01<00:00, 1631.12it/s]warmup should be done:  93%|| 2801/3000 [00:01<00:00, 1645.19it/s]warmup should be done:  96%|| 2881/3000 [00:01<00:00, 1604.81it/s]warmup should be done:  95%|| 2858/3000 [00:01<00:00, 1591.28it/s]warmup should be done:  98%|| 2943/3000 [00:01<00:00, 1616.93it/s]warmup should be done:  90%| | 2712/3000 [00:01<00:00, 1504.80it/s]warmup should be done:  95%|| 2853/3000 [00:01<00:00, 1609.65it/s]warmup should be done:  98%|| 2953/3000 [00:01<00:00, 1642.44it/s]warmup should be done:  99%|| 2982/3000 [00:01<00:00, 1625.19it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1641.94it/s]warmup should be done:  99%|| 2966/3000 [00:01<00:00, 1640.16it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1626.83it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1625.16it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1616.65it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1592.81it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1579.78it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1576.68it/s]warmup should be done:  96%|| 2866/3000 [00:01<00:00, 1515.17it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1501.07it/s]




warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]


warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   6%|         | 165/3000 [00:00<00:01, 1646.89it/s]warmup should be done:   6%|         | 169/3000 [00:00<00:01, 1686.03it/s]warmup should be done:   5%|         | 163/3000 [00:00<00:01, 1627.82it/s]warmup should be done:   6%|         | 167/3000 [00:00<00:01, 1663.43it/s]warmup should be done:   5%|         | 164/3000 [00:00<00:01, 1635.44it/s]warmup should be done:   5%|         | 144/3000 [00:00<00:01, 1432.36it/s]warmup should be done:   6%|         | 167/3000 [00:00<00:01, 1660.63it/s]warmup should be done:   6%|         | 165/3000 [00:00<00:01, 1640.94it/s]warmup should be done:  11%|         | 332/3000 [00:00<00:01, 1656.59it/s]warmup should be done:  11%|         | 334/3000 [00:00<00:01, 1665.20it/s]warmup should be done:  11%|         | 327/3000 [00:00<00:01, 1631.07it/s]warmup should be done:  11%|        | 339/3000 [00:00<00:01, 1689.65it/s]warmup should be done:  11%|         | 330/3000 [00:00<00:01, 1646.16it/s]warmup should be done:  11%|         | 331/3000 [00:00<00:01, 1651.06it/s]warmup should be done:  10%|         | 301/3000 [00:00<00:01, 1509.29it/s]warmup should be done:  11%|         | 334/3000 [00:00<00:01, 1660.22it/s]warmup should be done:  17%|        | 499/3000 [00:00<00:01, 1662.12it/s]warmup should be done:  17%|        | 501/3000 [00:00<00:01, 1666.80it/s]warmup should be done:  17%|        | 497/3000 [00:00<00:01, 1654.65it/s]warmup should be done:  16%|        | 491/3000 [00:00<00:01, 1631.95it/s]warmup should be done:  17%|        | 497/3000 [00:00<00:01, 1654.60it/s]warmup should be done:  15%|        | 459/3000 [00:00<00:01, 1537.44it/s]warmup should be done:  17%|        | 505/3000 [00:00<00:01, 1678.76it/s]warmup should be done:  17%|        | 508/3000 [00:00<00:01, 1679.50it/s]warmup should be done:  22%|       | 669/3000 [00:00<00:01, 1670.16it/s]warmup should be done:  22%|       | 668/3000 [00:00<00:01, 1669.90it/s]warmup should be done:  22%|       | 663/3000 [00:00<00:01, 1654.50it/s]warmup should be done:  22%|       | 664/3000 [00:00<00:01, 1658.10it/s]warmup should be done:  23%|       | 676/3000 [00:00<00:01, 1689.29it/s]warmup should be done:  21%|        | 617/3000 [00:00<00:01, 1551.42it/s]warmup should be done:  22%|       | 655/3000 [00:00<00:01, 1629.58it/s]warmup should be done:  23%|       | 676/3000 [00:00<00:01, 1672.10it/s]warmup should be done:  28%|       | 837/3000 [00:00<00:01, 1673.26it/s]warmup should be done:  28%|       | 837/3000 [00:00<00:01, 1674.42it/s]warmup should be done:  28%|       | 830/3000 [00:00<00:01, 1656.74it/s]warmup should be done:  28%|       | 845/3000 [00:00<00:01, 1689.52it/s]warmup should be done:  26%|       | 774/3000 [00:00<00:01, 1557.31it/s]warmup should be done:  27%|       | 821/3000 [00:00<00:01, 1637.81it/s]warmup should be done:  28%|       | 830/3000 [00:00<00:01, 1644.58it/s]warmup should be done:  28%|       | 844/3000 [00:00<00:01, 1669.67it/s]warmup should be done:  34%|      | 1005/3000 [00:00<00:01, 1673.17it/s]warmup should be done:  34%|      | 1006/3000 [00:00<00:01, 1678.35it/s]warmup should be done:  33%|      | 996/3000 [00:00<00:01, 1656.42it/s]warmup should be done:  34%|      | 1014/3000 [00:00<00:01, 1687.24it/s]warmup should be done:  33%|      | 986/3000 [00:00<00:01, 1641.02it/s]warmup should be done:  31%|       | 932/3000 [00:00<00:01, 1561.97it/s]warmup should be done:  33%|      | 996/3000 [00:00<00:01, 1648.31it/s]warmup should be done:  34%|      | 1011/3000 [00:00<00:01, 1663.92it/s]warmup should be done:  39%|      | 1174/3000 [00:00<00:01, 1678.60it/s]warmup should be done:  39%|      | 1173/3000 [00:00<00:01, 1670.20it/s]warmup should be done:  39%|      | 1163/3000 [00:00<00:01, 1658.22it/s]warmup should be done:  36%|      | 1089/3000 [00:00<00:01, 1563.59it/s]warmup should be done:  38%|      | 1152/3000 [00:00<00:01, 1645.47it/s]warmup should be done:  39%|      | 1183/3000 [00:00<00:01, 1682.71it/s]warmup should be done:  39%|      | 1162/3000 [00:00<00:01, 1651.73it/s]warmup should be done:  39%|      | 1178/3000 [00:00<00:01, 1661.50it/s]warmup should be done:  45%|     | 1343/3000 [00:00<00:00, 1679.30it/s]warmup should be done:  44%|     | 1329/3000 [00:00<00:01, 1658.46it/s]warmup should be done:  45%|     | 1341/3000 [00:00<00:00, 1671.96it/s]warmup should be done:  44%|     | 1318/3000 [00:00<00:01, 1647.75it/s]warmup should be done:  45%|     | 1352/3000 [00:00<00:00, 1684.61it/s]warmup should be done:  42%|     | 1246/3000 [00:00<00:01, 1561.11it/s]warmup should be done:  44%|     | 1328/3000 [00:00<00:01, 1653.28it/s]warmup should be done:  45%|     | 1345/3000 [00:00<00:00, 1663.74it/s]warmup should be done:  50%|     | 1496/3000 [00:00<00:00, 1661.32it/s]warmup should be done:  50%|     | 1512/3000 [00:00<00:00, 1680.24it/s]warmup should be done:  50%|     | 1509/3000 [00:00<00:00, 1672.03it/s]warmup should be done:  51%|     | 1521/3000 [00:00<00:00, 1683.84it/s]warmup should be done:  50%|     | 1485/3000 [00:00<00:00, 1651.82it/s]warmup should be done:  47%|     | 1404/3000 [00:00<00:01, 1565.50it/s]warmup should be done:  50%|     | 1495/3000 [00:00<00:00, 1657.50it/s]warmup should be done:  50%|     | 1512/3000 [00:00<00:00, 1664.52it/s]warmup should be done:  56%|    | 1681/3000 [00:01<00:00, 1682.29it/s]warmup should be done:  55%|    | 1664/3000 [00:01<00:00, 1664.70it/s]warmup should be done:  56%|    | 1677/3000 [00:01<00:00, 1671.71it/s]warmup should be done:  55%|    | 1652/3000 [00:01<00:00, 1656.17it/s]warmup should be done:  56%|    | 1690/3000 [00:01<00:00, 1683.50it/s]warmup should be done:  52%|    | 1561/3000 [00:01<00:00, 1565.13it/s]warmup should be done:  55%|    | 1662/3000 [00:01<00:00, 1661.12it/s]warmup should be done:  56%|    | 1679/3000 [00:01<00:00, 1666.04it/s]warmup should be done:  62%|   | 1850/3000 [00:01<00:00, 1683.97it/s]warmup should be done:  61%|    | 1832/3000 [00:01<00:00, 1666.88it/s]warmup should be done:  62%|   | 1845/3000 [00:01<00:00, 1671.63it/s]warmup should be done:  61%|    | 1819/3000 [00:01<00:00, 1658.14it/s]warmup should be done:  62%|   | 1859/3000 [00:01<00:00, 1683.47it/s]warmup should be done:  57%|    | 1718/3000 [00:01<00:00, 1564.73it/s]warmup should be done:  61%|    | 1830/3000 [00:01<00:00, 1663.92it/s]warmup should be done:  62%|   | 1847/3000 [00:01<00:00, 1667.35it/s]warmup should be done:  67%|   | 2019/3000 [00:01<00:00, 1684.46it/s]warmup should be done:  67%|   | 1999/3000 [00:01<00:00, 1666.31it/s]warmup should be done:  67%|   | 2013/3000 [00:01<00:00, 1670.87it/s]warmup should be done:  66%|   | 1985/3000 [00:01<00:00, 1657.42it/s]warmup should be done:  68%|   | 2028/3000 [00:01<00:00, 1684.56it/s]warmup should be done:  63%|   | 1876/3000 [00:01<00:00, 1566.97it/s]warmup should be done:  67%|   | 1997/3000 [00:01<00:00, 1663.00it/s]warmup should be done:  67%|   | 2014/3000 [00:01<00:00, 1666.58it/s]warmup should be done:  72%|  | 2166/3000 [00:01<00:00, 1665.67it/s]warmup should be done:  73%|  | 2197/3000 [00:01<00:00, 1685.93it/s]warmup should be done:  72%|  | 2151/3000 [00:01<00:00, 1657.15it/s]warmup should be done:  68%|   | 2033/3000 [00:01<00:00, 1566.98it/s]warmup should be done:  73%|  | 2181/3000 [00:01<00:00, 1668.29it/s]warmup should be done:  73%|  | 2188/3000 [00:01<00:00, 1671.28it/s]warmup should be done:  72%|  | 2164/3000 [00:01<00:00, 1662.33it/s]warmup should be done:  73%|  | 2181/3000 [00:01<00:00, 1663.68it/s]warmup should be done:  78%|  | 2334/3000 [00:01<00:00, 1666.96it/s]warmup should be done:  79%|  | 2366/3000 [00:01<00:00, 1685.11it/s]warmup should be done:  77%|  | 2318/3000 [00:01<00:00, 1658.47it/s]warmup should be done:  73%|  | 2190/3000 [00:01<00:00, 1566.16it/s]warmup should be done:  78%|  | 2349/3000 [00:01<00:00, 1669.39it/s]warmup should be done:  78%|  | 2331/3000 [00:01<00:00, 1664.05it/s]warmup should be done:  79%|  | 2357/3000 [00:01<00:00, 1675.97it/s]warmup should be done:  78%|  | 2348/3000 [00:01<00:00, 1664.17it/s]warmup should be done:  83%| | 2502/3000 [00:01<00:00, 1667.98it/s]warmup should be done:  84%| | 2535/3000 [00:01<00:00, 1683.67it/s]warmup should be done:  83%| | 2485/3000 [00:01<00:00, 1659.23it/s]warmup should be done:  78%|  | 2348/3000 [00:01<00:00, 1568.07it/s]warmup should be done:  83%| | 2498/3000 [00:01<00:00, 1665.54it/s]warmup should be done:  84%| | 2526/3000 [00:01<00:00, 1679.92it/s]warmup should be done:  84%| | 2517/3000 [00:01<00:00, 1669.85it/s]warmup should be done:  84%| | 2515/3000 [00:01<00:00, 1664.68it/s]warmup should be done:  89%| | 2669/3000 [00:01<00:00, 1667.20it/s]warmup should be done:  88%| | 2651/3000 [00:01<00:00, 1657.50it/s]warmup should be done:  90%| | 2695/3000 [00:01<00:00, 1682.88it/s]warmup should be done:  84%| | 2506/3000 [00:01<00:00, 1570.04it/s]warmup should be done:  90%| | 2704/3000 [00:01<00:00, 1681.42it/s]warmup should be done:  89%| | 2665/3000 [00:01<00:00, 1663.82it/s]warmup should be done:  90%| | 2685/3000 [00:01<00:00, 1671.42it/s]warmup should be done:  89%| | 2682/3000 [00:01<00:00, 1664.72it/s]warmup should be done:  95%|| 2836/3000 [00:01<00:00, 1666.53it/s]warmup should be done:  95%|| 2864/3000 [00:01<00:00, 1683.29it/s]warmup should be done:  94%|| 2817/3000 [00:01<00:00, 1655.45it/s]warmup should be done:  94%|| 2832/3000 [00:01<00:00, 1663.16it/s]warmup should be done:  89%| | 2664/3000 [00:01<00:00, 1568.13it/s]warmup should be done:  95%|| 2853/3000 [00:01<00:00, 1665.32it/s]warmup should be done:  95%|| 2849/3000 [00:01<00:00, 1663.46it/s]warmup should be done:  96%|| 2873/3000 [00:01<00:00, 1664.30it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1679.61it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1678.25it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1669.55it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1667.11it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1662.78it/s]warmup should be done:  99%|| 2984/3000 [00:01<00:00, 1656.90it/s]warmup should be done: 100%|| 2999/3000 [00:01<00:00, 1664.84it/s]warmup should be done:  94%|| 2821/3000 [00:01<00:00, 1567.91it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1658.81it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1650.57it/s]warmup should be done:  99%|| 2979/3000 [00:01<00:00, 1569.54it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1560.73it/s]
WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f73bb353e80>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f73bb356d30>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f73ba8100d0>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f73bb3559d0>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f73ba810190>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f73ba80e1c0>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f73ba8101f0>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f73ba81e2b0>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
2022-12-12 08:18:36.220065: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f6eeb0294f0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 08:18:36.220137: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 08:18:36.228126: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 08:18:36.363901: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f6ee7031910 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 08:18:36.363956: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 08:18:36.373413: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 08:18:36.732694: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f6ee282fd90 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 08:18:36.732753: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 08:18:36.740889: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 08:18:36.881433: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f6ee6f91fd0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 08:18:36.881504: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 08:18:36.890183: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 08:18:37.001406: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f6eef02d620 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 08:18:37.001475: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 08:18:37.011035: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 08:18:37.051510: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f6eee82fea0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 08:18:37.051581: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 08:18:37.055834: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f6eea830820 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 08:18:37.055893: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 08:18:37.061571: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 08:18:37.065935: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 08:18:37.067663: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f6ef2837570 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 08:18:37.067703: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 08:18:37.075476: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 08:18:43.421334: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 08:18:43.447520: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 08:18:43.806280: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 08:18:43.812446: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 08:18:43.902164: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 08:18:43.918179: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 08:18:43.930147: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 08:18:43.976137: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
[HCTR][08:19:32.703][ERROR][RK0][tid #140114969745152]: replica 5 reaches 1000, calling init pre replica
[HCTR][08:19:32.703][ERROR][RK0][tid #140114969745152]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][08:19:32.709][ERROR][RK0][tid #140114969745152]: coll ps creation done
[HCTR][08:19:32.709][ERROR][RK0][tid #140114969745152]: replica 5 waits for coll ps creation barrier
[HCTR][08:19:32.777][ERROR][RK0][tid #140114692916992]: replica 7 reaches 1000, calling init pre replica
[HCTR][08:19:32.777][ERROR][RK0][tid #140114692916992]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][08:19:32.782][ERROR][RK0][tid #140114692916992]: coll ps creation done
[HCTR][08:19:32.782][ERROR][RK0][tid #140114692916992]: replica 7 waits for coll ps creation barrier
[HCTR][08:19:33.061][ERROR][RK0][main]: replica 4 reaches 1000, calling init pre replica
[HCTR][08:19:33.061][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][08:19:33.066][ERROR][RK0][main]: coll ps creation done
[HCTR][08:19:33.066][ERROR][RK0][main]: replica 4 waits for coll ps creation barrier
[HCTR][08:19:33.101][ERROR][RK0][tid #140114642593536]: replica 1 reaches 1000, calling init pre replica
[HCTR][08:19:33.102][ERROR][RK0][tid #140114642593536]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][08:19:33.106][ERROR][RK0][tid #140114642593536]: coll ps creation done
[HCTR][08:19:33.107][ERROR][RK0][tid #140114642593536]: replica 1 waits for coll ps creation barrier
[HCTR][08:19:33.110][ERROR][RK0][main]: replica 6 reaches 1000, calling init pre replica
[HCTR][08:19:33.110][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][08:19:33.115][ERROR][RK0][main]: coll ps creation done
[HCTR][08:19:33.115][ERROR][RK0][main]: replica 6 waits for coll ps creation barrier
[HCTR][08:19:33.121][ERROR][RK0][main]: replica 3 reaches 1000, calling init pre replica
[HCTR][08:19:33.122][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][08:19:33.128][ERROR][RK0][main]: coll ps creation done
[HCTR][08:19:33.128][ERROR][RK0][main]: replica 3 waits for coll ps creation barrier
[HCTR][08:19:33.202][ERROR][RK0][main]: replica 0 reaches 1000, calling init pre replica
[HCTR][08:19:33.202][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][08:19:33.207][ERROR][RK0][main]: coll ps creation done
[HCTR][08:19:33.207][ERROR][RK0][main]: replica 0 waits for coll ps creation barrier
[HCTR][08:19:33.218][ERROR][RK0][tid #140114961352448]: replica 2 reaches 1000, calling init pre replica
[HCTR][08:19:33.218][ERROR][RK0][tid #140114961352448]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][08:19:33.225][ERROR][RK0][tid #140114961352448]: coll ps creation done
[HCTR][08:19:33.225][ERROR][RK0][tid #140114961352448]: replica 2 waits for coll ps creation barrier
[HCTR][08:19:33.225][ERROR][RK0][main]: replica 0 preparing frequency
[HCTR][08:19:34.077][ERROR][RK0][main]: replica 0 preparing frequency done
[HCTR][08:19:34.126][ERROR][RK0][main]: replica 0 calling init per replica
[HCTR][08:19:34.126][ERROR][RK0][tid #140114692916992]: replica 7 calling init per replica
[HCTR][08:19:34.126][ERROR][RK0][main]: replica 6 calling init per replica
[HCTR][08:19:34.126][ERROR][RK0][main]: replica 4 calling init per replica
[HCTR][08:19:34.126][ERROR][RK0][main]: Calling build_v2
[HCTR][08:19:34.126][ERROR][RK0][main]: replica 3 calling init per replica
[HCTR][08:19:34.126][ERROR][RK0][tid #140114642593536]: replica 1 calling init per replica
[HCTR][08:19:34.126][ERROR][RK0][tid #140114961352448]: replica 2 calling init per replica
[HCTR][08:19:34.126][ERROR][RK0][tid #140114642593536]: Calling build_v2
[HCTR][08:19:34.126][ERROR][RK0][tid #140114969745152]: replica 5 calling init per replica
[HCTR][08:19:34.126][ERROR][RK0][main]: Calling build_v2
[HCTR][08:19:34.126][ERROR][RK0][tid #140114961352448]: Calling build_v2
[HCTR][08:19:34.126][ERROR][RK0][tid #140114692916992]: Calling build_v2
[HCTR][08:19:34.126][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][08:19:34.126][ERROR][RK0][main]: Calling build_v2
[HCTR][08:19:34.126][ERROR][RK0][main]: Calling build_v2
[HCTR][08:19:34.126][ERROR][RK0][tid #140114642593536]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][08:19:34.126][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][08:19:34.126][ERROR][RK0][tid #140114969745152]: Calling build_v2
[HCTR][08:19:34.126][ERROR][RK0][tid #140114692916992]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][08:19:34.126][ERROR][RK0][tid #140114961352448]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][08:19:34.126][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][08:19:34.126][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][08:19:34.126][ERROR][RK0][tid #140114969745152]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[[[[2022-12-12 08:19:34[[2022-12-12 08:19:34.2022-12-12 08:19:34[[2022-12-12 08:19:34.2022-12-12 08:19:342022-12-12 08:19:34126357..126367.2022-12-12 08:19:34.: 2022-12-12 08:19:34126378126385: 126371.126384E.: : E: 126391:  126396EE E: E/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc:   /hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc E :E/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc:/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc /hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc136 ::136:/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc:] /hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc136136] 136:136using concurrent impl MPS:] ] using concurrent impl MPS] 136] 
136using concurrent impl MPSusing concurrent impl MPS
using concurrent impl MPS] using concurrent impl MPS] 


using concurrent impl MPS
using concurrent impl MPS

[2022-12-12 08:19:34.131072: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:178] v100x8, slow pcie
[2022-12-12 08:19:34.131110: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:196[] 2022-12-12 08:19:34assigning 8 to cpu.
131120: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:178] v100x8, slow pcie
[[2022-12-12 08:19:342022-12-12 08:19:34..131173131181: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[::[2022-12-12 08:19:341781962022-12-12 08:19:34.] ] .131211v100x8, slow pcieassigning 8 to cpu131221: 

: [EE[2022-12-12 08:19:34  2022-12-12 08:19:34./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.131279[::131297: 2022-12-12 08:19:34212178[: E.] ] E2022-12-12 08:19:34 131337build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8[v100x8, slow pcie ./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: 
2022-12-12 08:19:34
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc131343:E.:[: 178[ [1313891962022-12-12 08:19:34E] 2022-12-12 08:19:34/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 08:19:34: ] . v100x8, slow pcie.:.Eassigning 8 to cpu131452/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
131459212 131449
: :: [] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: E178E2022-12-12 08:19:34build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8:E ]  [.
178 /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccv100x8, slow pcie/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 08:19:34131574] :/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
[:.: v100x8, slow pcie196:2022-12-12 08:19:34213[131623E
] 178[.] 2022-12-12 08:19:34:  assigning 8 to cpu] 2022-12-12 08:19:34131680remote time is 8.68421.E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
v100x8, slow pcie.: 
131710 :
131755E: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[196:  [E:2022-12-12 08:19:34[] E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 08:19:34 212.2022-12-12 08:19:34assigning 8 to cpu :./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] 131841.
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc213131857:build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8: 131867:] : 196
E: 196remote time is 8.68421E]  E[] [
 assigning 8 to cpu/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc 2022-12-12 08:19:34assigning 8 to cpu2022-12-12 08:19:34/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[
:/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.
.:2022-12-12 08:19:34214:132003132006196.] 212[: : ] 132043cpu time is 97.0588] 2022-12-12 08:19:34EEassigning 8 to cpu[: 
build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8.  
2022-12-12 08:19:34E
132102/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc. : :[:132141/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE 2122022-12-12 08:19:34213: :[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] .] E2142022-12-12 08:19:34:build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8132222remote time is 8.68421 ] .212
: 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cccpu time is 97.0588132255] E:
[[: build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8 2122022-12-12 08:19:342022-12-12 08:19:34E
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] .. :build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8132359132361: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[213
: E:2022-12-12 08:19:34] E 212[.remote time is 8.68421 /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] 2022-12-12 08:19:34132425
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8.: :214[
132462E213] 2022-12-12 08:19:34:  ] [cpu time is 97.0588.E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccremote time is 8.684212022-12-12 08:19:34
132531 :
.: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc213132577E[:] :  2022-12-12 08:19:34213remote time is 8.68421E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.] 
 :132649remote time is 8.68421/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc214: [
:] E2022-12-12 08:19:34213cpu time is 97.0588[ .] 
2022-12-12 08:19:34/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc132716remote time is 8.68421.:: 
132749214E: ]  [Ecpu time is 97.0588/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 08:19:34 
:./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc214132817:] : 214cpu time is 97.0588E] 
 cpu time is 97.0588/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
:214] cpu time is 97.0588
[2022-12-12 08:20:53.846408: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:118] solver created. now build & solve
[2022-12-12 08:20:53.886458: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 381.47 MB
[2022-12-12 08:20:53.886546: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 381.47 MB
[2022-12-12 08:20:53.887610: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:74] mapping nid to rank...
[2022-12-12 08:20:53.966137: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:91] counting slots...
[2022-12-12 08:20:54.354530: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:105] Final num slot is 49
[2022-12-12 08:20:54.354624: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:109] counting blocks...
[2022-12-12 08:21:01.279283: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:118] Final num block is 1024
[2022-12-12 08:21:01.279376: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:123] counting freq and density...
[2022-12-12 08:21:02.991313: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:154] averaging freq and density...
[2022-12-12 08:21:02.991416: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:155] 1024
[2022-12-12 08:21:02.994178: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:123] solver built. now solve
[2022-12-12 08:21:02.994238: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:354] constructing optimal solver, device=8, stream=1
1024 blocks, 8 devices
[2022-12-12 08:21:03.325923: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:527] Add Var...
[2022-12-12 08:21:03.354483: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:545] Capacity...
[2022-12-12 08:21:03.355926: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:548] Connect CPU...
[2022-12-12 08:21:03.376827: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:550] Connect Access To Storage...
[2022-12-12 08:21:03.900284: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:554] Time...
[2022-12-12 08:21:16.757074: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:569] Coll Cache init block placement array
[2022-12-12 08:21:16.767220: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:645] Coll Cache init block placement array done
[2022-12-12 08:21:16.767432: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:647] Coll Cache model reset done
[2022-12-12 08:21:16.811110: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:127] solver solved
[2022-12-12 08:21:16.811222: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:195] 0 solved master
[2022-12-12 08:21:16.811254: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 0 solved
[2022-12-12 08:21:16.811283: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 0 initing device 0
[2022-12-12 08:21:16.811961: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-12 08:21:16.812021: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 08:21:16.813124: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 08:21:16.813791: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 08:21:16.826637: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 6 solved
[2022-12-12 08:21:16.826711: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 6 initing device 6
[2022-12-12 08:21:16.826921: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 5 solved
[2022-12-12 08:21:16.826986: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 5 initing device 5
[2022-12-12 08:21:16.826994: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 3 solved
[2022-12-12 08:21:16.827064: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 3 initing device 3
[2022-12-12 08:21:16.827181: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-12 08:21:16.827236: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 08:21:16.827414: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-12 08:21:16.827459: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 08:21:16.827520: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-12 08:21:16.827569: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 08:21:16.828220: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 7 solved
[2022-12-12 08:21:16[.2022-12-12 08:21:16828274.: 828261[E2022-12-12 08:21:16:  .E/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc828290 :: /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc205E:]  202worker 0 thread 7 initing device 7/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc] 
:2 solved202
] [4 solved[2022-12-12 08:21:16
2022-12-12 08:21:16..828398828418[: : 2022-12-12 08:21:16EE.  828444/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc: ::E202205 ] ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc1 solvedworker 0 thread 2 initing device 2:

205] [worker 0 thread 4 initing device 42022-12-12 08:21:16
.828548: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 1 initing device 1
[2022-12-12 08:21:16.828797: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-12 08:21:16.828839: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 08:21:16.828945: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8[
2022-12-12 08:21:16[.2022-12-12 08:21:16828975.: [828985E2022-12-12 08:21:16:  .E/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu829005 :: /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu1815E:]  1815Building Coll Cache with ... num gpu device is 8/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu] 
:Building Coll Cache with ... num gpu device is 81980
] eager alloc mem 381.47 MB
[2022-12-12 08:21:16.829112[: 2022-12-12 08:21:16E. 829120/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu: :E1980 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cueager alloc mem 381.47 MB:
1980] eager alloc mem 381.47 MB
[2022-12-12 08:21:16.829829: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 08:21:16.831402: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 08:21:16.831471: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 08:21:16.833377: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 08:21:16.833609: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 08:21:16.833660: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 08:21:16.833725: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 08:21:16.834333: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 08:21:16.835277: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 08:21:16.835832: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 08:21:16.837708: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 08:21:16.837883: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 08:21:16.837938: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 08:21:16.838069: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 08:21:16.892099: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1024.00 Bytes
[2022-12-12 08:21:16.897567: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-12 08:21:16.897703: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 08:21:16.898528: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 114.82 MB
[2022-12-12 08:21:16.899244: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 08:21:16.900376: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 08:21:16.900426: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 114.40 MB
[[[[2022-12-12 08:21:162022-12-12 08:21:162022-12-12 08:21:162022-12-12 08:21:16...915493915493.915488: : 915493: EE: E  E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:19801980:1980] ] 1980] eager alloc mem 1024.00 Byteseager alloc mem 1024.00 Bytes] eager alloc mem 1024.00 Bytes

eager alloc mem 1024.00 Bytes

[2022-12-12 08:21:16.917319: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1024.00 Bytes
[2022-12-12 08:21:16.921189: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1024.00 Bytes
[2022-12-12 08:21:16.921581: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-12 08:21:16.921674: E [/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc2022-12-12 08:21:16:.638921669] : eager release cuda mem 400000000E
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-12 08:21:16.921742: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638[] 2022-12-12 08:21:16eager release cuda mem 1024.
921781: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 08:21:16.[9218292022-12-12 08:21:16: .E921823 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccE: 638/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc] :eager release cuda mem 400000000638
] eager release cuda mem 1024
[2022-12-12 08:21:16.921919: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 08:21:16.922167: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1024.00 Bytes
[2022-12-12 08:21:16.922312: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-12 08:21:16.922386: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 08:21:16.922772: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 114.82 MB
[2022-12-12 08:21:16.923388: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 114.82 MB
[2022-12-12 08:21:16.924220: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 114.82 MB
[2022-12-12 08:21:16.924907: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 114.82 MB
[2022-12-12 08:21:16.931105: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 114.82 MB
[2022-12-12 08:21:16.931826: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 08:21:16.932196: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 08:21:16.932414: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 08:21:16.932463: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 08:21:16.932558: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 08:21:16.932937: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 08:21:16.932989: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 114.43 MB
[2022-12-12 08:21:16.933281: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 08:21:16.933328: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 114.28 MB
[2022-12-12 08:21:16.933457: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 08:21:16.933504: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 107.73 MB
[2022-12-12 08:21:16.933532: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc[:2022-12-12 08:21:16638.] 933537eager release cuda mem 625663: 
E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-12 08:21:16.933621: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 114.39 MB
[2022-12-12 08:21:16[.2022-12-12 08:21:16933646.: 933653E:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc :/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc638:] 638eager release cuda mem 625663] 
eager release cuda mem 400000000
[[2022-12-12 08:21:162022-12-12 08:21:16..933732933745: : EW  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc::63843] ] eager release cuda mem 1024WORKER[0] alloc host memory 114.40 MB

[2022-12-12 08:21:16.933856: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 08:21:16.934564: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 114.82 MB
[2022-12-12 08:21:16.935378: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 114.82 MB
[2022-12-12 08:21:16.935912: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 08:21:16.936098: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 08:21:16.937022: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 08:21:16.937070: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 114.33 MB
[2022-12-12 08:21:16.937192: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 08:21:16.937240: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 114.19 MB
[2022-12-12 08:21:16.977730: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 08:21:16.978378: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 08:21:16.978424: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 14.30 GB
[2022-12-12 08:21:17.  4704: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 08:21:17.  5341: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 08:21:17.  5387: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 13.47 GB
[2022-12-12 08:21:17.  6856: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 08:21:17.  7500: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 08:21:17.  7549: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 14.31 GB
[2022-12-12 08:21:17.  8196: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 08:21:17.  8257: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 08:21:17.  8820: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 08:21:17.  8862: E [/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu2022-12-12 08:21:17:.1980  8871] : eager alloc mem 14.31 GBE
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 08:21:17.  8926: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 14.30 GB
[2022-12-12 08:21:17.  9624: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 08:21:17.  9977: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 08:21:17. 10264: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 08:21:17. 10309: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 14.29 GB
[2022-12-12 08:21:17. 10587: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 08:21:17. 10632: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 14.30 GB
[2022-12-12 08:21:17. 10850: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 08:21:17. 11456: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 08:21:17. 11504: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 14.28 GB
[[[[[[[[2022-12-12 08:21:212022-12-12 08:21:212022-12-12 08:21:212022-12-12 08:21:212022-12-12 08:21:212022-12-12 08:21:212022-12-12 08:21:212022-12-12 08:21:21........531482531483531482531483531483531484531483531483: : : : : : : : EEEEEEEE        /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::::::::19261926192619261926192619261926] ] Device 5 init p2p of link 6] ] ] ] ] ] Device 6 init p2p of link 0
Device 4 init p2p of link 5Device 3 init p2p of link 2Device 7 init p2p of link 4Device 2 init p2p of link 1Device 0 init p2p of link 3Device 1 init p2p of link 7






[[2022-12-12 08:21:212022-12-12 08:21:21.[[[.532028[[2022-12-12 08:21:21[2022-12-12 08:21:212022-12-12 08:21:21532033: 2022-12-12 08:21:212022-12-12 08:21:21.2022-12-12 08:21:21..: E..532037.532045532039E 532044532044: 532050: :  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu: : E: EE/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:EE E  :1980  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu1980] /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::] eager alloc mem 611.00 KB::1980:19801980eager alloc mem 611.00 KB
19801980] 1980] ] 
] ] eager alloc mem 611.00 KB] eager alloc mem 611.00 KBeager alloc mem 611.00 KBeager alloc mem 611.00 KBeager alloc mem 611.00 KB
eager alloc mem 611.00 KB




[2022-12-12 08:21:21.533100: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 08:21:21.533150: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663[
[2022-12-12 08:21:212022-12-12 08:21:21..533170[533171: 2022-12-12 08:21:21: E.E [533191[ /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc[2022-12-12 08:21:21: 2022-12-12 08:21:21/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:2022-12-12 08:21:21.E.:638.533206 533210638] 533217: /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc: ] eager release cuda mem 625663: E:Eeager release cuda mem 625663
E 638 
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc] /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:eager release cuda mem 625663::638
638638] ] ] eager release cuda mem 625663eager release cuda mem 625663eager release cuda mem 625663


[2022-12-12 08:21:21.553899: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 6 init p2p of link 5
[2022-12-12 08:21:21.554076: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 08:21:21.555044: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 08:21:21.556690: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 3 init p2p of link 0
[2022-12-12 08:21:21.556845: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 08:21:21.556917: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 2
[2022-12-12 08:21:21.557066: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 08:21:21.557320: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 5 init p2p of link 4
[2022-12-12 08:21:21.557495: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 08:21:21.557789: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 08:21:21.557869: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 0 init p2p of link 6
[2022-12-12 08:21:21.557998: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 08:21:21.558035: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 08:21:21.558386: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 4 init p2p of link 7
[2022-12-12 08:21:21.558490: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 08:21:21.558554: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 08:21:21.558735: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 2 init p2p of link 3
[2022-12-12 08:21:21.558893: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 08:21:21.558923: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 08:21:21.558956: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 7 init p2p of link 1
[2022-12-12 08:21:21.559110: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 08:21:21.559484: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 08:21:21.559797: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 08:21:21.559978: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 08:21:21.564793: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 6 init p2p of link 4
[2022-12-12 08:21:21.564933: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 08:21:21.565864: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 08:21:21.580384: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 7 init p2p of link 6
[2022-12-12 08:21:21.580506: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 08:21:21.580720: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 3
[2022-12-12 08:21:21.580822: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926[] 2022-12-12 08:21:21Device 3 init p2p of link 5.
580853: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 08:21:21.580954: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 08:21:21.581047: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 0 init p2p of link 1
[2022-12-12 08:21:21.581175: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 08:21:21.581424: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 08:21:21.581751: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 08:21:21.581850: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 08:21:21.582083: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 08:21:21.582113: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 2 init p2p of link 0
[2022-12-12 08:21:21.582237: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 08:21:21.582264: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 5 init p2p of link 7
[2022-12-12 08:21:21.[5823812022-12-12 08:21:21: .E582402 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE: 1926/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu] :Device 4 init p2p of link 21980
] eager alloc mem 611.00 KB
[2022-12-12 08:21:21.582527: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 08:21:21.583169: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 08:21:21.583325: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 08:21:21.583436: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 08:21:21.590893: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 6 init p2p of link 7
[2022-12-12 08:21:21.591019: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 08:21:21.591913: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 08:21:21.610390: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 3 init p2p of link 1
[2022-12-12 08:21:21.610512: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 08:21:21.611452: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 08:21:21.611895: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 0
[2022-12-12 08:21:21.612010: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 08:21:21.612756: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 7 init p2p of link 5
[2022-12-12 08:21:21.612874: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 08:21:21.612937: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 08:21:21.613144: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 4 init p2p of link 6
[2022-12-12 08:21:21.613272: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 08:21:21.613427: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 0 init p2p of link 2
[2022-12-12 08:21:21.613551: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 08:21:21.613822: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 08:21:21.613934: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 5 init p2p of link 3
[2022-12-12 08:21:21.614062: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 08:21:21.614158: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 08:21:21.614467: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 08:21:21.614904: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 08:21:21.615552: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 2 init p2p of link 4
[2022-12-12 08:21:21.615643: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638[] 2022-12-12 08:21:21eager release cuda mem 120400004.
615674: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 08:21:21.616550: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 08:21:21.621161: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 29997042 / 100000000 nodes ( 30.00 %~30.00 %) | remote 70002958 / 100000000 nodes ( 70.00 %) | cpu 0 / 100000000 nodes ( 0.00 %) | 14.31 GB | 4.79394 secs 
[2022-12-12 08:21:21.638130: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 120400004
[2022-12-12 08:21:21.639885: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 120400004
[2022-12-12 08:21:21.641416: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 120400004
[2022-12-12 08:21:21.641583: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 120400004
[2022-12-12 08:21:21.642179: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 120400004
[2022-12-12 08:21:21.643987: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 120400004
[2022-12-12 08:21:21.644251: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 120400004
[2022-12-12 08:21:21.660678: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 29987827 / 100000000 nodes ( 29.99 %~30.00 %) | remote 70012173 / 100000000 nodes ( 70.01 %) | cpu 0 / 100000000 nodes ( 0.00 %) | 14.30 GB | 4.83312 secs 
[2022-12-12 08:21:21.660889: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 29972217 / 100000000 nodes ( 29.97 %~30.00 %) | remote 70027783 / 100000000 nodes ( 70.03 %) | cpu 0 / 100000000 nodes ( 0.00 %) | 14.30 GB | 4.83344 secs 
[2022-12-12 08:21:21.661104: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 28240741 / 100000000 nodes ( 28.24 %~30.00 %) | remote 71759259 / 100000000 nodes ( 71.76 %) | cpu 0 / 100000000 nodes ( 0.00 %) | 13.47 GB | 4.83211 secs 
[2022-12-12 08:21:21.661317: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 29933920 / 100000000 nodes ( 29.93 %~30.00 %) | remote 70066080 / 100000000 nodes ( 70.07 %) | cpu 0 / 100000000 nodes ( 0.00 %) | 14.28 GB | 4.83221 secs 
[2022-12-12 08:21:21.661538: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 29989852 / 100000000 nodes ( 29.99 %~30.00 %) | remote 70010148 / 100000000 nodes ( 70.01 %) | cpu 0 / 100000000 nodes ( 0.00 %) | 14.31 GB | 4.83242 secs 
[2022-12-12 08:21:21.661750: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 29988939 / 100000000 nodes ( 29.99 %~30.00 %) | remote 70011061 / 100000000 nodes ( 70.01 %) | cpu 0 / 100000000 nodes ( 0.00 %) | 14.30 GB | 4.84974 secs 
[2022-12-12 08:21:21.662002: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 29958957 / 100000000 nodes ( 29.96 %~30.00 %) | remote 70041043 / 100000000 nodes ( 70.04 %) | cpu 0 / 100000000 nodes ( 0.00 %) | 14.29 GB | 4.83317 secs 
[2022-12-12 08:21:21.662276: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 22.62 GB
[2022-12-12 08:21:23.402156: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 22.89 GB
[2022-12-12 08:21:23.404492: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 22.89 GB
[2022-12-12 08:21:23.405625: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 22.89 GB
[2022-12-12 08:21:24.697116: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 23.15 GB
[2022-12-12 08:21:24.697998: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 23.15 GB
[2022-12-12 08:21:24.698803: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 23.15 GB
[2022-12-12 08:21:25.650284: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 23.37 GB
[2022-12-12 08:21:25.650885: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 23.37 GB
[2022-12-12 08:21:25.651239: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 23.37 GB
[2022-12-12 08:21:26.995865: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 23.58 GB
[2022-12-12 08:21:26.996031: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 23.58 GB
[2022-12-12 08:21:26.997626: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 23.58 GB
[2022-12-12 08:21:28. 67649: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 24.04 GB
[2022-12-12 08:21:28. 69084: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 24.04 GB
[2022-12-12 08:21:28. 70730: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 24.04 GB
[2022-12-12 08:21:29.768443: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 24.24 GB
[2022-12-12 08:21:29.769164: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 24.24 GB
[HCTR][08:21:31.401][ERROR][RK0][tid #140114969745152]: replica 5 calling init per replica done, doing barrier
[HCTR][08:21:31.401][ERROR][RK0][main]: replica 0 calling init per replica done, doing barrier
[HCTR][08:21:31.401][ERROR][RK0][main]: replica 4 calling init per replica done, doing barrier
[HCTR][08:21:31.401][ERROR][RK0][main]: replica 6 calling init per replica done, doing barrier
[HCTR][08:21:31.401][ERROR][RK0][tid #140114692916992]: replica 7 calling init per replica done, doing barrier
[HCTR][08:21:31.401][ERROR][RK0][main]: replica 3 calling init per replica done, doing barrier
[HCTR][08:21:31.401][ERROR][RK0][tid #140114961352448]: replica 2 calling init per replica done, doing barrier
[HCTR][08:21:31.401][ERROR][RK0][tid #140114642593536]: replica 1 calling init per replica done, doing barrier
[HCTR][08:21:31.401][ERROR][RK0][tid #140114969745152]: replica 5 calling init per replica done, doing barrier done
[HCTR][08:21:31.401][ERROR][RK0][tid #140114692916992]: replica 7 calling init per replica done, doing barrier done
[HCTR][08:21:31.401][ERROR][RK0][main]: replica 3 calling init per replica done, doing barrier done
[HCTR][08:21:31.401][ERROR][RK0][main]: replica 0 calling init per replica done, doing barrier done
[HCTR][08:21:31.401][ERROR][RK0][main]: replica 4 calling init per replica done, doing barrier done
[HCTR][08:21:31.401][ERROR][RK0][tid #140114961352448]: replica 2 calling init per replica done, doing barrier done
[HCTR][08:21:31.401][ERROR][RK0][main]: replica 6 calling init per replica done, doing barrier done
[HCTR][08:21:31.401][ERROR][RK0][tid #140114642593536]: replica 1 calling init per replica done, doing barrier done
[HCTR][08:21:31.401][ERROR][RK0][tid #140114969745152]: init per replica done
[HCTR][08:21:31.401][ERROR][RK0][tid #140114692916992]: init per replica done
[HCTR][08:21:31.401][ERROR][RK0][main]: init per replica done
[HCTR][08:21:31.401][ERROR][RK0][main]: init per replica done
[HCTR][08:21:31.401][ERROR][RK0][tid #140114961352448]: init per replica done
[HCTR][08:21:31.401][ERROR][RK0][main]: init per replica done
[HCTR][08:21:31.401][ERROR][RK0][tid #140114642593536]: init per replica done
[HCTR][08:21:31.405][ERROR][RK0][main]: init per replica done
[HCTR][08:21:31.440][ERROR][RK0][tid #140114692916992]: 7 allocated 3276800 at 0x7f4fbc238400
[HCTR][08:21:31.440][ERROR][RK0][tid #140114692916992]: 7 allocated 6553600 at 0x7f4fbc558400
[HCTR][08:21:31.440][ERROR][RK0][tid #140114692916992]: 7 allocated 3276800 at 0x7f4fbcb98400
[HCTR][08:21:31.440][ERROR][RK0][tid #140114692916992]: 7 allocated 6553600 at 0x7f4fbceb8400
[HCTR][08:21:31.440][ERROR][RK0][main]: 4 allocated 3276800 at 0x7f4edc238400
[HCTR][08:21:31.440][ERROR][RK0][main]: 6 allocated 3276800 at 0x7f501c238400
[HCTR][08:21:31.440][ERROR][RK0][main]: 4 allocated 6553600 at 0x7f4edc558400
[HCTR][08:21:31.440][ERROR][RK0][main]: 6 allocated 6553600 at 0x7f501c558400
[HCTR][08:21:31.440][ERROR][RK0][main]: 4 allocated 3276800 at 0x7f4edcb98400
[HCTR][08:21:31.440][ERROR][RK0][main]: 6 allocated 3276800 at 0x7f501cb98400
[HCTR][08:21:31.440][ERROR][RK0][main]: 4 allocated 6553600 at 0x7f4edceb8400
[HCTR][08:21:31.440][ERROR][RK0][main]: 6 allocated 6553600 at 0x7f501ceb8400
[HCTR][08:21:31.440][ERROR][RK0][main]: 5 allocated 3276800 at 0x7f501c238400
[HCTR][08:21:31.440][ERROR][RK0][main]: 5 allocated 6553600 at 0x7f501c558400
[HCTR][08:21:31.440][ERROR][RK0][main]: 5 allocated 3276800 at 0x7f501cb98400
[HCTR][08:21:31.440][ERROR][RK0][main]: 5 allocated 6553600 at 0x7f501ceb8400
[HCTR][08:21:31.440][ERROR][RK0][main]: 2 allocated 3276800 at 0x7f4fe0238400
[HCTR][08:21:31.440][ERROR][RK0][main]: 2 allocated 6553600 at 0x7f4fe0558400
[HCTR][08:21:31.440][ERROR][RK0][main]: 2 allocated 3276800 at 0x7f4fe0b98400
[HCTR][08:21:31.440][ERROR][RK0][main]: 2 allocated 6553600 at 0x7f4fe0eb8400
[HCTR][08:21:31.440][ERROR][RK0][main]: 1 allocated 3276800 at 0x7f501c238400
[HCTR][08:21:31.440][ERROR][RK0][main]: 1 allocated 6553600 at 0x7f501c558400
[HCTR][08:21:31.440][ERROR][RK0][main]: 1 allocated 3276800 at 0x7f501cb98400
[HCTR][08:21:31.440][ERROR][RK0][tid #140114701309696]: 3 allocated 3276800 at 0x7f4ed0238400
[HCTR][08:21:31.440][ERROR][RK0][main]: 1 allocated 6553600 at 0x7f501ceb8400
[HCTR][08:21:31.440][ERROR][RK0][tid #140114701309696]: 3 allocated 6553600 at 0x7f4ed0558400
[HCTR][08:21:31.440][ERROR][RK0][tid #140114701309696]: 3 allocated 3276800 at 0x7f4ed0b98400
[HCTR][08:21:31.440][ERROR][RK0][tid #140114701309696]: 3 allocated 6553600 at 0x7f4ed0eb8400
[HCTR][08:21:31.443][ERROR][RK0][tid #140114835527424]: 0 allocated 3276800 at 0x7f4fe0320000
[HCTR][08:21:31.443][ERROR][RK0][tid #140114835527424]: 0 allocated 6553600 at 0x7f4fe0640000
[HCTR][08:21:31.443][ERROR][RK0][tid #140114835527424]: 0 allocated 3276800 at 0x7f4fe0c80000
[HCTR][08:21:31.443][ERROR][RK0][tid #140114835527424]: 0 allocated 6553600 at 0x7f4fe0fa0000
