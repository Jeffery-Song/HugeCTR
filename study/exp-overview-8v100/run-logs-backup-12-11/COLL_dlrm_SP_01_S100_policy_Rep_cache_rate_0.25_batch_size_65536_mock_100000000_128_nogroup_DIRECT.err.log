2022-12-12 06:17:49.298548: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.306677: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.313292: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.318752: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.324648: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.337258: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.346251: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.351503: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.406682: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.406751: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.415711: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.416110: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.419369: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.419522: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.420975: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.421383: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.422366: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.423155: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.424007: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.424903: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.426269: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.427446: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.427801: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.429011: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.429319: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.430842: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.431746: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.432762: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.433667: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.434659: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.435628: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.436669: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.438508: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.439654: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.440572: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.441571: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.442613: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.443675: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.444627: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.445557: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.450708: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 06:17:49.451752: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.452772: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.453893: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.455507: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.456859: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.456902: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.458309: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.458750: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.459741: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.460118: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.461050: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.462134: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.462723: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.463468: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.463575: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.464392: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.466111: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.466282: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.468812: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.468959: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.469512: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.471482: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.471674: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.472423: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.474518: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.474737: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.475514: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.475640: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.476544: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.478231: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.478828: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.479288: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.480265: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.481459: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.481931: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.482614: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.483604: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.484610: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.484865: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.485982: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.487194: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.488219: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.488885: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.489143: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.490057: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.490655: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.491650: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.491741: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.492637: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.492971: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.505413: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.507306: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.507363: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.508677: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.509267: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.509394: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.509864: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.510937: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.511900: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.522237: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.532249: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.546230: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.547475: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.548188: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.548434: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.548617: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.549510: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.549606: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.551377: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.552495: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.552834: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.553013: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.553734: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.553876: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.556540: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.557308: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.557381: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.558357: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.558983: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.559104: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.562086: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.562196: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.562235: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.562959: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.563124: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.566584: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.566631: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.566673: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.567410: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.567519: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.570493: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.570586: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.570675: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.571215: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.571416: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.574154: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.574196: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.574798: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.574919: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.574997: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.577286: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.577329: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.578332: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.578458: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.578488: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.580661: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.580699: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.581606: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.581892: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.582025: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.584162: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.584243: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.585269: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.585595: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.585961: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.587714: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.587758: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.588603: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.588780: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.589461: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.590267: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.591166: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.591213: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.592276: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.592464: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.593134: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.594095: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.594938: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.595078: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.596264: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.596479: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.597026: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.598290: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.598831: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.598927: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.600049: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.600237: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.600865: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.602021: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.602602: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.602646: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.604044: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.604662: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.605222: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.605521: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.607488: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.607507: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.607575: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.608022: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.608680: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.608921: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.609165: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 06:17:49.611142: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.611198: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.611381: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.611704: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.612364: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.612665: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.615170: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.615497: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.615511: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.615647: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.616336: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.616646: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.618568: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.618848: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.619332: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.619397: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.619518: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.620391: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.620632: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.623029: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.623533: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.623936: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.624113: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.624299: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.625181: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.625517: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.627525: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.628116: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.628530: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.628550: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.628998: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.629521: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.629913: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.632611: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.632985: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.633073: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.633413: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.633856: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.634415: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.637402: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.637660: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.638068: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.638654: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.639222: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.642044: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 06:17:49.642236: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.642476: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.642832: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.644897: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 06:17:49.645232: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 06:17:49.645815: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.646003: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.648293: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.648764: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.649359: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 06:17:49.650585: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.650934: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.651906: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.653120: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.653696: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.653918: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.654505: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.655167: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.656916: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.657802: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.658256: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.658621: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.658658: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.659465: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.661446: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.662624: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.662949: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.663026: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.664331: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.666607: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.678779: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.680752: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.683613: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.715398: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.717600: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.720962: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.750565: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.754533: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.756223: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.762888: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.764448: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 06:17:49.769528: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.773394: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.774537: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.779424: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.784106: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 06:17:49.792867: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.798008: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.814183: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:49.821614: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:50.803443: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:50.804297: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:50.805186: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:50.805679: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 06:17:50.805740: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-12 06:17:50.824568: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:50.825212: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:50.826172: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:50.826771: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:50.827317: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:50.827933: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-12 06:17:50.873400: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 06:17:50.873604: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 06:17:50.911761: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12340
2022-12-12 06:17:51.010282: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:51.011281: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:51.011816: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:51.012295: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 06:17:51.012354: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-12 06:17:51.031029: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:51.031680: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:51.032198: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:51.032771: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:51.033637: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:51.034256: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:1/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-12 06:17:51.076187: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:51.076815: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:51.077436: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:51.077922: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 06:17:51.077980: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-12 06:17:51.090007: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:51.090642: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:51.091197: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:51.091668: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 06:17:51.091726: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-12 06:17:51.094290: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:51.094888: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:51.095435: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:51.095535: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:51.096391: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 06:17:51.096470: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-12 06:17:51.096748: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:51.097346: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:51.098204: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:51.099007: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:51.099520: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:2/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-12 06:17:51.105633: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:51.106216: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:51.106734: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:51.107215: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 06:17:51.107266: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-12 06:17:51.109630: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:51.110269: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:51.110778: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:51.111626: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:51.112145: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:51.112627: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:4/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-12 06:17:51.113975: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 06:17:51.114121: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 06:17:51.114436: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:51.115021: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:51.115424: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12341
2022-12-12 06:17:51.115554: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:51.116116: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:51.116847: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:51.117326: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:5/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-12 06:17:51.124997: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:51.125614: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:51.126112: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:51.126687: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:51.127221: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:51.128125: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:6/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-12 06:17:51.144256: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:51.144878: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:51.145410: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:51.145874: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 06:17:51.145930: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-12 06:17:51.154309: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:51.154984: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:51.155577: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:51.156080: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 06:17:51.156143: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-12 06:17:51.158813: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 06:17:51.159006: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 06:17:51.160920: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12344
2022-12-12 06:17:51.164021: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 06:17:51.164150: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:51.164157: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 06:17:51.164807: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:51.165331: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:51.165914: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:51.166013: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12345
2022-12-12 06:17:51.166449: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:51.166925: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:7/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-12 06:17:51.173996: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 06:17:51.174070: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:51.174134: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 06:17:51.174742: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:51.175280: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:51.175859: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:51.176009: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12346
2022-12-12 06:17:51.176385: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:17:51.176857: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:3/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-12 06:17:51.187645: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 06:17:51.187775: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 06:17:51.189428: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12342
2022-12-12 06:17:51.212042: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 06:17:51.212230: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 06:17:51.213937: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12347
2022-12-12 06:17:51.222270: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 06:17:51.222444: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 06:17:51.224281: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12343
[HCTR][06:17:52.494][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][06:17:52.494][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][06:17:52.494][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][06:17:52.494][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][06:17:52.494][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][06:17:52.494][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][06:17:52.494][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][06:17:52.494][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 1it [00:01,  1.53s/it]warmup run: 1it [00:01,  1.57s/it]warmup run: 1it [00:01,  1.51s/it]warmup run: 1it [00:01,  1.54s/it]warmup run: 1it [00:01,  1.51s/it]warmup run: 99it [00:01, 84.41it/s]warmup run: 92it [00:01, 76.25it/s]warmup run: 100it [00:01, 86.03it/s]warmup run: 73it [00:01, 61.88it/s]warmup run: 103it [00:01, 88.77it/s]warmup run: 201it [00:01, 186.20it/s]warmup run: 185it [00:01, 166.85it/s]warmup run: 202it [00:01, 188.44it/s]warmup run: 171it [00:01, 161.39it/s]warmup run: 1it [00:01,  1.53s/it]warmup run: 206it [00:01, 192.09it/s]warmup run: 305it [00:01, 300.90it/s]warmup run: 281it [00:01, 271.05it/s]warmup run: 303it [00:01, 299.70it/s]warmup run: 1it [00:01,  1.48s/it]warmup run: 271it [00:01, 273.58it/s]warmup run: 1it [00:01,  1.48s/it]warmup run: 101it [00:01, 85.92it/s]warmup run: 309it [00:01, 305.67it/s]warmup run: 408it [00:01, 418.13it/s]warmup run: 379it [00:01, 382.87it/s]warmup run: 404it [00:01, 414.65it/s]warmup run: 98it [00:01, 85.91it/s]warmup run: 370it [00:01, 388.53it/s]warmup run: 94it [00:01, 82.40it/s]warmup run: 201it [00:01, 185.07it/s]warmup run: 412it [00:01, 423.21it/s]warmup run: 508it [00:02, 526.82it/s]warmup run: 477it [00:02, 492.41it/s]warmup run: 505it [00:02, 525.98it/s]warmup run: 197it [00:01, 186.65it/s]warmup run: 464it [00:02, 491.19it/s]warmup run: 189it [00:01, 179.04it/s]warmup run: 300it [00:01, 292.88it/s]warmup run: 515it [00:02, 537.39it/s]warmup run: 610it [00:02, 630.77it/s]warmup run: 576it [00:02, 594.97it/s]warmup run: 604it [00:02, 623.08it/s]warmup run: 297it [00:01, 298.25it/s]warmup run: 557it [00:02, 582.85it/s]warmup run: 284it [00:01, 284.63it/s]warmup run: 400it [00:01, 406.05it/s]warmup run: 621it [00:02, 646.98it/s]warmup run: 712it [00:02, 720.69it/s]warmup run: 675it [00:02, 685.31it/s]warmup run: 703it [00:02, 708.02it/s]warmup run: 397it [00:01, 413.33it/s]warmup run: 650it [00:02, 662.71it/s]warmup run: 379it [00:01, 393.10it/s]warmup run: 500it [00:02, 516.59it/s]warmup run: 726it [00:02, 738.97it/s]warmup run: 815it [00:02, 795.83it/s]warmup run: 802it [00:02, 776.97it/s]warmup run: 496it [00:01, 522.27it/s]warmup run: 744it [00:02, 731.05it/s]warmup run: 770it [00:02, 717.40it/s]warmup run: 473it [00:01, 496.47it/s]warmup run: 601it [00:02, 619.61it/s]warmup run: 830it [00:02, 812.52it/s]warmup run: 916it [00:02, 851.07it/s]warmup run: 903it [00:02, 837.72it/s]warmup run: 597it [00:02, 625.33it/s]warmup run: 838it [00:02, 785.05it/s]warmup run: 871it [00:02, 790.08it/s]warmup run: 568it [00:02, 592.10it/s]warmup run: 700it [00:02, 703.83it/s]warmup run: 933it [00:02, 869.08it/s]warmup run: 1017it [00:02, 891.95it/s]warmup run: 1005it [00:02, 886.70it/s]warmup run: 698it [00:02, 713.98it/s]warmup run: 931it [00:02, 824.40it/s]warmup run: 975it [00:02, 854.17it/s]warmup run: 669it [00:02, 688.62it/s]warmup run: 799it [00:02, 773.28it/s]warmup run: 1037it [00:02, 914.49it/s]warmup run: 1119it [00:02, 926.26it/s]warmup run: 1107it [00:02, 923.25it/s]warmup run: 800it [00:02, 789.22it/s]warmup run: 1029it [00:02, 865.95it/s]warmup run: 1078it [00:02, 900.86it/s]warmup run: 773it [00:02, 775.93it/s]warmup run: 897it [00:02, 826.78it/s]warmup run: 1140it [00:02, 944.78it/s]warmup run: 1221it [00:02, 950.61it/s]warmup run: 1208it [00:02, 941.60it/s]warmup run: 904it [00:02, 854.94it/s]warmup run: 1124it [00:02, 888.96it/s]warmup run: 1180it [00:02, 932.33it/s]warmup run: 876it [00:02, 842.51it/s]warmup run: 995it [00:02, 867.06it/s]warmup run: 1245it [00:02, 972.91it/s]warmup run: 1322it [00:02, 962.38it/s]warmup run: 1309it [00:02, 961.20it/s]warmup run: 1007it [00:02, 900.75it/s]warmup run: 1219it [00:02, 901.84it/s]warmup run: 1285it [00:02, 964.86it/s]warmup run: 980it [00:02, 894.27it/s]warmup run: 1096it [00:02, 905.77it/s]warmup run: 1349it [00:02, 991.81it/s]warmup run: 1424it [00:02, 977.35it/s]warmup run: 1411it [00:02, 975.39it/s]warmup run: 1110it [00:02, 935.50it/s]warmup run: 1313it [00:02, 909.86it/s]warmup run: 1391it [00:03, 990.64it/s]warmup run: 1084it [00:02, 933.53it/s]warmup run: 1198it [00:02, 936.42it/s]warmup run: 1453it [00:02, 1005.81it/s]warmup run: 1525it [00:03, 976.94it/s]warmup run: 1513it [00:03, 985.93it/s]warmup run: 1212it [00:02, 950.46it/s]warmup run: 1497it [00:03, 1010.53it/s]warmup run: 1407it [00:03, 914.07it/s]warmup run: 1188it [00:02, 961.65it/s]warmup run: 1298it [00:02, 950.99it/s]warmup run: 1557it [00:03, 1013.65it/s]warmup run: 1627it [00:03, 987.58it/s]warmup run: 1616it [00:03, 996.91it/s]warmup run: 1313it [00:02, 965.87it/s]warmup run: 1505it [00:03, 931.55it/s]warmup run: 1601it [00:03, 1006.13it/s]warmup run: 1290it [00:02, 970.56it/s]warmup run: 1398it [00:02, 961.48it/s]warmup run: 1661it [00:03, 1016.53it/s]warmup run: 1728it [00:03, 994.06it/s]warmup run: 1718it [00:03, 999.46it/s]warmup run: 1414it [00:02, 971.02it/s]warmup run: 1604it [00:03, 947.65it/s]warmup run: 1707it [00:03, 1020.27it/s]warmup run: 1393it [00:02, 985.55it/s]warmup run: 1499it [00:03, 974.04it/s]warmup run: 1765it [00:03, 1017.68it/s]warmup run: 1829it [00:03, 998.26it/s]warmup run: 1819it [00:03, 993.32it/s]warmup run: 1704it [00:03, 960.66it/s]warmup run: 1514it [00:02, 969.49it/s]warmup run: 1813it [00:03, 1030.06it/s]warmup run: 1496it [00:02, 998.04it/s]warmup run: 1601it [00:03, 985.88it/s]warmup run: 1869it [00:03, 1021.74it/s]warmup run: 1931it [00:03, 1003.11it/s]warmup run: 1920it [00:03, 988.79it/s]warmup run: 1804it [00:03, 969.52it/s]warmup run: 1918it [00:03, 1034.02it/s]warmup run: 1613it [00:03, 967.51it/s]warmup run: 1600it [00:03, 1009.49it/s]warmup run: 1705it [00:03, 1000.31it/s]warmup run: 1972it [00:03, 1024.04it/s]warmup run: 2039it [00:03, 1024.05it/s]warmup run: 2023it [00:03, 998.80it/s]warmup run: 1904it [00:03, 976.53it/s]warmup run: 2024it [00:03, 1040.08it/s]warmup run: 1712it [00:03, 967.64it/s]warmup run: 1706it [00:03, 1023.77it/s]warmup run: 1809it [00:03, 1011.27it/s]warmup run: 2086it [00:03, 1058.02it/s]warmup run: 2162it [00:03, 1083.14it/s]warmup run: 2142it [00:03, 1054.25it/s]warmup run: 2006it [00:03, 988.12it/s]warmup run: 2148it [00:03, 1097.02it/s]warmup run: 1813it [00:03, 978.01it/s]warmup run: 1810it [00:03, 1027.65it/s]warmup run: 1913it [00:03, 1019.11it/s]warmup run: 2205it [00:03, 1095.72it/s]warmup run: 2285it [00:03, 1124.37it/s]warmup run: 2260it [00:03, 1091.23it/s]warmup run: 2127it [00:03, 1053.01it/s]warmup run: 2271it [00:03, 1136.45it/s]warmup run: 1913it [00:03, 983.88it/s]warmup run: 1914it [00:03, 1030.43it/s]warmup run: 2019it [00:03, 1030.83it/s]warmup run: 2326it [00:03, 1128.49it/s]warmup run: 2408it [00:03, 1154.00it/s]warmup run: 2379it [00:03, 1117.94it/s]warmup run: 2248it [00:03, 1098.55it/s]warmup run: 2395it [00:03, 1165.25it/s]warmup run: 2015it [00:03, 993.80it/s]warmup run: 2019it [00:03, 1035.78it/s]warmup run: 2140it [00:03, 1083.90it/s]warmup run: 2449it [00:03, 1158.73it/s]warmup run: 2531it [00:03, 1175.11it/s]warmup run: 2498it [00:03, 1137.30it/s]warmup run: 2370it [00:03, 1132.19it/s]warmup run: 2519it [00:04, 1185.96it/s]warmup run: 2137it [00:03, 1058.55it/s]warmup run: 2140it [00:03, 1085.94it/s]warmup run: 2262it [00:03, 1121.94it/s]warmup run: 2572it [00:03, 1179.99it/s]warmup run: 2653it [00:04, 1185.89it/s]warmup run: 2618it [00:04, 1155.77it/s]warmup run: 2492it [00:04, 1155.83it/s]warmup run: 2641it [00:04, 1195.80it/s]warmup run: 2259it [00:03, 1104.97it/s]warmup run: 2261it [00:03, 1121.11it/s]warmup run: 2383it [00:03, 1148.09it/s]warmup run: 2694it [00:04, 1190.28it/s]warmup run: 2776it [00:04, 1196.52it/s]warmup run: 2737it [00:04, 1165.31it/s]warmup run: 2614it [00:04, 1173.12it/s]warmup run: 2764it [00:04, 1205.69it/s]warmup run: 2381it [00:03, 1138.32it/s]warmup run: 2382it [00:03, 1145.79it/s]warmup run: 2505it [00:03, 1166.84it/s]warmup run: 2817it [00:04, 1201.45it/s]warmup run: 2898it [00:04, 1203.43it/s]warmup run: 2857it [00:04, 1175.43it/s]warmup run: 2735it [00:04, 1182.22it/s]warmup run: 2887it [00:04, 1212.86it/s]warmup run: 2502it [00:03, 1158.97it/s]warmup run: 2503it [00:03, 1162.41it/s]warmup run: 2627it [00:04, 1180.25it/s]warmup run: 2940it [00:04, 1207.69it/s]warmup run: 3000it [00:04, 691.66it/s] warmup run: 2978it [00:04, 1183.52it/s]warmup run: 3000it [00:04, 680.57it/s] warmup run: 2857it [00:04, 1191.06it/s]warmup run: 2621it [00:04, 1166.33it/s]warmup run: 3000it [00:04, 701.12it/s] warmup run: 3000it [00:04, 688.40it/s] warmup run: 2622it [00:03, 1170.61it/s]warmup run: 2747it [00:04, 1185.48it/s]warmup run: 2977it [00:04, 1187.50it/s]warmup run: 2739it [00:04, 1167.95it/s]warmup run: 2741it [00:04, 1174.29it/s]warmup run: 2867it [00:04, 1188.32it/s]warmup run: 3000it [00:04, 669.75it/s] warmup run: 2859it [00:04, 1176.11it/s]warmup run: 2861it [00:04, 1179.65it/s]warmup run: 2988it [00:04, 1193.69it/s]warmup run: 3000it [00:04, 687.45it/s] warmup run: 2980it [00:04, 1184.83it/s]warmup run: 2981it [00:04, 1183.97it/s]warmup run: 3000it [00:04, 692.75it/s] warmup run: 3000it [00:04, 694.31it/s] 

warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]


warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]

warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   6%|▌         | 166/3000 [00:00<00:01, 1659.57it/s]warmup should be done:   6%|▌         | 165/3000 [00:00<00:01, 1647.81it/s]warmup should be done:   5%|▌         | 161/3000 [00:00<00:01, 1606.31it/s]warmup should be done:   5%|▌         | 161/3000 [00:00<00:01, 1608.79it/s]warmup should be done:   6%|▌         | 166/3000 [00:00<00:01, 1659.83it/s]warmup should be done:   6%|▌         | 165/3000 [00:00<00:01, 1644.57it/s]warmup should be done:   5%|▌         | 164/3000 [00:00<00:01, 1632.97it/s]warmup should be done:   6%|▌         | 165/3000 [00:00<00:01, 1640.32it/s]warmup should be done:  11%|█         | 324/3000 [00:00<00:01, 1620.24it/s]warmup should be done:  11%|█         | 331/3000 [00:00<00:01, 1652.60it/s]warmup should be done:  11%|█         | 334/3000 [00:00<00:01, 1667.83it/s]warmup should be done:  11%|█         | 335/3000 [00:00<00:01, 1671.70it/s]warmup should be done:  11%|█         | 333/3000 [00:00<00:01, 1662.23it/s]warmup should be done:  11%|█         | 331/3000 [00:00<00:01, 1651.50it/s]warmup should be done:  11%|█         | 329/3000 [00:00<00:01, 1638.87it/s]warmup should be done:  11%|█         | 322/3000 [00:00<00:01, 1588.77it/s]warmup should be done:  16%|█▌        | 487/3000 [00:00<00:01, 1621.37it/s]warmup should be done:  17%|█▋        | 502/3000 [00:00<00:01, 1670.33it/s]warmup should be done:  17%|█▋        | 500/3000 [00:00<00:01, 1662.89it/s]warmup should be done:  17%|█▋        | 503/3000 [00:00<00:01, 1671.39it/s]warmup should be done:  16%|█▋        | 493/3000 [00:00<00:01, 1636.65it/s]warmup should be done:  17%|█▋        | 497/3000 [00:00<00:01, 1647.50it/s]warmup should be done:  17%|█▋        | 497/3000 [00:00<00:01, 1650.52it/s]warmup should be done:  16%|█▌        | 481/3000 [00:00<00:01, 1580.71it/s]warmup should be done:  22%|██▏       | 650/3000 [00:00<00:01, 1620.01it/s]warmup should be done:  22%|██▏       | 657/3000 [00:00<00:01, 1636.30it/s]warmup should be done:  22%|██▏       | 671/3000 [00:00<00:01, 1672.21it/s]warmup should be done:  22%|██▏       | 670/3000 [00:00<00:01, 1669.47it/s]warmup should be done:  22%|██▏       | 663/3000 [00:00<00:01, 1652.53it/s]warmup should be done:  22%|██▏       | 667/3000 [00:00<00:01, 1662.01it/s]warmup should be done:  22%|██▏       | 662/3000 [00:00<00:01, 1645.42it/s]warmup should be done:  21%|██▏       | 640/3000 [00:00<00:01, 1573.36it/s]warmup should be done:  27%|██▋       | 821/3000 [00:00<00:01, 1635.45it/s]warmup should be done:  28%|██▊       | 839/3000 [00:00<00:01, 1670.40it/s]warmup should be done:  28%|██▊       | 829/3000 [00:00<00:01, 1650.89it/s]warmup should be done:  27%|██▋       | 813/3000 [00:00<00:01, 1617.53it/s]warmup should be done:  28%|██▊       | 827/3000 [00:00<00:01, 1643.70it/s]warmup should be done:  28%|██▊       | 834/3000 [00:00<00:01, 1659.17it/s]warmup should be done:  28%|██▊       | 837/3000 [00:00<00:01, 1642.95it/s]warmup should be done:  27%|██▋       | 798/3000 [00:00<00:01, 1554.89it/s]warmup should be done:  33%|███▎      | 985/3000 [00:00<00:01, 1635.07it/s]warmup should be done:  33%|███▎      | 995/3000 [00:00<00:01, 1648.39it/s]warmup should be done:  34%|███▎      | 1007/3000 [00:00<00:01, 1666.95it/s]warmup should be done:  33%|███▎      | 1000/3000 [00:00<00:01, 1653.33it/s]warmup should be done:  33%|███▎      | 992/3000 [00:00<00:01, 1638.10it/s]warmup should be done:  32%|███▎      | 975/3000 [00:00<00:01, 1608.56it/s]warmup should be done:  33%|███▎      | 1002/3000 [00:00<00:01, 1617.76it/s]warmup should be done:  32%|███▏      | 954/3000 [00:00<00:01, 1538.17it/s]warmup should be done:  38%|███▊      | 1149/3000 [00:00<00:01, 1631.19it/s]warmup should be done:  39%|███▉      | 1174/3000 [00:00<00:01, 1667.52it/s]warmup should be done:  39%|███▊      | 1161/3000 [00:00<00:01, 1649.92it/s]warmup should be done:  39%|███▉      | 1166/3000 [00:00<00:01, 1653.96it/s]warmup should be done:  38%|███▊      | 1138/3000 [00:00<00:01, 1615.09it/s]warmup should be done:  39%|███▊      | 1157/3000 [00:00<00:01, 1639.12it/s]warmup should be done:  39%|███▉      | 1164/3000 [00:00<00:01, 1608.55it/s]warmup should be done:  37%|███▋      | 1108/3000 [00:00<00:01, 1525.60it/s]warmup should be done:  44%|████▍     | 1326/3000 [00:00<00:01, 1648.95it/s]warmup should be done:  45%|████▍     | 1341/3000 [00:00<00:00, 1665.62it/s]warmup should be done:  44%|████▍     | 1313/3000 [00:00<00:01, 1629.32it/s]warmup should be done:  44%|████▍     | 1332/3000 [00:00<00:01, 1654.98it/s]warmup should be done:  43%|████▎     | 1302/3000 [00:00<00:01, 1620.77it/s]warmup should be done:  44%|████▍     | 1322/3000 [00:00<00:01, 1639.59it/s]warmup should be done:  44%|████▍     | 1325/3000 [00:00<00:01, 1588.68it/s]warmup should be done:  42%|████▏     | 1261/3000 [00:00<00:01, 1513.30it/s]warmup should be done:  49%|████▉     | 1476/3000 [00:00<00:00, 1629.13it/s]warmup should be done:  50%|████▉     | 1491/3000 [00:00<00:00, 1647.60it/s]warmup should be done:  50%|█████     | 1508/3000 [00:00<00:00, 1664.27it/s]warmup should be done:  50%|████▉     | 1498/3000 [00:00<00:00, 1654.39it/s]warmup should be done:  49%|████▉     | 1465/3000 [00:00<00:00, 1623.18it/s]warmup should be done:  50%|████▉     | 1486/3000 [00:00<00:00, 1638.95it/s]warmup should be done:  49%|████▉     | 1484/3000 [00:00<00:00, 1578.79it/s]warmup should be done:  47%|████▋     | 1413/3000 [00:00<00:01, 1506.70it/s]warmup should be done:  55%|█████▍    | 1639/3000 [00:01<00:00, 1629.16it/s]warmup should be done:  55%|█████▌    | 1656/3000 [00:01<00:00, 1647.88it/s]warmup should be done:  55%|█████▌    | 1664/3000 [00:01<00:00, 1655.37it/s]warmup should be done:  56%|█████▌    | 1675/3000 [00:01<00:00, 1664.94it/s]warmup should be done:  55%|█████▌    | 1650/3000 [00:01<00:00, 1639.21it/s]warmup should be done:  54%|█████▍    | 1629/3000 [00:01<00:00, 1626.10it/s]warmup should be done:  55%|█████▍    | 1647/3000 [00:01<00:00, 1591.55it/s]warmup should be done:  52%|█████▏    | 1567/3000 [00:01<00:00, 1516.57it/s]warmup should be done:  60%|██████    | 1802/3000 [00:01<00:00, 1629.14it/s]warmup should be done:  61%|██████    | 1821/3000 [00:01<00:00, 1648.19it/s]warmup should be done:  61%|██████▏   | 1842/3000 [00:01<00:00, 1664.15it/s]warmup should be done:  61%|██████    | 1816/3000 [00:01<00:00, 1644.40it/s]warmup should be done:  60%|█████▉    | 1793/3000 [00:01<00:00, 1628.63it/s]warmup should be done:  61%|██████    | 1830/3000 [00:01<00:00, 1641.51it/s]warmup should be done:  60%|██████    | 1811/3000 [00:01<00:00, 1605.87it/s]warmup should be done:  58%|█████▊    | 1726/3000 [00:01<00:00, 1536.43it/s]warmup should be done:  66%|██████▌   | 1965/3000 [00:01<00:00, 1628.50it/s]warmup should be done:  66%|██████▌   | 1986/3000 [00:01<00:00, 1648.10it/s]warmup should be done:  67%|██████▋   | 2009/3000 [00:01<00:00, 1663.95it/s]warmup should be done:  65%|██████▌   | 1956/3000 [00:01<00:00, 1628.86it/s]warmup should be done:  66%|██████▌   | 1984/3000 [00:01<00:00, 1653.66it/s]warmup should be done:  67%|██████▋   | 1996/3000 [00:01<00:00, 1644.74it/s]warmup should be done:  66%|██████▌   | 1972/3000 [00:01<00:00, 1605.07it/s]warmup should be done:  63%|██████▎   | 1884/3000 [00:01<00:00, 1547.01it/s]warmup should be done:  72%|███████▏  | 2151/3000 [00:01<00:00, 1647.65it/s]warmup should be done:  71%|███████   | 2129/3000 [00:01<00:00, 1629.03it/s]warmup should be done:  73%|███████▎  | 2176/3000 [00:01<00:00, 1663.82it/s]warmup should be done:  72%|███████▏  | 2152/3000 [00:01<00:00, 1659.35it/s]warmup should be done:  71%|███████   | 2120/3000 [00:01<00:00, 1629.21it/s]warmup should be done:  72%|███████▏  | 2162/3000 [00:01<00:00, 1647.02it/s]warmup should be done:  68%|██████▊   | 2039/3000 [00:01<00:00, 1541.71it/s]warmup should be done:  71%|███████   | 2133/3000 [00:01<00:00, 1586.81it/s]warmup should be done:  76%|███████▋  | 2292/3000 [00:01<00:00, 1628.75it/s]warmup should be done:  77%|███████▋  | 2316/3000 [00:01<00:00, 1645.14it/s]warmup should be done:  77%|███████▋  | 2319/3000 [00:01<00:00, 1661.16it/s]warmup should be done:  76%|███████▌  | 2283/3000 [00:01<00:00, 1627.75it/s]warmup should be done:  78%|███████▊  | 2343/3000 [00:01<00:00, 1660.90it/s]warmup should be done:  78%|███████▊  | 2327/3000 [00:01<00:00, 1646.78it/s]warmup should be done:  73%|███████▎  | 2194/3000 [00:01<00:00, 1537.27it/s]warmup should be done:  76%|███████▋  | 2292/3000 [00:01<00:00, 1572.71it/s]warmup should be done:  82%|████████▏ | 2455/3000 [00:01<00:00, 1626.92it/s]warmup should be done:  83%|████████▎ | 2481/3000 [00:01<00:00, 1644.27it/s]warmup should be done:  83%|████████▎ | 2487/3000 [00:01<00:00, 1665.28it/s]warmup should be done:  82%|████████▏ | 2447/3000 [00:01<00:00, 1628.85it/s]warmup should be done:  84%|████████▎ | 2510/3000 [00:01<00:00, 1662.32it/s]warmup should be done:  83%|████████▎ | 2493/3000 [00:01<00:00, 1649.26it/s]warmup should be done:  78%|███████▊  | 2348/3000 [00:01<00:00, 1537.75it/s]warmup should be done:  82%|████████▏ | 2450/3000 [00:01<00:00, 1572.94it/s]warmup should be done:  88%|████████▊ | 2646/3000 [00:01<00:00, 1644.84it/s]warmup should be done:  87%|████████▋ | 2619/3000 [00:01<00:00, 1628.09it/s]warmup should be done:  88%|████████▊ | 2655/3000 [00:01<00:00, 1668.74it/s]warmup should be done:  89%|████████▉ | 2677/3000 [00:01<00:00, 1663.52it/s]warmup should be done:  87%|████████▋ | 2610/3000 [00:01<00:00, 1623.51it/s]warmup should be done:  89%|████████▊ | 2659/3000 [00:01<00:00, 1650.97it/s]warmup should be done:  84%|████████▎ | 2507/3000 [00:01<00:00, 1551.94it/s]warmup should be done:  87%|████████▋ | 2614/3000 [00:01<00:00, 1592.05it/s]warmup should be done:  93%|█████████▎| 2783/3000 [00:01<00:00, 1629.16it/s]warmup should be done:  94%|█████████▎| 2812/3000 [00:01<00:00, 1646.38it/s]warmup should be done:  94%|█████████▍| 2823/3000 [00:01<00:00, 1671.67it/s]warmup should be done:  95%|█████████▍| 2845/3000 [00:01<00:00, 1665.73it/s]warmup should be done:  92%|█████████▏| 2773/3000 [00:01<00:00, 1619.36it/s]warmup should be done:  94%|█████████▍| 2825/3000 [00:01<00:00, 1652.73it/s]warmup should be done:  93%|█████████▎| 2778/3000 [00:01<00:00, 1604.10it/s]warmup should be done:  89%|████████▉ | 2663/3000 [00:01<00:00, 1523.56it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1666.12it/s]warmup should be done:  99%|█████████▉| 2977/3000 [00:01<00:00, 1646.73it/s]warmup should be done:  98%|█████████▊| 2948/3000 [00:01<00:00, 1634.02it/s]warmup should be done: 100%|█████████▉| 2992/3000 [00:01<00:00, 1676.45it/s]warmup should be done: 100%|█████████▉| 2992/3000 [00:01<00:00, 1656.39it/s]warmup should be done:  98%|█████████▊| 2935/3000 [00:01<00:00, 1609.56it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1655.40it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1652.85it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1647.48it/s]warmup should be done:  98%|█████████▊| 2943/3000 [00:01<00:00, 1617.41it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1631.17it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1619.93it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1609.99it/s]warmup should be done:  94%|█████████▍| 2816/3000 [00:01<00:00, 1374.39it/s]warmup should be done:  99%|█████████▉| 2973/3000 [00:01<00:00, 1427.49it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1510.82it/s]


warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]




warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   6%|▌         | 169/3000 [00:00<00:01, 1689.05it/s]warmup should be done:   6%|▌         | 171/3000 [00:00<00:01, 1709.78it/s]warmup should be done:   6%|▌         | 168/3000 [00:00<00:01, 1677.63it/s]warmup should be done:   6%|▌         | 167/3000 [00:00<00:01, 1667.40it/s]warmup should be done:   6%|▌         | 167/3000 [00:00<00:01, 1665.81it/s]warmup should be done:   6%|▌         | 171/3000 [00:00<00:01, 1703.67it/s]warmup should be done:   5%|▌         | 164/3000 [00:00<00:01, 1634.39it/s]warmup should be done:   6%|▌         | 165/3000 [00:00<00:01, 1640.93it/s]warmup should be done:  11%|█▏        | 340/3000 [00:00<00:01, 1698.99it/s]warmup should be done:  11%|█▏        | 338/3000 [00:00<00:01, 1686.28it/s]warmup should be done:  11%|█▏        | 344/3000 [00:00<00:01, 1715.88it/s]warmup should be done:  11%|█         | 335/3000 [00:00<00:01, 1670.52it/s]warmup should be done:  11%|█         | 337/3000 [00:00<00:01, 1682.19it/s]warmup should be done:  11%|█▏        | 343/3000 [00:00<00:01, 1709.54it/s]warmup should be done:  11%|█         | 330/3000 [00:00<00:01, 1644.73it/s]warmup should be done:  11%|█         | 330/3000 [00:00<00:01, 1634.92it/s]warmup should be done:  17%|█▋        | 511/3000 [00:00<00:01, 1702.93it/s]warmup should be done:  17%|█▋        | 508/3000 [00:00<00:01, 1691.66it/s]warmup should be done:  17%|█▋        | 517/3000 [00:00<00:01, 1721.82it/s]warmup should be done:  17%|█▋        | 508/3000 [00:00<00:01, 1694.00it/s]warmup should be done:  17%|█▋        | 503/3000 [00:00<00:01, 1672.78it/s]warmup should be done:  17%|█▋        | 496/3000 [00:00<00:01, 1649.68it/s]warmup should be done:  17%|█▋        | 515/3000 [00:00<00:01, 1709.83it/s]warmup should be done:  16%|█▋        | 494/3000 [00:00<00:01, 1635.57it/s]warmup should be done:  23%|██▎       | 678/3000 [00:00<00:01, 1693.51it/s]warmup should be done:  23%|██▎       | 682/3000 [00:00<00:01, 1702.32it/s]warmup should be done:  22%|██▏       | 671/3000 [00:00<00:01, 1675.44it/s]warmup should be done:  23%|██▎       | 680/3000 [00:00<00:01, 1701.45it/s]warmup should be done:  23%|██▎       | 691/3000 [00:00<00:01, 1725.43it/s]warmup should be done:  22%|██▏       | 661/3000 [00:00<00:01, 1648.98it/s]warmup should be done:  23%|██▎       | 686/3000 [00:00<00:01, 1708.81it/s]warmup should be done:  22%|██▏       | 659/3000 [00:00<00:01, 1639.25it/s]warmup should be done:  28%|██▊       | 839/3000 [00:00<00:01, 1675.64it/s]warmup should be done:  28%|██▊       | 853/3000 [00:00<00:01, 1702.90it/s]warmup should be done:  29%|██▉       | 864/3000 [00:00<00:01, 1726.28it/s]warmup should be done:  28%|██▊       | 851/3000 [00:00<00:01, 1703.20it/s]warmup should be done:  28%|██▊       | 830/3000 [00:00<00:01, 1661.11it/s]warmup should be done:  28%|██▊       | 848/3000 [00:00<00:01, 1691.03it/s]warmup should be done:  29%|██▊       | 857/3000 [00:00<00:01, 1706.87it/s]warmup should be done:  28%|██▊       | 826/3000 [00:00<00:01, 1648.36it/s]warmup should be done:  34%|███▍      | 1024/3000 [00:00<00:01, 1704.04it/s]warmup should be done:  34%|███▎      | 1007/3000 [00:00<00:01, 1674.80it/s]warmup should be done:  34%|███▍      | 1022/3000 [00:00<00:01, 1704.20it/s]warmup should be done:  35%|███▍      | 1037/3000 [00:00<00:01, 1725.12it/s]warmup should be done:  33%|███▎      | 998/3000 [00:00<00:01, 1666.80it/s]warmup should be done:  34%|███▍      | 1018/3000 [00:00<00:01, 1692.71it/s]warmup should be done:  34%|███▍      | 1028/3000 [00:00<00:01, 1705.15it/s]warmup should be done:  33%|███▎      | 991/3000 [00:00<00:01, 1640.90it/s]warmup should be done:  40%|███▉      | 1193/3000 [00:00<00:01, 1705.61it/s]warmup should be done:  40%|███▉      | 1195/3000 [00:00<00:01, 1704.60it/s]warmup should be done:  39%|███▉      | 1175/3000 [00:00<00:01, 1673.21it/s]warmup should be done:  40%|███▉      | 1188/3000 [00:00<00:01, 1693.32it/s]warmup should be done:  39%|███▉      | 1167/3000 [00:00<00:01, 1671.01it/s]warmup should be done:  40%|███▉      | 1199/3000 [00:00<00:01, 1705.61it/s]warmup should be done:  40%|████      | 1210/3000 [00:00<00:01, 1720.13it/s]warmup should be done:  39%|███▊      | 1161/3000 [00:00<00:01, 1658.16it/s]warmup should be done:  46%|████▌     | 1366/3000 [00:00<00:00, 1705.74it/s]warmup should be done:  46%|████▌     | 1365/3000 [00:00<00:00, 1709.21it/s]warmup should be done:  45%|████▍     | 1343/3000 [00:00<00:00, 1674.52it/s]warmup should be done:  45%|████▌     | 1359/3000 [00:00<00:00, 1696.06it/s]warmup should be done:  46%|████▌     | 1371/3000 [00:00<00:00, 1708.87it/s]warmup should be done:  44%|████▍     | 1330/3000 [00:00<00:01, 1667.41it/s]warmup should be done:  46%|████▌     | 1383/3000 [00:00<00:00, 1711.68it/s]warmup should be done:  44%|████▍     | 1335/3000 [00:00<00:01, 1659.39it/s]warmup should be done:  51%|█████     | 1536/3000 [00:00<00:00, 1708.69it/s]warmup should be done:  51%|█████     | 1537/3000 [00:00<00:00, 1705.21it/s]warmup should be done:  51%|█████     | 1529/3000 [00:00<00:00, 1695.34it/s]warmup should be done:  51%|█████▏    | 1542/3000 [00:00<00:00, 1706.51it/s]warmup should be done:  50%|█████     | 1500/3000 [00:00<00:00, 1676.68it/s]warmup should be done:  50%|█████     | 1511/3000 [00:00<00:00, 1663.33it/s]warmup should be done:  50%|█████     | 1503/3000 [00:00<00:00, 1665.58it/s]warmup should be done:  52%|█████▏    | 1555/3000 [00:00<00:00, 1703.89it/s]warmup should be done:  57%|█████▋    | 1707/3000 [00:01<00:00, 1707.98it/s]warmup should be done:  57%|█████▋    | 1708/3000 [00:01<00:00, 1705.59it/s]warmup should be done:  57%|█████▋    | 1699/3000 [00:01<00:00, 1695.78it/s]warmup should be done:  57%|█████▋    | 1714/3000 [00:01<00:00, 1707.48it/s]warmup should be done:  56%|█████▌    | 1671/3000 [00:01<00:00, 1684.21it/s]warmup should be done:  56%|█████▌    | 1672/3000 [00:01<00:00, 1671.93it/s]warmup should be done:  56%|█████▌    | 1678/3000 [00:01<00:00, 1660.20it/s]warmup should be done:  58%|█████▊    | 1726/3000 [00:01<00:00, 1697.59it/s]warmup should be done:  63%|██████▎   | 1878/3000 [00:01<00:00, 1707.96it/s]warmup should be done:  63%|██████▎   | 1880/3000 [00:01<00:00, 1707.19it/s]warmup should be done:  62%|██████▏   | 1869/3000 [00:01<00:00, 1694.95it/s]warmup should be done:  63%|██████▎   | 1886/3000 [00:01<00:00, 1710.32it/s]warmup should be done:  61%|██████▏   | 1842/3000 [00:01<00:00, 1689.18it/s]warmup should be done:  61%|██████▏   | 1841/3000 [00:01<00:00, 1676.10it/s]warmup should be done:  62%|██████▏   | 1846/3000 [00:01<00:00, 1664.26it/s]warmup should be done:  63%|██████▎   | 1896/3000 [00:01<00:00, 1694.47it/s]warmup should be done:  68%|██████▊   | 2050/3000 [00:01<00:00, 1708.65it/s]warmup should be done:  68%|██████▊   | 2051/3000 [00:01<00:00, 1707.90it/s]warmup should be done:  68%|██████▊   | 2039/3000 [00:01<00:00, 1696.00it/s]warmup should be done:  69%|██████▊   | 2058/3000 [00:01<00:00, 1713.12it/s]warmup should be done:  67%|██████▋   | 2009/3000 [00:01<00:00, 1674.98it/s]warmup should be done:  67%|██████▋   | 2013/3000 [00:01<00:00, 1692.54it/s]warmup should be done:  67%|██████▋   | 2014/3000 [00:01<00:00, 1666.44it/s]warmup should be done:  69%|██████▉   | 2066/3000 [00:01<00:00, 1693.79it/s]warmup should be done:  74%|███████▍  | 2222/3000 [00:01<00:00, 1705.47it/s]warmup should be done:  74%|███████▎  | 2209/3000 [00:01<00:00, 1695.05it/s]warmup should be done:  74%|███████▍  | 2230/3000 [00:01<00:00, 1711.27it/s]warmup should be done:  74%|███████▍  | 2221/3000 [00:01<00:00, 1695.27it/s]warmup should be done:  73%|███████▎  | 2183/3000 [00:01<00:00, 1692.86it/s]warmup should be done:  73%|███████▎  | 2182/3000 [00:01<00:00, 1669.83it/s]warmup should be done:  73%|███████▎  | 2177/3000 [00:01<00:00, 1664.82it/s]warmup should be done:  75%|███████▍  | 2236/3000 [00:01<00:00, 1693.32it/s]warmup should be done:  80%|███████▉  | 2393/3000 [00:01<00:00, 1703.33it/s]warmup should be done:  79%|███████▉  | 2379/3000 [00:01<00:00, 1695.91it/s]warmup should be done:  80%|████████  | 2402/3000 [00:01<00:00, 1711.61it/s]warmup should be done:  80%|███████▉  | 2393/3000 [00:01<00:00, 1700.34it/s]warmup should be done:  78%|███████▊  | 2353/3000 [00:01<00:00, 1692.06it/s]warmup should be done:  78%|███████▊  | 2351/3000 [00:01<00:00, 1673.78it/s]warmup should be done:  78%|███████▊  | 2344/3000 [00:01<00:00, 1660.02it/s]warmup should be done:  80%|████████  | 2406/3000 [00:01<00:00, 1689.12it/s]warmup should be done:  85%|████████▌ | 2564/3000 [00:01<00:00, 1705.01it/s]warmup should be done:  85%|████████▌ | 2550/3000 [00:01<00:00, 1697.62it/s]warmup should be done:  86%|████████▌ | 2574/3000 [00:01<00:00, 1710.85it/s]warmup should be done:  85%|████████▌ | 2564/3000 [00:01<00:00, 1702.87it/s]warmup should be done:  84%|████████▍ | 2524/3000 [00:01<00:00, 1694.82it/s]warmup should be done:  84%|████████▍ | 2520/3000 [00:01<00:00, 1675.59it/s]warmup should be done:  84%|████████▎ | 2511/3000 [00:01<00:00, 1658.14it/s]warmup should be done:  86%|████████▌ | 2575/3000 [00:01<00:00, 1656.57it/s]warmup should be done:  91%|█████████ | 2736/3000 [00:01<00:00, 1707.18it/s]warmup should be done:  91%|█████████ | 2720/3000 [00:01<00:00, 1697.79it/s]warmup should be done:  92%|█████████▏| 2746/3000 [00:01<00:00, 1710.08it/s]warmup should be done:  91%|█████████ | 2735/3000 [00:01<00:00, 1701.94it/s]warmup should be done:  90%|████████▉ | 2688/3000 [00:01<00:00, 1675.27it/s]warmup should be done:  90%|████████▉ | 2695/3000 [00:01<00:00, 1696.74it/s]warmup should be done:  89%|████████▉ | 2677/3000 [00:01<00:00, 1651.76it/s]warmup should be done:  91%|█████████▏| 2742/3000 [00:01<00:00, 1657.60it/s]warmup should be done:  97%|█████████▋| 2907/3000 [00:01<00:00, 1706.39it/s]warmup should be done:  96%|█████████▋| 2890/3000 [00:01<00:00, 1690.31it/s]warmup should be done:  97%|█████████▋| 2918/3000 [00:01<00:00, 1709.18it/s]warmup should be done:  96%|█████████▌| 2865/3000 [00:01<00:00, 1696.45it/s]warmup should be done:  95%|█████████▌| 2856/3000 [00:01<00:00, 1673.81it/s]warmup should be done:  95%|█████████▍| 2843/3000 [00:01<00:00, 1649.73it/s]warmup should be done:  97%|█████████▋| 2906/3000 [00:01<00:00, 1665.24it/s]warmup should be done:  97%|█████████▋| 2908/3000 [00:01<00:00, 1657.92it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1708.34it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1704.78it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1694.12it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1693.23it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1690.51it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1677.96it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1671.31it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1659.35it/s]2022-12-12 06:19:27.245613: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fda73832be0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 06:19:27.245682: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 06:19:27.262321: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fbb20029c00 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 06:19:27.262376: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 06:19:28.232658: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fda7b8310c0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 06:19:28.232726: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 06:19:28.322245: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fda7b8332d0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 06:19:28.322317: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 06:19:28.433470: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fbbb402de40 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 06:19:28.433549: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 06:19:28.437200: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fda7b795a40 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 06:19:28.437261: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 06:19:28.473590: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fda77830ea0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 06:19:28.473654: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 06:19:28.527468: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fbbf002a490 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 06:19:28.527540: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 06:19:29.476143: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 06:19:29.481921: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 06:19:30.559614: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 06:19:30.610569: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 06:19:30.672379: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 06:19:30.743108: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 06:19:30.776524: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 06:19:30.822195: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 06:19:32.311546: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 06:19:32.390283: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 06:19:33.504596: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 06:19:33.534080: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 06:19:33.585689: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 06:19:33.669226: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 06:19:33.684392: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 06:19:33.712065: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
[HCTR][06:19:57.300][ERROR][RK0][tid #140576536119040]: replica 3 reaches 1000, calling init pre replica
[HCTR][06:19:57.300][ERROR][RK0][tid #140576536119040]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][06:19:57.307][ERROR][RK0][tid #140576536119040]: coll ps creation done
[HCTR][06:19:57.307][ERROR][RK0][tid #140576536119040]: replica 3 waits for coll ps creation barrier
[HCTR][06:19:57.330][ERROR][RK0][tid #140576745838336]: replica 4 reaches 1000, calling init pre replica
[HCTR][06:19:57.330][ERROR][RK0][tid #140576745838336]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][06:19:57.335][ERROR][RK0][tid #140576745838336]: coll ps creation done
[HCTR][06:19:57.335][ERROR][RK0][tid #140576745838336]: replica 4 waits for coll ps creation barrier
[HCTR][06:19:57.361][ERROR][RK0][tid #140576536119040]: replica 2 reaches 1000, calling init pre replica
[HCTR][06:19:57.362][ERROR][RK0][tid #140576536119040]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][06:19:57.367][ERROR][RK0][tid #140576536119040]: coll ps creation done
[HCTR][06:19:57.367][ERROR][RK0][tid #140576536119040]: replica 2 waits for coll ps creation barrier
[HCTR][06:19:57.375][ERROR][RK0][tid #140576544511744]: replica 6 reaches 1000, calling init pre replica
[HCTR][06:19:57.375][ERROR][RK0][tid #140576544511744]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][06:19:57.383][ERROR][RK0][tid #140576544511744]: coll ps creation done
[HCTR][06:19:57.383][ERROR][RK0][tid #140576544511744]: replica 6 waits for coll ps creation barrier
[HCTR][06:19:57.393][ERROR][RK0][tid #140576544511744]: replica 7 reaches 1000, calling init pre replica
[HCTR][06:19:57.393][ERROR][RK0][tid #140576544511744]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][06:19:57.395][ERROR][RK0][main]: replica 5 reaches 1000, calling init pre replica
[HCTR][06:19:57.395][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][06:19:57.400][ERROR][RK0][main]: coll ps creation done
[HCTR][06:19:57.400][ERROR][RK0][main]: replica 5 waits for coll ps creation barrier
[HCTR][06:19:57.401][ERROR][RK0][tid #140576544511744]: coll ps creation done
[HCTR][06:19:57.401][ERROR][RK0][tid #140576544511744]: replica 7 waits for coll ps creation barrier
[HCTR][06:19:57.440][ERROR][RK0][tid #140576603227904]: replica 1 reaches 1000, calling init pre replica
[HCTR][06:19:57.441][ERROR][RK0][tid #140576603227904]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][06:19:57.445][ERROR][RK0][tid #140576603227904]: coll ps creation done
[HCTR][06:19:57.445][ERROR][RK0][tid #140576603227904]: replica 1 waits for coll ps creation barrier
[HCTR][06:19:57.485][ERROR][RK0][tid #140576737445632]: replica 0 reaches 1000, calling init pre replica
[HCTR][06:19:57.485][ERROR][RK0][tid #140576737445632]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][06:19:57.490][ERROR][RK0][tid #140576737445632]: coll ps creation done
[HCTR][06:19:57.490][ERROR][RK0][tid #140576737445632]: replica 0 waits for coll ps creation barrier
[HCTR][06:19:57.490][ERROR][RK0][tid #140576737445632]: replica 0 preparing frequency
[HCTR][06:19:58.332][ERROR][RK0][tid #140576737445632]: replica 0 preparing frequency done
[HCTR][06:19:58.383][ERROR][RK0][tid #140576737445632]: replica 0 calling init per replica
[HCTR][06:19:58.383][ERROR][RK0][tid #140576544511744]: replica 6 calling init per replica
[HCTR][06:19:58.383][ERROR][RK0][tid #140576536119040]: replica 3 calling init per replica
[HCTR][06:19:58.383][ERROR][RK0][main]: replica 5 calling init per replica
[HCTR][06:19:58.383][ERROR][RK0][tid #140576544511744]: replica 7 calling init per replica
[HCTR][06:19:58.383][ERROR][RK0][tid #140576536119040]: replica 2 calling init per replica
[HCTR][06:19:58.383][ERROR][RK0][tid #140576603227904]: replica 1 calling init per replica
[HCTR][06:19:58.383][ERROR][RK0][tid #140576745838336]: replica 4 calling init per replica
[HCTR][06:19:58.384][ERROR][RK0][tid #140576737445632]: Calling build_v2
[HCTR][06:19:58.384][ERROR][RK0][tid #140576544511744]: Calling build_v2
[HCTR][06:19:58.384][ERROR][RK0][tid #140576536119040]: Calling build_v2
[HCTR][06:19:58.384][ERROR][RK0][main]: Calling build_v2
[HCTR][06:19:58.384][ERROR][RK0][tid #140576544511744]: Calling build_v2
[HCTR][06:19:58.384][ERROR][RK0][tid #140576536119040]: Calling build_v2
[HCTR][06:19:58.384][ERROR][RK0][tid #140576603227904]: Calling build_v2
[HCTR][06:19:58.384][ERROR][RK0][tid #140576737445632]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][06:19:58.384][ERROR][RK0][tid #140576745838336]: Calling build_v2
[HCTR][06:19:58.384][ERROR][RK0][tid #140576544511744]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][06:19:58.384][ERROR][RK0][tid #140576536119040]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][06:19:58.384][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][06:19:58.384][ERROR][RK0][tid #140576745838336]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][06:19:58.384][ERROR][RK0][tid #140576544511744]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][06:19:58.384][ERROR][RK0][tid #140576536119040]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][06:19:58.384][ERROR][RK0][tid #140576603227904]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[2022-12-12 06:19:58.[388672: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:2022-12-12 06:19:58178.] 388721v100x8, slow pcie: 
E[ /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[2022-12-12 06:19:58:2022-12-12 06:19:58.178.388760] 388781: v100x8, slow pcie: E
E [ /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:2022-12-12 06:19:58:1782022-12-12 06:19:58.196] .388832] v100x8, slow pcie388810: assigning 0 to cpu
: E
E  [/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 06:19:58::.196178388881] ] : assigning 0 to cpuv100x8, slow pcieE

 /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:196] [assigning 0 to cpu[[2022-12-12 06:19:58
2022-12-12 06:19:58..388929388931: : E[E2022-12-12 06:19:58 2022-12-12 06:19:58 [./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 06:19:58388915:388961:.: 196[: 212388975E] E] [: 2022-12-12 06:19:58 assigning 0 to cpu build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8E.2022-12-12 06:19:58/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
 388968.::/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: 2022-12-12 06:19:58389008178212[:[E.: ] ] v100x8, slow pcie2022-12-12 06:19:582122022-12-12 06:19:58 389051Ebuild asymm link desc with 8X Tesla V100-SXM2-32GB out of 8
.] ./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:  
389113build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8389132:[E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: 
: 178[2022-12-12 06:19:58 :EE] [2022-12-12 06:19:58./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc178  v100x8, slow pcie2022-12-12 06:19:58.389188:] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
.389214: 178v100x8, slow pcie::389242: [E] 
213212: E2022-12-12 06:19:58 v100x8, slow pcie] ] E[ ./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
remote time is 8.68421build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8 [2022-12-12 06:19:58/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc389328:

/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 06:19:58.:: 196:[.[389372213E213] 2022-12-12 06:19:583894252022-12-12 06:19:58: ]  ] assigning 0 to cpu.: .Eremote time is 8.68421/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccremote time is 8.68421
389454E389460 
:
:  : /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc196[[E[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE:] 2022-12-12 06:19:582022-12-12 06:19:58 2022-12-12 06:19:58: 196assigning 0 to cpu../hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.196/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] 
389591389596:389602] :assigning 0 to cpu: : 214: assigning 0 to cpu213
EE] [E
]   cpu time is 97.05882022-12-12 06:19:58 remote time is 8.68421/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
:[:[389742:2122022-12-12 06:19:58[2142022-12-12 06:19:58: 214] .2022-12-12 06:19:58] .E] build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8389805.cpu time is 97.0588389824 cpu time is 97.0588
: 389825
: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
E: E:[ E 2122022-12-12 06:19:58/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] .:/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8389943212:214
: ] 212] Ebuild asymm link desc with 8X Tesla V100-SXM2-32GB out of 8] cpu time is 97.0588[
 build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8
2022-12-12 06:19:58/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
[.:2022-12-12 06:19:58390076[213.: 2022-12-12 06:19:58] 390111E.remote time is 8.68421:  390134
E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:  :E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc213 /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[:] :2022-12-12 06:19:58213remote time is 8.68421213.] 
] 390203[remote time is 8.68421remote time is 8.68421: 2022-12-12 06:19:58

E.[ [3902582022-12-12 06:19:58/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 06:19:58: .:.E390284214390287 : ] : /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccEcpu time is 97.0588E: 
 214/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] ::cpu time is 97.0588214214
] ] cpu time is 97.0588cpu time is 97.0588

[2022-12-12 06:21:17.338367: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:118] solver created. now build & solve
[2022-12-12 06:21:17.379911: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:123] solver built. now solve
[2022-12-12 06:21:17.379986: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:919] num_cached_nodes = 25000000
[2022-12-12 06:21:17.499505: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:127] solver solved
[2022-12-12 06:21:17.499626: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:195] 0 solved master
[2022-12-12 06:21:17.532410: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 0 solved
[2022-12-12 06:21:17.532484: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 0 initing device 0
[2022-12-12 06:21:17.533026: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 06:21:17.534381: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 06:21:17.535548: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 06:21:17.547723: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 4 solved
[2022-12-12 06:21:17.547781: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 4 initing device 4
[2022-12-12 06:21:17.547955: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 5 solved
[2022-12-12 06:21:17.548035: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 5 initing device 5
[[2022-12-12 06:21:172022-12-12 06:21:17..[5481395481652022-12-12 06:21:17: : .EE548170  : /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE:: 2021980/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc] ] :6 solvedeager alloc mem 381.47 MB202

] [7 solved2022-12-12 06:21:17
.548311: E[ 2022-12-12 06:21:17/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc.:548328205: ] Eworker 0 thread 6 initing device 6 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 7 initing device 7
[2022-12-12 06:21:17.548442: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 06:21:17[.2022-12-12 06:21:17548735.: 548738E:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu :/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu1980:] 1980eager alloc mem 381.47 MB] 
eager alloc mem 381.47 MB
[2022-12-12 06:21:17.551115: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 06:21:17.551226: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 06:21:17.551288: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 06:21:17.551345: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 06:21:17.551958: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 1 solved
[2022-12-12 06:21:17.552010: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 1 initing device 1
[2022-12-12 06:21:17.552042: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 3 solved
[2022-12-12 06:21:17.552099: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 3 initing device 3
[2022-12-12 06:21:17.552401: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 06:21:17.552481: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 06:21:17.554910: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 06:21:17.554960: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 06:21:17.555180: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 06:21:17.555193: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 2 solved
[2022-12-12 06:21:17.555272: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc[:2022-12-12 06:21:17205.] 555289worker 0 thread 2 initing device 2: 
E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 06:21:17.555857: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 06:21:17.556252: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 06:21:17.556310: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[[2022-12-12 06:21:172022-12-12 06:21:17..560295560310: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::19801980] ] eager alloc mem 381.47 MBeager alloc mem 381.47 MB[

2022-12-12 06:21:17.560408: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 06:21:17.562832: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 06:21:17.637471: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 2.00 Bytes
[2022-12-12 06:21:17.637858: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1024.00 Bytes
[2022-12-12 06:21:17.642236: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 2.00 Bytes
[2022-12-12 06:21:17.642575: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1024.00 Bytes
[2022-12-12 06:21:17.643053: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-12 06:21:17.643119: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 2
[2022-12-12 06:21:17.643174: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 06:21:17.644011: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 95.75 MB
[2022-12-12 06:21:17.644614: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:21:17.645720: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:21:17.645814: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 06:21:17.646493: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 06:21:17.646534: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 11.92 GB
[[2022-12-12 06:21:172022-12-12 06:21:17..646686646686: : EE[[  2022-12-12 06:21:172022-12-12 06:21:17/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu..::64674764674619801980: : ] ] EEeager alloc mem 2.00 Byteseager alloc mem 2.00 Bytes  

/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::19801980] ] eager alloc mem 2.00 Byteseager alloc mem 2.00 Bytes

[[2022-12-12 06:21:172022-12-12 06:21:17[.[.2022-12-12 06:21:176470792022-12-12 06:21:17647080.: .: 647087E647089E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1024.00 Bytes
:  : E/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE : /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu1980/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:] :1980eager alloc mem 1024.00 Bytes1980] 
] eager alloc mem 1024.00 Byteseager alloc mem 1024.00 Bytes

[2022-12-12 06:21:17.647508: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-12 06:21:17.647585: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 2
[2022-12-12 06:21:17.647628: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 06:21:17.648008: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 2.00 Bytes
[2022-12-12 06:21:17.648300: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1024.00 Bytes
[2022-12-12 06:21:17.662125: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 95.75 MB
[2022-12-12 06:21:17.662325: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 2.00 Bytes
[2022-12-12 06:21:17.662628: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1024.00 Bytes
[2022-12-12 06:21:17.663405: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:21:17.663987: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-12 06:21:17.664057: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 2
[2022-12-12 06:21:17.664070: [E2022-12-12 06:21:17 ./hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc664099:: 638E]  eager release cuda mem 1024/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc
:638] eager release cuda mem 400000000
[2022-12-12 06:21:17.[6641272022-12-12 06:21:17: .E664149 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccE: 638/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc] :eager release cuda mem 1024638
] eager release cuda mem 2
[2022-12-12 06:21:17.[6642092022-12-12 06:21:17: .E664216 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccE: 638/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc] :[eager release cuda mem 26382022-12-12 06:21:17
] .eager release cuda mem 400000000664233
: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:[6382022-12-12 06:21:17] .eager release cuda mem 1024664292
: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 06:21:17.664327: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 2
[2022-12-12 06:21:17.664368: E [/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc2022-12-12 06:21:17:.638664364] : eager release cuda mem 400000000E
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-12 06:21:17.664463: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] [eager release cuda mem 22022-12-12 06:21:17
.664479: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:21:17.664512: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 06:21:17.664579: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 06:21:17.665086: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 95.75 MB
[2022-12-12 06:21:17.665250: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 06:21:17.665291: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 11.92 GB
[2022-12-12 06:21:17.666441: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 95.75 MB
[2022-12-12 06:21:17.680171: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 95.75 MB
[2022-12-12 06:21:17.680894: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 95.75 MB
[2022-12-12 06:21:17.681939: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 95.75 MB
[2022-12-12 06:21:17.683231: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:21:17.683924: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-12 06:21:17.683987: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 2
[[2022-12-12 06:21:172022-12-12 06:21:17..684010684026: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::1980638] ] eager alloc mem 611.00 KBeager release cuda mem 400000000

[2022-12-12 06:21:17.684133: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:21:17.684284: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:21:17.684370: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 06:21:17.684417: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:21:17.684472: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:21:17.684930: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 06:21:17.684972: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 11.92 GB
[2022-12-12 06:21:17.685069: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:[19802022-12-12 06:21:17] .eager alloc mem 95.75 MB685088
: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:21:17.685193: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu[:2022-12-12 06:21:171980.] 685203eager alloc mem 25.25 KB: 
E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:21:17.685310: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 06:21:17.685487: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:21:17.685541: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:21:17.685576: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 06:21:17.685626: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 06:21:17.685771: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 06:21:17.685813: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 11.92 GB
[2022-12-12 06:21:17.685977: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 06:21:17.686017: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 11.92 GB
[2022-12-12 06:21:17.686244: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 06:21:17.686283: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:[19802022-12-12 06:21:17] .eager alloc mem 11.92 GB686296
: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 06:21:17.686358: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 11.92 GB
[2022-12-12 06:21:17.753134: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:21:17.754196: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:21:17.754284: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 06:21:17.754844: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 06:21:17.754883: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 11.92 GB
[[[[[[[[2022-12-12 06:21:202022-12-12 06:21:202022-12-12 06:21:202022-12-12 06:21:202022-12-12 06:21:202022-12-12 06:21:202022-12-12 06:21:202022-12-12 06:21:20........239655239662239657239654239659239659239659239659: : : : : : : : EEEEEEEE        /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::::::::19801980198019801980198019801980] ] ] ] ] ] ] ] eager alloc mem 611.00 KBeager alloc mem 611.00 KBeager alloc mem 611.00 KBeager alloc mem 611.00 KBeager alloc mem 611.00 KBeager alloc mem 611.00 KBeager alloc mem 611.00 KBeager alloc mem 611.00 KB







[2022-12-12 06:21:20.240772: E [[/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc2022-12-12 06:21:202022-12-12 06:21:20[:.[[[.2022-12-12 06:21:206382407862022-12-12 06:21:20[2022-12-12 06:21:202022-12-12 06:21:20240787.] : .2022-12-12 06:21:20..: 240793eager release cuda mem 625663E240795.240795240798E: 
 : 240815: :  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccE: EE/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc : E  :/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc638/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc638[:] :/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::] 2022-12-12 06:21:20638eager release cuda mem 625663638:638638eager release cuda mem 625663.] 
] 638] ] 
240942eager release cuda mem 625663eager release cuda mem 625663] eager release cuda mem 625663eager release cuda mem 625663: 

eager release cuda mem 625663

E
 [/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu2022-12-12 06:21:20:.1980[241076] 2022-12-12 06:21:20: eager alloc mem 611.00 KB.E
241090 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE:[ [1980[2022-12-12 06:21:20[/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu2022-12-12 06:21:20[] 2022-12-12 06:21:20.2022-12-12 06:21:20:.2022-12-12 06:21:20eager alloc mem 611.00 KB.241116.1980241117.
241119: 241123] : 241133: E: eager alloc mem 611.00 KBE: E E
 E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980:1980:1980] 1980] 1980] eager alloc mem 611.00 KB] eager alloc mem 611.00 KB] eager alloc mem 611.00 KB
eager alloc mem 611.00 KB
eager alloc mem 611.00 KB


[2022-12-12 06:21:20.241841: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:21:20.241914: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:21:20.241946: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:21:20.242005: E[ 2022-12-12 06:21:20/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc.:242016638: ] Eeager release cuda mem 625663 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:21:20.242082[: [[[[2022-12-12 06:21:20E2022-12-12 06:21:202022-12-12 06:21:202022-12-12 06:21:202022-12-12 06:21:20. ....242087/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc242087242087242090242094: :: : : : E638EEEE ]     /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cceager release cuda mem 625663/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:
::::6386386381980638] ] ] ] ] eager release cuda mem 625663eager release cuda mem 625663eager release cuda mem 625663eager alloc mem 611.00 KBeager release cuda mem 625663

[


2022-12-12 06:21:20.242319: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[[2022-12-12 06:21:202022-12-12 06:21:20.[.2423782022-12-12 06:21:20242377: [.: E2022-12-12 06:21:20242389E .:  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu242398E/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::  :1980E/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu1980]  :] eager alloc mem 611.00 KB/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu1980eager alloc mem 611.00 KB
:] 
1980eager alloc mem 611.00 KB] 
eager alloc mem 611.00 KB
[2022-12-12 06:21:20.242661: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:21:20.242731: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:21:20.242781: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:21:20.242849: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:21:20.243074: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:21:20.243100: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:21:20.243156: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:21:20.243182: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:21:20[.2022-12-12 06:21:20[243203.[2022-12-12 06:21:20: 2432102022-12-12 06:21:20.E: .243215 E243219: /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc : E:/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccE 638: /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc] 638/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:eager release cuda mem 625663] :638
eager release cuda mem 625663638] 
] eager release cuda mem 625663eager release cuda mem 625663

[2022-12-12 06:21:20.243338[: 2022-12-12 06:21:20E.[ 243346[2022-12-12 06:21:20/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu: 2022-12-12 06:21:20.:E.2433541980 243356: ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu: Eeager alloc mem 611.00 KB:E 
1980 /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu] /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:eager alloc mem 611.00 KB:1980
1980] ] eager alloc mem 611.00 KBeager alloc mem 611.00 KB

[2022-12-12 06:21:20.243481: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:21:20.243551: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:21:20.243611: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:21:20.243681: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:21:20.243908: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] [eager release cuda mem 6256632022-12-12 06:21:20
.243928: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:21:20.243980: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB[
2022-12-12 06:21:20.244001: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:21:20.244141: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:[6382022-12-12 06:21:20] .eager release cuda mem 625663[244158
2022-12-12 06:21:20: [.E2022-12-12 06:21:20244169 .: /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc244177E::  638E/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc]  :eager release cuda mem 625663/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc638[
:] 2022-12-12 06:21:20638eager release cuda mem 625663.] 
244221eager release cuda mem 625663: 
E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:21:20.244272: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980[] 2022-12-12 06:21:20eager alloc mem 611.00 KB.[
2442892022-12-12 06:21:20[: .2022-12-12 06:21:20E244298. : 244301/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE: : E1980/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu ] :/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cceager alloc mem 611.00 KB1980:
] 638eager alloc mem 611.00 KB] 
eager release cuda mem 625663
[2022-12-12 06:21:20.244430: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:21:20.244471: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB[
2022-12-12 06:21:20.244496: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:21:20.244729: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:21:20.244753: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:21:20.244796: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:21:20.244821: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:21:20.244990: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:21:20.245030: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:21:20.245061: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:21:20.245098: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:21:20.245133: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] [eager release cuda mem 6256632022-12-12 06:21:20
.245152: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:21:20.245204: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:21:20.[2452262022-12-12 06:21:20: .E[245231 2022-12-12 06:21:20: /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu.E:245251 1980: /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc] E:eager alloc mem 611.00 KB 638
/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc] :eager release cuda mem 625663638
] eager release cuda mem 625663
[2022-12-12 06:21:20.[2453882022-12-12 06:21:20: .E245392 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE: 1980/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu] :eager alloc mem 611.00 KB1980
] eager alloc mem 611.00 KB
[2022-12-12 06:21:20.245543: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:21:20.245570: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:21:20.245611: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:21:20.245637: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:21:20.245804: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:21:20.245844: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:21:20.245874: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:21:20.245911: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:21:20.245949: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:21:20.246016: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:21:20.246061: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:21:20.246131: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[[2022-12-12 06:21:202022-12-12 06:21:20..246164246168: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::638638] ] eager release cuda mem 625663eager release cuda mem 625663

[[2022-12-12 06:21:202022-12-12 06:21:20..246225246227: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::638638] ] eager release cuda mem 100400000eager release cuda mem 100400000

[2022-12-12 06:21:20.246359: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:21:20.246386[: 2022-12-12 06:21:20E. 246395/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc: :E638 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cceager release cuda mem 625663:
638] eager release cuda mem 100400000
[2022-12-12 06:21:20.246445: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 100400000
[2022-12-12 06:21:20.246620: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:21:20[.2022-12-12 06:21:20246659.: 246660E:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc :/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc638:] 638eager release cuda mem 100400000] 
eager release cuda mem 625663
[2022-12-12 06:21:20.246720: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 100400000
[2022-12-12 06:21:20.246762: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:21:20.246799: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 100400000
[2022-12-12 06:21:20.246887: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:21:20.246928: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 100400000
[2022-12-12 06:21:20.246991: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 25000000 / 100000000 nodes ( 25.00 %~25.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 75000000 / 100000000 nodes ( 75.00 %) | 11.92 GB | 2.69826 secs 
[2022-12-12 06:21:20.247370: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 25000000 / 100000000 nodes ( 25.00 %~25.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 75000000 / 100000000 nodes ( 75.00 %) | 11.92 GB | 2.69864 secs 
[2022-12-12 06:21:20.247859: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 25000000 / 100000000 nodes ( 25.00 %~25.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 75000000 / 100000000 nodes ( 75.00 %) | 11.92 GB | 2.69943 secs 
[2022-12-12 06:21:20.248275: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 25000000 / 100000000 nodes ( 25.00 %~25.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 75000000 / 100000000 nodes ( 75.00 %) | 11.92 GB | 2.69588 secs 
[2022-12-12 06:21:20.248978: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 25000000 / 100000000 nodes ( 25.00 %~25.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 75000000 / 100000000 nodes ( 75.00 %) | 11.92 GB | 2.70082 secs 
[2022-12-12 06:21:20.249164: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 25000000 / 100000000 nodes ( 25.00 %~25.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 75000000 / 100000000 nodes ( 75.00 %) | 11.92 GB | 2.69669 secs 
[2022-12-12 06:21:20.249345: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 25000000 / 100000000 nodes ( 25.00 %~25.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 75000000 / 100000000 nodes ( 75.00 %) | 11.92 GB | 2.6935 secs 
[2022-12-12 06:21:20.249855: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 25000000 / 100000000 nodes ( 25.00 %~25.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 75000000 / 100000000 nodes ( 75.00 %) | 11.92 GB | 2.71684 secs 
[HCTR][06:21:20.250][ERROR][RK0][main]: replica 5 calling init per replica done, doing barrier
[HCTR][06:21:20.250][ERROR][RK0][tid #140576544511744]: replica 7 calling init per replica done, doing barrier
[HCTR][06:21:20.250][ERROR][RK0][tid #140576544511744]: replica 6 calling init per replica done, doing barrier
[HCTR][06:21:20.250][ERROR][RK0][tid #140576603227904]: replica 1 calling init per replica done, doing barrier
[HCTR][06:21:20.250][ERROR][RK0][tid #140576745838336]: replica 4 calling init per replica done, doing barrier
[HCTR][06:21:20.250][ERROR][RK0][tid #140576536119040]: replica 3 calling init per replica done, doing barrier
[HCTR][06:21:20.250][ERROR][RK0][tid #140576536119040]: replica 2 calling init per replica done, doing barrier
[HCTR][06:21:20.250][ERROR][RK0][tid #140576737445632]: replica 0 calling init per replica done, doing barrier
[HCTR][06:21:20.250][ERROR][RK0][tid #140576536119040]: replica 2 calling init per replica done, doing barrier done
[HCTR][06:21:20.250][ERROR][RK0][tid #140576737445632]: replica 0 calling init per replica done, doing barrier done
[HCTR][06:21:20.250][ERROR][RK0][tid #140576544511744]: replica 6 calling init per replica done, doing barrier done
[HCTR][06:21:20.250][ERROR][RK0][tid #140576536119040]: replica 3 calling init per replica done, doing barrier done
[HCTR][06:21:20.250][ERROR][RK0][tid #140576544511744]: replica 7 calling init per replica done, doing barrier done
[HCTR][06:21:20.250][ERROR][RK0][tid #140576603227904]: replica 1 calling init per replica done, doing barrier done
[HCTR][06:21:20.250][ERROR][RK0][tid #140576745838336]: replica 4 calling init per replica done, doing barrier done
[HCTR][06:21:20.250][ERROR][RK0][tid #140576536119040]: init per replica done
[HCTR][06:21:20.250][ERROR][RK0][main]: replica 5 calling init per replica done, doing barrier done
[HCTR][06:21:20.250][ERROR][RK0][tid #140576536119040]: init per replica done
[HCTR][06:21:20.250][ERROR][RK0][tid #140576544511744]: init per replica done
[HCTR][06:21:20.250][ERROR][RK0][tid #140576544511744]: init per replica done
[HCTR][06:21:20.250][ERROR][RK0][main]: init per replica done
[HCTR][06:21:20.250][ERROR][RK0][tid #140576603227904]: init per replica done
[HCTR][06:21:20.250][ERROR][RK0][tid #140576745838336]: init per replica done
[HCTR][06:21:20.252][ERROR][RK0][tid #140576737445632]: init per replica done








