2022-12-11 23:09:46.600862: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.607336: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.611675: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.616204: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.628926: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.637425: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.645226: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.649840: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.701285: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.708949: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.713780: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.716165: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.724837: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.735180: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.745380: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.749464: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.750570: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.751607: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.752877: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.754851: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.755868: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.755958: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.757503: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.757624: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.758939: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.759382: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.760485: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.761117: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.762668: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.763015: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.764111: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.764686: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.765974: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.766259: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.767999: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.768010: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.769632: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.770690: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.771735: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.772691: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.777999: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 23:09:46.786204: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.786325: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.786989: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.788078: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.788107: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.788969: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.789880: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.790022: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.790971: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.792025: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.793494: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.794855: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.796609: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.797485: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.797851: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.800068: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.801350: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.801778: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.802255: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.803580: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.805035: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.805444: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.806346: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.806474: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.807306: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.808827: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.810165: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.810321: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.810673: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.812125: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.813296: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.813430: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.813803: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.814655: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.815917: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.816077: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.816348: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.817235: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.818371: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.818477: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.818702: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.819359: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.824486: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.827028: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.827092: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.827663: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.828136: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.830038: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.830185: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.830742: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.831028: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.854263: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.855203: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.867912: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.868272: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.868387: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.870053: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.870109: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.871608: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.871898: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.873030: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.873693: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.873875: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.874888: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.876945: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.877534: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.877675: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.879329: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.880343: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.880986: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.881060: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.882453: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.883933: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.884605: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.884651: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.885916: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.887194: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.888110: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.888391: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.889726: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.890218: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.891076: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.891572: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.892746: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.893004: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.893819: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.894273: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.895752: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.895974: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.896620: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.897181: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.898567: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.898883: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.899845: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.900822: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.902866: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.903242: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.903633: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.904298: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.905951: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.906210: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.906994: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.907694: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.909175: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.909438: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.909819: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.910734: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.912345: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.912353: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.912909: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.912946: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.913428: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.915224: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.915640: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.915898: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.916442: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.916831: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.917076: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.919024: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.919503: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.919748: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.920524: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.921324: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.921548: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.922528: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.924437: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.924937: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.925253: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.925764: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.926552: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.926675: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.927559: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.928709: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.929823: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.930124: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.930156: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.931298: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.931383: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.932087: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.933340: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.934184: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.934848: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.934906: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.935929: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.935967: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.936498: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.937878: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.938695: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.939312: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.939425: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.940638: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.940778: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.941115: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.942620: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.943771: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.944926: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.945097: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.946600: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.946653: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.946877: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.947909: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.948832: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.949678: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.949953: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.951305: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.952153: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.952220: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.952879: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.953528: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.953803: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.954754: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.955667: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.955838: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.955936: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 23:09:46.956633: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.957185: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.957261: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.959109: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.960029: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.960334: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.960564: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.962690: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 23:09:46.962804: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 23:09:46.963053: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.964118: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.964496: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.964535: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.966207: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.966986: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.967684: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.967755: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.967839: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 23:09:46.969189: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.969641: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.970377: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.970515: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.970939: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.971096: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.972078: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.972750: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.973799: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.974611: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.974755: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.975646: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.976303: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.978846: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.978902: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.980177: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.980264: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.980966: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.981892: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.983218: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.983241: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.985110: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.986061: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.987489: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.988874: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.990029: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:46.991621: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:47.022012: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:47.023025: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:47.024420: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:47.026228: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:47.027605: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:47.028968: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:47.031282: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:47.033281: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:47.034505: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:47.036340: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:47.039108: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:47.040086: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:47.042269: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:47.043591: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:47.047352: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:47.049627: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 23:09:47.053766: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 23:09:47.054525: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:47.058051: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:47.059541: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 23:09:47.062082: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:47.067493: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:47.100191: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:47.101710: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:47.102689: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:47.106246: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:47.107742: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:47.107869: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:48.108966: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:48.109852: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:48.110767: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:48.111349: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 23:09:48.111410: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-11 23:09:48.129418: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:48.130068: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:48.130777: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:48.131507: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:48.132198: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:48.132687: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-11 23:09:48.179140: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 23:09:48.179364: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 23:09:48.231784: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12340
2022-12-11 23:09:48.357948: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:48.358554: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:48.359544: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:48.360028: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 23:09:48.360080: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-11 23:09:48.365277: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:48.366139: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:48.367190: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:48.367994: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 23:09:48.368062: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-11 23:09:48.378912: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:48.379553: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:48.380070: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:48.380638: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:48.381167: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:48.381770: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:1/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-11 23:09:48.385363: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:48.386217: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:48.386721: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:48.387316: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:48.387843: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:48.388538: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:2/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-11 23:09:48.392205: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:48.392799: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:48.393333: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:48.393808: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 23:09:48.393866: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-11 23:09:48.412640: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:48.413265: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:48.413766: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:48.414564: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:48.415088: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:48.415572: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:3/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-11 23:09:48.436160: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:48.436756: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:48.437289: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:48.437755: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 23:09:48.437806: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-11 23:09:48.456063: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:48.456683: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:48.457197: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:48.457764: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:48.458298: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:48.458462: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:48.459444: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:6/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-11 23:09:48.459673: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:48.460232: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:48.460699: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 23:09:48.460745: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-11 23:09:48.461911: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 23:09:48.462074: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 23:09:48.462336: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 23:09:48.462484: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 23:09:48.463028: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12341
2022-12-11 23:09:48.463498: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12343
2022-12-11 23:09:48.465038: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:48.465616: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:48.466372: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:48.466849: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 23:09:48.466895: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-11 23:09:48.472452: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:48.473060: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:48.473587: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:48.474060: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 23:09:48.474109: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-11 23:09:48.478683: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:48.479171: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 23:09:48.479336: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:48.479351: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 23:09:48.479853: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:48.480431: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:48.480955: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:48.481097: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12342
2022-12-11 23:09:48.481424: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:5/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-11 23:09:48.484362: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:48.484986: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:48.485488: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:48.486064: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:48.486573: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:48.487046: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:7/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-11 23:09:48.491560: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:48.492181: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:48.492680: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:48.493254: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:48.493758: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:09:48.494232: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:4/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-11 23:09:48.504799: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 23:09:48.505007: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 23:09:48.506892: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12346
2022-12-11 23:09:48.527125: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 23:09:48.527345: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 23:09:48.529210: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12345
2022-12-11 23:09:48.532163: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 23:09:48.532318: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 23:09:48.534156: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12347
2022-12-11 23:09:48.539034: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 23:09:48.539193: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 23:09:48.540790: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12344
[HCTR][23:09:49.800][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][23:09:49.800][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][23:09:49.809][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][23:09:49.809][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][23:09:49.809][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][23:09:49.809][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][23:09:49.809][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][23:09:49.810][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 1it [00:01,  1.52s/it]warmup run: 1it [00:01,  1.54s/it]warmup run: 1it [00:01,  1.54s/it]warmup run: 1it [00:01,  1.52s/it]warmup run: 100it [00:01, 85.42it/s]warmup run: 90it [00:01, 76.23it/s]warmup run: 1it [00:01,  1.50s/it]warmup run: 101it [00:01, 85.70it/s]warmup run: 1it [00:01,  1.49s/it]warmup run: 97it [00:01, 82.79it/s]warmup run: 202it [00:01, 187.24it/s]warmup run: 179it [00:01, 164.32it/s]warmup run: 101it [00:01, 87.46it/s]warmup run: 202it [00:01, 185.72it/s]warmup run: 100it [00:01, 87.36it/s]warmup run: 192it [00:01, 177.18it/s]warmup run: 1it [00:01,  1.48s/it]warmup run: 305it [00:01, 300.57it/s]warmup run: 269it [00:01, 262.64it/s]warmup run: 201it [00:01, 188.05it/s]warmup run: 1it [00:01,  1.50s/it]warmup run: 303it [00:01, 296.04it/s]warmup run: 195it [00:01, 183.11it/s]warmup run: 289it [00:01, 283.81it/s]warmup run: 99it [00:01, 86.91it/s]warmup run: 408it [00:01, 418.02it/s]warmup run: 360it [00:01, 366.49it/s]warmup run: 302it [00:01, 299.71it/s]warmup run: 84it [00:01, 72.73it/s]warmup run: 405it [00:01, 411.66it/s]warmup run: 286it [00:01, 282.35it/s]warmup run: 386it [00:01, 394.09it/s]warmup run: 196it [00:01, 185.55it/s]warmup run: 511it [00:02, 532.11it/s]warmup run: 451it [00:02, 467.48it/s]warmup run: 404it [00:01, 416.23it/s]warmup run: 167it [00:01, 156.24it/s]warmup run: 507it [00:02, 524.80it/s]warmup run: 377it [00:01, 384.33it/s]warmup run: 484it [00:02, 502.94it/s]warmup run: 296it [00:01, 297.42it/s]warmup run: 615it [00:02, 638.90it/s]warmup run: 545it [00:02, 566.04it/s]warmup run: 506it [00:02, 529.73it/s]warmup run: 251it [00:01, 248.93it/s]warmup run: 610it [00:02, 630.89it/s]warmup run: 468it [00:01, 483.59it/s]warmup run: 583it [00:02, 604.30it/s]warmup run: 397it [00:01, 413.83it/s]warmup run: 720it [00:02, 732.68it/s]warmup run: 640it [00:02, 653.89it/s]warmup run: 609it [00:02, 635.28it/s]warmup run: 334it [00:01, 343.02it/s]warmup run: 713it [00:02, 721.91it/s]warmup run: 560it [00:02, 575.25it/s]warmup run: 682it [00:02, 691.77it/s]warmup run: 497it [00:01, 524.80it/s]warmup run: 822it [00:02, 802.56it/s]warmup run: 735it [00:02, 726.03it/s]warmup run: 711it [00:02, 724.58it/s]warmup run: 417it [00:02, 434.73it/s]warmup run: 815it [00:02, 795.72it/s]warmup run: 652it [00:02, 655.08it/s]warmup run: 781it [00:02, 764.26it/s]warmup run: 597it [00:02, 625.04it/s]warmup run: 923it [00:02, 855.15it/s]warmup run: 829it [00:02, 779.82it/s]warmup run: 812it [00:02, 795.46it/s]warmup run: 502it [00:02, 521.72it/s]warmup run: 918it [00:02, 855.12it/s]warmup run: 746it [00:02, 725.05it/s]warmup run: 878it [00:02, 816.99it/s]warmup run: 698it [00:02, 713.42it/s]warmup run: 1025it [00:02, 899.69it/s]warmup run: 923it [00:02, 822.12it/s]warmup run: 912it [00:02, 847.68it/s]warmup run: 594it [00:02, 612.82it/s]warmup run: 1021it [00:02, 900.41it/s]warmup run: 840it [00:02, 780.67it/s]warmup run: 977it [00:02, 861.58it/s]warmup run: 800it [00:02, 788.63it/s]warmup run: 1127it [00:02, 931.15it/s]warmup run: 1017it [00:02, 852.55it/s]warmup run: 1013it [00:02, 889.95it/s]warmup run: 691it [00:02, 701.46it/s]warmup run: 1123it [00:02, 933.52it/s]warmup run: 936it [00:02, 828.08it/s]warmup run: 1075it [00:02, 893.80it/s]warmup run: 900it [00:02, 844.07it/s]warmup run: 1229it [00:02, 954.21it/s]warmup run: 1114it [00:02, 884.73it/s]warmup run: 1115it [00:02, 925.84it/s]warmup run: 788it [00:02, 771.00it/s]warmup run: 1225it [00:02, 947.90it/s]warmup run: 1033it [00:02, 867.39it/s]warmup run: 1173it [00:02, 917.36it/s]warmup run: 1001it [00:02, 887.10it/s]warmup run: 1332it [00:02, 974.73it/s]warmup run: 1212it [00:02, 909.45it/s]warmup run: 1217it [00:02, 951.75it/s]warmup run: 886it [00:02, 825.60it/s]warmup run: 1326it [00:02, 964.95it/s]warmup run: 1131it [00:02, 899.12it/s]warmup run: 1271it [00:02, 934.66it/s]warmup run: 1103it [00:02, 921.93it/s]warmup run: 1308it [00:02, 923.67it/s]warmup run: 1318it [00:02, 967.46it/s]warmup run: 983it [00:02, 865.20it/s]warmup run: 1428it [00:02, 978.28it/s]warmup run: 1434it [00:02, 891.33it/s]warmup run: 1229it [00:02, 921.17it/s]warmup run: 1370it [00:02, 950.30it/s]warmup run: 1205it [00:02, 947.70it/s]warmup run: 1406it [00:03, 937.48it/s]warmup run: 1420it [00:02, 982.21it/s]warmup run: 1079it [00:02, 891.65it/s]warmup run: 1529it [00:03, 983.95it/s]warmup run: 1531it [00:03, 911.46it/s]warmup run: 1326it [00:02, 933.78it/s]warmup run: 1470it [00:03, 963.50it/s]warmup run: 1306it [00:02, 952.36it/s]warmup run: 1504it [00:03, 948.24it/s]warmup run: 1522it [00:03, 991.29it/s]warmup run: 1174it [00:02, 907.19it/s]warmup run: 1631it [00:03, 991.97it/s]warmup run: 1626it [00:03, 921.56it/s]warmup run: 1424it [00:02, 946.25it/s]warmup run: 1570it [00:03, 973.23it/s]warmup run: 1406it [00:02, 965.02it/s]warmup run: 1602it [00:03, 957.09it/s]warmup run: 1624it [00:03, 997.87it/s]warmup run: 1270it [00:02, 919.80it/s]warmup run: 1734it [00:03, 1001.85it/s]warmup run: 1722it [00:03, 932.35it/s]warmup run: 1521it [00:03, 942.27it/s]warmup run: 1671it [00:03, 982.04it/s]warmup run: 1506it [00:02, 972.60it/s]warmup run: 1699it [00:03, 955.60it/s]warmup run: 1727it [00:03, 1006.15it/s]warmup run: 1365it [00:03, 925.83it/s]warmup run: 1838it [00:03, 1011.86it/s]warmup run: 1819it [00:03, 942.54it/s]warmup run: 1621it [00:03, 958.40it/s]warmup run: 1771it [00:03, 985.50it/s]warmup run: 1606it [00:03, 979.88it/s]warmup run: 1796it [00:03, 952.20it/s]warmup run: 1830it [00:03, 1010.56it/s]warmup run: 1460it [00:03, 929.76it/s]warmup run: 1917it [00:03, 951.84it/s]warmup run: 1943it [00:03, 1020.71it/s]warmup run: 1721it [00:03, 970.69it/s]warmup run: 1872it [00:03, 991.10it/s]warmup run: 1708it [00:03, 991.25it/s]warmup run: 1893it [00:03, 954.95it/s]warmup run: 1934it [00:03, 1019.27it/s]warmup run: 1556it [00:03, 936.45it/s]warmup run: 2054it [00:03, 1046.26it/s]warmup run: 2018it [00:03, 967.37it/s]warmup run: 1823it [00:03, 984.02it/s]warmup run: 1974it [00:03, 996.99it/s]warmup run: 1811it [00:03, 1002.03it/s]warmup run: 1993it [00:03, 966.04it/s]warmup run: 2043it [00:03, 1039.76it/s]warmup run: 1652it [00:03, 942.99it/s]warmup run: 2139it [00:03, 1038.70it/s]warmup run: 2177it [00:03, 1098.86it/s]warmup run: 1925it [00:03, 992.72it/s]warmup run: 2089it [00:03, 1040.22it/s]warmup run: 1913it [00:03, 1005.14it/s]warmup run: 2113it [00:03, 1034.63it/s]warmup run: 2163it [00:03, 1085.46it/s]warmup run: 1749it [00:03, 948.76it/s]warmup run: 2260it [00:03, 1088.71it/s]warmup run: 2300it [00:03, 1136.64it/s]warmup run: 2031it [00:03, 1010.28it/s]warmup run: 2209it [00:03, 1086.47it/s]warmup run: 2018it [00:03, 1016.43it/s]warmup run: 2236it [00:03, 1091.53it/s]warmup run: 2283it [00:03, 1118.63it/s]warmup run: 1845it [00:03, 951.79it/s]warmup run: 2381it [00:03, 1124.57it/s]warmup run: 2423it [00:03, 1163.60it/s]warmup run: 2151it [00:03, 1065.18it/s]warmup run: 2330it [00:03, 1120.73it/s]warmup run: 2139it [00:03, 1071.52it/s]warmup run: 2359it [00:03, 1131.36it/s]warmup run: 2403it [00:03, 1141.21it/s]warmup run: 1943it [00:03, 957.63it/s]warmup run: 2503it [00:03, 1150.45it/s]warmup run: 2546it [00:03, 1183.20it/s]warmup run: 2271it [00:03, 1104.69it/s]warmup run: 2451it [00:03, 1145.72it/s]warmup run: 2260it [00:03, 1112.31it/s]warmup run: 2482it [00:04, 1159.40it/s]warmup run: 2523it [00:03, 1156.46it/s]warmup run: 2051it [00:03, 993.42it/s]warmup run: 2625it [00:04, 1168.92it/s]warmup run: 2670it [00:04, 1198.10it/s]warmup run: 2391it [00:03, 1133.01it/s]warmup run: 2572it [00:04, 1163.68it/s]warmup run: 2382it [00:03, 1141.99it/s]warmup run: 2605it [00:04, 1178.24it/s]warmup run: 2643it [00:04, 1168.20it/s]warmup run: 2174it [00:03, 1061.68it/s]warmup run: 2792it [00:04, 1203.31it/s]warmup run: 2747it [00:04, 1181.71it/s]warmup run: 2511it [00:04, 1152.75it/s]warmup run: 2693it [00:04, 1175.83it/s]warmup run: 2504it [00:03, 1163.25it/s]warmup run: 2728it [00:04, 1192.76it/s]warmup run: 2761it [00:04, 1171.47it/s]warmup run: 2297it [00:03, 1109.72it/s]warmup run: 2915it [00:04, 1210.30it/s]warmup run: 2867it [00:04, 1185.48it/s]warmup run: 2631it [00:04, 1166.36it/s]warmup run: 2813it [00:04, 1180.18it/s]warmup run: 2626it [00:03, 1178.55it/s]warmup run: 2850it [00:04, 1198.00it/s]warmup run: 3000it [00:04, 693.17it/s] warmup run: 2881it [00:04, 1177.64it/s]warmup run: 2419it [00:04, 1142.43it/s]warmup run: 2988it [00:04, 1192.36it/s]warmup run: 3000it [00:04, 683.34it/s] warmup run: 2751it [00:04, 1173.91it/s]warmup run: 2934it [00:04, 1187.15it/s]warmup run: 2748it [00:04, 1190.78it/s]warmup run: 2973it [00:04, 1206.24it/s]warmup run: 3000it [00:04, 694.89it/s] warmup run: 2542it [00:04, 1165.81it/s]warmup run: 3000it [00:04, 668.74it/s] warmup run: 3000it [00:04, 681.40it/s] warmup run: 2869it [00:04, 1173.68it/s]warmup run: 2868it [00:04, 1192.93it/s]warmup run: 2663it [00:04, 1177.14it/s]warmup run: 2988it [00:04, 1178.05it/s]warmup run: 3000it [00:04, 678.48it/s] warmup run: 2990it [00:04, 1199.12it/s]warmup run: 3000it [00:04, 695.59it/s] warmup run: 2783it [00:04, 1181.64it/s]warmup run: 2907it [00:04, 1196.79it/s]warmup run: 3000it [00:04, 666.67it/s] 





warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   6%|▌         | 168/3000 [00:00<00:01, 1678.73it/s]warmup should be done:   5%|▌         | 155/3000 [00:00<00:01, 1549.25it/s]warmup should be done:   6%|▌         | 166/3000 [00:00<00:01, 1658.31it/s]warmup should be done:   6%|▌         | 166/3000 [00:00<00:01, 1657.03it/s]warmup should be done:   6%|▌         | 165/3000 [00:00<00:01, 1642.83it/s]warmup should be done:   5%|▌         | 163/3000 [00:00<00:01, 1624.83it/s]warmup should be done:   5%|▌         | 161/3000 [00:00<00:01, 1602.43it/s]warmup should be done:   6%|▌         | 166/3000 [00:00<00:01, 1651.42it/s]warmup should be done:  11%|█         | 337/3000 [00:00<00:01, 1681.94it/s]warmup should be done:  11%|█         | 318/3000 [00:00<00:01, 1593.52it/s]warmup should be done:  11%|█         | 331/3000 [00:00<00:01, 1651.76it/s]warmup should be done:  11%|█         | 334/3000 [00:00<00:01, 1667.45it/s]warmup should be done:  11%|█         | 334/3000 [00:00<00:01, 1668.30it/s]warmup should be done:  11%|█         | 330/3000 [00:00<00:01, 1650.42it/s]warmup should be done:  11%|█         | 333/3000 [00:00<00:01, 1659.88it/s]warmup should be done:  11%|█         | 324/3000 [00:00<00:01, 1613.46it/s]warmup should be done:  16%|█▌        | 481/3000 [00:00<00:01, 1608.22it/s]warmup should be done:  17%|█▋        | 499/3000 [00:00<00:01, 1659.44it/s]warmup should be done:  17%|█▋        | 501/3000 [00:00<00:01, 1666.79it/s]warmup should be done:  17%|█▋        | 501/3000 [00:00<00:01, 1666.05it/s]warmup should be done:  17%|█▋        | 506/3000 [00:00<00:01, 1680.05it/s]warmup should be done:  17%|█▋        | 497/3000 [00:00<00:01, 1649.81it/s]warmup should be done:  16%|█▌        | 486/3000 [00:00<00:01, 1610.01it/s]warmup should be done:  17%|█▋        | 496/3000 [00:00<00:01, 1638.96it/s]warmup should be done:  22%|██▏       | 668/3000 [00:00<00:01, 1665.13it/s]warmup should be done:  22%|██▏       | 666/3000 [00:00<00:01, 1659.94it/s]warmup should be done:  22%|██▎       | 675/3000 [00:00<00:01, 1681.65it/s]warmup should be done:  22%|██▏       | 668/3000 [00:00<00:01, 1664.14it/s]warmup should be done:  21%|██▏       | 642/3000 [00:00<00:01, 1603.59it/s]warmup should be done:  22%|██▏       | 663/3000 [00:00<00:01, 1651.96it/s]warmup should be done:  22%|██▏       | 648/3000 [00:00<00:01, 1608.62it/s]warmup should be done:  22%|██▏       | 663/3000 [00:00<00:01, 1649.38it/s]warmup should be done:  28%|██▊       | 832/3000 [00:00<00:01, 1659.04it/s]warmup should be done:  27%|██▋       | 806/3000 [00:00<00:01, 1615.78it/s]warmup should be done:  28%|██▊       | 829/3000 [00:00<00:01, 1653.37it/s]warmup should be done:  28%|██▊       | 835/3000 [00:00<00:01, 1661.68it/s]warmup should be done:  28%|██▊       | 844/3000 [00:00<00:01, 1679.29it/s]warmup should be done:  28%|██▊       | 835/3000 [00:00<00:01, 1661.07it/s]warmup should be done:  27%|██▋       | 809/3000 [00:00<00:01, 1606.97it/s]warmup should be done:  28%|██▊       | 830/3000 [00:00<00:01, 1655.14it/s]warmup should be done:  33%|███▎      | 998/3000 [00:00<00:01, 1656.54it/s]warmup should be done:  32%|███▏      | 970/3000 [00:00<00:01, 1620.68it/s]warmup should be done:  33%|███▎      | 995/3000 [00:00<00:01, 1651.16it/s]warmup should be done:  33%|███▎      | 1002/3000 [00:00<00:01, 1661.68it/s]warmup should be done:  34%|███▎      | 1012/3000 [00:00<00:01, 1675.26it/s]warmup should be done:  33%|███▎      | 1002/3000 [00:00<00:01, 1659.18it/s]warmup should be done:  33%|███▎      | 996/3000 [00:00<00:01, 1655.33it/s]warmup should be done:  32%|███▏      | 970/3000 [00:00<00:01, 1602.64it/s]warmup should be done:  38%|███▊      | 1133/3000 [00:00<00:01, 1620.95it/s]warmup should be done:  39%|███▉      | 1164/3000 [00:00<00:01, 1651.29it/s]warmup should be done:  39%|███▉      | 1169/3000 [00:00<00:01, 1659.03it/s]warmup should be done:  39%|███▉      | 1168/3000 [00:00<00:01, 1656.65it/s]warmup should be done:  39%|███▊      | 1161/3000 [00:00<00:01, 1647.58it/s]warmup should be done:  39%|███▊      | 1162/3000 [00:00<00:01, 1652.87it/s]warmup should be done:  39%|███▉      | 1180/3000 [00:00<00:01, 1670.15it/s]warmup should be done:  38%|███▊      | 1131/3000 [00:00<00:01, 1597.08it/s]warmup should be done:  43%|████▎     | 1296/3000 [00:00<00:01, 1623.73it/s]warmup should be done:  44%|████▍     | 1334/3000 [00:00<00:01, 1656.79it/s]warmup should be done:  44%|████▍     | 1330/3000 [00:00<00:01, 1650.44it/s]warmup should be done:  44%|████▍     | 1326/3000 [00:00<00:01, 1647.67it/s]warmup should be done:  45%|████▍     | 1336/3000 [00:00<00:01, 1659.58it/s]warmup should be done:  44%|████▍     | 1328/3000 [00:00<00:01, 1653.87it/s]warmup should be done:  45%|████▍     | 1348/3000 [00:00<00:00, 1670.62it/s]warmup should be done:  43%|████▎     | 1291/3000 [00:00<00:01, 1595.88it/s]warmup should be done:  49%|████▊     | 1460/3000 [00:00<00:00, 1625.96it/s]warmup should be done:  50%|█████     | 1500/3000 [00:00<00:00, 1656.57it/s]warmup should be done:  50%|████▉     | 1496/3000 [00:00<00:00, 1651.34it/s]warmup should be done:  50%|████▉     | 1491/3000 [00:00<00:00, 1646.37it/s]warmup should be done:  50%|█████     | 1503/3000 [00:00<00:00, 1660.23it/s]warmup should be done:  50%|████▉     | 1494/3000 [00:00<00:00, 1653.32it/s]warmup should be done:  51%|█████     | 1516/3000 [00:00<00:00, 1669.71it/s]warmup should be done:  48%|████▊     | 1452/3000 [00:00<00:00, 1597.56it/s]warmup should be done:  54%|█████▍    | 1623/3000 [00:01<00:00, 1627.04it/s]warmup should be done:  55%|█████▌    | 1656/3000 [00:01<00:00, 1647.43it/s]warmup should be done:  56%|█████▌    | 1666/3000 [00:01<00:00, 1656.46it/s]warmup should be done:  55%|█████▌    | 1662/3000 [00:01<00:00, 1652.46it/s]warmup should be done:  55%|█████▌    | 1660/3000 [00:01<00:00, 1655.34it/s]warmup should be done:  56%|█████▌    | 1683/3000 [00:01<00:00, 1666.93it/s]warmup should be done:  54%|█████▍    | 1613/3000 [00:01<00:00, 1601.02it/s]warmup should be done:  56%|█████▌    | 1670/3000 [00:01<00:00, 1648.45it/s]warmup should be done:  60%|█████▉    | 1787/3000 [00:01<00:00, 1628.96it/s]warmup should be done:  61%|██████    | 1821/3000 [00:01<00:00, 1647.78it/s]warmup should be done:  61%|██████    | 1832/3000 [00:01<00:00, 1655.16it/s]warmup should be done:  61%|██████    | 1828/3000 [00:01<00:00, 1652.74it/s]warmup should be done:  61%|██████    | 1826/3000 [00:01<00:00, 1655.99it/s]warmup should be done:  62%|██████▏   | 1851/3000 [00:01<00:00, 1667.98it/s]warmup should be done:  59%|█████▉    | 1777/3000 [00:01<00:00, 1612.60it/s]warmup should be done:  61%|██████    | 1835/3000 [00:01<00:00, 1638.87it/s]warmup should be done:  65%|██████▌   | 1950/3000 [00:01<00:00, 1629.08it/s]warmup should be done:  66%|██████▌   | 1986/3000 [00:01<00:00, 1647.58it/s]warmup should be done:  67%|██████▋   | 1998/3000 [00:01<00:00, 1656.38it/s]warmup should be done:  66%|██████▋   | 1994/3000 [00:01<00:00, 1651.84it/s]warmup should be done:  66%|██████▋   | 1992/3000 [00:01<00:00, 1653.39it/s]warmup should be done:  65%|██████▍   | 1941/3000 [00:01<00:00, 1619.61it/s]warmup should be done:  67%|██████▋   | 2018/3000 [00:01<00:00, 1666.00it/s]warmup should be done:  67%|██████▋   | 2002/3000 [00:01<00:00, 1645.65it/s]warmup should be done:  70%|███████   | 2113/3000 [00:01<00:00, 1628.18it/s]warmup should be done:  72%|███████▏  | 2164/3000 [00:01<00:00, 1657.23it/s]warmup should be done:  72%|███████▏  | 2151/3000 [00:01<00:00, 1646.87it/s]warmup should be done:  72%|███████▏  | 2160/3000 [00:01<00:00, 1649.23it/s]warmup should be done:  70%|███████   | 2105/3000 [00:01<00:00, 1625.08it/s]warmup should be done:  72%|███████▏  | 2158/3000 [00:01<00:00, 1648.65it/s]warmup should be done:  73%|███████▎  | 2185/3000 [00:01<00:00, 1666.05it/s]warmup should be done:  72%|███████▏  | 2169/3000 [00:01<00:00, 1650.61it/s]warmup should be done:  76%|███████▌  | 2276/3000 [00:01<00:00, 1626.28it/s]warmup should be done:  78%|███████▊  | 2330/3000 [00:01<00:00, 1656.63it/s]warmup should be done:  77%|███████▋  | 2316/3000 [00:01<00:00, 1644.43it/s]warmup should be done:  78%|███████▊  | 2325/3000 [00:01<00:00, 1647.35it/s]warmup should be done:  76%|███████▌  | 2269/3000 [00:01<00:00, 1628.55it/s]warmup should be done:  78%|███████▊  | 2352/3000 [00:01<00:00, 1663.25it/s]warmup should be done:  77%|███████▋  | 2323/3000 [00:01<00:00, 1639.09it/s]warmup should be done:  78%|███████▊  | 2336/3000 [00:01<00:00, 1653.74it/s]warmup should be done:  81%|████████▏ | 2439/3000 [00:01<00:00, 1621.78it/s]warmup should be done:  83%|████████▎ | 2496/3000 [00:01<00:00, 1653.87it/s]warmup should be done:  83%|████████▎ | 2481/3000 [00:01<00:00, 1640.15it/s]warmup should be done:  81%|████████  | 2432/3000 [00:01<00:00, 1628.68it/s]warmup should be done:  83%|████████▎ | 2490/3000 [00:01<00:00, 1646.46it/s]warmup should be done:  84%|████████▍ | 2519/3000 [00:01<00:00, 1663.91it/s]warmup should be done:  83%|████████▎ | 2487/3000 [00:01<00:00, 1628.87it/s]warmup should be done:  83%|████████▎ | 2502/3000 [00:01<00:00, 1653.22it/s]warmup should be done:  87%|████████▋ | 2602/3000 [00:01<00:00, 1623.43it/s]warmup should be done:  89%|████████▊ | 2662/3000 [00:01<00:00, 1653.56it/s]warmup should be done:  88%|████████▊ | 2646/3000 [00:01<00:00, 1641.20it/s]warmup should be done:  87%|████████▋ | 2596/3000 [00:01<00:00, 1631.58it/s]warmup should be done:  89%|████████▊ | 2656/3000 [00:01<00:00, 1648.03it/s]warmup should be done:  90%|████████▉ | 2686/3000 [00:01<00:00, 1664.74it/s]warmup should be done:  89%|████████▉ | 2668/3000 [00:01<00:00, 1655.16it/s]warmup should be done:  88%|████████▊ | 2650/3000 [00:01<00:00, 1626.48it/s]warmup should be done:  92%|█████████▏| 2765/3000 [00:01<00:00, 1623.87it/s]warmup should be done:  94%|█████████▍| 2829/3000 [00:01<00:00, 1656.39it/s]warmup should be done:  94%|█████████▎| 2811/3000 [00:01<00:00, 1642.69it/s]warmup should be done:  92%|█████████▏| 2760/3000 [00:01<00:00, 1633.26it/s]warmup should be done:  94%|█████████▍| 2821/3000 [00:01<00:00, 1648.55it/s]warmup should be done:  95%|█████████▌| 2854/3000 [00:01<00:00, 1666.98it/s]warmup should be done:  94%|█████████▍| 2835/3000 [00:01<00:00, 1659.56it/s]warmup should be done:  94%|█████████▍| 2813/3000 [00:01<00:00, 1624.75it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1669.37it/s]warmup should be done:  98%|█████████▊| 2930/3000 [00:01<00:00, 1628.92it/s]warmup should be done: 100%|█████████▉| 2997/3000 [00:01<00:00, 1660.62it/s]warmup should be done:  99%|█████████▉| 2977/3000 [00:01<00:00, 1647.36it/s]warmup should be done:  98%|█████████▊| 2926/3000 [00:01<00:00, 1638.93it/s]warmup should be done: 100%|█████████▉| 2987/3000 [00:01<00:00, 1650.88it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1658.04it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1656.28it/s]warmup should be done:  99%|█████████▉| 2977/3000 [00:01<00:00, 1627.68it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1651.66it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1646.76it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1641.84it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1622.02it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1618.67it/s]





warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]

warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   6%|▌         | 168/3000 [00:00<00:01, 1679.71it/s]warmup should be done:   6%|▌         | 170/3000 [00:00<00:01, 1697.78it/s]warmup should be done:   6%|▌         | 169/3000 [00:00<00:01, 1686.33it/s]warmup should be done:   6%|▌         | 167/3000 [00:00<00:01, 1666.79it/s]warmup should be done:   5%|▌         | 161/3000 [00:00<00:01, 1605.63it/s]warmup should be done:   5%|▌         | 162/3000 [00:00<00:01, 1615.88it/s]warmup should be done:   6%|▌         | 166/3000 [00:00<00:01, 1653.74it/s]warmup should be done:   6%|▌         | 165/3000 [00:00<00:01, 1641.46it/s]warmup should be done:  11%|█         | 336/3000 [00:00<00:01, 1679.35it/s]warmup should be done:  11%|█▏        | 340/3000 [00:00<00:01, 1698.28it/s]warmup should be done:  11%|█         | 335/3000 [00:00<00:01, 1672.95it/s]warmup should be done:  11%|█         | 332/3000 [00:00<00:01, 1663.08it/s]warmup should be done:  11%|█▏        | 338/3000 [00:00<00:01, 1684.55it/s]warmup should be done:  11%|█         | 326/3000 [00:00<00:01, 1626.60it/s]warmup should be done:  11%|█         | 330/3000 [00:00<00:01, 1643.22it/s]warmup should be done:  11%|█         | 332/3000 [00:00<00:01, 1632.25it/s]warmup should be done:  17%|█▋        | 505/3000 [00:00<00:01, 1682.93it/s]warmup should be done:  17%|█▋        | 511/3000 [00:00<00:01, 1699.85it/s]warmup should be done:  17%|█▋        | 501/3000 [00:00<00:01, 1672.66it/s]warmup should be done:  17%|█▋        | 504/3000 [00:00<00:01, 1676.45it/s]warmup should be done:  17%|█▋        | 500/3000 [00:00<00:01, 1667.92it/s]warmup should be done:  17%|█▋        | 509/3000 [00:00<00:01, 1692.29it/s]warmup should be done:  17%|█▋        | 498/3000 [00:00<00:01, 1640.88it/s]warmup should be done:  16%|█▋        | 489/3000 [00:00<00:01, 1604.50it/s]warmup should be done:  22%|██▏       | 674/3000 [00:00<00:01, 1683.68it/s]warmup should be done:  23%|██▎       | 682/3000 [00:00<00:01, 1701.81it/s]warmup should be done:  22%|██▏       | 673/3000 [00:00<00:01, 1689.12it/s]warmup should be done:  22%|██▏       | 671/3000 [00:00<00:01, 1682.64it/s]warmup should be done:  22%|██▏       | 673/3000 [00:00<00:01, 1679.00it/s]warmup should be done:  23%|██▎       | 681/3000 [00:00<00:01, 1700.01it/s]warmup should be done:  22%|██▏       | 666/3000 [00:00<00:01, 1656.06it/s]warmup should be done:  22%|██▏       | 655/3000 [00:00<00:01, 1624.21it/s]warmup should be done:  28%|██▊       | 843/3000 [00:00<00:01, 1681.45it/s]warmup should be done:  28%|██▊       | 853/3000 [00:00<00:01, 1703.09it/s]warmup should be done:  28%|██▊       | 845/3000 [00:00<00:01, 1699.14it/s]warmup should be done:  28%|██▊       | 842/3000 [00:00<00:01, 1691.37it/s]warmup should be done:  28%|██▊       | 842/3000 [00:00<00:01, 1679.77it/s]warmup should be done:  28%|██▊       | 853/3000 [00:00<00:01, 1704.95it/s]warmup should be done:  28%|██▊       | 832/3000 [00:00<00:01, 1624.48it/s]warmup should be done:  27%|██▋       | 818/3000 [00:00<00:01, 1597.48it/s]warmup should be done:  34%|███▍      | 1016/3000 [00:00<00:01, 1702.55it/s]warmup should be done:  34%|███▎      | 1012/3000 [00:00<00:01, 1693.43it/s]warmup should be done:  34%|███▍      | 1024/3000 [00:00<00:01, 1700.40it/s]warmup should be done:  34%|███▍      | 1024/3000 [00:00<00:01, 1705.51it/s]warmup should be done:  34%|███▎      | 1011/3000 [00:00<00:01, 1680.47it/s]warmup should be done:  34%|███▎      | 1012/3000 [00:00<00:01, 1677.10it/s]warmup should be done:  33%|███▎      | 995/3000 [00:00<00:01, 1617.13it/s]warmup should be done:  33%|███▎      | 978/3000 [00:00<00:01, 1584.69it/s]warmup should be done:  40%|███▉      | 1195/3000 [00:00<00:01, 1705.08it/s]warmup should be done:  39%|███▉      | 1182/3000 [00:00<00:01, 1690.28it/s]warmup should be done:  40%|███▉      | 1187/3000 [00:00<00:01, 1698.09it/s]warmup should be done:  40%|███▉      | 1195/3000 [00:00<00:01, 1699.11it/s]warmup should be done:  39%|███▉      | 1180/3000 [00:00<00:01, 1672.17it/s]warmup should be done:  39%|███▉      | 1180/3000 [00:00<00:01, 1676.04it/s]warmup should be done:  39%|███▊      | 1161/3000 [00:00<00:01, 1628.92it/s]warmup should be done:  38%|███▊      | 1142/3000 [00:00<00:01, 1599.57it/s]warmup should be done:  46%|████▌     | 1367/3000 [00:00<00:00, 1709.34it/s]warmup should be done:  45%|████▌     | 1352/3000 [00:00<00:00, 1690.09it/s]warmup should be done:  45%|████▌     | 1359/3000 [00:00<00:00, 1702.14it/s]warmup should be done:  46%|████▌     | 1367/3000 [00:00<00:00, 1703.19it/s]warmup should be done:  45%|████▍     | 1349/3000 [00:00<00:00, 1675.08it/s]warmup should be done:  45%|████▍     | 1349/3000 [00:00<00:00, 1677.51it/s]warmup should be done:  44%|████▍     | 1328/3000 [00:00<00:01, 1638.97it/s]warmup should be done:  44%|████▎     | 1309/3000 [00:00<00:01, 1620.97it/s]warmup should be done:  51%|█████▏    | 1539/3000 [00:00<00:00, 1709.94it/s]warmup should be done:  51%|█████     | 1530/3000 [00:00<00:00, 1703.51it/s]warmup should be done:  51%|█████     | 1522/3000 [00:00<00:00, 1691.81it/s]warmup should be done:  51%|█████▏    | 1538/3000 [00:00<00:00, 1701.96it/s]warmup should be done:  51%|█████     | 1517/3000 [00:00<00:00, 1677.44it/s]warmup should be done:  51%|█████     | 1519/3000 [00:00<00:00, 1680.00it/s]warmup should be done:  50%|████▉     | 1496/3000 [00:00<00:00, 1650.28it/s]warmup should be done:  49%|████▉     | 1476/3000 [00:00<00:00, 1635.19it/s]warmup should be done:  57%|█████▋    | 1711/3000 [00:01<00:00, 1711.18it/s]warmup should be done:  57%|█████▋    | 1702/3000 [00:01<00:00, 1706.90it/s]warmup should be done:  56%|█████▋    | 1693/3000 [00:01<00:00, 1694.73it/s]warmup should be done:  56%|█████▌    | 1686/3000 [00:01<00:00, 1679.94it/s]warmup should be done:  56%|█████▋    | 1689/3000 [00:01<00:00, 1684.78it/s]warmup should be done:  57%|█████▋    | 1709/3000 [00:01<00:00, 1694.78it/s]warmup should be done:  56%|█████▌    | 1665/3000 [00:01<00:00, 1659.38it/s]warmup should be done:  55%|█████▍    | 1644/3000 [00:01<00:00, 1646.13it/s]warmup should be done:  63%|██████▎   | 1883/3000 [00:01<00:00, 1712.63it/s]warmup should be done:  62%|██████▏   | 1874/3000 [00:01<00:00, 1708.61it/s]warmup should be done:  62%|██████▏   | 1864/3000 [00:01<00:00, 1697.36it/s]warmup should be done:  62%|██████▏   | 1855/3000 [00:01<00:00, 1681.22it/s]warmup should be done:  62%|██████▏   | 1859/3000 [00:01<00:00, 1688.27it/s]warmup should be done:  63%|██████▎   | 1879/3000 [00:01<00:00, 1691.64it/s]warmup should be done:  61%|██████    | 1833/3000 [00:01<00:00, 1664.99it/s]warmup should be done:  60%|██████    | 1809/3000 [00:01<00:00, 1639.96it/s]warmup should be done:  69%|██████▊   | 2056/3000 [00:01<00:00, 1714.92it/s]warmup should be done:  68%|██████▊   | 2046/3000 [00:01<00:00, 1709.78it/s]warmup should be done:  68%|██████▊   | 2034/3000 [00:01<00:00, 1696.69it/s]warmup should be done:  67%|██████▋   | 2024/3000 [00:01<00:00, 1679.43it/s]warmup should be done:  68%|██████▊   | 2028/3000 [00:01<00:00, 1679.51it/s]warmup should be done:  68%|██████▊   | 2049/3000 [00:01<00:00, 1690.36it/s]warmup should be done:  67%|██████▋   | 2000/3000 [00:01<00:00, 1664.91it/s]warmup should be done:  66%|██████▌   | 1975/3000 [00:01<00:00, 1645.94it/s]warmup should be done:  74%|███████▍  | 2228/3000 [00:01<00:00, 1715.10it/s]warmup should be done:  74%|███████▍  | 2217/3000 [00:01<00:00, 1708.64it/s]warmup should be done:  73%|███████▎  | 2204/3000 [00:01<00:00, 1693.95it/s]warmup should be done:  73%|███████▎  | 2192/3000 [00:01<00:00, 1678.06it/s]warmup should be done:  73%|███████▎  | 2196/3000 [00:01<00:00, 1675.26it/s]warmup should be done:  74%|███████▍  | 2219/3000 [00:01<00:00, 1687.19it/s]warmup should be done:  72%|███████▏  | 2167/3000 [00:01<00:00, 1666.14it/s]warmup should be done:  71%|███████▏  | 2143/3000 [00:01<00:00, 1654.34it/s]warmup should be done:  80%|███████▉  | 2388/3000 [00:01<00:00, 1708.19it/s]warmup should be done:  80%|████████  | 2400/3000 [00:01<00:00, 1713.89it/s]warmup should be done:  79%|███████▉  | 2374/3000 [00:01<00:00, 1694.88it/s]warmup should be done:  79%|███████▊  | 2360/3000 [00:01<00:00, 1674.91it/s]warmup should be done:  79%|███████▉  | 2364/3000 [00:01<00:00, 1674.16it/s]warmup should be done:  80%|███████▉  | 2388/3000 [00:01<00:00, 1684.39it/s]warmup should be done:  78%|███████▊  | 2335/3000 [00:01<00:00, 1669.73it/s]warmup should be done:  77%|███████▋  | 2312/3000 [00:01<00:00, 1662.59it/s]warmup should be done:  86%|████████▌ | 2572/3000 [00:01<00:00, 1713.79it/s]warmup should be done:  85%|████████▌ | 2560/3000 [00:01<00:00, 1709.42it/s]warmup should be done:  84%|████████▍ | 2528/3000 [00:01<00:00, 1672.69it/s]warmup should be done:  84%|████████▍ | 2532/3000 [00:01<00:00, 1673.12it/s]warmup should be done:  85%|████████▌ | 2557/3000 [00:01<00:00, 1684.30it/s]warmup should be done:  85%|████████▍ | 2544/3000 [00:01<00:00, 1657.00it/s]warmup should be done:  83%|████████▎ | 2503/3000 [00:01<00:00, 1671.20it/s]warmup should be done:  83%|████████▎ | 2480/3000 [00:01<00:00, 1666.27it/s]warmup should be done:  91%|█████████ | 2731/3000 [00:01<00:00, 1709.54it/s]warmup should be done:  91%|█████████▏| 2744/3000 [00:01<00:00, 1713.09it/s]warmup should be done:  90%|████████▉ | 2696/3000 [00:01<00:00, 1668.02it/s]warmup should be done:  90%|█████████ | 2700/3000 [00:01<00:00, 1672.65it/s]warmup should be done:  91%|█████████ | 2726/3000 [00:01<00:00, 1661.87it/s]warmup should be done:  90%|█████████ | 2710/3000 [00:01<00:00, 1644.87it/s]warmup should be done:  88%|████████▊ | 2648/3000 [00:01<00:00, 1667.89it/s]warmup should be done:  89%|████████▉ | 2671/3000 [00:01<00:00, 1657.95it/s]warmup should be done:  97%|█████████▋| 2902/3000 [00:01<00:00, 1707.21it/s]warmup should be done:  97%|█████████▋| 2916/3000 [00:01<00:00, 1713.18it/s]warmup should be done:  95%|█████████▌| 2863/3000 [00:01<00:00, 1665.64it/s]warmup should be done:  96%|█████████▌| 2868/3000 [00:01<00:00, 1665.11it/s]warmup should be done:  96%|█████████▋| 2895/3000 [00:01<00:00, 1667.43it/s]warmup should be done:  96%|█████████▌| 2878/3000 [00:01<00:00, 1654.64it/s]warmup should be done:  94%|█████████▍| 2816/3000 [00:01<00:00, 1668.52it/s]warmup should be done:  95%|█████████▍| 2837/3000 [00:01<00:00, 1645.71it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1708.80it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1701.29it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1687.50it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1676.18it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1675.59it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1674.08it/s]warmup should be done: 100%|█████████▉| 2985/3000 [00:01<00:00, 1673.05it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1647.76it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1642.21it/s]
WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f8971c351f0>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f8971f37e80>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f8971f399d0>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f8971c332b0>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f8971c34070>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f8971c25130>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f8971f3ad30>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f8971c331c0>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
2022-12-11 23:11:19.468837: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f84aa82fec0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 23:11:19.468898: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 23:11:19.479858: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 23:11:19.715645: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f84a6830b50 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 23:11:19.715719: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 23:11:19.723635: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 23:11:20.444644: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f84a302d520 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 23:11:20.444708: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 23:11:20.452675: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 23:11:20.534603: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f84a6834be0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 23:11:20.534670: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 23:11:20.535414: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f84ab0291b0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 23:11:20.535463: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 23:11:20.544275: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 23:11:20.544283: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 23:11:20.610231: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f8497030fe0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 23:11:20.610300: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 23:11:20.618238: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 23:11:20.626788: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f84ae795200 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 23:11:20.626845: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 23:11:20.634383: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 23:11:20.660851: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f849a837f80 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 23:11:20.660915: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 23:11:20.670040: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 23:11:26.747947: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 23:11:26.924540: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 23:11:27.198220: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 23:11:27.338660: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 23:11:27.483834: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 23:11:27.517200: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 23:11:27.525284: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 23:11:27.597566: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
[HCTR][23:12:26.490][ERROR][RK0][main]: replica 5 reaches 1000, calling init pre replica
[HCTR][23:12:26.491][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][23:12:26.501][ERROR][RK0][main]: coll ps creation done
[HCTR][23:12:26.501][ERROR][RK0][main]: replica 5 waits for coll ps creation barrier
[HCTR][23:12:26.739][ERROR][RK0][main]: replica 4 reaches 1000, calling init pre replica
[HCTR][23:12:26.739][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][23:12:26.744][ERROR][RK0][main]: coll ps creation done
[HCTR][23:12:26.744][ERROR][RK0][main]: replica 4 waits for coll ps creation barrier
[HCTR][23:12:26.822][ERROR][RK0][tid #140208049739520]: replica 7 reaches 1000, calling init pre replica
[HCTR][23:12:26.822][ERROR][RK0][tid #140208049739520]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][23:12:26.831][ERROR][RK0][tid #140208049739520]: coll ps creation done
[HCTR][23:12:26.831][ERROR][RK0][tid #140208049739520]: replica 7 waits for coll ps creation barrier
[HCTR][23:12:26.979][ERROR][RK0][main]: replica 0 reaches 1000, calling init pre replica
[HCTR][23:12:26.979][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][23:12:26.987][ERROR][RK0][main]: coll ps creation done
[HCTR][23:12:26.987][ERROR][RK0][main]: replica 0 waits for coll ps creation barrier
[HCTR][23:12:27.026][ERROR][RK0][tid #140208502716160]: replica 1 reaches 1000, calling init pre replica
[HCTR][23:12:27.027][ERROR][RK0][tid #140208502716160]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][23:12:27.031][ERROR][RK0][tid #140208502716160]: coll ps creation done
[HCTR][23:12:27.031][ERROR][RK0][tid #140208502716160]: replica 1 waits for coll ps creation barrier
[HCTR][23:12:27.080][ERROR][RK0][tid #140208041346816]: replica 6 reaches 1000, calling init pre replica
[HCTR][23:12:27.080][ERROR][RK0][tid #140208041346816]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][23:12:27.085][ERROR][RK0][tid #140208041346816]: coll ps creation done
[HCTR][23:12:27.085][ERROR][RK0][tid #140208041346816]: replica 6 waits for coll ps creation barrier
[HCTR][23:12:27.150][ERROR][RK0][tid #140208100062976]: replica 3 reaches 1000, calling init pre replica
[HCTR][23:12:27.150][ERROR][RK0][tid #140208100062976]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][23:12:27.154][ERROR][RK0][tid #140208100062976]: coll ps creation done
[HCTR][23:12:27.154][ERROR][RK0][tid #140208100062976]: replica 3 waits for coll ps creation barrier
[HCTR][23:12:27.185][ERROR][RK0][tid #140208511108864]: replica 2 reaches 1000, calling init pre replica
[HCTR][23:12:27.186][ERROR][RK0][tid #140208511108864]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][23:12:27.193][ERROR][RK0][tid #140208511108864]: coll ps creation done
[HCTR][23:12:27.193][ERROR][RK0][tid #140208511108864]: replica 2 waits for coll ps creation barrier
[HCTR][23:12:27.193][ERROR][RK0][main]: replica 0 preparing frequency
[HCTR][23:12:28.026][ERROR][RK0][main]: replica 0 preparing frequency done
[HCTR][23:12:28.060][ERROR][RK0][main]: replica 0 calling init per replica
[HCTR][23:12:28.060][ERROR][RK0][tid #140208049739520]: replica 7 calling init per replica
[HCTR][23:12:28.060][ERROR][RK0][tid #140208100062976]: replica 3 calling init per replica
[HCTR][23:12:28.060][ERROR][RK0][main]: replica 5 calling init per replica
[HCTR][23:12:28.060][ERROR][RK0][tid #140208511108864]: replica 2 calling init per replica
[HCTR][23:12:28.060][ERROR][RK0][main]: replica 4 calling init per replica
[HCTR][23:12:28.060][ERROR][RK0][tid #140208502716160]: replica 1 calling init per replica
[HCTR][23:12:28.060][ERROR][RK0][tid #140208041346816]: replica 6 calling init per replica
[HCTR][23:12:28.060][ERROR][RK0][main]: Calling build_v2
[HCTR][23:12:28.060][ERROR][RK0][tid #140208049739520]: Calling build_v2
[HCTR][23:12:28.060][ERROR][RK0][tid #140208100062976]: Calling build_v2
[HCTR][23:12:28.060][ERROR][RK0][main]: Calling build_v2
[HCTR][23:12:28.060][ERROR][RK0][tid #140208511108864]: Calling build_v2
[HCTR][23:12:28.060][ERROR][RK0][tid #140208049739520]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][23:12:28.060][ERROR][RK0][main]: Calling build_v2
[HCTR][23:12:28.060][ERROR][RK0][tid #140208502716160]: Calling build_v2
[HCTR][23:12:28.060][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][23:12:28.060][ERROR][RK0][tid #140208041346816]: Calling build_v2
[HCTR][23:12:28.060][ERROR][RK0][tid #140208100062976]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][23:12:28.060][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][23:12:28.060][ERROR][RK0][tid #140208511108864]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][23:12:28.060][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][23:12:28.060][ERROR][RK0][tid #140208502716160]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][23:12:28.060][ERROR][RK0][tid #140208041346816]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[2022-12-11 23:12:28. 64532: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:178[] v100x8, slow pcie
2022-12-11 23:12:28.[ 645762022-12-11 23:12:28: .E 64616 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE: [178/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] :v100x8, slow pcie196
] 2022-12-11 23:12:28assigning 0 to cpu.
[ 646272022-12-11 23:12:28: .E[ 64667 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE: 178/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[] :2022-12-11 23:12:28v100x8, slow pcie196.
]  647012022-12-11 23:12:28assigning 0 to cpu: .[
[E 646682022-12-11 23:12:28 : ./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE2022-12-11 23:12:28 64725: .: 212/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc 64715E] ::  build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8178E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[
[]  v100x8, slow pcie:2022-12-11 23:12:28/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
2022-12-11 23:12:28196.:[.]  64784[178[2022-12-11 23:12:28 64768assigning 0 to cpu: 2022-12-11 23:12:28] .[: 
2022-12-11 23:12:28E.v100x8, slow pcie 64816E2022-12-11 23:12:28.  64823
:  . 64813/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc 64859[[: :E :: 2022-12-11 23:12:282022-12-11 23:12:28E212 /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc178E.. ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:]   64923 64926/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccbuild asymm link desc with 8X Tesla V100-SXM2-32GB out of 8:213v100x8, slow pcie/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: : :
196] 
:EE178] remote time is 8.68421178[ [ ] assigning 0 to cpu
] 2022-12-11 23:12:28/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-11 23:12:28/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccv100x8, slow pcie
v100x8, slow pcie[.:.:

2022-12-11 23:12:28 65082196 65083212.: [[[] : ]  65133E2022-12-11 23:12:282022-12-11 23:12:282022-12-11 23:12:28assigning 0 to cpuEbuild asymm link desc with 8X Tesla V100-SXM2-32GB out of 8:  ...
 
E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc 65185 65185 65187/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc :: [: : :[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc213E2022-12-11 23:12:28EE1962022-12-11 23:12:28:]  .  ] .214remote time is 8.68421/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc 65293/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccassigning 0 to cpu 65303] 
:: ::
: cpu time is 97.0588196E212[196E
]  ] 2022-12-11 23:12:28]  assigning 0 to cpu/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccbuild asymm link desc with 8X Tesla V100-SXM2-32GB out of 8.assigning 0 to cpu/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[
:
 65426
:2022-12-11 23:12:28213: 212[.] E] 2022-12-11 23:12:28[ 65485remote time is 8.68421 build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8.2022-12-11 23:12:28[: 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
 65528.2022-12-11 23:12:28E[:[:  65555. 2022-12-11 23:12:282142022-12-11 23:12:28E:  65562/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.] . E: : 65601cpu time is 97.0588 65592/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc E212: 
: :/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc ] EE213:/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccbuild asymm link desc with 8X Tesla V100-SXM2-32GB out of 8  ] 212:
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccremote time is 8.68421] 212::
build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8] [213214
build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8[2022-12-11 23:12:28] ] 
2022-12-11 23:12:28.[remote time is 8.68421[cpu time is 97.0588. 657542022-12-11 23:12:28
2022-12-11 23:12:28
 65768[: ..: 2022-12-11 23:12:28E 65792 65811E. : :   65840/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccEE/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: :  :E213/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc214 ] ::] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccremote time is 8.68421213213cpu time is 97.0588:
] ] 
[214remote time is 8.68421remote time is 8.684212022-12-11 23:12:28] 

.cpu time is 97.0588 65966[[
: 2022-12-11 23:12:282022-12-11 23:12:28E..  65996 65998/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: : :EE214  ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cccpu time is 97.0588::
214214] ] cpu time is 97.0588cpu time is 97.0588

[2022-12-11 23:13:44.845559: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:118] solver created. now build & solve
[2022-12-11 23:13:44.885593: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:123] solver built. now solve
block 0 storage is 00010001
	access is	0	0	0	0	4	4	4	4	
block 1 storage is 00100010
	access is	1	1	1	1	5	5	5	5	
block 2 storage is 01000100
	access is	2	2	2	2	6	6	6	6	
block 3 storage is 10001000
	access is	3	3	3	3	7	7	7	7	
block 4 storage is 00000000
	access is	8	8	8	8	8	8	8	8	
[2022-12-11 23:13:45. 12828: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:127] solver solved
[2022-12-11 23:13:45. 12889: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:195] 0 solved master
[2022-12-11 23:13:45. 12920: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 0 solved
[2022-12-11 23:13:45. 12949: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 0 initing device 0
[2022-12-11 23:13:45. 13498: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-11 23:13:45. 13542: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 23:13:45. 17642: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 23:13:45. 21667: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 23:13:45. 28148: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 4 solved
[2022-12-11 23:13:45. 28205: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 4 initing device 4
[2022-12-11 23:13:45. 28474: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 7 solved
[2022-12-11 23:13:45. 28546: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 7 initing device 7
[2022-12-11 23:13:45. 28632: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-11 23:13:45. 28686: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 23:13:45. 29013: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-11 23:13:45. 29064: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 23:13:45. 30117: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 23:13:45. 30177: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 23:13:45. 31379: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 23:13:45. 31426: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 23:13:45. 31584: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 2 solved
[2022-12-11 23:13:45.[ 316422022-12-11 23:13:45: .E 31631 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.ccE: 205/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc] :worker 0 thread 2 initing device 2202
] 1 solved
[2022-12-11 23:13:45. 31752: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 1 initing device 1
[2022-12-11 23:13:45. 32126: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-11 23:13:45. 32171: E [/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu2022-12-11 23:13:45:.1980 32181] : eager alloc mem 381.47 MBE
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-11 23:13:45. 32257: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[[2022-12-11 23:13:452022-12-11 23:13:45.. 33628 33631: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc::202202] ] 5 solved6 solved

[[2022-12-11 23:13:452022-12-11 23:13:45.. 33767 33769: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc::205205] ] worker 0 thread 5 initing device 5worker 0 thread 6 initing device 6

[2022-12-11 23:13:45. 33994: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 23:13:45. 34045: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 23:13:45. 34273[: 2022-12-11 23:13:45E.  34282/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu: :E1815 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuBuilding Coll Cache with ... num gpu device is 8:
1815] Building Coll Cache with ... num gpu device is 8
[2022-12-11 23:13:45. 34370: [E2022-12-11 23:13:45 ./hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu 34379:: 1980E]  eager alloc mem 381.47 MB/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu
:1980] eager alloc mem 381.47 MB
[2022-12-11 23:13:45. 36521: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 23:13:45. 36580: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 23:13:45. 36662: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 23:13:45. 36718: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 23:13:45. 37752: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 3 solved
[2022-12-11 23:13:45. 37825: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 3 initing device 3
[2022-12-11 23:13:45. 38397: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-11 23:13:45. 38458: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 23:13:45. 39909: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 23:13:45. 39964: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 23:13:45. 41058: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 23:13:45. 42308: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 23:13:45.100709: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.00 Bytes
[2022-12-11 23:13:45.106130: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5
[2022-12-11 23:13:45.106248: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-11 23:13:45.107160: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 11.83 MB
[2022-12-11 23:13:45.107925: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:13:45.108926: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:13:45.108974: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 11.44 MB
[2022-12-11 23:13:45.112752: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.00 Bytes
[2022-12-11 23:13:45.113837: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 23:13:45.114593: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 23:13:45.114652: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1.44 GB
[2022-12-11 23:13:45.120191: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.00 Bytes
[[2022-12-11 23:13:452022-12-11 23:13:45..121360121360: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::19801980] ] eager alloc mem 5.00 Byteseager alloc mem 5.00 Bytes

[2022-12-11 23:13:45.124445: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5
[2022-12-11 23:13:45.124579: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-11 23:13:45.126452: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.00 Bytes
[[2022-12-11 23:13:452022-12-11 23:13:45..126519126519: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::19801980] ] eager alloc mem 5.00 Byteseager alloc mem 5.00 Bytes

[2022-12-11 23:13:45.129415: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5
[2022-12-11 23:13:45.129495: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-11 23:13:45.133278: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 11.83 MB
[2022-12-11 23:13:45.134289: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5
[2022-12-11 23:13:45.134364: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-11 23:13:45.134502: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 11.83 MB
[2022-12-11 23:13:45.135306: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:13:45.135501: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5
[2022-12-11 23:13:45.135576: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638[] 2022-12-11 23:13:45eager release cuda mem 400000000.
135582: E[ 2022-12-11 23:13:45/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc.:135609638] : eager release cuda mem 5E
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:[19802022-12-11 23:13:45] .eager alloc mem 11.83 MB135662
[: 2022-12-11 23:13:45E. 135703/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc: :E638 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cceager release cuda mem 5:
638] eager release cuda mem 400000000
[2022-12-11 23:13:45.135781: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-11 23:13:45.136056: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:13:45.136282: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:13:45.136328: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 11.44 MB
[2022-12-11 23:13:45.136784: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 11.83 MB
[2022-12-11 23:13:45.137030: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:13:45.137077: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 11.44 MB
[2022-12-11 23:13:45.137496: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 11.83 MB
[2022-12-11 23:13:45.138109: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 11.83 MB
[2022-12-11 23:13:45.138373: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:13:45.138903: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:13:45.139000: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:13:45.139104: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:13:45.139367: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:13:45.139414: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 11.44 MB
[2022-12-11 23:13:45.139877: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:13:45.139926: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 11.44 MB
[2022-12-11 23:13:45.139962: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:13:45.140010: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 11.44 MB
[2022-12-11 23:13:45.140089: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:13:45.140137: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 11.44 MB
[2022-12-11 23:13:45.140341: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5
[2022-12-11 23:13:45.140449: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-11 23:13:45.141343: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 11.83 MB
[2022-12-11 23:13:45.141887: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:13:45.142855: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:13:45.142903: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 11.44 MB
[2022-12-11 23:13:45.144659: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 23:13:45.145193: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 23:13:45.145315: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 23:13:45.145360: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1.44 GB
[2022-12-11 23:13:45.145907: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 23:13:45.145953: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1.44 GB
[2022-12-11 23:13:45.147764: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 23:13:45.148227: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 23:13:45.148304: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 23:13:45.148372: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 23:13:45.148416: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1.44 GB
[2022-12-11 23:13:45.148939: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 23:13:45.148985: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1.44 GB
[2022-12-11 23:13:45.149018: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 23:13:45.149062: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1.44 GB
[2022-12-11 23:13:45.149197: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 23:13:45.149802: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 23:13:45.149845: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1.44 GB
[2022-12-11 23:13:45.152203: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 23:13:45.152924: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 23:13:45.152968: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1.44 GB
[[[[[[[[2022-12-11 23:13:452022-12-11 23:13:452022-12-11 23:13:452022-12-11 23:13:452022-12-11 23:13:452022-12-11 23:13:452022-12-11 23:13:452022-12-11 23:13:45........677979677979677979677979677980677979677979677979: : : : : : : : EEEEEEEE        /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::::::::19261926192619261926192619261926] ] ] ] Device 4 init p2p of link 5] ] ] ] Device 2 init p2p of link 1Device 6 init p2p of link 0Device 3 init p2p of link 2
Device 0 init p2p of link 3Device 1 init p2p of link 7Device 7 init p2p of link 4Device 5 init p2p of link 6






[[[[2022-12-11 23:13:45[2022-12-11 23:13:452022-12-11 23:13:452022-12-11 23:13:45.[2022-12-11 23:13:45.[[..6785052022-12-11 23:13:45.6785052022-12-11 23:13:452022-12-11 23:13:45678505678508: .678517: ..: : E678518: E678522678523EE : E : :   /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuEE/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu: /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:  ::1980/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu19801980] :1980] ::] ] eager alloc mem 611.00 KB1980] eager alloc mem 611.00 KB19801980eager alloc mem 611.00 KBeager alloc mem 611.00 KB
] eager alloc mem 611.00 KB
] ] 

eager alloc mem 611.00 KB
eager alloc mem 611.00 KBeager alloc mem 611.00 KB


[2022-12-11 23:13:45.679574: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:13:45.[6796102022-12-11 23:13:45[[: .[2022-12-11 23:13:452022-12-11 23:13:45E679616[[2022-12-11 23:13:45.. : 2022-12-11 23:13:452022-12-11 23:13:45.679626679628/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccE..679633: : : 679645679644: EE638/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc: : E  ] :EE /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cceager release cuda mem 625663638  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::
] /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638638eager release cuda mem 625663::638] ] 
638638] eager release cuda mem 625663eager release cuda mem 625663] ] eager release cuda mem 625663

eager release cuda mem 625663eager release cuda mem 625663


[2022-12-11 23:13:45.692150: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 6 init p2p of link 5
[2022-12-11 23:13:45.692306: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:13:45.692435: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 2
[2022-12-11 23:13:45.692590: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:13:45.693111: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:13:45.693416: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:13:45.693855: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 4 init p2p of link 7
[2022-12-11 23:13:45.694021: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:13:45.694084: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 2 init p2p of link 3
[2022-12-11 23:13:45.694251: E [/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu2022-12-11 23:13:45:.1980694252] : eager alloc mem 611.00 KBE
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 0 init p2p of link 6
[2022-12-11 23:13:45.694410: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:13:45.694489: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 3 init p2p of link 0
[2022-12-11 23:13:45.694648: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:13:45.694729: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 7 init p2p of link 1
[2022-12-11 23:13:45.694840: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:13:45.694894: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:13:45.694912: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 5 init p2p of link 4
[2022-12-11 23:13:45[.2022-12-11 23:13:45695070.: 695075E:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc :/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu638:] 1980eager release cuda mem 625663] 
eager alloc mem 611.00 KB
[2022-12-11 23:13:45.695180: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:13:45.695448: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:13:45.695667: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:13:45.695946: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:13:45.706989: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 6 init p2p of link 4
[2022-12-11 23:13:45.707107: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:13:45.707318: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 0 init p2p of link 1
[2022-12-11 23:13:45.707437: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:13:45.707485: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 3
[2022-12-11 23:13:45.707611: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:13:45.707682: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 7 init p2p of link 6
[2022-12-11 23:13:45.707797: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:13:45.707917: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:13:45.708018: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 4 init p2p of link 2
[2022-12-11 23:13:45.708139: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:13:45.708259: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:13:45.708300: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 2 init p2p of link 0
[2022-12-11 23:13:45.708417: E[ 2022-12-11 23:13:45/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu.:7084261980: ] Eeager alloc mem 611.00 KB 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:13:45.708599: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:13:45.708709: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 3 init p2p of link 5
[2022-12-11 23:13:45.708824: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:13:45.708918: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:13:45.709182: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 5 init p2p of link 7
[2022-12-11 23:13:45.709254: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:13:45.709311: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:13:45.709585: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:13:45.710109: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:13:45.724046: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 4 init p2p of link 6
[2022-12-11 23:13:45.724164: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:13:45.724680: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 3 init p2p of link 1
[2022-12-11 23:13:45.724793: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:13:45.724927: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 6 init p2p of link 7
[2022-12-11 23:13:45.724978: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:13:45.725041: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:13:45.725325: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 0 init p2p of link 2
[2022-12-11 23:13:45.725440: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:13:45.725606: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:13:45.725678: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 0
[2022-12-11 23:13:45.725796: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:13:45.725849: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:13:45.726065: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 7 init p2p of link 5
[2022-12-11 23:13:45.726179: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:13:45.726247: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:13:45.726593: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:13:45.726788: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 2 init p2p of link 4
[2022-12-11 23:13:45.726905: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:13:45.726982: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:13:45.727367: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 5 init p2p of link 3
[2022-12-11 23:13:45.727491: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:13:45.727671: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:13:45.728258: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:13:45.741852: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 12399996
[2022-12-11 23:13:45.742318: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 12399996
[2022-12-11 23:13:45.742706: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 12399996
[2022-12-11 23:13:45.742855: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 12399996
[2022-12-11 23:13:45.743328: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 12399996
[2022-12-11 23:13:45.743362: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 2999999 / 100000000 nodes ( 3.00 %~3.00 %) | remote 8999997 / 100000000 nodes ( 9.00 %) | cpu 88000004 / 100000000 nodes ( 88.00 %) | 1.44 GB | 0.714681 secs 
[2022-12-11 23:13:45.743579: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 2999999 / 100000000 nodes ( 3.00 %~3.00 %) | remote 8999997 / 100000000 nodes ( 9.00 %) | cpu 88000004 / 100000000 nodes ( 88.00 %) | 1.44 GB | 0.711415 secs 
[2022-12-11 23:13:45.743708: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 12399996
[2022-12-11 23:13:45.743815: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 2999999 / 100000000 nodes ( 3.00 %~3.00 %) | remote 8999997 / 100000000 nodes ( 9.00 %) | cpu 88000004 / 100000000 nodes ( 88.00 %) | 1.44 GB | 0.709443 secs 
[2022-12-11 23:13:45.743947: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 2999999 / 100000000 nodes ( 3.00 %~3.00 %) | remote 8999997 / 100000000 nodes ( 9.00 %) | cpu 88000004 / 100000000 nodes ( 88.00 %) | 1.44 GB | 0.705498 secs 
[2022-12-11 23:13:45.744157: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 12399996
[2022-12-11 23:13:45.744211: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 2999999 / 100000000 nodes ( 3.00 %~3.00 %) | remote 8999997 / 100000000 nodes ( 9.00 %) | cpu 88000004 / 100000000 nodes ( 88.00 %) | 1.44 GB | 0.709849 secs 
[2022-12-11 23:13:45.744436: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 12399996
[2022-12-11 23:13:45.744816: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 2999999 / 100000000 nodes ( 3.00 %~3.00 %) | remote 8999997 / 100000000 nodes ( 9.00 %) | cpu 88000004 / 100000000 nodes ( 88.00 %) | 1.44 GB | 0.731282 secs 
[2022-12-11 23:13:45.745147: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 2999999 / 100000000 nodes ( 3.00 %~3.00 %) | remote 8999997 / 100000000 nodes ( 9.00 %) | cpu 88000004 / 100000000 nodes ( 88.00 %) | 1.44 GB | 0.712898 secs 
[2022-12-11 23:13:45.745366: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 2999999 / 100000000 nodes ( 3.00 %~3.00 %) | remote 8999997 / 100000000 nodes ( 9.00 %) | cpu 88000004 / 100000000 nodes ( 88.00 %) | 1.44 GB | 0.716317 secs 
[HCTR][23:13:45.745][ERROR][RK0][main]: replica 0 calling init per replica done, doing barrier
[HCTR][23:13:45.745][ERROR][RK0][tid #140208041346816]: replica 6 calling init per replica done, doing barrier
[HCTR][23:13:45.745][ERROR][RK0][tid #140208502716160]: replica 1 calling init per replica done, doing barrier
[HCTR][23:13:45.745][ERROR][RK0][tid #140208049739520]: replica 7 calling init per replica done, doing barrier
[HCTR][23:13:45.745][ERROR][RK0][main]: replica 5 calling init per replica done, doing barrier
[HCTR][23:13:45.745][ERROR][RK0][main]: replica 4 calling init per replica done, doing barrier
[HCTR][23:13:45.745][ERROR][RK0][tid #140208100062976]: replica 3 calling init per replica done, doing barrier
[HCTR][23:13:45.745][ERROR][RK0][tid #140208511108864]: replica 2 calling init per replica done, doing barrier
[HCTR][23:13:45.745][ERROR][RK0][tid #140208041346816]: replica 6 calling init per replica done, doing barrier done
[HCTR][23:13:45.745][ERROR][RK0][tid #140208511108864]: replica 2 calling init per replica done, doing barrier done
[HCTR][23:13:45.745][ERROR][RK0][main]: replica 5 calling init per replica done, doing barrier done
[HCTR][23:13:45.745][ERROR][RK0][tid #140208100062976]: replica 3 calling init per replica done, doing barrier done
[HCTR][23:13:45.745][ERROR][RK0][main]: replica 0 calling init per replica done, doing barrier done
[HCTR][23:13:45.745][ERROR][RK0][tid #140208049739520]: replica 7 calling init per replica done, doing barrier done
[HCTR][23:13:45.745][ERROR][RK0][tid #140208502716160]: replica 1 calling init per replica done, doing barrier done
[HCTR][23:13:45.745][ERROR][RK0][main]: replica 4 calling init per replica done, doing barrier done
[HCTR][23:13:45.745][ERROR][RK0][tid #140208041346816]: init per replica done
[HCTR][23:13:45.745][ERROR][RK0][tid #140208511108864]: init per replica done
[HCTR][23:13:45.745][ERROR][RK0][main]: init per replica done
[HCTR][23:13:45.745][ERROR][RK0][main]: init per replica done
[HCTR][23:13:45.745][ERROR][RK0][tid #140208100062976]: init per replica done
[HCTR][23:13:45.745][ERROR][RK0][tid #140208049739520]: init per replica done
[HCTR][23:13:45.745][ERROR][RK0][tid #140208502716160]: init per replica done
[HCTR][23:13:45.747][ERROR][RK0][main]: init per replica done
