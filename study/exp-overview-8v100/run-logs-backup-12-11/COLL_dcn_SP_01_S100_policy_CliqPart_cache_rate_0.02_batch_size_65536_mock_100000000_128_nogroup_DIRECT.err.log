2022-12-11 21:58:14.726009: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:14.730974: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:14.735940: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:14.741101: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:14.748512: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:14.759646: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:14.767225: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:14.771772: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:14.817545: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:14.820916: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:14.822320: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:14.823929: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:14.831644: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:14.833655: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:14.843400: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:14.844205: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:14.847595: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:14.848511: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:14.849484: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:14.849967: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:14.851048: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:14.851432: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:14.852620: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:14.852957: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:14.854431: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:14.855595: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:14.856701: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:14.857766: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:14.858866: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:14.859935: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:14.860999: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:14.862028: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:14.863903: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:14.865216: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:14.866224: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:14.867278: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:14.868436: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:14.869465: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:14.870520: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:14.871536: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:14.875147: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:14.876655: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:14.877088: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 21:58:14.877587: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:14.878545: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:14.879497: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:14.880552: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:14.881798: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:14.882737: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:14.885601: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:14.886100: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:14.887433: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:14.888093: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:14.889868: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:14.890113: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:14.890268: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:14.892158: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:14.892444: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:14.894418: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:14.895065: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:14.897398: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:14.898277: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:14.898354: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:14.899325: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:14.901223: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:14.901989: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:14.902603: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:14.903579: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:14.904136: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:14.905078: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:14.905718: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:14.906655: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:14.907650: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:14.908445: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:14.909673: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:14.910933: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:14.911643: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:14.912327: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:14.912959: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:14.914366: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:14.915060: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:14.915856: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:14.916120: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:14.917982: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:14.918063: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:14.918820: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:14.918878: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:14.920955: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:14.920988: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:14.921356: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:14.921563: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:14.925565: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:14.925583: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:14.925630: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:14.927118: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:14.927601: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:14.929544: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:14.929598: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:14.930186: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:14.930637: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:14.932231: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:14.932746: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:14.953247: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:14.960947: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:14.966628: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:14.967965: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:14.968692: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:14.969137: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:14.969219: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:14.969290: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:14.971389: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:14.972798: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:14.972797: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:14.973200: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:14.973301: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:14.973397: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:14.976065: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:14.976142: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:14.977682: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:14.977926: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:14.978002: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:14.978090: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:14.978180: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:14.980997: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:14.981206: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:14.982705: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:14.983291: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:14.983381: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:14.983465: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:14.985189: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:14.985333: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:14.987261: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:14.987535: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:14.987623: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:14.987714: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:14.989422: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:14.990191: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:14.991339: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:14.991633: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:14.991769: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:14.991862: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:14.993489: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:14.994125: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:14.995148: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:14.995333: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:14.995460: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:14.995775: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:14.997148: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:14.998048: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:14.998737: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:14.999003: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:14.999143: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:14.999461: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:15.001445: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:15.002471: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:15.003117: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:15.003403: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:15.003716: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:15.003941: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:15.005459: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:15.006395: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:15.007099: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:15.007443: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:15.007821: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:15.008123: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:15.009488: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:15.010099: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:15.010843: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:15.011137: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:15.011731: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:15.011868: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:15.013482: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:15.014391: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:15.015367: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:15.015562: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:15.016104: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:15.016213: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:15.017498: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:15.018321: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:15.018871: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:15.019311: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:15.019917: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:15.020007: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:15.021189: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:15.022676: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:15.022879: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:15.023395: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:15.024204: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:15.024331: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:15.025773: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:15.025962: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:15.026481: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:15.026571: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 21:58:15.027672: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:15.027734: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:15.028018: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:15.029244: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:15.030711: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:15.030822: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:15.031546: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:15.031580: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:15.031940: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:15.033130: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:15.034666: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:15.034919: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:15.035677: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:15.035877: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:15.035891: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:15.036072: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:15.038808: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:15.039471: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:15.039562: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:15.040259: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:15.040510: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:15.040601: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:15.041212: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:15.043733: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:15.044094: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:15.044290: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:15.045064: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:15.045323: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:15.045585: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:15.046201: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:15.048986: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:15.049376: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:15.049402: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:15.050373: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:15.050997: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:15.051643: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:15.053783: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:15.053981: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:15.054024: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:15.054847: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:15.055679: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:15.056060: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:15.058710: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:15.058829: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:15.059047: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:15.060407: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:15.060662: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:15.062760: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:15.062903: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:15.063112: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:15.064053: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 21:58:15.064741: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:15.065046: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:15.066757: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:15.068471: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:15.069234: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:15.070056: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:15.070523: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 21:58:15.070523: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 21:58:15.071601: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:15.072326: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:15.073002: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:15.074870: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:15.076113: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:15.076436: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 21:58:15.077447: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:15.078752: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:15.079534: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 21:58:15.079704: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:15.079782: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:15.079837: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:15.083319: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:15.083361: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:15.083427: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:15.085292: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:15.087914: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:15.089631: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:15.089923: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:15.089976: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:15.091949: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:15.093212: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:15.095395: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:15.096350: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:15.097709: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:15.127873: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:15.160239: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:15.166326: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:15.171732: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:15.180603: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:15.187310: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:15.193391: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:15.206878: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 21:58:15.215604: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:15.246673: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:15.257612: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:16.197427: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:16.198239: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:16.198848: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:16.199339: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 21:58:16.199398: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-11 21:58:16.217240: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:16.218080: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:16.218954: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:16.219558: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:16.220105: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:16.220580: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-11 21:58:16.264141: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 21:58:16.264352: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 21:58:16.283774: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12340
2022-12-11 21:58:16.378695: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:16.379512: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:16.380045: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:16.380691: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 21:58:16.380744: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-11 21:58:16.398083: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:16.399012: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:16.399975: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:16.400559: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:16.401099: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:16.401565: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:1/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-11 21:58:16.452125: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:16.452758: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:16.453290: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:16.453771: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 21:58:16.453826: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-11 21:58:16.461728: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:16.462329: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:16.462854: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:16.463424: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:16.464131: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 21:58:16.464205: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-11 21:58:16.464423: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:16.465040: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:16.465812: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 21:58:16.465874: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-11 21:58:16.470744: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:16.471406: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:16.471928: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:16.472498: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:16.473020: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:16.473491: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:2/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-11 21:58:16.481868: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:16.482327: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:16.482513: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:16.483478: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:16.483504: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:16.483597: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:16.483654: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:16.485416: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:16.485639: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:16.485786: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:16.485903: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:16.487523: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:16.487580: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:16.487703: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:16.487980: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:16.488938: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 21:58:16.489093: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 21:58:16.489545: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:16.489585: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 21:58:16.489640: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-11 21:58:16.489679: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:6/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-11 21:58:16.489863: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 21:58:16.489918: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-11 21:58:16.490595: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:5/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-11 21:58:16.490731: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12341
2022-12-11 21:58:16.501956: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:16.502567: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:16.503103: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:16.503639: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 21:58:16.503698: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-11 21:58:16.506589: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:16.506911: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:16.507647: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:16.508015: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:16.508628: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:16.509034: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:16.509712: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:16.510108: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:16.510604: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:16.511119: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:16.511576: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:3/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-11 21:58:16.511863: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:4/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-11 21:58:16.520634: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:16.521287: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:16.521806: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:16.522384: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:16.522908: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:58:16.523422: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:7/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-11 21:58:16.532931: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 21:58:16.533131: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 21:58:16.533929: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 21:58:16.534078: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 21:58:16.534985: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12346
2022-12-11 21:58:16.535878: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12345
2022-12-11 21:58:16.554553: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 21:58:16.554765: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 21:58:16.555544: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 21:58:16.555697: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 21:58:16.556507: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12343
2022-12-11 21:58:16.557272: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12344
2022-12-11 21:58:16.561208: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 21:58:16.561397: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 21:58:16.563294: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12342
2022-12-11 21:58:16.566567: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 21:58:16.566731: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 21:58:16.568443: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12347
[HCTR][21:58:17.835][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][21:58:17.836][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][21:58:17.836][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][21:58:17.836][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][21:58:17.836][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][21:58:17.836][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][21:58:17.837][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][21:58:17.837][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 1it [00:01,  1.53s/it]warmup run: 1it [00:01,  1.54s/it]warmup run: 1it [00:01,  1.52s/it]warmup run: 97it [00:01, 82.80it/s]warmup run: 98it [00:01, 82.97it/s]warmup run: 1it [00:01,  1.56s/it]warmup run: 95it [00:01, 81.12it/s]warmup run: 1it [00:01,  1.50s/it]warmup run: 1it [00:01,  1.50s/it]warmup run: 195it [00:01, 180.53it/s]warmup run: 198it [00:01, 182.02it/s]warmup run: 97it [00:01, 81.25it/s]warmup run: 191it [00:01, 176.84it/s]warmup run: 1it [00:01,  1.49s/it]warmup run: 100it [00:01, 86.90it/s]warmup run: 1it [00:01,  1.47s/it]warmup run: 99it [00:01, 85.88it/s]warmup run: 296it [00:01, 292.13it/s]warmup run: 299it [00:01, 292.58it/s]warmup run: 195it [00:01, 177.40it/s]warmup run: 289it [00:01, 284.73it/s]warmup run: 100it [00:01, 87.02it/s]warmup run: 199it [00:01, 186.74it/s]warmup run: 101it [00:01, 89.15it/s]warmup run: 199it [00:01, 186.88it/s]warmup run: 397it [00:01, 407.67it/s]warmup run: 399it [00:01, 405.98it/s]warmup run: 293it [00:01, 283.70it/s]warmup run: 389it [00:01, 399.48it/s]warmup run: 201it [00:01, 189.25it/s]warmup run: 299it [00:01, 297.61it/s]warmup run: 202it [00:01, 192.51it/s]warmup run: 300it [00:01, 298.87it/s]warmup run: 499it [00:02, 521.37it/s]warmup run: 498it [00:02, 514.21it/s]warmup run: 391it [00:01, 394.31it/s]warmup run: 485it [00:02, 503.93it/s]warmup run: 302it [00:01, 301.22it/s]warmup run: 399it [00:01, 411.75it/s]warmup run: 303it [00:01, 305.23it/s]warmup run: 401it [00:01, 414.03it/s]warmup run: 602it [00:02, 628.46it/s]warmup run: 597it [00:02, 612.98it/s]warmup run: 489it [00:02, 502.15it/s]warmup run: 583it [00:02, 603.52it/s]warmup run: 403it [00:01, 416.76it/s]warmup run: 499it [00:01, 522.65it/s]warmup run: 405it [00:01, 422.70it/s]warmup run: 502it [00:02, 525.51it/s]warmup run: 706it [00:02, 722.10it/s]warmup run: 696it [00:02, 698.66it/s]warmup run: 589it [00:02, 605.28it/s]warmup run: 679it [00:02, 685.54it/s]warmup run: 503it [00:01, 526.89it/s]warmup run: 601it [00:02, 627.37it/s]warmup run: 507it [00:01, 536.27it/s]warmup run: 605it [00:02, 631.24it/s]warmup run: 810it [00:02, 799.55it/s]warmup run: 795it [00:02, 770.06it/s]warmup run: 688it [00:02, 693.49it/s]warmup run: 776it [00:02, 754.64it/s]warmup run: 604it [00:02, 629.02it/s]warmup run: 703it [00:02, 717.16it/s]warmup run: 611it [00:02, 643.68it/s]warmup run: 707it [00:02, 720.74it/s]warmup run: 914it [00:02, 862.10it/s]warmup run: 893it [00:02, 823.98it/s]warmup run: 787it [00:02, 765.06it/s]warmup run: 875it [00:02, 814.01it/s]warmup run: 705it [00:02, 716.82it/s]warmup run: 805it [00:02, 791.86it/s]warmup run: 715it [00:02, 735.50it/s]warmup run: 806it [00:02, 787.30it/s]warmup run: 1019it [00:02, 911.98it/s]warmup run: 993it [00:02, 870.59it/s]warmup run: 886it [00:02, 822.33it/s]warmup run: 974it [00:02, 861.45it/s]warmup run: 806it [00:02, 789.76it/s]warmup run: 904it [00:02, 843.38it/s]warmup run: 818it [00:02, 807.93it/s]warmup run: 905it [00:02, 838.53it/s]warmup run: 1124it [00:02, 949.06it/s]warmup run: 1094it [00:02, 908.58it/s]warmup run: 985it [00:02, 865.86it/s]warmup run: 1074it [00:02, 899.39it/s]warmup run: 908it [00:02, 848.37it/s]warmup run: 1003it [00:02, 882.20it/s]warmup run: 920it [00:02, 862.25it/s]warmup run: 1004it [00:02, 876.77it/s]warmup run: 1228it [00:02, 974.32it/s]warmup run: 1197it [00:02, 942.24it/s]warmup run: 1083it [00:02, 897.07it/s]warmup run: 1175it [00:02, 929.11it/s]warmup run: 1009it [00:02, 890.37it/s]warmup run: 1102it [00:02, 909.94it/s]warmup run: 1021it [00:02, 902.00it/s]warmup run: 1103it [00:02, 906.80it/s]warmup run: 1332it [00:02, 991.59it/s]warmup run: 1297it [00:02, 958.90it/s]warmup run: 1183it [00:02, 925.24it/s]warmup run: 1277it [00:02, 954.86it/s]warmup run: 1110it [00:02, 923.15it/s]warmup run: 1122it [00:02, 930.78it/s]warmup run: 1204it [00:02, 935.27it/s]warmup run: 1201it [00:02, 875.11it/s]warmup run: 1436it [00:02, 1004.82it/s]warmup run: 1399it [00:02, 976.22it/s]warmup run: 1283it [00:02, 944.66it/s]warmup run: 1380it [00:02, 974.71it/s]warmup run: 1212it [00:02, 950.35it/s]warmup run: 1225it [00:02, 956.89it/s]warmup run: 1304it [00:02, 952.67it/s]warmup run: 1302it [00:02, 911.17it/s]warmup run: 1541it [00:03, 1017.91it/s]warmup run: 1500it [00:03, 975.77it/s]warmup run: 1383it [00:02, 960.69it/s]warmup run: 1482it [00:03, 985.82it/s]warmup run: 1313it [00:02, 951.87it/s]warmup run: 1328it [00:02, 977.37it/s]warmup run: 1408it [00:02, 976.05it/s]warmup run: 1402it [00:02, 934.45it/s]warmup run: 1646it [00:03, 1025.70it/s]warmup run: 1600it [00:03, 978.39it/s]warmup run: 1483it [00:03, 970.73it/s]warmup run: 1583it [00:03, 985.10it/s]warmup run: 1417it [00:02, 975.20it/s]warmup run: 1432it [00:02, 994.99it/s]warmup run: 1513it [00:03, 995.98it/s]warmup run: 1504it [00:03, 958.22it/s]warmup run: 1751it [00:03, 1031.21it/s]warmup run: 1700it [00:03, 982.14it/s]warmup run: 1583it [00:03, 978.68it/s]warmup run: 1683it [00:03, 962.27it/s]warmup run: 1521it [00:03, 993.37it/s]warmup run: 1535it [00:02, 1005.17it/s]warmup run: 1617it [00:03, 1008.35it/s]warmup run: 1607it [00:03, 977.03it/s]warmup run: 1856it [00:03, 1036.08it/s]warmup run: 1802it [00:03, 992.49it/s]warmup run: 1683it [00:03, 982.64it/s]warmup run: 1785it [00:03, 977.50it/s]warmup run: 1624it [00:03, 1001.92it/s]warmup run: 1638it [00:03, 1011.80it/s]warmup run: 1721it [00:03, 1015.77it/s]warmup run: 1709it [00:03, 987.05it/s]warmup run: 1961it [00:03, 1032.38it/s]warmup run: 1903it [00:03, 996.73it/s]warmup run: 1783it [00:03, 986.16it/s]warmup run: 1890it [00:03, 995.84it/s]warmup run: 1726it [00:03, 1004.80it/s]warmup run: 1741it [00:03, 1013.11it/s]warmup run: 1825it [00:03, 1022.08it/s]warmup run: 1812it [00:03, 997.98it/s]warmup run: 2075it [00:03, 1064.10it/s]warmup run: 2008it [00:03, 1010.86it/s]warmup run: 1884it [00:03, 991.56it/s]warmup run: 1993it [00:03, 1003.80it/s]warmup run: 1829it [00:03, 1011.40it/s]warmup run: 1844it [00:03, 1005.01it/s]warmup run: 1929it [00:03, 1026.40it/s]warmup run: 1916it [00:03, 1008.68it/s]warmup run: 2197it [00:03, 1109.37it/s]warmup run: 2131it [00:03, 1075.85it/s]warmup run: 1986it [00:03, 999.41it/s]warmup run: 2111it [00:03, 1054.63it/s]warmup run: 1932it [00:03, 1016.35it/s]warmup run: 1946it [00:03, 999.91it/s] warmup run: 2035it [00:03, 1035.67it/s]warmup run: 2023it [00:03, 1026.59it/s]warmup run: 2319it [00:03, 1141.35it/s]warmup run: 2255it [00:03, 1122.25it/s]warmup run: 2105it [00:03, 1056.22it/s]warmup run: 2228it [00:03, 1086.93it/s]warmup run: 2042it [00:03, 1039.04it/s]warmup run: 2054it [00:03, 1021.77it/s]warmup run: 2157it [00:03, 1089.89it/s]warmup run: 2147it [00:03, 1089.27it/s]warmup run: 2438it [00:03, 1154.50it/s]warmup run: 2379it [00:03, 1155.01it/s]warmup run: 2229it [00:03, 1110.17it/s]warmup run: 2345it [00:03, 1111.41it/s]warmup run: 2166it [00:03, 1097.66it/s]warmup run: 2175it [00:03, 1075.68it/s]warmup run: 2280it [00:03, 1129.74it/s]warmup run: 2271it [00:03, 1133.11it/s]warmup run: 2559it [00:03, 1170.29it/s]warmup run: 2503it [00:03, 1178.69it/s]warmup run: 2353it [00:03, 1147.81it/s]warmup run: 2462it [00:03, 1127.87it/s]warmup run: 2290it [00:03, 1139.05it/s]warmup run: 2296it [00:03, 1113.11it/s]warmup run: 2403it [00:03, 1157.95it/s]warmup run: 2394it [00:03, 1160.90it/s]warmup run: 2681it [00:04, 1182.46it/s]warmup run: 2625it [00:04, 1190.49it/s]warmup run: 2477it [00:03, 1173.67it/s]warmup run: 2579it [00:04, 1139.91it/s]warmup run: 2414it [00:03, 1167.57it/s]warmup run: 2416it [00:03, 1138.65it/s]warmup run: 2526it [00:03, 1176.98it/s]warmup run: 2517it [00:03, 1180.72it/s]warmup run: 2801it [00:04, 1185.53it/s]warmup run: 2748it [00:04, 1199.55it/s]warmup run: 2601it [00:04, 1191.31it/s]warmup run: 2697it [00:04, 1150.28it/s]warmup run: 2538it [00:03, 1187.33it/s]warmup run: 2536it [00:03, 1155.99it/s]warmup run: 2649it [00:04, 1190.50it/s]warmup run: 2640it [00:04, 1194.87it/s]warmup run: 2923it [00:04, 1193.93it/s]warmup run: 2869it [00:04, 1201.04it/s]warmup run: 2725it [00:04, 1202.96it/s]warmup run: 2814it [00:04, 1153.61it/s]warmup run: 2662it [00:04, 1200.49it/s]warmup run: 3000it [00:04, 696.35it/s] warmup run: 2656it [00:03, 1168.33it/s]warmup run: 2770it [00:04, 1194.86it/s]warmup run: 2762it [00:04, 1199.50it/s]warmup run: 2991it [00:04, 1203.79it/s]warmup run: 2847it [00:04, 1206.87it/s]warmup run: 3000it [00:04, 686.38it/s] warmup run: 2932it [00:04, 1160.11it/s]warmup run: 2783it [00:04, 1201.54it/s]warmup run: 2775it [00:04, 1174.16it/s]warmup run: 2892it [00:04, 1202.17it/s]warmup run: 2884it [00:04, 1204.01it/s]warmup run: 3000it [00:04, 679.97it/s] warmup run: 2970it [00:04, 1211.34it/s]warmup run: 3000it [00:04, 681.45it/s] warmup run: 2904it [00:04, 1203.24it/s]warmup run: 2895it [00:04, 1181.41it/s]warmup run: 3000it [00:04, 696.35it/s] warmup run: 3000it [00:04, 691.97it/s] warmup run: 3000it [00:04, 698.11it/s] warmup run: 3000it [00:04, 700.55it/s] 


warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]



warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   6%|▌         | 165/3000 [00:00<00:01, 1647.44it/s]warmup should be done:   6%|▌         | 169/3000 [00:00<00:01, 1686.74it/s]warmup should be done:   6%|▌         | 168/3000 [00:00<00:01, 1677.78it/s]warmup should be done:   6%|▌         | 169/3000 [00:00<00:01, 1681.55it/s]warmup should be done:   6%|▌         | 169/3000 [00:00<00:01, 1684.66it/s]warmup should be done:   5%|▌         | 163/3000 [00:00<00:01, 1624.33it/s]warmup should be done:   5%|▌         | 155/3000 [00:00<00:01, 1544.29it/s]warmup should be done:   6%|▌         | 166/3000 [00:00<00:01, 1652.25it/s]warmup should be done:  11%|█▏        | 339/3000 [00:00<00:01, 1693.37it/s]warmup should be done:  11%|█▏        | 338/3000 [00:00<00:01, 1689.14it/s]warmup should be done:  11%|█         | 333/3000 [00:00<00:01, 1662.04it/s]warmup should be done:  11%|█         | 332/3000 [00:00<00:01, 1655.55it/s]warmup should be done:  11%|█▏        | 338/3000 [00:00<00:01, 1682.60it/s]warmup should be done:  10%|█         | 312/3000 [00:00<00:01, 1556.57it/s]warmup should be done:  11%|█         | 326/3000 [00:00<00:01, 1623.66it/s]warmup should be done:  11%|█▏        | 339/3000 [00:00<00:01, 1688.65it/s]warmup should be done:  17%|█▋        | 498/3000 [00:00<00:01, 1656.40it/s]warmup should be done:  17%|█▋        | 508/3000 [00:00<00:01, 1690.32it/s]warmup should be done:  16%|█▌        | 468/3000 [00:00<00:01, 1555.53it/s]warmup should be done:  17%|█▋        | 509/3000 [00:00<00:01, 1688.22it/s]warmup should be done:  17%|█▋        | 500/3000 [00:00<00:01, 1658.73it/s]warmup should be done:  17%|█▋        | 507/3000 [00:00<00:01, 1677.73it/s]warmup should be done:  17%|█▋        | 508/3000 [00:00<00:01, 1679.34it/s]warmup should be done:  16%|█▋        | 489/3000 [00:00<00:01, 1606.77it/s]warmup should be done:  22%|██▏       | 664/3000 [00:00<00:01, 1656.18it/s]warmup should be done:  23%|██▎       | 678/3000 [00:00<00:01, 1691.65it/s]warmup should be done:  22%|██▏       | 666/3000 [00:00<00:01, 1657.43it/s]warmup should be done:  21%|██        | 624/3000 [00:00<00:01, 1552.47it/s]warmup should be done:  22%|██▎       | 675/3000 [00:00<00:01, 1674.60it/s]warmup should be done:  23%|██▎       | 676/3000 [00:00<00:01, 1675.81it/s]warmup should be done:  23%|██▎       | 678/3000 [00:00<00:01, 1667.57it/s]warmup should be done:  22%|██▏       | 650/3000 [00:00<00:01, 1595.06it/s]warmup should be done:  28%|██▊       | 830/3000 [00:00<00:01, 1652.28it/s]warmup should be done:  28%|██▊       | 848/3000 [00:00<00:01, 1689.59it/s]warmup should be done:  28%|██▊       | 832/3000 [00:00<00:01, 1654.77it/s]warmup should be done:  26%|██▌       | 780/3000 [00:00<00:01, 1551.92it/s]warmup should be done:  28%|██▊       | 843/3000 [00:00<00:01, 1670.39it/s]warmup should be done:  28%|██▊       | 844/3000 [00:00<00:01, 1672.15it/s]warmup should be done:  28%|██▊       | 845/3000 [00:00<00:01, 1635.48it/s]warmup should be done:  27%|██▋       | 810/3000 [00:00<00:01, 1567.85it/s]warmup should be done:  34%|███▍      | 1017/3000 [00:00<00:01, 1687.68it/s]warmup should be done:  31%|███       | 936/3000 [00:00<00:01, 1546.29it/s]warmup should be done:  33%|███▎      | 998/3000 [00:00<00:01, 1645.67it/s]warmup should be done:  34%|███▎      | 1012/3000 [00:00<00:01, 1670.22it/s]warmup should be done:  34%|███▎      | 1011/3000 [00:00<00:01, 1665.76it/s]warmup should be done:  33%|███▎      | 996/3000 [00:00<00:01, 1632.76it/s]warmup should be done:  32%|███▏      | 967/3000 [00:00<00:01, 1558.32it/s]warmup should be done:  34%|███▎      | 1009/3000 [00:00<00:01, 1615.75it/s]warmup should be done:  40%|███▉      | 1186/3000 [00:00<00:01, 1682.34it/s]warmup should be done:  36%|███▋      | 1094/3000 [00:00<00:01, 1554.39it/s]warmup should be done:  39%|███▉      | 1163/3000 [00:00<00:01, 1643.21it/s]warmup should be done:  39%|███▉      | 1180/3000 [00:00<00:01, 1668.50it/s]warmup should be done:  39%|███▊      | 1160/3000 [00:00<00:01, 1630.56it/s]warmup should be done:  39%|███▉      | 1178/3000 [00:00<00:01, 1633.24it/s]warmup should be done:  37%|███▋      | 1123/3000 [00:00<00:01, 1547.74it/s]warmup should be done:  39%|███▉      | 1171/3000 [00:00<00:01, 1600.23it/s]warmup should be done:  45%|████▌     | 1355/3000 [00:00<00:00, 1682.61it/s]warmup should be done:  42%|████▏     | 1252/3000 [00:00<00:01, 1562.32it/s]warmup should be done:  44%|████▍     | 1328/3000 [00:00<00:01, 1644.58it/s]warmup should be done:  45%|████▍     | 1349/3000 [00:00<00:00, 1673.15it/s]warmup should be done:  44%|████▍     | 1324/3000 [00:00<00:01, 1630.88it/s]warmup should be done:  44%|████▍     | 1332/3000 [00:00<00:01, 1593.83it/s]warmup should be done:  43%|████▎     | 1278/3000 [00:00<00:01, 1515.14it/s]warmup should be done:  45%|████▍     | 1342/3000 [00:00<00:01, 1546.79it/s]warmup should be done:  51%|█████     | 1524/3000 [00:00<00:00, 1681.57it/s]warmup should be done:  47%|████▋     | 1411/3000 [00:00<00:01, 1568.73it/s]warmup should be done:  50%|████▉     | 1493/3000 [00:00<00:00, 1646.16it/s]warmup should be done:  51%|█████     | 1518/3000 [00:00<00:00, 1677.48it/s]warmup should be done:  50%|████▉     | 1490/3000 [00:00<00:00, 1638.31it/s]warmup should be done:  50%|████▉     | 1492/3000 [00:00<00:00, 1595.55it/s]warmup should be done:  48%|████▊     | 1434/3000 [00:00<00:01, 1527.32it/s]warmup should be done:  50%|█████     | 1503/3000 [00:00<00:00, 1564.39it/s]warmup should be done:  56%|█████▋    | 1693/3000 [00:01<00:00, 1680.28it/s]warmup should be done:  52%|█████▏    | 1570/3000 [00:01<00:00, 1573.51it/s]warmup should be done:  55%|█████▌    | 1659/3000 [00:01<00:00, 1648.29it/s]warmup should be done:  56%|█████▌    | 1687/3000 [00:01<00:00, 1680.81it/s]warmup should be done:  55%|█████▌    | 1656/3000 [00:01<00:00, 1642.50it/s]warmup should be done:  55%|█████▌    | 1653/3000 [00:01<00:00, 1597.10it/s]warmup should be done:  53%|█████▎    | 1594/3000 [00:01<00:00, 1548.79it/s]warmup should be done:  56%|█████▌    | 1671/3000 [00:01<00:00, 1597.95it/s]warmup should be done:  61%|██████    | 1824/3000 [00:01<00:00, 1648.41it/s]warmup should be done:  58%|█████▊    | 1729/3000 [00:01<00:00, 1577.09it/s]warmup should be done:  62%|██████▏   | 1862/3000 [00:01<00:00, 1679.37it/s]warmup should be done:  62%|██████▏   | 1856/3000 [00:01<00:00, 1677.20it/s]warmup should be done:  61%|██████    | 1822/3000 [00:01<00:00, 1645.20it/s]warmup should be done:  60%|██████    | 1813/3000 [00:01<00:00, 1597.73it/s]warmup should be done:  58%|█████▊    | 1753/3000 [00:01<00:00, 1559.85it/s]warmup should be done:  61%|██████    | 1836/3000 [00:01<00:00, 1611.91it/s]warmup should be done:  63%|██████▎   | 1888/3000 [00:01<00:00, 1580.91it/s]warmup should be done:  66%|██████▋   | 1989/3000 [00:01<00:00, 1647.88it/s]warmup should be done:  68%|██████▊   | 2030/3000 [00:01<00:00, 1677.88it/s]warmup should be done:  66%|██████▌   | 1987/3000 [00:01<00:00, 1645.23it/s]warmup should be done:  67%|██████▋   | 2024/3000 [00:01<00:00, 1663.42it/s]warmup should be done:  66%|██████▌   | 1973/3000 [00:01<00:00, 1597.01it/s]warmup should be done:  64%|██████▍   | 1915/3000 [00:01<00:00, 1576.29it/s]warmup should be done:  67%|██████▋   | 1999/3000 [00:01<00:00, 1615.42it/s]warmup should be done:  68%|██████▊   | 2047/3000 [00:01<00:00, 1583.00it/s]warmup should be done:  72%|███████▏  | 2154/3000 [00:01<00:00, 1647.55it/s]warmup should be done:  73%|███████▎  | 2198/3000 [00:01<00:00, 1677.73it/s]warmup should be done:  72%|███████▏  | 2153/3000 [00:01<00:00, 1647.25it/s]warmup should be done:  73%|███████▎  | 2191/3000 [00:01<00:00, 1657.28it/s]warmup should be done:  71%|███████   | 2133/3000 [00:01<00:00, 1596.81it/s]warmup should be done:  69%|██████▉   | 2083/3000 [00:01<00:00, 1606.05it/s]warmup should be done:  72%|███████▏  | 2163/3000 [00:01<00:00, 1622.16it/s]warmup should be done:  77%|███████▋  | 2319/3000 [00:01<00:00, 1648.06it/s]warmup should be done:  74%|███████▎  | 2206/3000 [00:01<00:00, 1583.30it/s]warmup should be done:  79%|███████▉  | 2366/3000 [00:01<00:00, 1673.93it/s]warmup should be done:  77%|███████▋  | 2319/3000 [00:01<00:00, 1648.92it/s]warmup should be done:  79%|███████▊  | 2357/3000 [00:01<00:00, 1651.41it/s]warmup should be done:  76%|███████▋  | 2293/3000 [00:01<00:00, 1595.80it/s]warmup should be done:  75%|███████▌  | 2251/3000 [00:01<00:00, 1628.00it/s]warmup should be done:  78%|███████▊  | 2328/3000 [00:01<00:00, 1628.27it/s]warmup should be done:  79%|███████▉  | 2368/3000 [00:01<00:00, 1592.91it/s]warmup should be done:  83%|████████▎ | 2484/3000 [00:01<00:00, 1645.21it/s]warmup should be done:  84%|████████▍ | 2534/3000 [00:01<00:00, 1674.13it/s]warmup should be done:  83%|████████▎ | 2484/3000 [00:01<00:00, 1648.81it/s]warmup should be done:  84%|████████▍ | 2523/3000 [00:01<00:00, 1648.88it/s]warmup should be done:  82%|████████▏ | 2453/3000 [00:01<00:00, 1587.98it/s]warmup should be done:  81%|████████  | 2419/3000 [00:01<00:00, 1642.43it/s]warmup should be done:  83%|████████▎ | 2492/3000 [00:01<00:00, 1630.47it/s]warmup should be done:  84%|████████▍ | 2531/3000 [00:01<00:00, 1601.65it/s]warmup should be done:  88%|████████▊ | 2650/3000 [00:01<00:00, 1647.00it/s]warmup should be done:  90%|█████████ | 2702/3000 [00:01<00:00, 1674.18it/s]warmup should be done:  88%|████████▊ | 2650/3000 [00:01<00:00, 1652.09it/s]warmup should be done:  90%|████████▉ | 2688/3000 [00:01<00:00, 1647.07it/s]warmup should be done:  87%|████████▋ | 2612/3000 [00:01<00:00, 1585.60it/s]warmup should be done:  86%|████████▋ | 2588/3000 [00:01<00:00, 1654.93it/s]warmup should be done:  89%|████████▊ | 2658/3000 [00:01<00:00, 1637.72it/s]warmup should be done:  90%|████████▉ | 2694/3000 [00:01<00:00, 1608.61it/s]warmup should be done:  94%|█████████▍| 2816/3000 [00:01<00:00, 1648.90it/s]warmup should be done:  96%|█████████▌| 2870/3000 [00:01<00:00, 1672.80it/s]warmup should be done:  94%|█████████▍| 2817/3000 [00:01<00:00, 1654.49it/s]warmup should be done:  95%|█████████▌| 2854/3000 [00:01<00:00, 1650.38it/s]warmup should be done:  92%|█████████▏| 2771/3000 [00:01<00:00, 1585.46it/s]warmup should be done:  92%|█████████▏| 2757/3000 [00:01<00:00, 1662.65it/s]warmup should be done:  94%|█████████▍| 2826/3000 [00:01<00:00, 1647.94it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1678.50it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1663.92it/s]warmup should be done:  95%|█████████▌| 2857/3000 [00:01<00:00, 1614.25it/s]warmup should be done:  99%|█████████▉| 2983/3000 [00:01<00:00, 1654.61it/s]warmup should be done:  99%|█████████▉| 2984/3000 [00:01<00:00, 1657.43it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1649.80it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1647.52it/s]warmup should be done:  98%|█████████▊| 2932/3000 [00:01<00:00, 1592.46it/s]warmup should be done:  98%|█████████▊| 2926/3000 [00:01<00:00, 1670.05it/s]warmup should be done: 100%|█████████▉| 2994/3000 [00:01<00:00, 1656.54it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1633.46it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1606.98it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1604.78it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1582.98it/s]





warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   6%|▌         | 170/3000 [00:00<00:01, 1698.69it/s]warmup should be done:   6%|▌         | 170/3000 [00:00<00:01, 1697.05it/s]warmup should be done:   6%|▌         | 168/3000 [00:00<00:01, 1676.07it/s]warmup should be done:   6%|▌         | 168/3000 [00:00<00:01, 1676.32it/s]warmup should be done:   6%|▌         | 170/3000 [00:00<00:01, 1695.48it/s]warmup should be done:   6%|▌         | 171/3000 [00:00<00:01, 1704.45it/s]warmup should be done:   6%|▌         | 170/3000 [00:00<00:01, 1693.41it/s]warmup should be done:   6%|▌         | 169/3000 [00:00<00:01, 1680.21it/s]warmup should be done:  11%|█▏        | 340/3000 [00:00<00:01, 1693.89it/s]warmup should be done:  11%|█▏        | 340/3000 [00:00<00:01, 1693.79it/s]warmup should be done:  11%|█▏        | 340/3000 [00:00<00:01, 1694.54it/s]warmup should be done:  11%|█▏        | 340/3000 [00:00<00:01, 1693.01it/s]warmup should be done:  11%|█         | 336/3000 [00:00<00:01, 1672.71it/s]warmup should be done:  11%|█         | 336/3000 [00:00<00:01, 1672.13it/s]warmup should be done:  11%|█▏        | 338/3000 [00:00<00:01, 1676.01it/s]warmup should be done:  11%|█▏        | 342/3000 [00:00<00:01, 1685.12it/s]warmup should be done:  17%|█▋        | 510/3000 [00:00<00:01, 1696.16it/s]warmup should be done:  17%|█▋        | 510/3000 [00:00<00:01, 1694.60it/s]warmup should be done:  17%|█▋        | 504/3000 [00:00<00:01, 1674.47it/s]warmup should be done:  17%|█▋        | 510/3000 [00:00<00:01, 1692.89it/s]warmup should be done:  17%|█▋        | 510/3000 [00:00<00:01, 1692.05it/s]warmup should be done:  17%|█▋        | 504/3000 [00:00<00:01, 1659.64it/s]warmup should be done:  17%|█▋        | 511/3000 [00:00<00:01, 1682.12it/s]warmup should be done:  17%|█▋        | 506/3000 [00:00<00:01, 1648.33it/s]warmup should be done:  23%|██▎       | 680/3000 [00:00<00:01, 1694.72it/s]warmup should be done:  23%|██▎       | 680/3000 [00:00<00:01, 1695.10it/s]warmup should be done:  22%|██▏       | 674/3000 [00:00<00:01, 1682.04it/s]warmup should be done:  23%|██▎       | 684/3000 [00:00<00:01, 1709.08it/s]warmup should be done:  23%|██▎       | 681/3000 [00:00<00:01, 1695.91it/s]warmup should be done:  22%|██▏       | 670/3000 [00:00<00:01, 1657.07it/s]warmup should be done:  23%|██▎       | 683/3000 [00:00<00:01, 1695.92it/s]warmup should be done:  22%|██▎       | 675/3000 [00:00<00:01, 1664.11it/s]warmup should be done:  28%|██▊       | 851/3000 [00:00<00:01, 1697.96it/s]warmup should be done:  28%|██▊       | 844/3000 [00:00<00:01, 1686.23it/s]warmup should be done:  28%|██▊       | 851/3000 [00:00<00:01, 1696.88it/s]warmup should be done:  29%|██▊       | 858/3000 [00:00<00:01, 1717.52it/s]warmup should be done:  28%|██▊       | 852/3000 [00:00<00:01, 1698.20it/s]warmup should be done:  28%|██▊       | 836/3000 [00:00<00:01, 1657.53it/s]warmup should be done:  28%|██▊       | 855/3000 [00:00<00:01, 1703.42it/s]warmup should be done:  28%|██▊       | 844/3000 [00:00<00:01, 1672.91it/s]warmup should be done:  34%|███▍      | 1021/3000 [00:00<00:01, 1697.65it/s]warmup should be done:  34%|███▍      | 1014/3000 [00:00<00:01, 1688.28it/s]warmup should be done:  34%|███▍      | 1032/3000 [00:00<00:01, 1722.08it/s]warmup should be done:  34%|███▍      | 1022/3000 [00:00<00:01, 1697.09it/s]warmup should be done:  34%|███▍      | 1021/3000 [00:00<00:01, 1692.43it/s]warmup should be done:  33%|███▎      | 1003/3000 [00:00<00:01, 1661.29it/s]warmup should be done:  34%|███▍      | 1026/3000 [00:00<00:01, 1699.24it/s]warmup should be done:  34%|███▎      | 1012/3000 [00:00<00:01, 1674.09it/s]warmup should be done:  39%|███▉      | 1183/3000 [00:00<00:01, 1688.59it/s]warmup should be done:  40%|███▉      | 1191/3000 [00:00<00:01, 1694.59it/s]warmup should be done:  40%|████      | 1205/3000 [00:00<00:01, 1720.46it/s]warmup should be done:  40%|███▉      | 1192/3000 [00:00<00:01, 1694.40it/s]warmup should be done:  40%|███▉      | 1191/3000 [00:00<00:01, 1691.94it/s]warmup should be done:  39%|███▉      | 1170/3000 [00:00<00:01, 1662.54it/s]warmup should be done:  40%|███▉      | 1196/3000 [00:00<00:01, 1699.43it/s]warmup should be done:  39%|███▉      | 1180/3000 [00:00<00:01, 1673.34it/s]warmup should be done:  45%|████▌     | 1355/3000 [00:00<00:00, 1695.75it/s]warmup should be done:  45%|████▌     | 1362/3000 [00:00<00:00, 1696.78it/s]warmup should be done:  46%|████▌     | 1379/3000 [00:00<00:00, 1726.16it/s]warmup should be done:  45%|████▌     | 1362/3000 [00:00<00:00, 1696.53it/s]warmup should be done:  45%|████▌     | 1363/3000 [00:00<00:00, 1696.86it/s]warmup should be done:  45%|████▍     | 1338/3000 [00:00<00:00, 1665.93it/s]warmup should be done:  45%|████▍     | 1349/3000 [00:00<00:00, 1677.19it/s]warmup should be done:  46%|████▌     | 1366/3000 [00:00<00:00, 1683.18it/s]warmup should be done:  51%|█████     | 1527/3000 [00:00<00:00, 1702.13it/s]warmup should be done:  51%|█████     | 1532/3000 [00:00<00:00, 1697.00it/s]warmup should be done:  52%|█████▏    | 1553/3000 [00:00<00:00, 1727.97it/s]warmup should be done:  51%|█████     | 1533/3000 [00:00<00:00, 1696.44it/s]warmup should be done:  50%|█████     | 1505/3000 [00:00<00:00, 1666.93it/s]warmup should be done:  51%|█████     | 1518/3000 [00:00<00:00, 1679.36it/s]warmup should be done:  51%|█████     | 1535/3000 [00:00<00:00, 1675.99it/s]warmup should be done:  51%|█████     | 1532/3000 [00:00<00:00, 1663.04it/s]warmup should be done:  57%|█████▋    | 1698/3000 [00:01<00:00, 1702.85it/s]warmup should be done:  57%|█████▋    | 1702/3000 [00:01<00:00, 1697.37it/s]warmup should be done:  58%|█████▊    | 1726/3000 [00:01<00:00, 1727.86it/s]warmup should be done:  57%|█████▋    | 1703/3000 [00:01<00:00, 1696.20it/s]warmup should be done:  56%|█████▌    | 1673/3000 [00:01<00:00, 1668.46it/s]warmup should be done:  56%|█████▌    | 1687/3000 [00:01<00:00, 1681.54it/s]warmup should be done:  57%|█████▋    | 1705/3000 [00:01<00:00, 1682.77it/s]warmup should be done:  57%|█████▋    | 1699/3000 [00:01<00:00, 1651.68it/s]warmup should be done:  62%|██████▏   | 1872/3000 [00:01<00:00, 1695.15it/s]warmup should be done:  63%|██████▎   | 1900/3000 [00:01<00:00, 1730.63it/s]warmup should be done:  62%|██████▏   | 1869/3000 [00:01<00:00, 1700.57it/s]warmup should be done:  62%|██████▏   | 1873/3000 [00:01<00:00, 1696.75it/s]warmup should be done:  61%|██████▏   | 1841/3000 [00:01<00:00, 1668.97it/s]warmup should be done:  62%|██████▏   | 1858/3000 [00:01<00:00, 1687.97it/s]warmup should be done:  63%|██████▎   | 1876/3000 [00:01<00:00, 1688.07it/s]warmup should be done:  62%|██████▏   | 1865/3000 [00:01<00:00, 1632.06it/s]warmup should be done:  69%|██████▉   | 2074/3000 [00:01<00:00, 1731.80it/s]warmup should be done:  68%|██████▊   | 2042/3000 [00:01<00:00, 1693.96it/s]warmup should be done:  68%|██████▊   | 2043/3000 [00:01<00:00, 1695.85it/s]warmup should be done:  68%|██████▊   | 2040/3000 [00:01<00:00, 1698.83it/s]warmup should be done:  67%|██████▋   | 2008/3000 [00:01<00:00, 1667.85it/s]warmup should be done:  68%|██████▊   | 2029/3000 [00:01<00:00, 1691.84it/s]warmup should be done:  68%|██████▊   | 2045/3000 [00:01<00:00, 1684.56it/s]warmup should be done:  68%|██████▊   | 2030/3000 [00:01<00:00, 1635.27it/s]warmup should be done:  75%|███████▍  | 2248/3000 [00:01<00:00, 1732.23it/s]warmup should be done:  74%|███████▎  | 2212/3000 [00:01<00:00, 1693.88it/s]warmup should be done:  74%|███████▍  | 2213/3000 [00:01<00:00, 1695.38it/s]warmup should be done:  74%|███████▎  | 2210/3000 [00:01<00:00, 1697.61it/s]warmup should be done:  72%|███████▎  | 2175/3000 [00:01<00:00, 1662.08it/s]warmup should be done:  73%|███████▎  | 2199/3000 [00:01<00:00, 1691.95it/s]warmup should be done:  74%|███████▍  | 2214/3000 [00:01<00:00, 1676.12it/s]warmup should be done:  73%|███████▎  | 2200/3000 [00:01<00:00, 1653.52it/s]warmup should be done:  79%|███████▉  | 2383/3000 [00:01<00:00, 1695.48it/s]warmup should be done:  81%|████████  | 2422/3000 [00:01<00:00, 1730.90it/s]warmup should be done:  79%|███████▉  | 2382/3000 [00:01<00:00, 1692.06it/s]warmup should be done:  79%|███████▉  | 2380/3000 [00:01<00:00, 1696.11it/s]warmup should be done:  78%|███████▊  | 2342/3000 [00:01<00:00, 1662.43it/s]warmup should be done:  79%|███████▉  | 2369/3000 [00:01<00:00, 1692.66it/s]warmup should be done:  79%|███████▉  | 2382/3000 [00:01<00:00, 1673.40it/s]warmup should be done:  79%|███████▉  | 2371/3000 [00:01<00:00, 1668.30it/s]warmup should be done:  85%|████████▌ | 2553/3000 [00:01<00:00, 1696.65it/s]warmup should be done:  85%|████████▌ | 2552/3000 [00:01<00:00, 1693.02it/s]warmup should be done:  87%|████████▋ | 2596/3000 [00:01<00:00, 1728.70it/s]warmup should be done:  85%|████████▌ | 2550/3000 [00:01<00:00, 1693.78it/s]warmup should be done:  84%|████████▎ | 2509/3000 [00:01<00:00, 1664.04it/s]warmup should be done:  85%|████████▍ | 2540/3000 [00:01<00:00, 1695.29it/s]warmup should be done:  85%|████████▌ | 2551/3000 [00:01<00:00, 1675.60it/s]warmup should be done:  85%|████████▍ | 2542/3000 [00:01<00:00, 1679.77it/s]warmup should be done:  91%|█████████ | 2724/3000 [00:01<00:00, 1700.52it/s]warmup should be done:  91%|█████████ | 2722/3000 [00:01<00:00, 1694.68it/s]warmup should be done:  91%|█████████ | 2720/3000 [00:01<00:00, 1691.40it/s]warmup should be done:  89%|████████▉ | 2676/3000 [00:01<00:00, 1662.18it/s]warmup should be done:  92%|█████████▏| 2769/3000 [00:01<00:00, 1718.78it/s]warmup should be done:  90%|█████████ | 2711/3000 [00:01<00:00, 1698.60it/s]warmup should be done:  91%|█████████ | 2720/3000 [00:01<00:00, 1678.77it/s]warmup should be done:  90%|█████████ | 2713/3000 [00:01<00:00, 1688.07it/s]warmup should be done:  96%|█████████▋| 2895/3000 [00:01<00:00, 1701.02it/s]warmup should be done:  96%|█████████▋| 2892/3000 [00:01<00:00, 1693.73it/s]warmup should be done:  96%|█████████▋| 2890/3000 [00:01<00:00, 1686.47it/s]warmup should be done:  98%|█████████▊| 2943/3000 [00:01<00:00, 1722.85it/s]warmup should be done:  95%|█████████▍| 2843/3000 [00:01<00:00, 1660.81it/s]warmup should be done:  96%|█████████▌| 2881/3000 [00:01<00:00, 1694.03it/s]warmup should be done:  96%|█████████▋| 2888/3000 [00:01<00:00, 1674.09it/s]warmup should be done:  96%|█████████▌| 2884/3000 [00:01<00:00, 1693.14it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1721.90it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1697.35it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1694.70it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1690.26it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1684.23it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1684.10it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1678.75it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1663.48it/s]
WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f2745b5e220>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f2745b60100>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f2745b6f070>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f27466a3b80>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f2745b6f1c0>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f2745b63310>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f27466a4730>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f2745b630a0>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
2022-12-11 21:59:47.782545: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f22868308c0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 21:59:47.782609: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 21:59:47.785850: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f227682bc10 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 21:59:47.785906: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 21:59:47.792724: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 21:59:47.794558: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 21:59:48.370040: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f228702cf30 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 21:59:48.370107: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 21:59:48.379350: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 21:59:48.552088: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f2276f920f0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 21:59:48.552150: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 21:59:48.562356: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 21:59:48.689057: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f227e8305a0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 21:59:48.689122: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 21:59:48.693370: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f2287028f20 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 21:59:48.693414: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 21:59:48.699208: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f2273030e80 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 21:59:48.699270: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 21:59:48.699352: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 21:59:48.704644: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 21:59:48.708234: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 21:59:48.719163: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f227b030e20 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 21:59:48.719229: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 21:59:48.728890: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 21:59:54.995304: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 21:59:55.146857: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 21:59:55.307262: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 21:59:55.531293: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 21:59:55.571273: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 21:59:55.584577: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 21:59:55.634362: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 21:59:55.650409: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
[HCTR][22:00:56.677][ERROR][RK0][tid #139786648004352]: replica 5 reaches 1000, calling init pre replica
[HCTR][22:00:56.677][ERROR][RK0][tid #139786648004352]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][22:00:56.685][ERROR][RK0][tid #139786648004352]: coll ps creation done
[HCTR][22:00:56.685][ERROR][RK0][tid #139786648004352]: replica 5 waits for coll ps creation barrier
[HCTR][22:00:56.872][ERROR][RK0][main]: replica 4 reaches 1000, calling init pre replica
[HCTR][22:00:56.872][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][22:00:56.879][ERROR][RK0][main]: coll ps creation done
[HCTR][22:00:56.880][ERROR][RK0][main]: replica 4 waits for coll ps creation barrier
[HCTR][22:00:56.900][ERROR][RK0][tid #139788116006656]: replica 2 reaches 1000, calling init pre replica
[HCTR][22:00:56.900][ERROR][RK0][tid #139788116006656]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][22:00:56.907][ERROR][RK0][tid #139788116006656]: coll ps creation done
[HCTR][22:00:56.907][ERROR][RK0][tid #139788116006656]: replica 2 waits for coll ps creation barrier
[HCTR][22:00:57.000][ERROR][RK0][tid #139786639611648]: replica 7 reaches 1000, calling init pre replica
[HCTR][22:00:57.001][ERROR][RK0][tid #139786639611648]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][22:00:57.004][ERROR][RK0][main]: replica 3 reaches 1000, calling init pre replica
[HCTR][22:00:57.004][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][22:00:57.009][ERROR][RK0][tid #139786639611648]: coll ps creation done
[HCTR][22:00:57.009][ERROR][RK0][tid #139786639611648]: replica 7 waits for coll ps creation barrier
[HCTR][22:00:57.011][ERROR][RK0][main]: coll ps creation done
[HCTR][22:00:57.011][ERROR][RK0][main]: replica 3 waits for coll ps creation barrier
[HCTR][22:00:57.074][ERROR][RK0][main]: replica 0 reaches 1000, calling init pre replica
[HCTR][22:00:57.074][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][22:00:57.079][ERROR][RK0][main]: coll ps creation done
[HCTR][22:00:57.079][ERROR][RK0][main]: replica 0 waits for coll ps creation barrier
[HCTR][22:00:57.163][ERROR][RK0][tid #139787109373696]: replica 6 reaches 1000, calling init pre replica
[HCTR][22:00:57.163][ERROR][RK0][tid #139787109373696]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][22:00:57.167][ERROR][RK0][tid #139787109373696]: coll ps creation done
[HCTR][22:00:57.167][ERROR][RK0][tid #139787109373696]: replica 6 waits for coll ps creation barrier
[HCTR][22:00:57.268][ERROR][RK0][tid #139786790614784]: replica 1 reaches 1000, calling init pre replica
[HCTR][22:00:57.268][ERROR][RK0][tid #139786790614784]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][22:00:57.275][ERROR][RK0][tid #139786790614784]: coll ps creation done
[HCTR][22:00:57.275][ERROR][RK0][tid #139786790614784]: replica 1 waits for coll ps creation barrier
[HCTR][22:00:57.275][ERROR][RK0][main]: replica 0 preparing frequency
[HCTR][22:00:58.154][ERROR][RK0][main]: replica 0 preparing frequency done
[HCTR][22:00:58.198][ERROR][RK0][main]: replica 0 calling init per replica
[HCTR][22:00:58.198][ERROR][RK0][tid #139788116006656]: replica 2 calling init per replica
[HCTR][22:00:58.198][ERROR][RK0][tid #139786639611648]: replica 7 calling init per replica
[HCTR][22:00:58.198][ERROR][RK0][main]: replica 4 calling init per replica
[HCTR][22:00:58.198][ERROR][RK0][main]: replica 3 calling init per replica
[HCTR][22:00:58.198][ERROR][RK0][tid #139786790614784]: replica 1 calling init per replica
[HCTR][22:00:58.198][ERROR][RK0][tid #139787109373696]: replica 6 calling init per replica
[HCTR][22:00:58.198][ERROR][RK0][tid #139786648004352]: replica 5 calling init per replica
[HCTR][22:00:58.198][ERROR][RK0][main]: Calling build_v2
[HCTR][22:00:58.198][ERROR][RK0][tid #139788116006656]: Calling build_v2
[HCTR][22:00:58.198][ERROR][RK0][tid #139786639611648]: Calling build_v2
[HCTR][22:00:58.198][ERROR][RK0][main]: Calling build_v2
[HCTR][22:00:58.198][ERROR][RK0][main]: Calling build_v2
[HCTR][22:00:58.198][ERROR][RK0][tid #139786790614784]: Calling build_v2
[HCTR][22:00:58.198][ERROR][RK0][tid #139787109373696]: Calling build_v2
[HCTR][22:00:58.198][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][22:00:58.198][ERROR][RK0][tid #139786648004352]: Calling build_v2
[HCTR][22:00:58.198][ERROR][RK0][tid #139788116006656]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][22:00:58.198][ERROR][RK0][tid #139786639611648]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][22:00:58.198][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][22:00:58.198][ERROR][RK0][tid #139786648004352]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][22:00:58.198][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][22:00:58.198][ERROR][RK0][tid #139786790614784]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][22:00:58.198][ERROR][RK0][tid #139787109373696]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[2022-12-11 22:00:58.202703: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:178[] v100x8, slow pcie
2022-12-11 22:00:58.[2027442022-12-11 22:00:58: .E202781 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[E: 178/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] :v100x8, slow pcie196
] assigning 0 to cpu
[2022-12-11 22:00:58.2028322022-12-11 22:00:58: .E202787 [: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE:[ 1962022-12-11 22:00:582022-12-11 22:00:58/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] ..:assigning 0 to cpu202837202864178
: : ] EEv100x8, slow pcie  
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:[:1782022-12-11 22:00:58212[2022-12-11 22:00:58] .] 2022-12-11 22:00:58.[v100x8, slow pcie202919build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8.202887
: 
2022-12-11 22:00:58202931: E[[.: E [2022-12-11 22:00:582029292022-12-11 22:00:58E [/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-11 22:00:58.: . /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:.2029892022-12-11 22:00:58E202971/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:196203006: . : :178] : E203013/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE212] assigning 0 to cpuE : : ] v100x8, slow pcie
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE178/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccbuild asymm link desc with 8X Tesla V100-SXM2-32GB out of 8
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: ] :
:196[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccv100x8, slow pcie178213] [2022-12-11 22:00:58:
[] ] [assigning 0 to cpu2022-12-11 22:00:58.1782022-12-11 22:00:58v100x8, slow pcieremote time is 8.684212022-12-11 22:00:58
.203221] .

.203237: v100x8, slow pcie203246203277[: [[E
: : 2022-12-11 22:00:58E2022-12-11 22:00:582022-12-11 22:00:58 EE[. ../hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc  2022-12-11 22:00:58203341/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc203345203346:/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.: :: : 196::203386E212EE] 213196:  ]   assigning 0 to cpu] ] E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccbuild asymm link desc with 8X Tesla V100-SXM2-32GB out of 8/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
remote time is 8.68421assigning 0 to cpu :
::

/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc196214212[[:[] ] ] 2022-12-11 22:00:58[2022-12-11 22:00:581962022-12-11 22:00:58assigning 0 to cpucpu time is 97.0588build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8.2022-12-11 22:00:58.] .


203590.203594assigning 0 to cpu203597: 203613: [[
: E: E2022-12-11 22:00:582022-12-11 22:00:58E E .. /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc203702203720[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:: : 2022-12-11 22:00:58:212:213EE.214] 212]   203775] build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8] remote time is 8.68421/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: cpu time is 97.0588
build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8
::E

213212[[ ] ] 2022-12-11 22:00:58[2022-12-11 22:00:58/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccremote time is 8.68421build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8.2022-12-11 22:00:58.:

203912.203915212[: 203938: [] 2022-12-11 22:00:58E: E2022-12-11 22:00:58build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8. E .
203999/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc204007: :/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[:: E214:2022-12-11 22:00:58213E ] 213.]  /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cccpu time is 97.0588] 204069remote time is 8.68421/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:
remote time is 8.68421: 
:214
E213[[]  ] 2022-12-11 22:00:582022-12-11 22:00:58cpu time is 97.0588/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccremote time is 8.68421..
:
204158204168213: : ] [EEremote time is 8.684212022-12-11 22:00:58  
./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc204219::[: 2142142022-12-11 22:00:58E] ] . cpu time is 97.0588cpu time is 97.0588204245/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc

: :E214 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cccpu time is 97.0588:
214] cpu time is 97.0588
[2022-12-11 22:02:15.689922: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:118] solver created. now build & solve
[2022-12-11 22:02:15.729994: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:123] solver built. now solve
block 0 storage is 00010001
	access is	0	0	0	0	4	4	4	4	
block 1 storage is 00100010
	access is	1	1	1	1	5	5	5	5	
block 2 storage is 01000100
	access is	2	2	2	2	6	6	6	6	
block 3 storage is 10001000
	access is	3	3	3	3	7	7	7	7	
block 4 storage is 00000000
	access is	8	8	8	8	8	8	8	8	
[2022-12-11 22:02:15.844262: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:127] solver solved
[2022-12-11 22:02:15.844338: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:195] 0 solved master
[2022-12-11 22:02:15.844367: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 0 solved
[2022-12-11 22:02:15.844395: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 0 initing device 0
[2022-12-11 22:02:15.844889: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-11 22:02:15.844931: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 22:02:15.845771: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 22:02:15.846434: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 22:02:15.859637: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 2 solved
[2022-12-11 22:02:15.859696: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 2 initing device 2
[[2022-12-11 22:02:152022-12-11 22:02:15..859846859851: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc::202202] ] 1 solved[4 solved
[2022-12-11 22:02:15
[.2022-12-11 22:02:152022-12-11 22:02:15[859899..2022-12-11 22:02:15: 859906859932.E: : 859944 EE: /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc  E:/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc 202::/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc] 202205:7 solved] ] 205
3 solvedworker 0 thread 1 initing device 1] 

worker 0 thread 4 initing device 4[
2022-12-11 22:02:15[.2022-12-11 22:02:15860022.: 860029E:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc :/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc205:] 205worker 0 thread 7 initing device 7] 
worker 0 thread 3 initing device 3
[2022-12-11 22:02:15.860125: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-11 22:02:15.860170: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[[2022-12-11 22:02:152022-12-11 22:02:15..860424860426: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::18151815[] [] 2022-12-11 22:02:15Building Coll Cache with ... num gpu device is 82022-12-11 22:02:15Building Coll Cache with ... num gpu device is 8.
.
860461860462: : EE  [/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu[2022-12-11 22:02:15::2022-12-11 22:02:15.18151815.860501] ] 860506: Building Coll Cache with ... num gpu device is 8Building Coll Cache with ... num gpu device is 8: E

E  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::1980[1980] [2022-12-11 22:02:15] eager alloc mem 381.47 MB2022-12-11 22:02:15.eager alloc mem 381.47 MB
.860561
860563: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::19801980] ] eager alloc mem 381.47 MBeager alloc mem 381.47 MB

[2022-12-11 22:02:15.860982: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 5 solved
[2022-12-11 22:02:15.861035: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 5 initing device 5
[2022-12-11 22:02:15.861033: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 6 solved
[2022-12-11 22:02:15.861110: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 6 initing device 6
[2022-12-11 22:02:15.861447: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-11 22:02:15.861493: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 22:02:15.861565: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-11 22:02:15.861615: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 22:02:15.866569: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 22:02:15.866717: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 22:02:15.866778: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 22:02:15.866823: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 22:02:15.866874: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 22:02:15.866943: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 22:02:15.867456: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 22:02:15.870989: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 22:02:15.871107: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 22:02:15.871157: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 22:02:15.871280: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 22:02:15.871350: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:[19802022-12-11 22:02:15] .eager alloc mem 381.47 MB871380
: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 22:02:15.871437: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 22:02:15.925118: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.00 Bytes
[2022-12-11 22:02:15.931191: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5
[2022-12-11 22:02:15.931325: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-11 22:02:15.932259: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 8.01 MB
[2022-12-11 22:02:15.932999: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:02:15.933999: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:02:15.935913: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 22:02:15.936659: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 22:02:15.936705: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 981.44 MB
[[[[[2022-12-11 22:02:152022-12-11 22:02:152022-12-11 22:02:152022-12-11 22:02:152022-12-11 22:02:15.....958509958523958522958525958524: : : : : EEEEE     /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::::[:19801980198019802022-12-11 22:02:151980] ] ] ] .] eager alloc mem 5.00 Byteseager alloc mem 5.00 Byteseager alloc mem 5.00 Byteseager alloc mem 5.00 Bytes958647eager alloc mem 5.00 Bytes



: 
E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.00 Bytes
[2022-12-11 22:02:15.961049: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.00 Bytes
[2022-12-11 22:02:15.969200: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5
[2022-12-11 22:02:15[.2022-12-11 22:02:15969302.: 969291E:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc :/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc638:] 638eager release cuda mem 400000000] 
eager release cuda mem 5
[2022-12-11 22:02:15.969354: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5[
2022-12-11 22:02:15.969396: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-11 22:02:15.969440: E[ 2022-12-11 22:02:15/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc.:969436638: ] Eeager release cuda mem 400000000 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5
[[2022-12-11 22:02:152022-12-11 22:02:15..969517969532: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::638638] ] eager release cuda mem 5eager release cuda mem 400000000

[2022-12-11 22:02:15.969602[: 2022-12-11 22:02:15E. 969624/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc: :E638 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cceager release cuda mem 5:
638] eager release cuda mem 400000000
[2022-12-11 22:02:15.969696: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-11 22:02:15.969899: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5
[2022-12-11 22:02:15.969980: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-11 22:02:15.980980: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 8.01 MB
[2022-12-11 22:02:15.981507: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 8.01 MB
[2022-12-11 22:02:15.982013: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 8.01 MB
[2022-12-11 22:02:15.982521: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 8.01 MB
[2022-12-11 22:02:15.983022: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 8.01 MB
[2022-12-11 22:02:15.983561: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 8.01 MB
[2022-12-11 22:02:15.984259: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 8.01 MB
[2022-12-11 22:02:15.990307: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:02:15.990345: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:02:15.990384: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:02:15.990442: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:02:15.990476: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:02:15.990525: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:02:15.990583: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[[2022-12-11 22:02:152022-12-11 22:02:15..991266991270: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::638638] ] eager release cuda mem 625663eager release cuda mem 625663

[2022-12-11 22:02:15.991338: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:02:15.991402: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663[
2022-12-11 22:02:15.991424: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:02:15.991482: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:02:15.991564: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[[2022-12-11 22:02:152022-12-11 22:02:15..997371997379: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::19801980] ] eager alloc mem 25.25 KBeager alloc mem 25.25 KB

[2022-12-11 22:02:15.997450: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 22:02:15.997514: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 22:02:15.997592: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 22:02:15.997681: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[[2022-12-11 22:02:152022-12-11 22:02:15..997998998000: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::638638] ] eager release cuda mem 25855eager release cuda mem 25855[

2022-12-11 22:02:15.998041: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638[] [2022-12-11 22:02:15eager release cuda mem 258552022-12-11 22:02:15.
.998066998068: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu[/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2022-12-11 22:02:15:1980.[1980] 9980992022-12-11 22:02:15] eager alloc mem 981.44 MB: .eager alloc mem 981.44 MB
E998108
 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE: 1980/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc] :eager alloc mem 981.44 MB638
] eager release cuda mem 25855
[[[2022-12-11 22:02:152022-12-11 22:02:152022-12-11 22:02:15...998163998184998187: : : EEE   /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:::[198063819802022-12-11 22:02:15] ] ] .eager alloc mem 25.25 KBeager release cuda mem 25855eager alloc mem 981.44 MB998284


: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 22:02:15.998353: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980[] 2022-12-11 22:02:15eager alloc mem 981.44 MB.
998370: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 981.44 MB
[2022-12-11 22:02:15.998886: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 22:02:15.998932: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 981.44 MB
[[[[[[[[2022-12-11 22:02:162022-12-11 22:02:162022-12-11 22:02:162022-12-11 22:02:162022-12-11 22:02:162022-12-11 22:02:162022-12-11 22:02:162022-12-11 22:02:16........357642357643357643357643357642357643357646357643: : : : : : : : EEEEEEEE        /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::::::::19261926192619261926192619261926] ] ] ] ] ] ] ] Device 6 init p2p of link 0Device 2 init p2p of link 1Device 7 init p2p of link 4Device 3 init p2p of link 2Device 4 init p2p of link 5Device 1 init p2p of link 7Device 0 init p2p of link 3Device 5 init p2p of link 6







[2022-12-11 22:02:16.358140: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] [[[[[[eager alloc mem 611.00 KB2022-12-11 22:02:162022-12-11 22:02:162022-12-11 22:02:16[2022-12-11 22:02:162022-12-11 22:02:162022-12-11 22:02:16
...2022-12-11 22:02:16...358158358157358157.358157358160358161: : : 358171: : : EEE: EEE   E   /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:::/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:::198019801980:198019801980] ] ] 1980] ] ] eager alloc mem 611.00 KBeager alloc mem 611.00 KBeager alloc mem 611.00 KB] eager alloc mem 611.00 KBeager alloc mem 611.00 KBeager alloc mem 611.00 KB


eager alloc mem 611.00 KB



[2022-12-11 22:02:16.358967: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:02:16.359110: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:[638[2022-12-11 22:02:16[[] 2022-12-11 22:02:16[.2022-12-11 22:02:162022-12-11 22:02:16[eager release cuda mem 625663.2022-12-11 22:02:16359136..2022-12-11 22:02:16
359137.: 359136359141.: 359144E: : 359151E:  EE:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc :/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc :/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc638::/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638:] 638638638] 638eager release cuda mem 625663] ] ] eager release cuda mem 625663] 
eager release cuda mem 625663eager release cuda mem 625663eager release cuda mem 625663
eager release cuda mem 625663



[2022-12-11 22:02:16.371696: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 6 init p2p of link 5
[2022-12-11 22:02:16.371845: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:02:16.371930: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 2
[2022-12-11 22:02:16.372072: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:02:16.372657: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:02:16.372869: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:02:16.372910: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 5 init p2p of link 4
[2022-12-11 22:02:16.373052: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:02:16.373156: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 7 init p2p of link 1
[2022-12-11 22:02:16.373297: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:02:16.373384: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 2 init p2p of link 3
[2022-12-11 22:02:16.373515: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:02:16.373565: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 4 init p2p of link 7
[2022-12-11 22:02:16.373706: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:02:16.373830: [E2022-12-11 22:02:16 ./hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu373852:: 1926E]  Device 3 init p2p of link 0/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc
:638] eager release cuda mem 625663
[2022-12-11 22:02:16.373982: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:02:16.374008: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 0 init p2p of link 6
[2022-12-11 22:02:16.374073: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:02:16.374157: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:02:16.374305: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:02:16.374495: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:02:16.374768: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:02:16.374925: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:02:16.385814: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 6 init p2p of link 4
[2022-12-11 22:02:16.385935: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:02:16.386097: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 7 init p2p of link 6
[2022-12-11 22:02:16.386215: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:02:16.386273: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 3
[2022-12-11 22:02:16.386393: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:02:16.[3867042022-12-11 22:02:16: .E386719 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE: 1926/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc] :Device 5 init p2p of link 7638
] eager release cuda mem 625663
[2022-12-11 22:02:16.386866: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:02:16.386946: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 0 init p2p of link 1
[2022-12-11 22:02:16.387026: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:02:16.387064: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:[19802022-12-11 22:02:16] .eager alloc mem 611.00 KB387072
: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 2 init p2p of link 0
[2022-12-11 22:02:16.387176: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:02:16.387203: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:02:16.387341: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 4 init p2p of link 2
[2022-12-11 22:02:16.387460: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:02:16.387511: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 3 init p2p of link 5
[2022-12-11 22:02:16.387626: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:02:16.387653: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:02:16.387889: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:02:16.387995: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:02:16.388233: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:02:16.388391: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:02:16.402330: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 6 init p2p of link 7
[2022-12-11 22:02:16.402447: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[[2022-12-11 22:02:162022-12-11 22:02:16..402928402930: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::19261926] ] Device 4 init p2p of link 6Device 7 init p2p of link 5

[2022-12-11 22:02:16[.2022-12-11 22:02:16403069.: 403072E:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu :/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu1980:] 1980eager alloc mem 611.00 KB] 
eager alloc mem 611.00 KB
[2022-12-11 22:02:16.403238: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[[2022-12-11 22:02:162022-12-11 22:02:16..403264403265: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::19261926] ] Device 3 init p2p of link 1Device 1 init p2p of link 0

[[2022-12-11 22:02:162022-12-11 22:02:16..403407403410: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::19801980] ] eager alloc mem 611.00 KBeager alloc mem 611.00 KB

[2022-12-11 22:02:16.403548: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 5 init p2p of link 3
[2022-12-11 22:02:16.403661: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:02:16.403830: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 0 init p2p of link 2
[[2022-12-11 22:02:162022-12-11 22:02:16..403893403895: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::638638] ] eager release cuda mem 625663eager release cuda mem 625663

[2022-12-11 22:02:16.403948: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:02:16.404164: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 2 init p2p of link 4
[[2022-12-11 22:02:162022-12-11 22:02:16..404218404220: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::638638] ] eager release cuda mem 625663eager release cuda mem 625663

[2022-12-11 22:02:16.404280: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:02:16.404426: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:02:16.404737: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:02:16.405042: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:02:16.418571: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 8399996
[2022-12-11 22:02:16.418896: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 8399996
[2022-12-11 22:02:16.419028: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 8399996
[2022-12-11 22:02:16.419465: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 8399996
[2022-12-11 22:02:16.419572: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 8399996
[2022-12-11 22:02:16.419792: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 8399996
[2022-12-11 22:02:16.420124: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 8399996
[2022-12-11 22:02:16.420431: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 1999999 / 100000000 nodes ( 2.00 %~2.00 %) | remote 5999997 / 100000000 nodes ( 6.00 %) | cpu 92000004 / 100000000 nodes ( 92.00 %) | 981.44 MB | 0.558828 secs 
[2022-12-11 22:02:16.420602: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 1999999 / 100000000 nodes ( 2.00 %~2.00 %) | remote 5999997 / 100000000 nodes ( 6.00 %) | cpu 92000004 / 100000000 nodes ( 92.00 %) | 981.44 MB | 0.559114 secs 
[2022-12-11 22:02:16.420679: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 1999999 / 100000000 nodes ( 2.00 %~2.00 %) | remote 5999997 / 100000000 nodes ( 6.00 %) | cpu 92000004 / 100000000 nodes ( 92.00 %) | 981.44 MB | 0.560123 secs 
[2022-12-11 22:02:16.420799: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 8399996
[2022-12-11 22:02:16.420851: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 1999999 / 100000000 nodes ( 2.00 %~2.00 %) | remote 5999997 / 100000000 nodes ( 6.00 %) | cpu 92000004 / 100000000 nodes ( 92.00 %) | 981.44 MB | 0.560353 secs 
[2022-12-11 22:02:16.421012: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 1999999 / 100000000 nodes ( 2.00 %~2.00 %) | remote 5999997 / 100000000 nodes ( 6.00 %) | cpu 92000004 / 100000000 nodes ( 92.00 %) | 981.44 MB | 0.560851 secs 
[2022-12-11 22:02:16.421264: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 1999999 / 100000000 nodes ( 2.00 %~2.00 %) | remote 5999997 / 100000000 nodes ( 6.00 %) | cpu 92000004 / 100000000 nodes ( 92.00 %) | 981.44 MB | 0.56077 secs 
[2022-12-11 22:02:16.421385: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 1999999 / 100000000 nodes ( 2.00 %~2.00 %) | remote 5999997 / 100000000 nodes ( 6.00 %) | cpu 92000004 / 100000000 nodes ( 92.00 %) | 981.44 MB | 0.560829 secs 
[2022-12-11 22:02:16.421908: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 1999999 / 100000000 nodes ( 2.00 %~2.00 %) | remote 5999997 / 100000000 nodes ( 6.00 %) | cpu 92000004 / 100000000 nodes ( 92.00 %) | 981.44 MB | 0.576985 secs 
[HCTR][22:02:16.422][ERROR][RK0][main]: replica 0 calling init per replica done, doing barrier
[HCTR][22:02:16.422][ERROR][RK0][tid #139786790614784]: replica 1 calling init per replica done, doing barrier
[HCTR][22:02:16.422][ERROR][RK0][tid #139786639611648]: replica 7 calling init per replica done, doing barrier
[HCTR][22:02:16.422][ERROR][RK0][main]: replica 4 calling init per replica done, doing barrier
[HCTR][22:02:16.422][ERROR][RK0][tid #139787109373696]: replica 6 calling init per replica done, doing barrier
[HCTR][22:02:16.422][ERROR][RK0][main]: replica 3 calling init per replica done, doing barrier
[HCTR][22:02:16.422][ERROR][RK0][tid #139788116006656]: replica 2 calling init per replica done, doing barrier
[HCTR][22:02:16.422][ERROR][RK0][tid #139786648004352]: replica 5 calling init per replica done, doing barrier
[HCTR][22:02:16.422][ERROR][RK0][tid #139786648004352]: replica 5 calling init per replica done, doing barrier done
[HCTR][22:02:16.422][ERROR][RK0][main]: replica 3 calling init per replica done, doing barrier done
[HCTR][22:02:16.422][ERROR][RK0][tid #139787109373696]: replica 6 calling init per replica done, doing barrier done
[HCTR][22:02:16.422][ERROR][RK0][tid #139788116006656]: replica 2 calling init per replica done, doing barrier done
[HCTR][22:02:16.422][ERROR][RK0][main]: replica 4 calling init per replica done, doing barrier done
[HCTR][22:02:16.422][ERROR][RK0][tid #139786648004352]: init per replica done
[HCTR][22:02:16.422][ERROR][RK0][main]: replica 0 calling init per replica done, doing barrier done
[HCTR][22:02:16.422][ERROR][RK0][tid #139786790614784]: replica 1 calling init per replica done, doing barrier done
[HCTR][22:02:16.422][ERROR][RK0][tid #139786639611648]: replica 7 calling init per replica done, doing barrier done
[HCTR][22:02:16.422][ERROR][RK0][main]: init per replica done
[HCTR][22:02:16.422][ERROR][RK0][tid #139787109373696]: init per replica done
[HCTR][22:02:16.422][ERROR][RK0][tid #139788116006656]: init per replica done
[HCTR][22:02:16.422][ERROR][RK0][main]: init per replica done
[HCTR][22:02:16.422][ERROR][RK0][tid #139786790614784]: init per replica done
[HCTR][22:02:16.422][ERROR][RK0][tid #139786639611648]: init per replica done
[HCTR][22:02:16.424][ERROR][RK0][main]: init per replica done
