2022-12-12 00:26:59.338505: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.349343: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.354053: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.359544: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.371410: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.378314: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.390253: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.396231: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.448122: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.452577: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.453929: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.455661: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.456321: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.456768: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.457936: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.458067: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.459398: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.459457: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.460746: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.460850: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.462156: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.462337: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.463538: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.463985: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.465066: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.465766: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.466580: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.467411: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.468076: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.469483: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.470488: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.471423: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.473148: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.474338: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.475376: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.476417: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.477448: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.478418: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.479349: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.480264: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.485147: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.485563: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 00:26:59.486964: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.487971: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.488306: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.489694: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.490207: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.492182: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.492792: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.493922: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.494384: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.494916: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.495466: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.496494: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.497044: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.497741: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.498136: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.499576: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.500184: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.500716: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.500819: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.502473: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.503050: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.504962: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.505477: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.507263: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.508947: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.510577: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.513657: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.515666: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.517236: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.517468: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.519176: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.519192: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.521265: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.521545: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.526985: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.527671: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.528076: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.529314: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.530158: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.531035: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.531366: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.531900: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.548320: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.552127: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.556770: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.567356: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.568428: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.569176: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.569432: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.570728: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.570861: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.571280: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.572191: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.573169: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.574512: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.574601: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.575932: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.575985: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.576234: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.578452: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.579574: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.579974: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.580464: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.582389: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.582438: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.582670: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.585408: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.585639: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.586621: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.587067: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.587164: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.587305: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.589900: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.590039: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.590989: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.591086: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.591234: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.591327: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.594180: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.594726: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.595312: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.595437: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.595532: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.595585: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.598329: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.599662: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.599714: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.599861: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.600113: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.601889: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.602999: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.603050: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.603219: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.605631: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.605743: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.605783: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.606066: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.608708: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.608819: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.608854: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.609086: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.611986: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.612081: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.612145: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.612273: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.614769: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.614900: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.614982: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.615067: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.617677: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.617913: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.617926: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.618014: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.620437: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.620721: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.620773: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.620818: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.623054: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.623423: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.623679: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.623725: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.626418: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.626743: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.627407: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.627999: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.628607: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.629873: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.630173: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.630446: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.631104: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.632292: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 00:26:59.632647: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.632963: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.633590: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.634019: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.635436: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.636111: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.636550: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.637207: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.638787: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.639625: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.639861: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.640561: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.641796: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.641962: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.642713: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.643227: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.643378: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.643844: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.645834: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.646363: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.646423: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.647107: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.647208: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.647387: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.647734: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.650449: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.650997: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.651190: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.651805: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.651962: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.652319: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.655774: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.656483: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.656692: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.656935: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 00:26:59.658765: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.659390: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.659504: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 00:26:59.659636: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.659835: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 00:26:59.661860: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.662690: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.662726: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.665303: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.665823: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.665874: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.666723: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.668453: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.669182: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.669371: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.669600: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.669822: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.670673: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.672959: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.673956: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.674114: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.674426: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.674952: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.675473: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.677904: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.678865: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.679256: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.679329: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.679882: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.682564: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.683295: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.683692: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.714926: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.715533: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.716020: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.720065: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.720807: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.720986: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.728904: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.729631: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.729828: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.734834: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.735687: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.735859: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.768386: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.769143: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.769216: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.774287: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.775824: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.776363: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.781273: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.781638: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.782795: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.786845: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.787488: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.787806: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.791533: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.791951: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.799095: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.811380: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.812985: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.815201: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 00:26:59.815806: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.816663: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.824584: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.850070: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.879115: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.907536: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.908811: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.910987: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.913772: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.915757: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.922014: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 00:26:59.925349: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 00:26:59.931760: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.933482: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.938693: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.939455: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.943502: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:26:59.944706: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:27:00.783160: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:27:00.783960: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:27:00.784497: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:27:00.785393: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 00:27:00.785454: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-12 00:27:00.803396: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:27:00.804044: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:27:00.804566: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:27:00.805270: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:27:00.805805: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:27:00.806271: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-12 00:27:00.851967: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 00:27:00.852174: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 00:27:00.879626: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12340
2022-12-12 00:27:01.007366: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:27:01.008202: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:27:01.008753: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:27:01.009551: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 00:27:01.009610: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-12 00:27:01.028970: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:27:01.029623: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:27:01.030317: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:27:01.031115: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:27:01.031884: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:27:01.032358: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:1/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-12 00:27:01.053910: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:27:01.054534: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:27:01.055067: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:27:01.055545: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 00:27:01.055604: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-12 00:27:01.073098: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:27:01.073726: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:27:01.074244: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:27:01.074714: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 00:27:01.074768: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-12 00:27:01.076496: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:27:01.077138: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:27:01.077661: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:27:01.078429: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:27:01.078966: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:27:01.079506: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:2/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-12 00:27:01.089200: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:27:01.089829: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:27:01.090347: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:27:01.090811: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 00:27:01.090862: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-12 00:27:01.092008: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:27:01.092620: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:27:01.093128: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:27:01.093708: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:27:01.094216: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:27:01.094714: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:3/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-12 00:27:01.104969: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:27:01.105577: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:27:01.106092: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:27:01.106560: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 00:27:01.106613: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-12 00:27:01.109802: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:27:01.110431: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:27:01.110966: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:27:01.111607: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:27:01.112143: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:27:01.112623: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:4/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-12 00:27:01.117652: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 00:27:01.117812: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 00:27:01.119605: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12341
2022-12-12 00:27:01.123233: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:27:01.123848: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:27:01.124367: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:27:01.124832: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 00:27:01.124882: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-12 00:27:01.125097: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:27:01.125668: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:27:01.125933: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:27:01.126568: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:27:01.126935: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:27:01.127426: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:27:01.127920: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:27:01.128241: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 00:27:01.128294: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-12 00:27:01.128810: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:27:01.129294: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:7/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-12 00:27:01.140750: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 00:27:01.140948: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 00:27:01.142641: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12343
2022-12-12 00:27:01.142985: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:27:01.143653: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:27:01.144164: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:27:01.144750: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:27:01.145263: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:27:01.145742: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:6/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-12 00:27:01.147806: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:27:01.148405: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:27:01.148933: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:27:01.149517: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:27:01.150046: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:27:01.150521: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:5/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-12 00:27:01.158641: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 00:27:01.158834: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 00:27:01.160914: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12344
2022-12-12 00:27:01.175781: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 00:27:01.175969: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 00:27:01.177792: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12347
2022-12-12 00:27:01.192467: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 00:27:01.192660: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 00:27:01.194507: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12346
2022-12-12 00:27:01.195921: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 00:27:01.196097: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 00:27:01.197898: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12345
2022-12-12 00:27:01.203874: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 00:27:01.204051: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 00:27:01.205890: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12342
[HCTR][00:27:02.443][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][00:27:02.446][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][00:27:02.452][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][00:27:02.454][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][00:27:02.460][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][00:27:02.460][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][00:27:02.501][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][00:27:02.519][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 1it [00:01,  1.53s/it]warmup run: 1it [00:01,  1.53s/it]warmup run: 1it [00:01,  1.54s/it]warmup run: 1it [00:01,  1.49s/it]warmup run: 1it [00:01,  1.59s/it]warmup run: 1it [00:01,  1.53s/it]warmup run: 99it [00:01, 84.46it/s]warmup run: 101it [00:01, 85.99it/s]warmup run: 102it [00:01, 86.44it/s]warmup run: 100it [00:01, 87.00it/s]warmup run: 1it [00:01,  1.54s/it]warmup run: 90it [00:01, 74.03it/s]warmup run: 99it [00:01, 84.33it/s]warmup run: 1it [00:01,  1.47s/it]warmup run: 199it [00:01, 184.13it/s]warmup run: 201it [00:01, 185.24it/s]warmup run: 205it [00:01, 188.46it/s]warmup run: 200it [00:01, 188.16it/s]warmup run: 99it [00:01, 83.80it/s]warmup run: 181it [00:01, 162.11it/s]warmup run: 198it [00:01, 182.81it/s]warmup run: 102it [00:01, 90.20it/s]warmup run: 300it [00:01, 295.27it/s]warmup run: 300it [00:01, 293.11it/s]warmup run: 308it [00:01, 300.97it/s]warmup run: 302it [00:01, 301.49it/s]warmup run: 191it [00:01, 174.11it/s]warmup run: 297it [00:01, 291.31it/s]warmup run: 273it [00:01, 260.99it/s]warmup run: 204it [00:01, 194.71it/s]warmup run: 402it [00:01, 411.60it/s]warmup run: 400it [00:01, 406.50it/s]warmup run: 411it [00:01, 417.47it/s]warmup run: 400it [00:01, 412.12it/s]warmup run: 290it [00:01, 283.18it/s]warmup run: 398it [00:01, 406.85it/s]warmup run: 364it [00:01, 363.05it/s]warmup run: 306it [00:01, 308.67it/s]warmup run: 504it [00:02, 524.78it/s]warmup run: 501it [00:02, 518.78it/s]warmup run: 514it [00:02, 530.72it/s]warmup run: 500it [00:01, 522.38it/s]warmup run: 391it [00:01, 399.24it/s]warmup run: 499it [00:02, 519.13it/s]warmup run: 456it [00:02, 464.64it/s]warmup run: 405it [00:01, 420.55it/s]warmup run: 608it [00:02, 632.02it/s]warmup run: 603it [00:02, 623.78it/s]warmup run: 617it [00:02, 634.69it/s]warmup run: 600it [00:02, 623.68it/s]warmup run: 490it [00:02, 508.63it/s]warmup run: 602it [00:02, 625.58it/s]warmup run: 548it [00:02, 557.85it/s]warmup run: 505it [00:01, 530.37it/s]warmup run: 707it [00:02, 713.20it/s]warmup run: 706it [00:02, 717.14it/s]warmup run: 720it [00:02, 724.44it/s]warmup run: 696it [00:02, 700.24it/s]warmup run: 592it [00:02, 615.59it/s]warmup run: 704it [00:02, 716.68it/s]warmup run: 640it [00:02, 639.81it/s]warmup run: 608it [00:02, 636.95it/s]warmup run: 805it [00:02, 778.92it/s]warmup run: 807it [00:02, 789.50it/s]warmup run: 822it [00:02, 795.85it/s]warmup run: 694it [00:02, 708.74it/s]warmup run: 792it [00:02, 755.53it/s]warmup run: 804it [00:02, 786.79it/s]warmup run: 732it [00:02, 707.72it/s]warmup run: 710it [00:02, 724.83it/s]warmup run: 907it [00:02, 840.07it/s]warmup run: 907it [00:02, 843.02it/s]warmup run: 923it [00:02, 849.88it/s]warmup run: 796it [00:02, 785.00it/s]warmup run: 904it [00:02, 841.71it/s]warmup run: 824it [00:02, 761.72it/s]warmup run: 887it [00:02, 789.26it/s]warmup run: 811it [00:02, 795.89it/s]warmup run: 1009it [00:02, 888.82it/s]warmup run: 1007it [00:02, 882.48it/s]warmup run: 1024it [00:02, 882.35it/s]warmup run: 898it [00:02, 845.24it/s]warmup run: 1004it [00:02, 882.59it/s]warmup run: 920it [00:02, 813.29it/s]warmup run: 912it [00:02, 851.69it/s]warmup run: 980it [00:02, 812.83it/s]warmup run: 1112it [00:02, 926.96it/s]warmup run: 1107it [00:02, 914.31it/s]warmup run: 1124it [00:02, 894.47it/s]warmup run: 1000it [00:02, 892.17it/s]warmup run: 1105it [00:02, 917.60it/s]warmup run: 1018it [00:02, 859.41it/s]warmup run: 1013it [00:02, 894.73it/s]warmup run: 1071it [00:02, 829.68it/s]warmup run: 1215it [00:02, 954.50it/s]warmup run: 1207it [00:02, 937.82it/s]warmup run: 1103it [00:02, 928.76it/s]warmup run: 1221it [00:02, 907.02it/s]warmup run: 1209it [00:02, 951.70it/s]warmup run: 1118it [00:02, 897.12it/s]warmup run: 1115it [00:02, 928.57it/s]warmup run: 1161it [00:02, 844.45it/s]warmup run: 1317it [00:02, 969.59it/s]warmup run: 1307it [00:02, 950.08it/s]warmup run: 1206it [00:02, 956.25it/s]warmup run: 1318it [00:02, 920.92it/s]warmup run: 1312it [00:02, 973.02it/s]warmup run: 1218it [00:02, 924.66it/s]warmup run: 1216it [00:02, 950.44it/s]warmup run: 1251it [00:02, 853.00it/s]warmup run: 1419it [00:02, 983.91it/s]warmup run: 1406it [00:02, 961.02it/s]warmup run: 1308it [00:02, 973.60it/s]warmup run: 1415it [00:02, 933.63it/s]warmup run: 1416it [00:02, 990.55it/s]warmup run: 1317it [00:02, 941.95it/s]warmup run: 1317it [00:02, 961.36it/s]warmup run: 1340it [00:02, 862.22it/s]warmup run: 1522it [00:03, 995.26it/s]warmup run: 1507it [00:03, 974.50it/s]warmup run: 1410it [00:02, 986.58it/s]warmup run: 1514it [00:03, 947.92it/s]warmup run: 1520it [00:03, 1002.18it/s]warmup run: 1417it [00:03, 957.54it/s]warmup run: 1417it [00:02, 972.28it/s]warmup run: 1429it [00:03, 868.46it/s]warmup run: 1624it [00:03, 1002.21it/s]warmup run: 1607it [00:03, 978.45it/s]warmup run: 1512it [00:03, 989.98it/s]warmup run: 1617it [00:03, 970.63it/s]warmup run: 1623it [00:03, 1006.38it/s]warmup run: 1517it [00:03, 968.02it/s]warmup run: 1517it [00:02, 968.95it/s]warmup run: 1525it [00:03, 894.74it/s]warmup run: 1728it [00:03, 1010.84it/s]warmup run: 1707it [00:03, 982.86it/s]warmup run: 1720it [00:03, 987.44it/s]warmup run: 1614it [00:03, 990.24it/s]warmup run: 1616it [00:03, 973.22it/s]warmup run: 1726it [00:03, 1007.00it/s]warmup run: 1616it [00:03, 966.81it/s]warmup run: 1622it [00:03, 915.59it/s]warmup run: 1831it [00:03, 1016.03it/s]warmup run: 1807it [00:03, 987.13it/s]warmup run: 1822it [00:03, 996.45it/s]warmup run: 1715it [00:03, 978.16it/s]warmup run: 1715it [00:03, 983.83it/s]warmup run: 1828it [00:03, 991.52it/s] warmup run: 1715it [00:03, 969.12it/s]warmup run: 1719it [00:03, 930.94it/s]warmup run: 1935it [00:03, 1020.96it/s]warmup run: 1907it [00:03, 989.18it/s]warmup run: 1925it [00:03, 1005.18it/s]warmup run: 1814it [00:03, 981.09it/s]warmup run: 1815it [00:03, 978.79it/s]warmup run: 1932it [00:03, 1003.35it/s]warmup run: 1813it [00:03, 957.93it/s]warmup run: 1815it [00:03, 939.30it/s]warmup run: 2043it [00:03, 1036.74it/s]warmup run: 2008it [00:03, 995.23it/s]warmup run: 2031it [00:03, 1020.56it/s]warmup run: 1913it [00:03, 963.11it/s]warmup run: 1914it [00:03, 972.14it/s]warmup run: 2041it [00:03, 1028.46it/s]warmup run: 1914it [00:03, 951.98it/s]warmup run: 1910it [00:03, 956.65it/s]warmup run: 2164it [00:03, 1086.34it/s]warmup run: 2127it [00:03, 1052.39it/s]warmup run: 2150it [00:03, 1070.57it/s]warmup run: 2163it [00:03, 1084.54it/s]warmup run: 2012it [00:03, 971.39it/s]warmup run: 2012it [00:03, 960.02it/s]warmup run: 2007it [00:03, 957.71it/s]warmup run: 2010it [00:03, 910.32it/s]warmup run: 2285it [00:03, 1121.00it/s]warmup run: 2247it [00:03, 1094.01it/s]warmup run: 2269it [00:03, 1105.92it/s]warmup run: 2285it [00:03, 1124.18it/s]warmup run: 2132it [00:03, 1037.61it/s]warmup run: 2122it [00:03, 999.22it/s]warmup run: 2126it [00:03, 1024.60it/s]warmup run: 2129it [00:03, 987.87it/s]warmup run: 2406it [00:03, 1145.14it/s]warmup run: 2367it [00:03, 1123.82it/s]warmup run: 2389it [00:03, 1131.84it/s]warmup run: 2407it [00:03, 1152.29it/s]warmup run: 2252it [00:03, 1084.44it/s]warmup run: 2233it [00:03, 1030.84it/s]warmup run: 2245it [00:03, 1072.50it/s]warmup run: 2248it [00:03, 1045.12it/s]warmup run: 2527it [00:03, 1162.36it/s]warmup run: 2486it [00:03, 1142.92it/s]warmup run: 2509it [00:03, 1150.81it/s]warmup run: 2529it [00:03, 1171.41it/s]warmup run: 2372it [00:03, 1118.34it/s]warmup run: 2342it [00:03, 1046.96it/s]warmup run: 2365it [00:03, 1107.56it/s]warmup run: 2367it [00:04, 1085.95it/s]warmup run: 2648it [00:04, 1174.83it/s]warmup run: 2605it [00:04, 1156.52it/s]warmup run: 2630it [00:04, 1167.08it/s]warmup run: 2651it [00:04, 1185.34it/s]warmup run: 2492it [00:03, 1142.36it/s]warmup run: 2453it [00:04, 1063.26it/s]warmup run: 2484it [00:03, 1131.87it/s]warmup run: 2486it [00:04, 1114.08it/s]warmup run: 2768it [00:04, 1180.02it/s]warmup run: 2724it [00:04, 1166.42it/s]warmup run: 2752it [00:04, 1180.94it/s]warmup run: 2772it [00:04, 1190.85it/s]warmup run: 2612it [00:04, 1158.80it/s]warmup run: 2563it [00:04, 1074.05it/s]warmup run: 2603it [00:03, 1148.00it/s]warmup run: 2605it [00:04, 1134.23it/s]warmup run: 2889it [00:04, 1187.63it/s]warmup run: 2842it [00:04, 1169.32it/s]warmup run: 2873it [00:04, 1187.29it/s]warmup run: 2894it [00:04, 1198.19it/s]warmup run: 2732it [00:04, 1170.23it/s]warmup run: 2672it [00:04, 1076.19it/s]warmup run: 2723it [00:04, 1160.67it/s]warmup run: 2724it [00:04, 1148.77it/s]warmup run: 3000it [00:04, 691.23it/s] warmup run: 2961it [00:04, 1174.32it/s]warmup run: 2994it [00:04, 1192.68it/s]warmup run: 3000it [00:04, 690.99it/s] warmup run: 2850it [00:04, 1171.30it/s]warmup run: 3000it [00:04, 684.97it/s] warmup run: 2782it [00:04, 1081.23it/s]warmup run: 2840it [00:04, 1161.39it/s]warmup run: 3000it [00:04, 684.24it/s] warmup run: 2841it [00:04, 1153.60it/s]warmup run: 2969it [00:04, 1176.19it/s]warmup run: 2893it [00:04, 1088.31it/s]warmup run: 2961it [00:04, 1175.06it/s]warmup run: 2961it [00:04, 1165.67it/s]warmup run: 3000it [00:04, 682.16it/s] warmup run: 3000it [00:04, 692.08it/s] warmup run: 3000it [00:04, 656.31it/s] warmup run: 3000it [00:04, 661.17it/s] 



warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]


warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   6%|▌         | 165/3000 [00:00<00:01, 1649.12it/s]warmup should be done:   5%|▌         | 160/3000 [00:00<00:01, 1599.03it/s]warmup should be done:   5%|▌         | 163/3000 [00:00<00:01, 1628.56it/s]warmup should be done:   5%|▌         | 164/3000 [00:00<00:01, 1634.54it/s]warmup should be done:   5%|▌         | 162/3000 [00:00<00:01, 1617.51it/s]warmup should be done:   5%|▌         | 155/3000 [00:00<00:01, 1540.43it/s]warmup should be done:   5%|▌         | 161/3000 [00:00<00:01, 1603.98it/s]warmup should be done:   5%|▌         | 163/3000 [00:00<00:01, 1620.89it/s]warmup should be done:  11%|█         | 330/3000 [00:00<00:01, 1647.16it/s]warmup should be done:  11%|█         | 321/3000 [00:00<00:01, 1603.18it/s]warmup should be done:  11%|█         | 329/3000 [00:00<00:01, 1642.04it/s]warmup should be done:  11%|█         | 323/3000 [00:00<00:01, 1613.30it/s]warmup should be done:  11%|█         | 328/3000 [00:00<00:01, 1635.87it/s]warmup should be done:  11%|█         | 326/3000 [00:00<00:01, 1625.48it/s]warmup should be done:  11%|█         | 328/3000 [00:00<00:01, 1633.78it/s]warmup should be done:  10%|█         | 310/3000 [00:00<00:01, 1527.71it/s]warmup should be done:  16%|█▋        | 495/3000 [00:00<00:01, 1643.03it/s]warmup should be done:  16%|█▋        | 492/3000 [00:00<00:01, 1634.32it/s]warmup should be done:  16%|█▋        | 494/3000 [00:00<00:01, 1644.35it/s]warmup should be done:  16%|█▋        | 494/3000 [00:00<00:01, 1638.37it/s]warmup should be done:  16%|█▌        | 482/3000 [00:00<00:01, 1596.92it/s]warmup should be done:  16%|█▋        | 489/3000 [00:00<00:01, 1621.95it/s]warmup should be done:  16%|█▌        | 485/3000 [00:00<00:01, 1606.73it/s]warmup should be done:  15%|█▌        | 464/3000 [00:00<00:01, 1529.99it/s]warmup should be done:  22%|██▏       | 661/3000 [00:00<00:01, 1650.81it/s]warmup should be done:  22%|██▏       | 658/3000 [00:00<00:01, 1635.31it/s]warmup should be done:  21%|██▏       | 642/3000 [00:00<00:01, 1594.51it/s]warmup should be done:  22%|██▏       | 646/3000 [00:00<00:01, 1604.99it/s]warmup should be done:  22%|██▏       | 660/3000 [00:00<00:01, 1635.51it/s]warmup should be done:  22%|██▏       | 652/3000 [00:00<00:01, 1618.46it/s]warmup should be done:  22%|██▏       | 656/3000 [00:00<00:01, 1624.96it/s]warmup should be done:  21%|██        | 618/3000 [00:00<00:01, 1520.54it/s]warmup should be done:  28%|██▊       | 827/3000 [00:00<00:01, 1652.94it/s]warmup should be done:  27%|██▋       | 802/3000 [00:00<00:01, 1594.15it/s]warmup should be done:  27%|██▋       | 807/3000 [00:00<00:01, 1603.12it/s]warmup should be done:  27%|██▋       | 814/3000 [00:00<00:01, 1617.01it/s]warmup should be done:  27%|██▋       | 820/3000 [00:00<00:01, 1629.26it/s]warmup should be done:  27%|██▋       | 822/3000 [00:00<00:01, 1630.00it/s]warmup should be done:  27%|██▋       | 824/3000 [00:00<00:01, 1625.44it/s]warmup should be done:  26%|██▌       | 772/3000 [00:00<00:01, 1524.96it/s]warmup should be done:  32%|███▏      | 962/3000 [00:00<00:01, 1594.50it/s]warmup should be done:  33%|███▎      | 993/3000 [00:00<00:01, 1650.26it/s]warmup should be done:  33%|███▎      | 984/3000 [00:00<00:01, 1631.93it/s]warmup should be done:  33%|███▎      | 976/3000 [00:00<00:01, 1612.06it/s]warmup should be done:  32%|███▏      | 968/3000 [00:00<00:01, 1598.33it/s]warmup should be done:  33%|███▎      | 986/3000 [00:00<00:01, 1626.79it/s]warmup should be done:  31%|███       | 926/3000 [00:00<00:01, 1527.54it/s]warmup should be done:  33%|███▎      | 987/3000 [00:00<00:01, 1617.17it/s]warmup should be done:  37%|███▋      | 1122/3000 [00:00<00:01, 1591.60it/s]warmup should be done:  38%|███▊      | 1148/3000 [00:00<00:01, 1631.30it/s]warmup should be done:  39%|███▊      | 1159/3000 [00:00<00:01, 1645.06it/s]warmup should be done:  38%|███▊      | 1149/3000 [00:00<00:01, 1622.60it/s]warmup should be done:  38%|███▊      | 1128/3000 [00:00<00:01, 1592.33it/s]warmup should be done:  38%|███▊      | 1138/3000 [00:00<00:01, 1606.39it/s]warmup should be done:  38%|███▊      | 1149/3000 [00:00<00:01, 1609.99it/s]warmup should be done:  36%|███▌      | 1079/3000 [00:00<00:01, 1518.64it/s]warmup should be done:  43%|████▎     | 1282/3000 [00:00<00:01, 1592.34it/s]warmup should be done:  44%|████▎     | 1312/3000 [00:00<00:01, 1632.92it/s]warmup should be done:  44%|████▍     | 1324/3000 [00:00<00:01, 1644.35it/s]warmup should be done:  44%|████▎     | 1312/3000 [00:00<00:01, 1621.93it/s]warmup should be done:  43%|████▎     | 1299/3000 [00:00<00:01, 1605.97it/s]warmup should be done:  43%|████▎     | 1288/3000 [00:00<00:01, 1591.59it/s]warmup should be done:  41%|████      | 1233/3000 [00:00<00:01, 1522.96it/s]warmup should be done:  44%|████▎     | 1311/3000 [00:00<00:01, 1608.18it/s]warmup should be done:  49%|████▉     | 1476/3000 [00:00<00:00, 1633.31it/s]warmup should be done:  48%|████▊     | 1442/3000 [00:00<00:00, 1591.99it/s]warmup should be done:  50%|████▉     | 1489/3000 [00:00<00:00, 1644.27it/s]warmup should be done:  49%|████▊     | 1460/3000 [00:00<00:00, 1604.91it/s]warmup should be done:  49%|████▉     | 1475/3000 [00:00<00:00, 1620.57it/s]warmup should be done:  48%|████▊     | 1448/3000 [00:00<00:00, 1590.81it/s]warmup should be done:  49%|████▉     | 1472/3000 [00:00<00:00, 1605.32it/s]warmup should be done:  46%|████▌     | 1386/3000 [00:00<00:01, 1517.73it/s]warmup should be done:  55%|█████▍    | 1640/3000 [00:01<00:00, 1633.26it/s]warmup should be done:  53%|█████▎    | 1602/3000 [00:01<00:00, 1590.90it/s]warmup should be done:  55%|█████▌    | 1654/3000 [00:01<00:00, 1645.73it/s]warmup should be done:  54%|█████▍    | 1621/3000 [00:01<00:00, 1605.03it/s]warmup should be done:  54%|█████▎    | 1608/3000 [00:01<00:00, 1590.82it/s]warmup should be done:  55%|█████▍    | 1638/3000 [00:01<00:00, 1619.43it/s]warmup should be done:  54%|█████▍    | 1633/3000 [00:01<00:00, 1603.25it/s]warmup should be done:  51%|█████▏    | 1539/3000 [00:01<00:00, 1519.91it/s]warmup should be done:  60%|██████    | 1804/3000 [00:01<00:00, 1632.64it/s]warmup should be done:  61%|██████    | 1819/3000 [00:01<00:00, 1645.64it/s]warmup should be done:  59%|█████▊    | 1762/3000 [00:01<00:00, 1590.04it/s]warmup should be done:  59%|█████▉    | 1782/3000 [00:01<00:00, 1605.74it/s]warmup should be done:  60%|██████    | 1800/3000 [00:01<00:00, 1618.29it/s]warmup should be done:  59%|█████▉    | 1768/3000 [00:01<00:00, 1591.30it/s]warmup should be done:  56%|█████▋    | 1692/3000 [00:01<00:00, 1522.13it/s]warmup should be done:  60%|█████▉    | 1794/3000 [00:01<00:00, 1602.10it/s]warmup should be done:  66%|██████▌   | 1984/3000 [00:01<00:00, 1645.69it/s]warmup should be done:  66%|██████▌   | 1968/3000 [00:01<00:00, 1631.74it/s]warmup should be done:  64%|██████▍   | 1922/3000 [00:01<00:00, 1590.15it/s]warmup should be done:  65%|██████▍   | 1943/3000 [00:01<00:00, 1605.38it/s]warmup should be done:  65%|██████▌   | 1962/3000 [00:01<00:00, 1617.45it/s]warmup should be done:  64%|██████▍   | 1928/3000 [00:01<00:00, 1591.56it/s]warmup should be done:  65%|██████▌   | 1955/3000 [00:01<00:00, 1600.42it/s]warmup should be done:  62%|██████▏   | 1845/3000 [00:01<00:00, 1515.26it/s]warmup should be done:  72%|███████▏  | 2149/3000 [00:01<00:00, 1645.59it/s]warmup should be done:  71%|███████   | 2132/3000 [00:01<00:00, 1631.52it/s]warmup should be done:  69%|██████▉   | 2082/3000 [00:01<00:00, 1589.64it/s]warmup should be done:  70%|███████   | 2104/3000 [00:01<00:00, 1604.88it/s]warmup should be done:  71%|███████   | 2124/3000 [00:01<00:00, 1617.26it/s]warmup should be done:  70%|██████▉   | 2088/3000 [00:01<00:00, 1591.70it/s]warmup should be done:  71%|███████   | 2116/3000 [00:01<00:00, 1600.41it/s]warmup should be done:  67%|██████▋   | 1998/3000 [00:01<00:00, 1518.78it/s]warmup should be done:  77%|███████▋  | 2314/3000 [00:01<00:00, 1644.72it/s]warmup should be done:  77%|███████▋  | 2296/3000 [00:01<00:00, 1632.50it/s]warmup should be done:  75%|███████▍  | 2244/3000 [00:01<00:00, 1597.19it/s]warmup should be done:  76%|███████▌  | 2265/3000 [00:01<00:00, 1603.66it/s]warmup should be done:  76%|███████▌  | 2286/3000 [00:01<00:00, 1617.25it/s]warmup should be done:  75%|███████▍  | 2248/3000 [00:01<00:00, 1590.16it/s]warmup should be done:  76%|███████▌  | 2277/3000 [00:01<00:00, 1599.59it/s]warmup should be done:  72%|███████▏  | 2150/3000 [00:01<00:00, 1499.08it/s]warmup should be done:  83%|████████▎ | 2479/3000 [00:01<00:00, 1641.20it/s]warmup should be done:  82%|████████▏ | 2460/3000 [00:01<00:00, 1629.25it/s]warmup should be done:  80%|████████  | 2407/3000 [00:01<00:00, 1605.44it/s]warmup should be done:  81%|████████  | 2426/3000 [00:01<00:00, 1603.01it/s]warmup should be done:  82%|████████▏ | 2449/3000 [00:01<00:00, 1618.57it/s]warmup should be done:  80%|████████  | 2408/3000 [00:01<00:00, 1587.46it/s]warmup should be done:  81%|████████  | 2437/3000 [00:01<00:00, 1596.68it/s]warmup should be done:  77%|███████▋  | 2301/3000 [00:01<00:00, 1500.40it/s]warmup should be done:  88%|████████▊ | 2644/3000 [00:01<00:00, 1642.84it/s]warmup should be done:  87%|████████▋ | 2623/3000 [00:01<00:00, 1629.05it/s]warmup should be done:  86%|████████▌ | 2570/3000 [00:01<00:00, 1610.18it/s]warmup should be done:  86%|████████▋ | 2588/3000 [00:01<00:00, 1607.52it/s]warmup should be done:  87%|████████▋ | 2612/3000 [00:01<00:00, 1621.11it/s]warmup should be done:  86%|████████▌ | 2568/3000 [00:01<00:00, 1589.07it/s]warmup should be done:  87%|████████▋ | 2597/3000 [00:01<00:00, 1596.83it/s]warmup should be done:  82%|████████▏ | 2452/3000 [00:01<00:00, 1498.64it/s]warmup should be done:  94%|█████████▎| 2809/3000 [00:01<00:00, 1643.59it/s]warmup should be done:  93%|█████████▎| 2788/3000 [00:01<00:00, 1633.64it/s]warmup should be done:  92%|█████████▏| 2752/3000 [00:01<00:00, 1616.12it/s]warmup should be done:  91%|█████████ | 2733/3000 [00:01<00:00, 1613.83it/s]warmup should be done:  93%|█████████▎| 2776/3000 [00:01<00:00, 1624.54it/s]warmup should be done:  91%|█████████ | 2728/3000 [00:01<00:00, 1590.51it/s]warmup should be done:  92%|█████████▏| 2757/3000 [00:01<00:00, 1597.57it/s]warmup should be done:  87%|████████▋ | 2602/3000 [00:01<00:00, 1495.65it/s]warmup should be done:  99%|█████████▉| 2975/3000 [00:01<00:00, 1647.63it/s]warmup should be done:  98%|█████████▊| 2955/3000 [00:01<00:00, 1642.74it/s]warmup should be done:  97%|█████████▋| 2897/3000 [00:01<00:00, 1620.35it/s]warmup should be done:  97%|█████████▋| 2918/3000 [00:01<00:00, 1626.49it/s]warmup should be done:  98%|█████████▊| 2942/3000 [00:01<00:00, 1633.26it/s]warmup should be done:  96%|█████████▋| 2889/3000 [00:01<00:00, 1596.02it/s]warmup should be done:  97%|█████████▋| 2920/3000 [00:01<00:00, 1604.45it/s]warmup should be done:  92%|█████████▏| 2756/3000 [00:01<00:00, 1506.29it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1644.97it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1633.91it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1624.95it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1613.56it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1608.89it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1602.30it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1594.65it/s]warmup should be done:  97%|█████████▋| 2907/3000 [00:01<00:00, 1507.14it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1513.97it/s]




warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]

warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   5%|▌         | 164/3000 [00:00<00:01, 1639.43it/s]warmup should be done:   5%|▌         | 160/3000 [00:00<00:01, 1597.70it/s]warmup should be done:   6%|▌         | 168/3000 [00:00<00:01, 1677.31it/s]warmup should be done:   6%|▌         | 166/3000 [00:00<00:01, 1653.40it/s]warmup should be done:   6%|▌         | 170/3000 [00:00<00:01, 1690.42it/s]warmup should be done:   6%|▌         | 169/3000 [00:00<00:01, 1680.34it/s]warmup should be done:   6%|▌         | 168/3000 [00:00<00:01, 1670.81it/s]warmup should be done:   6%|▌         | 168/3000 [00:00<00:01, 1670.01it/s]warmup should be done:  11%|█▏        | 338/3000 [00:00<00:01, 1690.26it/s]warmup should be done:  11%|█         | 323/3000 [00:00<00:01, 1612.97it/s]warmup should be done:  11%|█         | 329/3000 [00:00<00:01, 1641.15it/s]warmup should be done:  11%|█▏        | 338/3000 [00:00<00:01, 1686.17it/s]warmup should be done:  11%|█▏        | 340/3000 [00:00<00:01, 1691.43it/s]warmup should be done:  11%|█         | 337/3000 [00:00<00:01, 1676.49it/s]warmup should be done:  11%|█▏        | 339/3000 [00:00<00:01, 1686.05it/s]warmup should be done:  11%|█         | 332/3000 [00:00<00:01, 1643.68it/s]warmup should be done:  17%|█▋        | 509/3000 [00:00<00:01, 1696.45it/s]warmup should be done:  16%|█▌        | 486/3000 [00:00<00:01, 1617.84it/s]warmup should be done:  16%|█▋        | 495/3000 [00:00<00:01, 1645.55it/s]warmup should be done:  17%|█▋        | 509/3000 [00:00<00:01, 1694.50it/s]warmup should be done:  17%|█▋        | 506/3000 [00:00<00:01, 1679.90it/s]warmup should be done:  17%|█▋        | 509/3000 [00:00<00:01, 1689.57it/s]warmup should be done:  17%|█▋        | 510/3000 [00:00<00:01, 1690.35it/s]warmup should be done:  17%|█▋        | 498/3000 [00:00<00:01, 1648.75it/s]warmup should be done:  23%|██▎       | 679/3000 [00:00<00:01, 1697.62it/s]warmup should be done:  23%|██▎       | 679/3000 [00:00<00:01, 1695.48it/s]warmup should be done:  22%|██▏       | 649/3000 [00:00<00:01, 1619.62it/s]warmup should be done:  22%|██▏       | 661/3000 [00:00<00:01, 1648.63it/s]warmup should be done:  23%|██▎       | 680/3000 [00:00<00:01, 1694.00it/s]warmup should be done:  23%|██▎       | 677/3000 [00:00<00:01, 1689.00it/s]warmup should be done:  23%|██▎       | 679/3000 [00:00<00:01, 1690.83it/s]warmup should be done:  22%|██▏       | 667/3000 [00:00<00:01, 1661.76it/s]warmup should be done:  28%|██▊       | 849/3000 [00:00<00:01, 1696.46it/s]warmup should be done:  28%|██▊       | 849/3000 [00:00<00:01, 1691.90it/s]warmup should be done:  27%|██▋       | 815/3000 [00:00<00:01, 1631.64it/s]warmup should be done:  28%|██▊       | 828/3000 [00:00<00:01, 1654.27it/s]warmup should be done:  28%|██▊       | 848/3000 [00:00<00:01, 1696.08it/s]warmup should be done:  28%|██▊       | 851/3000 [00:00<00:01, 1696.79it/s]warmup should be done:  28%|██▊       | 836/3000 [00:00<00:01, 1671.42it/s]warmup should be done:  28%|██▊       | 849/3000 [00:00<00:01, 1686.87it/s]warmup should be done:  33%|███▎      | 994/3000 [00:00<00:01, 1655.31it/s]warmup should be done:  33%|███▎      | 982/3000 [00:00<00:01, 1642.91it/s]warmup should be done:  34%|███▍      | 1021/3000 [00:00<00:01, 1696.83it/s]warmup should be done:  34%|███▍      | 1019/3000 [00:00<00:01, 1692.03it/s]warmup should be done:  34%|███▍      | 1019/3000 [00:00<00:01, 1697.85it/s]warmup should be done:  34%|███▍      | 1019/3000 [00:00<00:01, 1685.96it/s]warmup should be done:  34%|███▎      | 1005/3000 [00:00<00:01, 1674.99it/s]warmup should be done:  34%|███▍      | 1018/3000 [00:00<00:01, 1680.05it/s]warmup should be done:  38%|███▊      | 1149/3000 [00:00<00:01, 1650.06it/s]warmup should be done:  39%|███▊      | 1161/3000 [00:00<00:01, 1657.62it/s]warmup should be done:  40%|███▉      | 1189/3000 [00:00<00:01, 1697.52it/s]warmup should be done:  40%|███▉      | 1191/3000 [00:00<00:01, 1692.02it/s]warmup should be done:  40%|███▉      | 1188/3000 [00:00<00:01, 1685.51it/s]warmup should be done:  39%|███▉      | 1173/3000 [00:00<00:01, 1673.92it/s]warmup should be done:  40%|███▉      | 1189/3000 [00:00<00:01, 1687.12it/s]warmup should be done:  40%|███▉      | 1187/3000 [00:00<00:01, 1679.67it/s]warmup should be done:  44%|████▍     | 1315/3000 [00:00<00:01, 1652.04it/s]warmup should be done:  45%|████▌     | 1361/3000 [00:00<00:00, 1702.44it/s]warmup should be done:  44%|████▍     | 1327/3000 [00:00<00:01, 1653.20it/s]warmup should be done:  45%|████▌     | 1362/3000 [00:00<00:00, 1695.43it/s]warmup should be done:  45%|████▍     | 1342/3000 [00:00<00:00, 1677.84it/s]warmup should be done:  45%|████▌     | 1358/3000 [00:00<00:00, 1687.71it/s]warmup should be done:  45%|████▌     | 1358/3000 [00:00<00:00, 1686.16it/s]warmup should be done:  45%|████▌     | 1355/3000 [00:00<00:00, 1675.15it/s]warmup should be done:  49%|████▉     | 1482/3000 [00:00<00:00, 1655.74it/s]warmup should be done:  51%|█████     | 1532/3000 [00:00<00:00, 1703.62it/s]warmup should be done:  50%|████▉     | 1493/3000 [00:00<00:00, 1653.80it/s]warmup should be done:  51%|█████     | 1532/3000 [00:00<00:00, 1695.39it/s]warmup should be done:  51%|█████     | 1527/3000 [00:00<00:00, 1686.89it/s]warmup should be done:  50%|█████     | 1510/3000 [00:00<00:00, 1674.90it/s]warmup should be done:  51%|█████     | 1527/3000 [00:00<00:00, 1683.35it/s]warmup should be done:  51%|█████     | 1523/3000 [00:00<00:00, 1668.73it/s]warmup should be done:  55%|█████▍    | 1649/3000 [00:01<00:00, 1658.24it/s]warmup should be done:  57%|█████▋    | 1703/3000 [00:01<00:00, 1705.11it/s]warmup should be done:  55%|█████▌    | 1659/3000 [00:01<00:00, 1655.61it/s]warmup should be done:  57%|█████▋    | 1702/3000 [00:01<00:00, 1695.44it/s]warmup should be done:  57%|█████▋    | 1696/3000 [00:01<00:00, 1687.14it/s]warmup should be done:  56%|█████▌    | 1678/3000 [00:01<00:00, 1674.87it/s]warmup should be done:  57%|█████▋    | 1696/3000 [00:01<00:00, 1681.37it/s]warmup should be done:  56%|█████▋    | 1690/3000 [00:01<00:00, 1665.71it/s]warmup should be done:  62%|██████▏   | 1874/3000 [00:01<00:00, 1706.44it/s]warmup should be done:  61%|██████    | 1816/3000 [00:01<00:00, 1659.49it/s]warmup should be done:  61%|██████    | 1825/3000 [00:01<00:00, 1654.10it/s]warmup should be done:  62%|██████▏   | 1867/3000 [00:01<00:00, 1691.37it/s]warmup should be done:  62%|██████▏   | 1873/3000 [00:01<00:00, 1696.84it/s]warmup should be done:  62%|██████▏   | 1865/3000 [00:01<00:00, 1681.89it/s]warmup should be done:  62%|██████▏   | 1846/3000 [00:01<00:00, 1668.40it/s]warmup should be done:  62%|██████▏   | 1857/3000 [00:01<00:00, 1664.70it/s]warmup should be done:  68%|██████▊   | 2045/3000 [00:01<00:00, 1707.38it/s]warmup should be done:  66%|██████▌   | 1982/3000 [00:01<00:00, 1657.19it/s]warmup should be done:  66%|██████▋   | 1991/3000 [00:01<00:00, 1653.03it/s]warmup should be done:  68%|██████▊   | 2043/3000 [00:01<00:00, 1697.48it/s]warmup should be done:  68%|██████▊   | 2038/3000 [00:01<00:00, 1694.97it/s]warmup should be done:  68%|██████▊   | 2034/3000 [00:01<00:00, 1681.84it/s]warmup should be done:  67%|██████▋   | 2013/3000 [00:01<00:00, 1659.79it/s]warmup should be done:  67%|██████▋   | 2024/3000 [00:01<00:00, 1661.41it/s]warmup should be done:  74%|███████▍  | 2216/3000 [00:01<00:00, 1706.68it/s]warmup should be done:  72%|███████▏  | 2148/3000 [00:01<00:00, 1657.07it/s]warmup should be done:  72%|███████▏  | 2158/3000 [00:01<00:00, 1656.56it/s]warmup should be done:  74%|███████▎  | 2208/3000 [00:01<00:00, 1696.11it/s]warmup should be done:  74%|███████▍  | 2213/3000 [00:01<00:00, 1695.88it/s]warmup should be done:  73%|███████▎  | 2203/3000 [00:01<00:00, 1680.10it/s]warmup should be done:  73%|███████▎  | 2179/3000 [00:01<00:00, 1652.64it/s]warmup should be done:  73%|███████▎  | 2191/3000 [00:01<00:00, 1657.84it/s]warmup should be done:  80%|███████▉  | 2387/3000 [00:01<00:00, 1704.18it/s]warmup should be done:  77%|███████▋  | 2315/3000 [00:01<00:00, 1659.81it/s]warmup should be done:  78%|███████▊  | 2325/3000 [00:01<00:00, 1660.05it/s]warmup should be done:  79%|███████▉  | 2378/3000 [00:01<00:00, 1697.00it/s]warmup should be done:  79%|███████▉  | 2383/3000 [00:01<00:00, 1696.08it/s]warmup should be done:  79%|███████▉  | 2372/3000 [00:01<00:00, 1680.05it/s]warmup should be done:  78%|███████▊  | 2345/3000 [00:01<00:00, 1649.75it/s]warmup should be done:  79%|███████▊  | 2357/3000 [00:01<00:00, 1657.94it/s]warmup should be done:  85%|████████▌ | 2558/3000 [00:01<00:00, 1702.59it/s]warmup should be done:  83%|████████▎ | 2482/3000 [00:01<00:00, 1660.58it/s]warmup should be done:  83%|████████▎ | 2492/3000 [00:01<00:00, 1661.67it/s]warmup should be done:  85%|████████▍ | 2549/3000 [00:01<00:00, 1698.65it/s]warmup should be done:  85%|████████▌ | 2554/3000 [00:01<00:00, 1698.35it/s]warmup should be done:  85%|████████▍ | 2541/3000 [00:01<00:00, 1680.43it/s]warmup should be done:  84%|████████▎ | 2510/3000 [00:01<00:00, 1649.19it/s]warmup should be done:  84%|████████▍ | 2524/3000 [00:01<00:00, 1658.93it/s]warmup should be done:  88%|████████▊ | 2649/3000 [00:01<00:00, 1660.78it/s]warmup should be done:  91%|█████████ | 2720/3000 [00:01<00:00, 1699.39it/s]warmup should be done:  91%|█████████ | 2725/3000 [00:01<00:00, 1699.58it/s]warmup should be done:  91%|█████████ | 2729/3000 [00:01<00:00, 1694.66it/s]warmup should be done:  89%|████████▊ | 2659/3000 [00:01<00:00, 1656.58it/s]warmup should be done:  90%|█████████ | 2710/3000 [00:01<00:00, 1679.44it/s]warmup should be done:  89%|████████▉ | 2675/3000 [00:01<00:00, 1646.74it/s]warmup should be done:  90%|████████▉ | 2690/3000 [00:01<00:00, 1658.60it/s]warmup should be done:  94%|█████████▍| 2816/3000 [00:01<00:00, 1659.12it/s]warmup should be done:  96%|█████████▋| 2890/3000 [00:01<00:00, 1697.53it/s]warmup should be done:  94%|█████████▍| 2825/3000 [00:01<00:00, 1656.24it/s]warmup should be done:  96%|█████████▋| 2895/3000 [00:01<00:00, 1693.64it/s]warmup should be done:  97%|█████████▋| 2899/3000 [00:01<00:00, 1687.06it/s]warmup should be done:  96%|█████████▌| 2878/3000 [00:01<00:00, 1675.84it/s]warmup should be done:  95%|█████████▌| 2856/3000 [00:01<00:00, 1655.95it/s]warmup should be done:  95%|█████████▍| 2840/3000 [00:01<00:00, 1641.76it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1695.32it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1694.13it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1693.22it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1682.08it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1663.40it/s]warmup should be done:  99%|█████████▉| 2982/3000 [00:01<00:00, 1655.21it/s]warmup should be done: 100%|█████████▉| 2992/3000 [00:01<00:00, 1658.40it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1657.60it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1654.82it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1649.61it/s]
WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f17b9f2f280>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f17ba2659d0>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f17b9f221c0>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f17ba263e80>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f17b9f240d0>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f17b9f21190>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f17ba266d30>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f17b9f211f0>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
2022-12-12 00:28:31.221565: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f12e7028ba0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 00:28:31.221623: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 00:28:31.224203: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f12ea9548d0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 00:28:31.224262: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 00:28:31.230993: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 00:28:31.233390: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 00:28:31.349489: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f12e6833f40 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 00:28:31.349546: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 00:28:31.357096: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 00:28:31.843782: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f12e68307e0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 00:28:31.843845: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 00:28:31.853671: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 00:28:31.924245: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f12e7028ab0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 00:28:31.924310: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 00:28:31.933795: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 00:28:32.031358: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f12ef02da30 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 00:28:32.031429: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 00:28:32.040247: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 00:28:32.071066: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f12ef030a10 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 00:28:32.071149: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 00:28:32.071565: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f12e6f95860 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 00:28:32.071609: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 00:28:32.079234: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 00:28:32.080857: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 00:28:38.346635: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 00:28:38.361558: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 00:28:38.515299: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 00:28:38.748083: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 00:28:38.797560: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 00:28:38.922767: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 00:28:38.981748: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 00:28:39.074354: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
[HCTR][00:29:37.086][ERROR][RK0][main]: replica 3 reaches 1000, calling init pre replica
[HCTR][00:29:37.086][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][00:29:37.091][ERROR][RK0][main]: coll ps creation done
[HCTR][00:29:37.091][ERROR][RK0][main]: replica 3 waits for coll ps creation barrier
[HCTR][00:29:37.206][ERROR][RK0][main]: replica 2 reaches 1000, calling init pre replica
[HCTR][00:29:37.206][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][00:29:37.212][ERROR][RK0][main]: coll ps creation done
[HCTR][00:29:37.212][ERROR][RK0][main]: replica 2 waits for coll ps creation barrier
[HCTR][00:29:37.356][ERROR][RK0][main]: replica 4 reaches 1000, calling init pre replica
[HCTR][00:29:37.356][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][00:29:37.363][ERROR][RK0][main]: coll ps creation done
[HCTR][00:29:37.363][ERROR][RK0][main]: replica 4 waits for coll ps creation barrier
[HCTR][00:29:37.378][ERROR][RK0][tid #139719379777280]: replica 7 reaches 1000, calling init pre replica
[HCTR][00:29:37.378][ERROR][RK0][tid #139719379777280]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][00:29:37.386][ERROR][RK0][tid #139719379777280]: coll ps creation done
[HCTR][00:29:37.386][ERROR][RK0][tid #139719379777280]: replica 7 waits for coll ps creation barrier
[HCTR][00:29:37.446][ERROR][RK0][main]: replica 5 reaches 1000, calling init pre replica
[HCTR][00:29:37.446][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][00:29:37.451][ERROR][RK0][main]: coll ps creation done
[HCTR][00:29:37.451][ERROR][RK0][main]: replica 5 waits for coll ps creation barrier
[HCTR][00:29:37.453][ERROR][RK0][main]: replica 1 reaches 1000, calling init pre replica
[HCTR][00:29:37.453][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][00:29:37.461][ERROR][RK0][main]: coll ps creation done
[HCTR][00:29:37.461][ERROR][RK0][main]: replica 1 waits for coll ps creation barrier
[HCTR][00:29:37.486][ERROR][RK0][main]: replica 6 reaches 1000, calling init pre replica
[HCTR][00:29:37.486][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][00:29:37.491][ERROR][RK0][main]: coll ps creation done
[HCTR][00:29:37.491][ERROR][RK0][main]: replica 6 waits for coll ps creation barrier
[HCTR][00:29:37.501][ERROR][RK0][main]: replica 0 reaches 1000, calling init pre replica
[HCTR][00:29:37.502][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][00:29:37.506][ERROR][RK0][main]: coll ps creation done
[HCTR][00:29:37.506][ERROR][RK0][main]: replica 0 waits for coll ps creation barrier
[HCTR][00:29:37.506][ERROR][RK0][main]: replica 0 preparing frequency
[HCTR][00:29:38.399][ERROR][RK0][main]: replica 0 preparing frequency done
[HCTR][00:29:38.433][ERROR][RK0][main]: replica 0 calling init per replica
[HCTR][00:29:38.433][ERROR][RK0][main]: replica 1 calling init per replica
[HCTR][00:29:38.433][ERROR][RK0][main]: replica 4 calling init per replica
[HCTR][00:29:38.433][ERROR][RK0][main]: replica 3 calling init per replica
[HCTR][00:29:38.433][ERROR][RK0][main]: replica 5 calling init per replica
[HCTR][00:29:38.433][ERROR][RK0][main]: replica 6 calling init per replica
[HCTR][00:29:38.433][ERROR][RK0][tid #139719379777280]: replica 7 calling init per replica
[HCTR][00:29:38.433][ERROR][RK0][main]: replica 2 calling init per replica
[HCTR][00:29:38.433][ERROR][RK0][main]: Calling build_v2
[HCTR][00:29:38.433][ERROR][RK0][main]: Calling build_v2
[HCTR][00:29:38.433][ERROR][RK0][main]: Calling build_v2
[HCTR][00:29:38.433][ERROR][RK0][main]: Calling build_v2
[HCTR][00:29:38.433][ERROR][RK0][main]: Calling build_v2
[HCTR][00:29:38.433][ERROR][RK0][main]: Calling build_v2
[HCTR][00:29:38.433][ERROR][RK0][tid #139719379777280]: Calling build_v2
[HCTR][00:29:38.433][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][00:29:38.433][ERROR][RK0][main]: Calling build_v2
[HCTR][00:29:38.433][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][00:29:38.433][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][00:29:38.433][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][00:29:38.433][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][00:29:38.433][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][00:29:38.433][ERROR][RK0][tid #139719379777280]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][00:29:38.433][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[[[[[[2022-12-12 00:29:38[2022-12-12 00:29:382022-12-12 00:29:38.2022-12-12 00:29:382022-12-12 00:29:382022-12-12 00:29:38..4334902022-12-12 00:29:38...433490433490: .433501433501433501[: : E433501: : : EE : EEE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.ccE2022-12-12 00:29:38   /hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc: ./hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc::136/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc433579:::136136] :: 136136136] ] using concurrent impl MPS136E] ] ] using concurrent impl MPSusing concurrent impl MPS
]  using concurrent impl MPSusing concurrent impl MPSusing concurrent impl MPS

using concurrent impl MPS/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc



:136] using concurrent impl MPS
[2022-12-12 00:29:38.437771: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:178] v100x8, slow pcie
[2022-12-12 00:29:38.437807: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:[1962022-12-12 00:29:38] .assigning 8 to cpu437817
: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:178] v100x8, slow pcie
[2022-12-12 00:29:38.437861[: 2022-12-12 00:29:38E. 437861/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: :E[196 2022-12-12 00:29:38] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.assigning 8 to cpu:437889
178: ] Ev100x8, slow pcie 
[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 00:29:38[:.2022-12-12 00:29:38212437914[.] : [2022-12-12 00:29:38437932build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8E2022-12-12 00:29:38.: 
 .437951E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc437955[: [ :: 2022-12-12 00:29:38E2022-12-12 00:29:38/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc178E. .[:]  438016/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc4380022022-12-12 00:29:38196v100x8, slow pcie/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: :[: .] 
:E2122022-12-12 00:29:38E438050assigning 8 to cpu178 [] . : 
] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 00:29:38build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8438097/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccEv100x8, slow pcie:.
: :[ 
213438149E1782022-12-12 00:29:38/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[] : [ ] .:2022-12-12 00:29:38remote time is 8.68421E2022-12-12 00:29:38/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccv100x8, slow pcie438244178.
 .:
: ] 438253/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc438263178[Ev100x8, slow pcie: [:: ] 2022-12-12 00:29:38 
E2022-12-12 00:29:38196Ev100x8, slow pcie./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc .] [ 
438334:/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc438344assigning 8 to cpu2022-12-12 00:29:38/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: 212[:: 
.:E] 2022-12-12 00:29:38213E438386196 build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8.]  : ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
438424remote time is 8.68421/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE[assigning 8 to cpu:: 
[: 2022-12-12 00:29:38
214E2022-12-12 00:29:38196[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.]  .] 2022-12-12 00:29:38:438519cpu time is 97.0588/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc438541assigning 8 to cpu.196: [
:: 
438570] E2022-12-12 00:29:38196E: assigning 8 to cpu .]  E
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc438636assigning 8 to cpu/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc :: [
:/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc212E2022-12-12 00:29:38213:]  .] 214[build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[438729remote time is 8.68421] 2022-12-12 00:29:38
:2022-12-12 00:29:38: 
cpu time is 97.0588.212.E[
438772[] 438775 2022-12-12 00:29:38: 2022-12-12 00:29:38build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.E.
E:438826 438832 212: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] E:2022-12-12 00:29:38E:build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8 212. 212
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] 438917/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] :build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8: :[build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8213
E2142022-12-12 00:29:38[
]  ] .2022-12-12 00:29:38remote time is 8.68421/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cccpu time is 97.0588[438986.
:
2022-12-12 00:29:38: 439026213[.E: ] 2022-12-12 00:29:38439045 Eremote time is 8.68421.: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc 
439075E:/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: [ 213:E2022-12-12 00:29:38/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] 213 .:remote time is 8.68421] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc439150213
remote time is 8.68421:: ] 
214[Eremote time is 8.68421] 2022-12-12 00:29:38[ 
cpu time is 97.0588.2022-12-12 00:29:38/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
439231.:[: 4392472142022-12-12 00:29:38E: ] .cpu time is 97.0588439275 E
: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc E:/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc 214:/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] 214:cpu time is 97.0588] 214
cpu time is 97.0588] 
cpu time is 97.0588
[2022-12-12 00:30:58.109062: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:118] solver created. now build & solve
[2022-12-12 00:30:58.149161: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:123] solver built. now solve
block 0 storage is 00010001
	access is	0	0	0	0	4	4	4	4	
block 1 storage is 00100010
	access is	1	1	1	1	5	5	5	5	
block 2 storage is 01000100
	access is	2	2	2	2	6	6	6	6	
block 3 storage is 10001000
	access is	3	3	3	3	7	7	7	7	
block 4 storage is 00000000
	access is	8	8	8	8	8	8	8	8	
[2022-12-12 00:30:58.285711: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:127] solver solved
[2022-12-12 00:30:58.285772: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:195] 0 solved master
[2022-12-12 00:30:58.285805: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 0 solved
[2022-12-12 00:30:58.285837: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 0 initing device 0
[2022-12-12 00:30:58.286324: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-12 00:30:58.286369: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 00:30:58.287269: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 00:30:58.288075: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 00:30:58.301259: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 1 solved
[2022-12-12 00:30:58.301332: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 1 initing device 1
[[2022-12-12 00:30:582022-12-12 00:30:58..301583301581: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc::[202[2022022-12-12 00:30:58] 2022-12-12 00:30:58] .4 solved.3 solved301640
301640
: : [E[E2022-12-12 00:30:58 [[2022-12-12 00:30:582022-12-12 00:30:58 ./hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc2022-12-12 00:30:58..[/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc301724:.3017303017462022-12-12 00:30:58:: 202301729: : .202E] : EE301774]  E2 solved  : 6 solved/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.ccE
:/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc::[ 205:205[2022022-12-12 00:30:58/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu] 202] 2022-12-12 00:30:58] .:worker 0 thread 4 initing device 4] worker 0 thread 3 initing device 3.3018975 solved1815
7 solved
301911: 
] 
: EBuilding Coll Cache with ... num gpu device is 8[E[ 
2022-12-12 00:30:58 2022-12-12 00:30:58/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc./hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc.:302003:[302009205: 2052022-12-12 00:30:58: ] E] .Eworker 0 thread 2 initing device 2 worker 0 thread 6 initing device 6302050 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc
: /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:E:205 205] /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu] worker 0 thread 5 initing device 5:worker 0 thread 7 initing device 7
1980
] eager alloc mem 381.47 MB
[2022-12-12 00:30:58.302388[: 2022-12-12 00:30:58E. 302397/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu: :E1815 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuBuilding Coll Cache with ... num gpu device is 8:
1815] Building Coll Cache with ... num gpu device is 8
[2022-12-12 00:30:58.302455[: 2022-12-12 00:30:58E. 302462/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu: :E1980 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cueager alloc mem 381.47 MB:
1980] eager alloc mem 381.47 MB
[2022-12-12 00:30:58.302517: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-12 00:30:58.302545: E[ 2022-12-12 00:30:58/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu.[:[3025612022-12-12 00:30:5818152022-12-12 00:30:58: .] .E302575Building Coll Cache with ... num gpu device is 8302576 : 
: /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuEE:  1980/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu] [::eager alloc mem 381.47 MB2022-12-12 00:30:5818151815
.] ] 302659Building Coll Cache with ... num gpu device is 8Building Coll Cache with ... num gpu device is 8: 

E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB[
[2022-12-12 00:30:582022-12-12 00:30:58..302738302741: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::19801980] ] eager alloc mem 381.47 MBeager alloc mem 381.47 MB

[2022-12-12 00:30:58.306745: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 00:30:58.306999: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 00:30:58.307050: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 00:30:58.307101: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 00:30:58.307177: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 00:30:58.307683: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 00:30:58.307737: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 00:30:58.311210: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 00:30:58.311435: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 00:30:58.311483: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 00:30:58.311536: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 00:30:58.311592: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 00:30:58.312133: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 00:30:58.312184: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 00:30:58.366935: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.00 Bytes
[2022-12-12 00:30:58.371925: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5
[2022-12-12 00:30:58.372014: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 00:30:58.372769: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 15.64 MB
[2022-12-12 00:30:58.373325: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:30:58.374322: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:30:58.374366: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 15.26 MB
[2022-12-12 00:30:58.378430: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 00:30:58.379153: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 00:30:58.379201: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1.91 GB
[2022-12-12 00:30:58.392589: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.00 Bytes
[2022-12-12 00:30:58.397096: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5
[2022-12-12 00:30:58.397175: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 00:30:58.397920: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 15.64 MB
[2022-12-12 00:30:58.398388: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:30:58.399364: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:30:58.399408: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 15.26 MB
[2022-12-12 00:30:58.405312: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.00 Bytes
[[[[2022-12-12 00:30:582022-12-12 00:30:582022-12-12 00:30:582022-12-12 00:30:58....405374405372405377405372: : : : EEEE    /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu[/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:::2022-12-12 00:30:58:198019801980.1980] ] ] 405457] eager alloc mem 5.00 Byteseager alloc mem 5.00 Byteseager alloc mem 5.00 Bytes: eager alloc mem 5.00 Bytes


E
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.00 Bytes
[2022-12-12 00:30:58.410425: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 00:30:58.411028: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 00:30:58.411068: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1.91 GB
[2022-12-12 00:30:58.411585: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5
[2022-12-12 00:30:58.411682: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 00:30:58.411855: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5
[[2022-12-12 00:30:582022-12-12 00:30:58..411925411944: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::638638] ] eager release cuda mem 5eager release cuda mem 400000000

[[2022-12-12 00:30:582022-12-12 00:30:58..412029412016: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::638638] ] eager release cuda mem 400000000eager release cuda mem 5

[[2022-12-12 00:30:582022-12-12 00:30:58..412111412126: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::638638] ] [eager release cuda mem 5eager release cuda mem 4000000002022-12-12 00:30:58

.412170: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5
[2022-12-12 00:30:58.412243: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] [eager release cuda mem 4000000002022-12-12 00:30:58
.412263: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 00:30:58.415211: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 15.64 MB
[2022-12-12 00:30:58.415735: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 15.64 MB
[2022-12-12 00:30:58.416245: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 15.64 MB
[2022-12-12 00:30:58.416749: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 15.64 MB
[2022-12-12 00:30:58.417336: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 15.64 MB
[2022-12-12 00:30:58.417837: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 15.64 MB
[2022-12-12 00:30:58.419331: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:30:58.419364: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:30:58.419419: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:30:58.419521: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:30:58.419568: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:30:58.419620: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:30:58.420305: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:30:58.420337[: 2022-12-12 00:30:58E. 420348/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc: :W638 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cceager release cuda mem 625663:
43] WORKER[0] alloc host memory 15.26 MB
[2022-12-12 00:30:58[.2022-12-12 00:30:58420393.: 420400E:  W/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc :/hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc638:] 43eager release cuda mem 625663] 
WORKER[0] alloc host memory 15.26 MB
[2022-12-12 00:30:58.420459: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 15.26 MB
[2022-12-12 00:30:58.420488: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:30:58.420533: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc[:2022-12-12 00:30:5843.] 420542WORKER[0] alloc host memory 15.26 MB: 
E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:30:58.420587[: 2022-12-12 00:30:58E. 420598/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc: :W638 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cceager release cuda mem 625663:
43] WORKER[0] alloc host memory 15.26 MB
[2022-12-12 00:30:58.420652: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 15.26 MB
[2022-12-12 00:30:58.424200: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 00:30:58.424402: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 00:30:58.424933: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 00:30:58.424975: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1.91 GB
[2022-12-12 00:30:58.425116: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 00:30:58.425159: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1.91 GB
[2022-12-12 00:30:58.430292: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 00:30:58.430897: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 00:30:58.430938: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1.91 GB
[2022-12-12 00:30:58.431241: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 00:30:58.431494: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 00:30:58.431563: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 00:30:58.431847: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 00:30:58.431888: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1.91 GB
[2022-12-12 00:30:58.432095: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 00:30:58.432136: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1.91 GB
[2022-12-12 00:30:58.432162: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 00:30:58.432213: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1.91 GB
[[[[[[[[2022-12-12 00:30:592022-12-12 00:30:592022-12-12 00:30:592022-12-12 00:30:592022-12-12 00:30:592022-12-12 00:30:592022-12-12 00:30:592022-12-12 00:30:59........118993118994118992118991118992118991118992: 118992: : : : : : E: EEEEEE E      /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::::::1926:192619261926192619261926] 1926Device 6 init p2p of link 0] ] ] ] ] ] ] 
Device 0 init p2p of link 3Device 2 init p2p of link 1Device 3 init p2p of link 2Device 5 init p2p of link 6Device 7 init p2p of link 4Device 4 init p2p of link 5Device 1 init p2p of link 7






[2022-12-12 00:30:59.119520: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[[[[[2022-12-12 00:30:59[2022-12-12 00:30:59[2022-12-12 00:30:592022-12-12 00:30:592022-12-12 00:30:59.2022-12-12 00:30:59.2022-12-12 00:30:59...119544.119544.119544119544119544: 119550: 119548: : : E: E: EEE E E   /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:::1980:1980:198019801980] 1980] 1980] ] ] eager alloc mem 611.00 KB] eager alloc mem 611.00 KB] eager alloc mem 611.00 KBeager alloc mem 611.00 KBeager alloc mem 611.00 KB
eager alloc mem 611.00 KB
eager alloc mem 611.00 KB




[2022-12-12 00:30:59.120327: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:30:59.120546: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] [[eager release cuda mem 6256632022-12-12 00:30:592022-12-12 00:30:59
.[[[.1205682022-12-12 00:30:592022-12-12 00:30:592022-12-12 00:30:59120567: ..[.: E1205781205782022-12-12 00:30:59120578E : .: :  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccE120601EE/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc: :   :638/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccE/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc638] : ::] eager release cuda mem 625663638/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc638638eager release cuda mem 625663
] :] ] 
eager release cuda mem 625663638eager release cuda mem 625663eager release cuda mem 625663
] 

eager release cuda mem 625663
[2022-12-12 00:30:59.133559: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 6 init p2p of link 5
[2022-12-12 00:30:59.133714: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:30:59.133792: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 2
[2022-12-12 00:30:59.133944: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:30:59.134524: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:30:59.134781: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:30:59.135682: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 2 init p2p of link 3
[2022-12-12 00:30:59.135841: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:30:59.135919: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 3 init p2p of link 0
[2022-12-12 00:30:59.136066: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:30:59.136096: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 5 init p2p of link 4
[2022-12-12 00:30:59.136252: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:30:59.136321: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 7 init p2p of link 1
[2022-12-12 00:30:59.136472: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:30:59.136517: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 0 init p2p of link 6
[2022-12-12 00:30:59.136642: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:30:59.136669: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:30:59.136720: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 4 init p2p of link 7
[2022-12-12 00:30:59.136859: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:30:59.136882: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:30:59.137072: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:30:59.137241: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:30:59.137440: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:30:59.137663: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:30:59.147055: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 6 init p2p of link 4
[2022-12-12 00:30:59.147215: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:30:59.147613: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 3
[2022-12-12 00:30:59.147733: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:30:59.148017: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:30:59.148527: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:30:59.148593: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 7 init p2p of link 6
[2022-12-12 00:30:59.148708: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:30:59.148852: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 0 init p2p of link 1
[2022-12-12 00:30:59.148974: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:30:59.149531: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:30:59.149634: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 2 init p2p of link 0
[2022-12-12 00:30:59.149746: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:30:59.149809: E [/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc2022-12-12 00:30:59:.638149816] : eager release cuda mem 625663E
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 3 init p2p of link 5
[2022-12-12 00:30:59.149956: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:30:59.150066: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 5 init p2p of link 7
[2022-12-12 00:30:59.150187: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:30:59.150543: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:30:59.150722: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:30:59.150740: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 4 init p2p of link 2
[2022-12-12 00:30:59.150852: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:30:59.150990: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:30:59.151618: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:30:59.164834: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 3 init p2p of link 1
[2022-12-12 00:30:59.164951: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:30:59.165450: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 4 init p2p of link 6
[2022-12-12 00:30:59.165568: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:30:59.165773: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:30:59.166195: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 6 init p2p of link 7
[2022-12-12 00:30:59.166318: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:30:59.166386: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:30:59.166626: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 0
[2022-12-12 00:30:59.166743: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:30:59[.2022-12-12 00:30:59167146.: 167149E:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc :/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu638:] 1926eager release cuda mem 625663] 
Device 7 init p2p of link 5
[2022-12-12 00:30:59.167316: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:30:59.167487: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 0 init p2p of link 2
[2022-12-12 00:30:59.167553: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:30:59.167608: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:30:59.167833: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 2 init p2p of link 4
[2022-12-12 00:30:59.167964: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:30:59.168123: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:30:59.168393: E [/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu2022-12-12 00:30:59:.1926168417] : Device 5 init p2p of link 3E
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:30:59.168539: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:30:59.168735: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:30:59.169311: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:30:59.180497: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 16399996
[2022-12-12 00:30:59.181363: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 16399996
[2022-12-12 00:30:59.181584: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 16399996
[2022-12-12 00:30:59.181968: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 16399996
[2022-12-12 00:30:59.182712: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 3999999 / 100000000 nodes ( 4.00 %~4.00 %) | remote 11999997 / 100000000 nodes ( 12.00 %) | cpu 84000004 / 100000000 nodes ( 84.00 %) | 1.91 GB | 0.880256 secs 
[2022-12-12 00:30:59.182884: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 3999999 / 100000000 nodes ( 4.00 %~4.00 %) | remote 11999997 / 100000000 nodes ( 12.00 %) | cpu 84000004 / 100000000 nodes ( 84.00 %) | 1.91 GB | 0.880437 secs 
[2022-12-12 00:30:59.183055: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 3999999 / 100000000 nodes ( 4.00 %~4.00 %) | remote 11999997 / 100000000 nodes ( 12.00 %) | cpu 84000004 / 100000000 nodes ( 84.00 %) | 1.91 GB | 0.880501 secs 
[2022-12-12 00:30:59.183302: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 16399996
[2022-12-12 00:30:59.183482: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 3999999 / 100000000 nodes ( 4.00 %~4.00 %) | remote 11999997 / 100000000 nodes ( 12.00 %) | cpu 84000004 / 100000000 nodes ( 84.00 %) | 1.91 GB | 0.880748 secs 
[2022-12-12 00:30:59.183790: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 3999999 / 100000000 nodes ( 4.00 %~4.00 %) | remote 11999997 / 100000000 nodes ( 12.00 %) | cpu 84000004 / 100000000 nodes ( 84.00 %) | 1.91 GB | 0.881138 secs 
[2022-12-12 00:30:59.183881: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 16399996
[2022-12-12 00:30:59.184216: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 16399996
[2022-12-12 00:30:59.184575: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 16399996
[2022-12-12 00:30:59.184772: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 3999999 / 100000000 nodes ( 4.00 %~4.00 %) | remote 11999997 / 100000000 nodes ( 12.00 %) | cpu 84000004 / 100000000 nodes ( 84.00 %) | 1.91 GB | 0.882735 secs 
[2022-12-12 00:30:59.188274: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 3999999 / 100000000 nodes ( 4.00 %~4.00 %) | remote 11999997 / 100000000 nodes ( 12.00 %) | cpu 84000004 / 100000000 nodes ( 84.00 %) | 1.91 GB | 0.885543 secs 
[2022-12-12 00:30:59.188829: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 3999999 / 100000000 nodes ( 4.00 %~4.00 %) | remote 11999997 / 100000000 nodes ( 12.00 %) | cpu 84000004 / 100000000 nodes ( 84.00 %) | 1.91 GB | 0.902467 secs 
[2022-12-12 00:30:59.191734: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 10.23 GB
[2022-12-12 00:31:00.550417: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 10.50 GB
[2022-12-12 00:31:00.555240: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 10.50 GB
[2022-12-12 00:31:00.557130: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 10.50 GB
[2022-12-12 00:31:02.  8740: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 10.76 GB
[2022-12-12 00:31:02.  9101: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 10.76 GB
[2022-12-12 00:31:02. 10793: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 10.76 GB
[2022-12-12 00:31:03.168284: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 10.97 GB
[2022-12-12 00:31:03.168418: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 10.97 GB
[2022-12-12 00:31:03.168749: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 10.97 GB
[2022-12-12 00:31:04.447845: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 11.19 GB
[2022-12-12 00:31:04.448172: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 11.19 GB
[2022-12-12 00:31:04.448511: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 11.19 GB
[2022-12-12 00:31:05.775954: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 11.65 GB
[2022-12-12 00:31:05.776318: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 11.65 GB
[2022-12-12 00:31:05.777331: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 11.65 GB
[2022-12-12 00:31:06.973231: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 11.84 GB
[2022-12-12 00:31:06.973622: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 11.84 GB
[HCTR][00:31:07.714][ERROR][RK0][main]: replica 5 calling init per replica done, doing barrier
[HCTR][00:31:07.714][ERROR][RK0][main]: replica 1 calling init per replica done, doing barrier
[HCTR][00:31:07.714][ERROR][RK0][tid #139719379777280]: replica 7 calling init per replica done, doing barrier
[HCTR][00:31:07.714][ERROR][RK0][main]: replica 3 calling init per replica done, doing barrier
[HCTR][00:31:07.714][ERROR][RK0][main]: replica 0 calling init per replica done, doing barrier
[HCTR][00:31:07.714][ERROR][RK0][main]: replica 6 calling init per replica done, doing barrier
[HCTR][00:31:07.714][ERROR][RK0][main]: replica 2 calling init per replica done, doing barrier
[HCTR][00:31:07.714][ERROR][RK0][main]: replica 4 calling init per replica done, doing barrier
[HCTR][00:31:07.715][ERROR][RK0][main]: replica 5 calling init per replica done, doing barrier done
[HCTR][00:31:07.715][ERROR][RK0][main]: replica 4 calling init per replica done, doing barrier done
[HCTR][00:31:07.715][ERROR][RK0][main]: replica 0 calling init per replica done, doing barrier done
[HCTR][00:31:07.715][ERROR][RK0][main]: replica 3 calling init per replica done, doing barrier done
[HCTR][00:31:07.715][ERROR][RK0][tid #139719379777280]: replica 7 calling init per replica done, doing barrier done
[HCTR][00:31:07.715][ERROR][RK0][main]: replica 1 calling init per replica done, doing barrier done
[HCTR][00:31:07.715][ERROR][RK0][main]: replica 6 calling init per replica done, doing barrier done
[HCTR][00:31:07.715][ERROR][RK0][main]: replica 2 calling init per replica done, doing barrier done
[HCTR][00:31:07.715][ERROR][RK0][main]: init per replica done
[HCTR][00:31:07.715][ERROR][RK0][main]: init per replica done
[HCTR][00:31:07.715][ERROR][RK0][main]: init per replica done
[HCTR][00:31:07.715][ERROR][RK0][tid #139719379777280]: init per replica done
[HCTR][00:31:07.715][ERROR][RK0][main]: init per replica done
[HCTR][00:31:07.715][ERROR][RK0][main]: init per replica done
[HCTR][00:31:07.715][ERROR][RK0][main]: init per replica done
[HCTR][00:31:07.718][ERROR][RK0][main]: init per replica done
[HCTR][00:31:07.753][ERROR][RK0][tid #139719438493440]: 1 allocated 3276800 at 0x7ef744238400
[HCTR][00:31:07.753][ERROR][RK0][tid #139719438493440]: 1 allocated 6553600 at 0x7ef744558400
[HCTR][00:31:07.753][ERROR][RK0][main]: 3 allocated 3276800 at 0x7ef720238400
[HCTR][00:31:07.753][ERROR][RK0][tid #139719438493440]: 1 allocated 3276800 at 0x7ef744b98400
[HCTR][00:31:07.753][ERROR][RK0][main]: 3 allocated 6553600 at 0x7ef720558400
[HCTR][00:31:07.753][ERROR][RK0][tid #139719438493440]: 1 allocated 6553600 at 0x7ef744eb8400
[HCTR][00:31:07.753][ERROR][RK0][main]: 3 allocated 3276800 at 0x7ef720b98400
[HCTR][00:31:07.753][ERROR][RK0][main]: 3 allocated 6553600 at 0x7ef720eb8400
[HCTR][00:31:07.753][ERROR][RK0][tid #139719371384576]: 2 allocated 3276800 at 0x7ef6cc238400
[HCTR][00:31:07.754][ERROR][RK0][tid #139719371384576]: 2 allocated 6553600 at 0x7ef6cc558400
[HCTR][00:31:07.754][ERROR][RK0][tid #139719371384576]: 2 allocated 3276800 at 0x7ef6ccb98400
[HCTR][00:31:07.754][ERROR][RK0][tid #139719371384576]: 2 allocated 6553600 at 0x7ef6cceb8400
[HCTR][00:31:07.754][ERROR][RK0][main]: 7 allocated 3276800 at 0x7ef6ec238400
[HCTR][00:31:07.754][ERROR][RK0][main]: 7 allocated 6553600 at 0x7ef6ec558400
[HCTR][00:31:07.754][ERROR][RK0][main]: 4 allocated 3276800 at 0x7ef6ac238400
[HCTR][00:31:07.754][ERROR][RK0][main]: 7 allocated 3276800 at 0x7ef6ecb98400
[HCTR][00:31:07.754][ERROR][RK0][main]: 4 allocated 6553600 at 0x7ef6ac558400
[HCTR][00:31:07.754][ERROR][RK0][main]: 7 allocated 6553600 at 0x7ef6eceb8400
[HCTR][00:31:07.754][ERROR][RK0][main]: 4 allocated 3276800 at 0x7ef6acb98400
[HCTR][00:31:07.754][ERROR][RK0][main]: 4 allocated 6553600 at 0x7ef6aceb8400
[HCTR][00:31:07.754][ERROR][RK0][main]: 6 allocated 3276800 at 0x7ef694238400
[HCTR][00:31:07.754][ERROR][RK0][main]: 6 allocated 6553600 at 0x7ef694558400
[HCTR][00:31:07.754][ERROR][RK0][main]: 6 allocated 3276800 at 0x7ef694b98400
[HCTR][00:31:07.754][ERROR][RK0][main]: 6 allocated 6553600 at 0x7ef694eb8400
[HCTR][00:31:07.754][ERROR][RK0][main]: 5 allocated 3276800 at 0x7ef614238400
[HCTR][00:31:07.754][ERROR][RK0][main]: 5 allocated 6553600 at 0x7ef614558400
[HCTR][00:31:07.754][ERROR][RK0][main]: 5 allocated 3276800 at 0x7ef614b98400
[HCTR][00:31:07.754][ERROR][RK0][main]: 5 allocated 6553600 at 0x7ef614eb8400
[HCTR][00:31:07.756][ERROR][RK0][tid #139719866291968]: 0 allocated 3276800 at 0x7ef714320000
[HCTR][00:31:07.756][ERROR][RK0][tid #139719866291968]: 0 allocated 6553600 at 0x7ef714640000
[HCTR][00:31:07.756][ERROR][RK0][tid #139719866291968]: 0 allocated 3276800 at 0x7ef714c80000
[HCTR][00:31:07.756][ERROR][RK0][tid #139719866291968]: 0 allocated 6553600 at 0x7ef714fa0000
