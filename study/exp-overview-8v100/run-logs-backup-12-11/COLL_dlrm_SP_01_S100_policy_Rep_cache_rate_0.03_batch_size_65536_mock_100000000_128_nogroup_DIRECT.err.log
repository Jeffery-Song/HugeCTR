2022-12-11 22:47:49.245313: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.257063: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.264256: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.275971: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.283511: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.287803: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.298680: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.304582: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.328419: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.337329: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.341265: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.342416: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.343496: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.344610: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.345709: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.346805: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.358459: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.359989: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.361419: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.361420: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.362836: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.363167: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.364314: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.364972: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.365774: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.366393: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.367107: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.367784: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.368370: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.369304: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.369620: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.371006: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.371149: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.372864: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.372989: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.375238: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.376397: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.376489: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.378373: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.378696: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.380709: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.381100: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.381125: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.383432: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.384332: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.384351: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.384595: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.387003: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.387448: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.387545: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.388236: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.388426: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.390286: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.391027: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.391072: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.391470: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.391760: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.392058: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.392654: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.393760: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.395081: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.395153: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.395390: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.395854: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.396112: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.397045: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.397642: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.398987: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.399043: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.399217: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.399919: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.400233: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.401289: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.401549: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.402828: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.403102: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.403951: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.404132: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.404896: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.404946: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.405948: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.406222: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.407282: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.407462: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.408033: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.408262: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.408853: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.410463: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.410503: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.410909: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.411104: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.411515: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.413286: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.413724: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.414291: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.415747: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.415801: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.416930: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.417448: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.417952: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.418001: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 22:47:49.418449: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.418960: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.419190: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.419680: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.420212: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.420553: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.421154: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.421791: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.422232: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.422285: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.423447: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.423459: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.424661: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.424759: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.425566: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.425745: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.426436: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.426689: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.427338: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.427671: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.428203: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.428382: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.428701: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.429629: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.429954: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.430266: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.431352: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.431749: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.432628: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.433417: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.434490: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.436284: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.436356: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 22:47:49.437437: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.438567: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.439709: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.441038: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.441767: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.446194: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.448072: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.449234: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.450125: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.450670: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.451200: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.452009: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.452716: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.452768: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.457312: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.458101: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 22:47:49.460355: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.460510: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.467768: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.490343: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.490666: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.492797: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.492832: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.492904: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.493026: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.495143: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.496185: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.498127: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.498214: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.498270: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.498409: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.500734: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.501354: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.504010: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.504100: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.504196: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.505888: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.506570: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.508433: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.508659: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.508756: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.510427: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.511777: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.513745: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.513975: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.514060: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.515977: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.517206: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.519371: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.519458: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.519560: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.521251: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.522214: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.524365: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.524407: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.524558: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.525952: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.527080: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.528874: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.528972: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.529197: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.530799: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.531757: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.533330: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.533380: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.533550: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.535246: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.536334: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.540636: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.540727: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.540819: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.543190: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.544113: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.545888: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.546173: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.546177: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.547818: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.548784: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.550591: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.550747: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.550808: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.552160: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.553069: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.583792: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.583921: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.586218: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.588695: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.590628: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.592104: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.592184: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.592437: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.593593: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.596807: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.597857: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.598134: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.599399: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.600935: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.630787: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.632904: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.632936: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.633012: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.633903: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.636776: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.639214: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.639256: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.639485: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.640023: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.643419: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.646430: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.646478: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.646746: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.647286: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.649878: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.652110: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.652168: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.652460: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.652516: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.656213: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.657485: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.657650: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.657890: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.657900: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.661997: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.663007: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.663409: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.663672: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.663821: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.668646: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.669002: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.669179: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.669276: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.671285: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 22:47:49.672883: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.673521: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.674093: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.677255: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 22:47:49.680916: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 22:47:49.680946: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 22:47:49.681262: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.681559: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 22:47:49.684691: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.686819: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.687778: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.690507: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.690969: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.691035: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.691515: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.695440: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.696366: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.696636: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.697249: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.701619: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.701729: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:49.702724: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:50.752124: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:50.753928: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:50.755065: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:50.755552: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 22:47:50.755615: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-11 22:47:50.774537: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:50.775158: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:50.775877: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:50.776441: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:50.776963: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:50.777628: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-11 22:47:50.823399: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 22:47:50.823608: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 22:47:50.824899: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:50.824928: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:50.826575: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:50.826729: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:50.827677: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:50.827761: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:50.828519: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 22:47:50.828582: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-11 22:47:50.828637: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 22:47:50.828691: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-11 22:47:50.847682: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:50.847682: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:50.848998: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:50.849025: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:50.849978: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:50.850017: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:50.851024: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:50.851051: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:50.852182: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:50.852263: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:50.853319: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:1/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-11 22:47:50.853395: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:2/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-11 22:47:50.859800: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12340
2022-12-11 22:47:50.926111: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 22:47:50.926315: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 22:47:50.927272: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12341
2022-12-11 22:47:50.944049: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 22:47:50.944248: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 22:47:50.945064: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12342
2022-12-11 22:47:50.966661: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:50.967509: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:50.968041: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:50.968324: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:50.968743: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 22:47:50.968805: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-11 22:47:50.969542: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:50.970086: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:50.970552: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 22:47:50.970604: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-11 22:47:50.982556: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:50.983197: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:50.983737: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:50.984195: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 22:47:50.984256: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-11 22:47:50.984581: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:50.985221: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:50.985753: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:50.986209: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 22:47:50.986264: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-11 22:47:50.986496: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:50.987146: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:50.987259: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:50.988115: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:50.988232: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:50.988254: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:50.989571: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:50.989705: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:50.989838: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:50.990961: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:50.991058: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 22:47:50.991111: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-11 22:47:50.991359: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:50.992110: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:5/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-11 22:47:50.992414: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:50.992944: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:50.993407: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:4/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-11 22:47:51.002075: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:51.002782: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:51.003329: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:51.003913: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:51.004425: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:51.004464: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:51.005522: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:3/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-11 22:47:51.005736: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:51.006274: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:51.006856: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:51.007385: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:51.007857: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:7/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-11 22:47:51.008676: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:51.009319: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:51.009847: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:51.010487: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:51.011034: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:47:51.011519: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:6/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-11 22:47:51.037939: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 22:47:51.038139: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 22:47:51.039958: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12345
2022-12-11 22:47:51.039936: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 22:47:51.040131: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 22:47:51.041051: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12344
2022-12-11 22:47:51.051055: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 22:47:51.051221: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 22:47:51.052938: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12343
2022-12-11 22:47:51.054216: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 22:47:51.054382: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 22:47:51.055314: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12347
2022-12-11 22:47:51.058040: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 22:47:51.058199: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 22:47:51.059170: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12346
[HCTR][22:47:52.296][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][22:47:52.296][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][22:47:52.296][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][22:47:52.308][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][22:47:52.311][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][22:47:52.318][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][22:47:52.318][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][22:47:52.318][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 1it [00:01,  1.55s/it]warmup run: 97it [00:01, 81.57it/s]warmup run: 194it [00:01, 177.08it/s]warmup run: 1it [00:01,  1.53s/it]warmup run: 1it [00:01,  1.53s/it]warmup run: 1it [00:01,  1.53s/it]warmup run: 1it [00:01,  1.50s/it]warmup run: 1it [00:01,  1.55s/it]warmup run: 296it [00:01, 289.22it/s]warmup run: 1it [00:01,  1.48s/it]warmup run: 100it [00:01, 85.05it/s]warmup run: 97it [00:01, 82.53it/s]warmup run: 98it [00:01, 83.40it/s]warmup run: 102it [00:01, 88.35it/s]warmup run: 100it [00:01, 84.17it/s]warmup run: 399it [00:01, 407.35it/s]warmup run: 1it [00:01,  1.49s/it]warmup run: 99it [00:01, 86.70it/s]warmup run: 201it [00:01, 185.36it/s]warmup run: 194it [00:01, 178.80it/s]warmup run: 199it [00:01, 183.92it/s]warmup run: 202it [00:01, 188.99it/s]warmup run: 200it [00:01, 182.71it/s]warmup run: 501it [00:02, 521.12it/s]warmup run: 99it [00:01, 86.43it/s]warmup run: 200it [00:01, 189.61it/s]warmup run: 303it [00:01, 297.04it/s]warmup run: 292it [00:01, 286.28it/s]warmup run: 300it [00:01, 294.50it/s]warmup run: 304it [00:01, 301.87it/s]warmup run: 297it [00:01, 287.48it/s]warmup run: 605it [00:02, 629.21it/s]warmup run: 199it [00:01, 187.97it/s]warmup run: 302it [00:01, 303.29it/s]warmup run: 402it [00:01, 408.05it/s]warmup run: 390it [00:01, 397.75it/s]warmup run: 401it [00:01, 408.96it/s]warmup run: 407it [00:01, 419.90it/s]warmup run: 395it [00:01, 397.88it/s]warmup run: 707it [00:02, 719.09it/s]warmup run: 299it [00:01, 299.07it/s]warmup run: 404it [00:01, 420.41it/s]warmup run: 499it [00:02, 511.98it/s]warmup run: 488it [00:02, 505.74it/s]warmup run: 501it [00:02, 518.73it/s]warmup run: 508it [00:02, 530.25it/s]warmup run: 494it [00:02, 507.37it/s]warmup run: 811it [00:02, 796.93it/s]warmup run: 400it [00:01, 415.29it/s]warmup run: 507it [00:01, 535.15it/s]warmup run: 596it [00:02, 607.70it/s]warmup run: 588it [00:02, 608.88it/s]warmup run: 603it [00:02, 623.86it/s]warmup run: 612it [00:02, 637.47it/s]warmup run: 596it [00:02, 614.74it/s]warmup run: 914it [00:02, 857.34it/s]warmup run: 500it [00:01, 525.37it/s]warmup run: 609it [00:02, 637.56it/s]warmup run: 697it [00:02, 699.70it/s]warmup run: 689it [00:02, 700.18it/s]warmup run: 706it [00:02, 716.81it/s]warmup run: 700it [00:02, 712.33it/s]warmup run: 710it [00:02, 713.08it/s]warmup run: 1016it [00:02, 900.85it/s]warmup run: 601it [00:02, 627.18it/s]warmup run: 712it [00:02, 727.64it/s]warmup run: 798it [00:02, 775.86it/s]warmup run: 789it [00:02, 774.41it/s]warmup run: 809it [00:02, 794.32it/s]warmup run: 800it [00:02, 782.50it/s]warmup run: 815it [00:02, 796.01it/s]warmup run: 1119it [00:02, 934.51it/s]warmup run: 703it [00:02, 717.24it/s]warmup run: 812it [00:02, 794.32it/s]warmup run: 900it [00:02, 839.24it/s]warmup run: 888it [00:02, 828.54it/s]warmup run: 911it [00:02, 853.19it/s]warmup run: 919it [00:02, 859.76it/s]warmup run: 899it [00:02, 833.09it/s]warmup run: 1221it [00:02, 912.16it/s]warmup run: 805it [00:02, 790.88it/s]warmup run: 912it [00:02, 846.05it/s]warmup run: 1003it [00:02, 888.92it/s]warmup run: 987it [00:02, 870.04it/s]warmup run: 1014it [00:02, 900.52it/s]warmup run: 1022it [00:02, 905.02it/s]warmup run: 998it [00:02, 869.71it/s]warmup run: 1318it [00:02, 914.69it/s]warmup run: 907it [00:02, 850.26it/s]warmup run: 1012it [00:02, 881.87it/s]warmup run: 1107it [00:02, 929.15it/s]warmup run: 1086it [00:02, 902.46it/s]warmup run: 1118it [00:02, 938.79it/s]warmup run: 1124it [00:02, 932.79it/s]warmup run: 1098it [00:02, 903.66it/s]warmup run: 1417it [00:02, 935.64it/s]warmup run: 1010it [00:02, 897.80it/s]warmup run: 1111it [00:02, 903.32it/s]warmup run: 1210it [00:02, 956.75it/s]warmup run: 1184it [00:02, 923.40it/s]warmup run: 1222it [00:02, 965.76it/s]warmup run: 1228it [00:02, 961.02it/s]warmup run: 1197it [00:02, 923.33it/s]warmup run: 1516it [00:03, 951.15it/s]warmup run: 1112it [00:02, 931.16it/s]warmup run: 1209it [00:02, 924.59it/s]warmup run: 1314it [00:02, 978.68it/s]warmup run: 1283it [00:02, 941.68it/s]warmup run: 1326it [00:02, 985.33it/s]warmup run: 1330it [00:02, 977.72it/s]warmup run: 1295it [00:02, 938.85it/s]warmup run: 1616it [00:03, 963.10it/s]warmup run: 1213it [00:02, 949.78it/s]warmup run: 1308it [00:02, 942.33it/s]warmup run: 1417it [00:02, 991.22it/s]warmup run: 1382it [00:02, 955.41it/s]warmup run: 1430it [00:02, 998.74it/s]warmup run: 1393it [00:02, 949.60it/s]warmup run: 1432it [00:02, 974.90it/s]warmup run: 1717it [00:03, 975.38it/s]warmup run: 1315it [00:02, 967.56it/s]warmup run: 1407it [00:02, 950.96it/s]warmup run: 1520it [00:03, 1001.36it/s]warmup run: 1483it [00:03, 969.85it/s]warmup run: 1535it [00:03, 1011.07it/s]warmup run: 1491it [00:03, 957.29it/s]warmup run: 1533it [00:03, 971.06it/s]warmup run: 1819it [00:03, 986.99it/s]warmup run: 1417it [00:02, 981.06it/s]warmup run: 1505it [00:02, 957.82it/s]warmup run: 1623it [00:03, 1008.55it/s]warmup run: 1584it [00:03, 980.65it/s]warmup run: 1639it [00:03, 1019.36it/s]warmup run: 1589it [00:03, 962.55it/s]warmup run: 1633it [00:03, 965.89it/s]warmup run: 1921it [00:03, 996.15it/s]warmup run: 1519it [00:02, 989.87it/s]warmup run: 1603it [00:03, 964.00it/s]warmup run: 1726it [00:03, 1006.65it/s]warmup run: 1686it [00:03, 991.21it/s]warmup run: 1743it [00:03, 1024.34it/s]warmup run: 1688it [00:03, 968.41it/s]warmup run: 1731it [00:03, 964.56it/s]warmup run: 2029it [00:03, 1020.29it/s]warmup run: 1621it [00:03, 997.93it/s]warmup run: 1701it [00:03, 965.08it/s]warmup run: 1828it [00:03, 1005.20it/s]warmup run: 1787it [00:03, 994.37it/s]warmup run: 1847it [00:03, 1023.88it/s]warmup run: 1786it [00:03, 969.78it/s]warmup run: 1834it [00:03, 983.49it/s]warmup run: 2151it [00:03, 1077.42it/s]warmup run: 1724it [00:03, 1006.15it/s]warmup run: 1799it [00:03, 968.17it/s]warmup run: 1930it [00:03, 999.53it/s] warmup run: 1888it [00:03, 986.09it/s]warmup run: 1951it [00:03, 1020.11it/s]warmup run: 1884it [00:03, 972.19it/s]warmup run: 1938it [00:03, 998.40it/s]warmup run: 2272it [00:03, 1116.65it/s]warmup run: 1826it [00:03, 1002.24it/s]warmup run: 1898it [00:03, 972.03it/s]warmup run: 2033it [00:03, 1008.33it/s]warmup run: 1989it [00:03, 990.76it/s]warmup run: 2063it [00:03, 1048.82it/s]warmup run: 1982it [00:03, 967.64it/s]warmup run: 2045it [00:03, 1017.09it/s]warmup run: 2395it [00:03, 1148.19it/s]warmup run: 1927it [00:03, 1000.55it/s]warmup run: 1996it [00:03, 973.70it/s]warmup run: 2154it [00:03, 1066.14it/s]warmup run: 2104it [00:03, 1036.79it/s]warmup run: 2184it [00:03, 1096.02it/s]warmup run: 2097it [00:03, 1019.73it/s]warmup run: 2517it [00:03, 1169.48it/s]warmup run: 2148it [00:03, 974.19it/s] warmup run: 2033it [00:03, 1017.35it/s]warmup run: 2115it [00:03, 1037.39it/s]warmup run: 2275it [00:03, 1106.48it/s]warmup run: 2222it [00:03, 1078.23it/s]warmup run: 2305it [00:03, 1127.92it/s]warmup run: 2218it [00:03, 1074.23it/s]warmup run: 2638it [00:04, 1180.08it/s]warmup run: 2250it [00:03, 985.86it/s]warmup run: 2155it [00:03, 1075.02it/s]warmup run: 2236it [00:03, 1088.63it/s]warmup run: 2396it [00:03, 1135.62it/s]warmup run: 2340it [00:03, 1107.02it/s]warmup run: 2426it [00:03, 1150.62it/s]warmup run: 2339it [00:03, 1113.27it/s]warmup run: 2760it [00:04, 1191.27it/s]warmup run: 2370it [00:03, 1046.78it/s]warmup run: 2277it [00:03, 1115.75it/s]warmup run: 2358it [00:03, 1125.19it/s]warmup run: 2517it [00:03, 1156.41it/s]warmup run: 2458it [00:03, 1126.66it/s]warmup run: 2547it [00:03, 1167.14it/s]warmup run: 2459it [00:03, 1137.56it/s]warmup run: 2882it [00:04, 1199.81it/s]warmup run: 2490it [00:03, 1091.64it/s]warmup run: 2398it [00:03, 1143.14it/s]warmup run: 2479it [00:03, 1150.09it/s]warmup run: 2637it [00:04, 1167.08it/s]warmup run: 2576it [00:04, 1141.19it/s]warmup run: 2668it [00:04, 1178.23it/s]warmup run: 2579it [00:04, 1155.06it/s]warmup run: 2610it [00:04, 1122.73it/s]warmup run: 3000it [00:04, 684.13it/s] warmup run: 2519it [00:03, 1161.32it/s]warmup run: 2600it [00:04, 1166.30it/s]warmup run: 2757it [00:04, 1174.34it/s]warmup run: 2693it [00:04, 1148.54it/s]warmup run: 2787it [00:04, 1179.87it/s]warmup run: 2698it [00:04, 1162.85it/s]warmup run: 2731it [00:04, 1145.94it/s]warmup run: 2639it [00:04, 1171.67it/s]warmup run: 2720it [00:04, 1175.20it/s]warmup run: 2875it [00:04, 1168.79it/s]warmup run: 2812it [00:04, 1158.32it/s]warmup run: 2908it [00:04, 1186.26it/s]warmup run: 2818it [00:04, 1171.73it/s]warmup run: 2853it [00:04, 1165.70it/s]warmup run: 2760it [00:04, 1181.29it/s]warmup run: 2841it [00:04, 1184.14it/s]warmup run: 3000it [00:04, 692.80it/s] warmup run: 2997it [00:04, 1182.75it/s]warmup run: 3000it [00:04, 687.41it/s] warmup run: 2930it [00:04, 1162.13it/s]warmup run: 2937it [00:04, 1176.41it/s]warmup run: 2975it [00:04, 1179.83it/s]warmup run: 2879it [00:04, 1182.68it/s]warmup run: 3000it [00:04, 679.58it/s] warmup run: 3000it [00:04, 685.61it/s] warmup run: 3000it [00:04, 677.31it/s] warmup run: 2962it [00:04, 1190.63it/s]warmup run: 3000it [00:04, 690.87it/s] warmup run: 2998it [00:04, 1160.55it/s]warmup run: 3000it [00:04, 694.02it/s] 
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]





warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   5%|▌         | 164/3000 [00:00<00:01, 1632.19it/s]warmup should be done:   5%|▌         | 164/3000 [00:00<00:01, 1636.09it/s]warmup should be done:   6%|▌         | 165/3000 [00:00<00:01, 1644.04it/s]warmup should be done:   5%|▌         | 158/3000 [00:00<00:01, 1573.69it/s]warmup should be done:   5%|▌         | 161/3000 [00:00<00:01, 1604.76it/s]warmup should be done:   5%|▌         | 162/3000 [00:00<00:01, 1611.54it/s]warmup should be done:   5%|▌         | 163/3000 [00:00<00:01, 1621.92it/s]warmup should be done:   6%|▌         | 166/3000 [00:00<00:01, 1651.02it/s]warmup should be done:  11%|█         | 329/3000 [00:00<00:01, 1643.03it/s]warmup should be done:  11%|█         | 327/3000 [00:00<00:01, 1633.91it/s]warmup should be done:  11%|█         | 327/3000 [00:00<00:01, 1632.31it/s]warmup should be done:  11%|█         | 333/3000 [00:00<00:01, 1660.44it/s]warmup should be done:  11%|█         | 323/3000 [00:00<00:01, 1613.57it/s]warmup should be done:  11%|█         | 332/3000 [00:00<00:01, 1652.80it/s]warmup should be done:  11%|█         | 324/3000 [00:00<00:01, 1613.45it/s]warmup should be done:  11%|█         | 328/3000 [00:00<00:01, 1625.49it/s]warmup should be done:  16%|█▌        | 487/3000 [00:00<00:01, 1624.18it/s]warmup should be done:  16%|█▋        | 494/3000 [00:00<00:01, 1641.00it/s]warmup should be done:  16%|█▋        | 493/3000 [00:00<00:01, 1641.51it/s]warmup should be done:  17%|█▋        | 500/3000 [00:00<00:01, 1660.72it/s]warmup should be done:  16%|█▋        | 491/3000 [00:00<00:01, 1630.02it/s]warmup should be done:  17%|█▋        | 498/3000 [00:00<00:01, 1651.45it/s]warmup should be done:  16%|█▌        | 486/3000 [00:00<00:01, 1612.45it/s]warmup should be done:  16%|█▋        | 491/3000 [00:00<00:01, 1617.99it/s]warmup should be done:  22%|██▏       | 651/3000 [00:00<00:01, 1629.26it/s]warmup should be done:  22%|██▏       | 659/3000 [00:00<00:01, 1645.29it/s]warmup should be done:  22%|██▏       | 667/3000 [00:00<00:01, 1661.36it/s]warmup should be done:  22%|██▏       | 659/3000 [00:00<00:01, 1639.42it/s]warmup should be done:  22%|██▏       | 664/3000 [00:00<00:01, 1652.01it/s]warmup should be done:  22%|██▏       | 655/3000 [00:00<00:01, 1628.83it/s]warmup should be done:  22%|██▏       | 648/3000 [00:00<00:01, 1610.87it/s]warmup should be done:  22%|██▏       | 653/3000 [00:00<00:01, 1612.99it/s]warmup should be done:  27%|██▋       | 814/3000 [00:00<00:01, 1629.48it/s]warmup should be done:  27%|██▋       | 823/3000 [00:00<00:01, 1638.06it/s]warmup should be done:  27%|██▋       | 824/3000 [00:00<00:01, 1643.01it/s]warmup should be done:  28%|██▊       | 834/3000 [00:00<00:01, 1659.83it/s]warmup should be done:  27%|██▋       | 818/3000 [00:00<00:01, 1626.04it/s]warmup should be done:  28%|██▊       | 830/3000 [00:00<00:01, 1649.57it/s]warmup should be done:  27%|██▋       | 810/3000 [00:00<00:01, 1601.22it/s]warmup should be done:  27%|██▋       | 815/3000 [00:00<00:01, 1604.69it/s]warmup should be done:  33%|███▎      | 977/3000 [00:00<00:01, 1626.53it/s]warmup should be done:  33%|███▎      | 987/3000 [00:00<00:01, 1632.59it/s]warmup should be done:  33%|███▎      | 1000/3000 [00:00<00:01, 1654.77it/s]warmup should be done:  33%|███▎      | 995/3000 [00:00<00:01, 1645.40it/s]warmup should be done:  33%|███▎      | 981/3000 [00:00<00:01, 1618.56it/s]warmup should be done:  33%|███▎      | 989/3000 [00:00<00:01, 1633.87it/s]warmup should be done:  32%|███▏      | 971/3000 [00:00<00:01, 1600.29it/s]warmup should be done:  33%|███▎      | 976/3000 [00:00<00:01, 1599.27it/s]warmup should be done:  38%|███▊      | 1141/3000 [00:00<00:01, 1628.06it/s]warmup should be done:  38%|███▊      | 1151/3000 [00:00<00:01, 1633.24it/s]warmup should be done:  39%|███▉      | 1166/3000 [00:00<00:01, 1655.48it/s]warmup should be done:  39%|███▊      | 1160/3000 [00:00<00:01, 1646.42it/s]warmup should be done:  38%|███▊      | 1143/3000 [00:00<00:01, 1617.90it/s]warmup should be done:  38%|███▊      | 1153/3000 [00:00<00:01, 1630.77it/s]warmup should be done:  38%|███▊      | 1135/3000 [00:00<00:01, 1612.02it/s]warmup should be done:  38%|███▊      | 1137/3000 [00:00<00:01, 1600.39it/s]warmup should be done:  44%|████▎     | 1305/3000 [00:00<00:01, 1629.74it/s]warmup should be done:  44%|████▍     | 1332/3000 [00:00<00:01, 1656.09it/s]warmup should be done:  44%|████▍     | 1325/3000 [00:00<00:01, 1647.03it/s]warmup should be done:  44%|████▍     | 1315/3000 [00:00<00:01, 1633.75it/s]warmup should be done:  44%|████▎     | 1305/3000 [00:00<00:01, 1617.22it/s]warmup should be done:  43%|████▎     | 1299/3000 [00:00<00:01, 1620.79it/s]warmup should be done:  44%|████▍     | 1317/3000 [00:00<00:01, 1626.47it/s]warmup should be done:  43%|████▎     | 1298/3000 [00:00<00:01, 1600.82it/s]warmup should be done:  49%|████▉     | 1469/3000 [00:00<00:00, 1630.61it/s]warmup should be done:  50%|████▉     | 1490/3000 [00:00<00:00, 1647.37it/s]warmup should be done:  50%|████▉     | 1498/3000 [00:00<00:00, 1655.67it/s]warmup should be done:  49%|████▉     | 1479/3000 [00:00<00:00, 1634.44it/s]warmup should be done:  49%|████▉     | 1467/3000 [00:00<00:00, 1616.65it/s]warmup should be done:  49%|████▊     | 1462/3000 [00:00<00:00, 1619.00it/s]warmup should be done:  49%|████▉     | 1480/3000 [00:00<00:00, 1624.53it/s]warmup should be done:  49%|████▊     | 1459/3000 [00:00<00:00, 1599.30it/s]warmup should be done:  54%|█████▍    | 1633/3000 [00:01<00:00, 1631.89it/s]warmup should be done:  55%|█████▌    | 1664/3000 [00:01<00:00, 1656.27it/s]warmup should be done:  55%|█████▍    | 1643/3000 [00:01<00:00, 1634.91it/s]warmup should be done:  55%|█████▌    | 1656/3000 [00:01<00:00, 1648.26it/s]warmup should be done:  54%|█████▍    | 1629/3000 [00:01<00:00, 1617.43it/s]warmup should be done:  54%|█████▍    | 1624/3000 [00:01<00:00, 1618.17it/s]warmup should be done:  55%|█████▍    | 1643/3000 [00:01<00:00, 1625.81it/s]warmup should be done:  54%|█████▍    | 1619/3000 [00:01<00:00, 1599.30it/s]warmup should be done:  60%|█████▉    | 1797/3000 [00:01<00:00, 1630.72it/s]warmup should be done:  61%|██████    | 1830/3000 [00:01<00:00, 1655.54it/s]warmup should be done:  61%|██████    | 1821/3000 [00:01<00:00, 1647.73it/s]warmup should be done:  60%|█████▉    | 1791/3000 [00:01<00:00, 1617.92it/s]warmup should be done:  60%|██████    | 1807/3000 [00:01<00:00, 1631.90it/s]warmup should be done:  60%|█████▉    | 1786/3000 [00:01<00:00, 1618.58it/s]warmup should be done:  60%|██████    | 1806/3000 [00:01<00:00, 1623.92it/s]warmup should be done:  59%|█████▉    | 1780/3000 [00:01<00:00, 1600.02it/s]warmup should be done:  65%|██████▌   | 1961/3000 [00:01<00:00, 1629.99it/s]warmup should be done:  67%|██████▋   | 1996/3000 [00:01<00:00, 1654.78it/s]warmup should be done:  66%|██████▌   | 1986/3000 [00:01<00:00, 1646.43it/s]warmup should be done:  65%|██████▌   | 1953/3000 [00:01<00:00, 1617.20it/s]warmup should be done:  66%|██████▌   | 1971/3000 [00:01<00:00, 1627.76it/s]warmup should be done:  65%|██████▍   | 1948/3000 [00:01<00:00, 1617.65it/s]warmup should be done:  65%|██████▍   | 1941/3000 [00:01<00:00, 1599.98it/s]warmup should be done:  66%|██████▌   | 1969/3000 [00:01<00:00, 1581.84it/s]warmup should be done:  71%|███████   | 2124/3000 [00:01<00:00, 1627.13it/s]warmup should be done:  72%|███████▏  | 2162/3000 [00:01<00:00, 1653.77it/s]warmup should be done:  70%|███████   | 2115/3000 [00:01<00:00, 1615.69it/s]warmup should be done:  71%|███████   | 2134/3000 [00:01<00:00, 1625.15it/s]warmup should be done:  72%|███████▏  | 2151/3000 [00:01<00:00, 1630.70it/s]warmup should be done:  70%|███████   | 2102/3000 [00:01<00:00, 1600.12it/s]warmup should be done:  70%|███████   | 2110/3000 [00:01<00:00, 1584.18it/s]warmup should be done:  71%|███████   | 2131/3000 [00:01<00:00, 1592.99it/s]warmup should be done:  78%|███████▊  | 2328/3000 [00:01<00:00, 1651.17it/s]warmup should be done:  76%|███████▌  | 2287/3000 [00:01<00:00, 1622.63it/s]warmup should be done:  76%|███████▌  | 2277/3000 [00:01<00:00, 1612.05it/s]warmup should be done:  77%|███████▋  | 2297/3000 [00:01<00:00, 1621.07it/s]warmup should be done:  75%|███████▌  | 2263/3000 [00:01<00:00, 1594.66it/s]warmup should be done:  76%|███████▌  | 2269/3000 [00:01<00:00, 1574.77it/s]warmup should be done:  77%|███████▋  | 2315/3000 [00:01<00:00, 1590.13it/s]warmup should be done:  76%|███████▋  | 2293/3000 [00:01<00:00, 1598.20it/s]warmup should be done:  83%|████████▎ | 2494/3000 [00:01<00:00, 1650.52it/s]warmup should be done:  82%|████████▏ | 2450/3000 [00:01<00:00, 1620.99it/s]warmup should be done:  81%|████████▏ | 2439/3000 [00:01<00:00, 1611.53it/s]warmup should be done:  82%|████████▏ | 2460/3000 [00:01<00:00, 1619.26it/s]warmup should be done:  81%|████████  | 2423/3000 [00:01<00:00, 1592.31it/s]warmup should be done:  81%|████████  | 2430/3000 [00:01<00:00, 1582.83it/s]warmup should be done:  82%|████████▏ | 2455/3000 [00:01<00:00, 1603.41it/s]warmup should be done:  82%|████████▎ | 2475/3000 [00:01<00:00, 1567.95it/s]warmup should be done:  89%|████████▊ | 2660/3000 [00:01<00:00, 1650.11it/s]warmup should be done:  87%|████████▋ | 2601/3000 [00:01<00:00, 1611.27it/s]warmup should be done:  87%|████████▋ | 2613/3000 [00:01<00:00, 1620.25it/s]warmup should be done:  87%|████████▋ | 2622/3000 [00:01<00:00, 1617.74it/s]warmup should be done:  86%|████████▌ | 2583/3000 [00:01<00:00, 1587.63it/s]warmup should be done:  86%|████████▋ | 2591/3000 [00:01<00:00, 1590.58it/s]warmup should be done:  87%|████████▋ | 2618/3000 [00:01<00:00, 1611.11it/s]warmup should be done:  88%|████████▊ | 2635/3000 [00:01<00:00, 1575.11it/s]warmup should be done:  92%|█████████▏| 2763/3000 [00:01<00:00, 1612.61it/s]warmup should be done:  94%|█████████▍| 2826/3000 [00:01<00:00, 1649.64it/s]warmup should be done:  93%|█████████▎| 2776/3000 [00:01<00:00, 1617.78it/s]warmup should be done:  93%|█████████▎| 2784/3000 [00:01<00:00, 1615.82it/s]warmup should be done:  91%|█████████▏| 2744/3000 [00:01<00:00, 1591.63it/s]warmup should be done:  92%|█████████▏| 2753/3000 [00:01<00:00, 1598.38it/s]warmup should be done:  93%|█████████▎| 2781/3000 [00:01<00:00, 1615.97it/s]warmup should be done:  93%|█████████▎| 2796/3000 [00:01<00:00, 1585.26it/s]warmup should be done:  98%|█████████▊| 2927/3000 [00:01<00:00, 1620.36it/s]warmup should be done: 100%|█████████▉| 2992/3000 [00:01<00:00, 1650.50it/s]warmup should be done:  98%|█████████▊| 2941/3000 [00:01<00:00, 1624.40it/s]warmup should be done:  98%|█████████▊| 2948/3000 [00:01<00:00, 1622.10it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1653.72it/s]warmup should be done:  97%|█████████▋| 2916/3000 [00:01<00:00, 1607.42it/s]warmup should be done:  98%|█████████▊| 2945/3000 [00:01<00:00, 1621.42it/s]warmup should be done:  97%|█████████▋| 2904/3000 [00:01<00:00, 1577.23it/s]warmup should be done:  99%|█████████▊| 2960/3000 [00:01<00:00, 1600.15it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1628.30it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1624.30it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1622.56it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1619.47it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1618.30it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1604.79it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1572.45it/s]




warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]

warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   6%|▌         | 169/3000 [00:00<00:01, 1689.76it/s]warmup should be done:   6%|▌         | 166/3000 [00:00<00:01, 1658.18it/s]warmup should be done:   5%|▌         | 164/3000 [00:00<00:01, 1634.58it/s]warmup should be done:   6%|▌         | 167/3000 [00:00<00:01, 1664.05it/s]warmup should be done:   6%|▌         | 165/3000 [00:00<00:01, 1647.55it/s]warmup should be done:   6%|▌         | 166/3000 [00:00<00:01, 1656.42it/s]warmup should be done:   6%|▌         | 166/3000 [00:00<00:01, 1654.32it/s]warmup should be done:   6%|▌         | 167/3000 [00:00<00:01, 1660.70it/s]warmup should be done:  11%|█▏        | 340/3000 [00:00<00:01, 1698.35it/s]warmup should be done:  11%|█         | 334/3000 [00:00<00:01, 1669.05it/s]warmup should be done:  11%|█         | 333/3000 [00:00<00:01, 1660.09it/s]warmup should be done:  11%|█         | 332/3000 [00:00<00:01, 1653.07it/s]warmup should be done:  11%|█         | 332/3000 [00:00<00:01, 1655.58it/s]warmup should be done:  11%|█         | 335/3000 [00:00<00:01, 1667.53it/s]warmup should be done:  11%|█         | 330/3000 [00:00<00:01, 1641.56it/s]warmup should be done:  11%|█         | 334/3000 [00:00<00:01, 1651.27it/s]warmup should be done:  17%|█▋        | 511/3000 [00:00<00:01, 1701.69it/s]warmup should be done:  17%|█▋        | 502/3000 [00:00<00:01, 1672.03it/s]warmup should be done:  17%|█▋        | 500/3000 [00:00<00:01, 1664.47it/s]warmup should be done:  17%|█▋        | 499/3000 [00:00<00:01, 1662.05it/s]warmup should be done:  17%|█▋        | 503/3000 [00:00<00:01, 1671.57it/s]warmup should be done:  17%|█▋        | 496/3000 [00:00<00:01, 1648.98it/s]warmup should be done:  17%|█▋        | 498/3000 [00:00<00:01, 1649.87it/s]warmup should be done:  17%|█▋        | 500/3000 [00:00<00:01, 1653.04it/s]warmup should be done:  23%|██▎       | 682/3000 [00:00<00:01, 1701.95it/s]warmup should be done:  22%|██▏       | 668/3000 [00:00<00:01, 1667.73it/s]warmup should be done:  22%|██▏       | 668/3000 [00:00<00:01, 1669.82it/s]warmup should be done:  22%|██▏       | 665/3000 [00:00<00:01, 1663.29it/s]warmup should be done:  22%|██▏       | 670/3000 [00:00<00:01, 1669.48it/s]warmup should be done:  22%|██▏       | 665/3000 [00:00<00:01, 1657.32it/s]warmup should be done:  22%|██▏       | 672/3000 [00:00<00:01, 1674.91it/s]warmup should be done:  22%|██▏       | 668/3000 [00:00<00:01, 1661.78it/s]warmup should be done:  28%|██▊       | 836/3000 [00:00<00:01, 1672.80it/s]warmup should be done:  28%|██▊       | 836/3000 [00:00<00:01, 1670.27it/s]warmup should be done:  28%|██▊       | 853/3000 [00:00<00:01, 1701.48it/s]warmup should be done:  28%|██▊       | 833/3000 [00:00<00:01, 1668.63it/s]warmup should be done:  28%|██▊       | 840/3000 [00:00<00:01, 1675.05it/s]warmup should be done:  28%|██▊       | 838/3000 [00:00<00:01, 1669.95it/s]warmup should be done:  28%|██▊       | 832/3000 [00:00<00:01, 1659.48it/s]warmup should be done:  28%|██▊       | 836/3000 [00:00<00:01, 1665.97it/s]warmup should be done:  33%|███▎      | 1000/3000 [00:00<00:01, 1668.62it/s]warmup should be done:  34%|███▍      | 1024/3000 [00:00<00:01, 1702.78it/s]warmup should be done:  33%|███▎      | 1004/3000 [00:00<00:01, 1671.33it/s]warmup should be done:  34%|███▎      | 1006/3000 [00:00<00:01, 1670.83it/s]warmup should be done:  33%|███▎      | 998/3000 [00:00<00:01, 1657.15it/s]warmup should be done:  34%|███▎      | 1008/3000 [00:00<00:01, 1672.61it/s]warmup should be done:  33%|███▎      | 1004/3000 [00:00<00:01, 1667.56it/s]warmup should be done:  33%|███▎      | 1003/3000 [00:00<00:01, 1665.28it/s]warmup should be done:  39%|███▉      | 1173/3000 [00:00<00:01, 1676.08it/s]warmup should be done:  39%|███▉      | 1168/3000 [00:00<00:01, 1669.16it/s]warmup should be done:  40%|███▉      | 1195/3000 [00:00<00:01, 1701.53it/s]warmup should be done:  39%|███▉      | 1164/3000 [00:00<00:01, 1655.30it/s]warmup should be done:  39%|███▉      | 1176/3000 [00:00<00:01, 1671.82it/s]warmup should be done:  39%|███▉      | 1174/3000 [00:00<00:01, 1666.21it/s]warmup should be done:  39%|███▉      | 1171/3000 [00:00<00:01, 1661.79it/s]warmup should be done:  39%|███▉      | 1170/3000 [00:00<00:01, 1663.26it/s]warmup should be done:  45%|████▍     | 1341/3000 [00:00<00:00, 1677.23it/s]warmup should be done:  45%|████▍     | 1336/3000 [00:00<00:00, 1671.27it/s]warmup should be done:  46%|████▌     | 1366/3000 [00:00<00:00, 1703.42it/s]warmup should be done:  45%|████▍     | 1344/3000 [00:00<00:00, 1672.03it/s]warmup should be done:  44%|████▍     | 1330/3000 [00:00<00:01, 1652.96it/s]warmup should be done:  45%|████▍     | 1337/3000 [00:00<00:00, 1665.06it/s]warmup should be done:  45%|████▍     | 1338/3000 [00:00<00:01, 1659.79it/s]warmup should be done:  45%|████▍     | 1341/3000 [00:00<00:00, 1661.60it/s]warmup should be done:  50%|█████     | 1509/3000 [00:00<00:00, 1675.78it/s]warmup should be done:  50%|█████     | 1504/3000 [00:00<00:00, 1672.40it/s]warmup should be done:  51%|█████     | 1537/3000 [00:00<00:00, 1703.36it/s]warmup should be done:  50%|████▉     | 1498/3000 [00:00<00:00, 1661.19it/s]warmup should be done:  50%|█████     | 1512/3000 [00:00<00:00, 1673.65it/s]warmup should be done:  50%|█████     | 1505/3000 [00:00<00:00, 1668.32it/s]warmup should be done:  50%|█████     | 1508/3000 [00:00<00:00, 1660.39it/s]warmup should be done:  50%|█████     | 1504/3000 [00:00<00:00, 1654.16it/s]warmup should be done:  56%|█████▌    | 1679/3000 [00:01<00:00, 1681.21it/s]warmup should be done:  56%|█████▌    | 1673/3000 [00:01<00:00, 1675.12it/s]warmup should be done:  57%|█████▋    | 1708/3000 [00:01<00:00, 1702.77it/s]warmup should be done:  56%|█████▌    | 1666/3000 [00:01<00:00, 1666.74it/s]warmup should be done:  56%|█████▌    | 1680/3000 [00:01<00:00, 1672.49it/s]warmup should be done:  56%|█████▌    | 1673/3000 [00:01<00:00, 1669.29it/s]warmup should be done:  56%|█████▌    | 1675/3000 [00:01<00:00, 1660.08it/s]warmup should be done:  56%|█████▌    | 1670/3000 [00:01<00:00, 1651.23it/s]warmup should be done:  62%|██████▏   | 1850/3000 [00:01<00:00, 1687.03it/s]warmup should be done:  63%|██████▎   | 1879/3000 [00:01<00:00, 1703.62it/s]warmup should be done:  61%|██████    | 1834/3000 [00:01<00:00, 1669.87it/s]warmup should be done:  61%|██████▏   | 1841/3000 [00:01<00:00, 1672.03it/s]warmup should be done:  62%|██████▏   | 1848/3000 [00:01<00:00, 1673.67it/s]warmup should be done:  61%|██████▏   | 1840/3000 [00:01<00:00, 1667.44it/s]warmup should be done:  61%|██████▏   | 1842/3000 [00:01<00:00, 1658.62it/s]warmup should be done:  61%|██████    | 1836/3000 [00:01<00:00, 1647.29it/s]warmup should be done:  67%|██████▋   | 2020/3000 [00:01<00:00, 1690.52it/s]warmup should be done:  68%|██████▊   | 2050/3000 [00:01<00:00, 1703.88it/s]warmup should be done:  67%|██████▋   | 2002/3000 [00:01<00:00, 1670.21it/s]warmup should be done:  67%|██████▋   | 2009/3000 [00:01<00:00, 1669.55it/s]warmup should be done:  67%|██████▋   | 2016/3000 [00:01<00:00, 1672.47it/s]warmup should be done:  67%|██████▋   | 2007/3000 [00:01<00:00, 1665.10it/s]warmup should be done:  67%|██████▋   | 2008/3000 [00:01<00:00, 1655.76it/s]warmup should be done:  67%|██████▋   | 2001/3000 [00:01<00:00, 1643.84it/s]warmup should be done:  73%|███████▎  | 2190/3000 [00:01<00:00, 1689.85it/s]warmup should be done:  74%|███████▍  | 2221/3000 [00:01<00:00, 1701.84it/s]warmup should be done:  73%|███████▎  | 2176/3000 [00:01<00:00, 1669.54it/s]warmup should be done:  72%|███████▏  | 2170/3000 [00:01<00:00, 1670.24it/s]warmup should be done:  73%|███████▎  | 2184/3000 [00:01<00:00, 1670.71it/s]warmup should be done:  72%|███████▏  | 2174/3000 [00:01<00:00, 1663.37it/s]warmup should be done:  72%|███████▏  | 2174/3000 [00:01<00:00, 1654.01it/s]warmup should be done:  72%|███████▏  | 2166/3000 [00:01<00:00, 1640.68it/s]warmup should be done:  79%|███████▊  | 2359/3000 [00:01<00:00, 1686.96it/s]warmup should be done:  78%|███████▊  | 2338/3000 [00:01<00:00, 1672.90it/s]warmup should be done:  78%|███████▊  | 2344/3000 [00:01<00:00, 1672.32it/s]warmup should be done:  80%|███████▉  | 2392/3000 [00:01<00:00, 1701.08it/s]warmup should be done:  78%|███████▊  | 2341/3000 [00:01<00:00, 1665.31it/s]warmup should be done:  78%|███████▊  | 2352/3000 [00:01<00:00, 1666.55it/s]warmup should be done:  78%|███████▊  | 2341/3000 [00:01<00:00, 1656.93it/s]warmup should be done:  78%|███████▊  | 2333/3000 [00:01<00:00, 1647.26it/s]warmup should be done:  84%|████████▍ | 2529/3000 [00:01<00:00, 1689.68it/s]warmup should be done:  84%|████████▎ | 2506/3000 [00:01<00:00, 1674.35it/s]warmup should be done:  84%|████████▍ | 2513/3000 [00:01<00:00, 1674.97it/s]warmup should be done:  85%|████████▌ | 2563/3000 [00:01<00:00, 1701.46it/s]warmup should be done:  84%|████████▎ | 2508/3000 [00:01<00:00, 1666.19it/s]warmup should be done:  84%|████████▍ | 2519/3000 [00:01<00:00, 1663.26it/s]warmup should be done:  84%|████████▎ | 2507/3000 [00:01<00:00, 1657.69it/s]warmup should be done:  83%|████████▎ | 2500/3000 [00:01<00:00, 1652.48it/s]warmup should be done:  90%|████████▉ | 2698/3000 [00:01<00:00, 1689.35it/s]warmup should be done:  89%|████████▉ | 2674/3000 [00:01<00:00, 1673.80it/s]warmup should be done:  89%|████████▉ | 2681/3000 [00:01<00:00, 1675.54it/s]warmup should be done:  91%|█████████ | 2734/3000 [00:01<00:00, 1700.23it/s]warmup should be done:  89%|████████▉ | 2675/3000 [00:01<00:00, 1664.70it/s]warmup should be done:  89%|████████▉ | 2673/3000 [00:01<00:00, 1653.83it/s]warmup should be done:  90%|████████▉ | 2686/3000 [00:01<00:00, 1657.15it/s]warmup should be done:  89%|████████▉ | 2666/3000 [00:01<00:00, 1654.22it/s]warmup should be done:  96%|█████████▌| 2867/3000 [00:01<00:00, 1688.65it/s]warmup should be done:  95%|█████████▍| 2849/3000 [00:01<00:00, 1674.61it/s]warmup should be done:  95%|█████████▍| 2842/3000 [00:01<00:00, 1671.95it/s]warmup should be done:  97%|█████████▋| 2905/3000 [00:01<00:00, 1697.54it/s]warmup should be done:  95%|█████████▍| 2842/3000 [00:01<00:00, 1660.18it/s]warmup should be done:  95%|█████████▍| 2839/3000 [00:01<00:00, 1651.40it/s]warmup should be done:  95%|█████████▌| 2852/3000 [00:01<00:00, 1652.17it/s]warmup should be done:  94%|█████████▍| 2832/3000 [00:01<00:00, 1651.66it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1700.56it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1681.53it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1670.08it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1665.72it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1665.55it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1664.43it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1659.35it/s]warmup should be done: 100%|█████████▉| 2999/3000 [00:01<00:00, 1655.73it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1654.58it/s]2022-12-11 22:49:26.765381: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fc2cb832a60 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 22:49:26.765447: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 22:49:27.673003: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fa700028c80 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 22:49:27.673066: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 22:49:27.675280: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fc2cb830bb0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 22:49:27.675335: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 22:49:27.748905: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fc2d38302a0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 22:49:27.748976: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 22:49:27.767111: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fc2cbf93400 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 22:49:27.767188: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 22:49:27.933313: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fc2d3830b80 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 22:49:27.933395: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 22:49:28.060192: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fc2cb833590 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 22:49:28.060266: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 22:49:28.060506: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fc2d382c520 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 22:49:28.060586: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 22:49:28.997649: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 22:49:29.913313: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 22:49:30.018535: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 22:49:30.052665: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 22:49:30.068792: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 22:49:30.270645: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 22:49:30.333563: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 22:49:30.415294: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 22:49:31.980925: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 22:49:32.752120: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 22:49:32.967985: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 22:49:32.981314: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 22:49:33.010421: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 22:49:33.195523: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 22:49:33.211163: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 22:49:33.359536: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
[HCTR][22:50:11.259][ERROR][RK0][tid #140475000407808]: replica 0 reaches 1000, calling init pre replica
[HCTR][22:50:11.260][ERROR][RK0][tid #140475000407808]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][22:50:11.268][ERROR][RK0][tid #140475000407808]: coll ps creation done
[HCTR][22:50:11.268][ERROR][RK0][tid #140475000407808]: replica 0 waits for coll ps creation barrier
[HCTR][22:50:11.277][ERROR][RK0][main]: replica 1 reaches 1000, calling init pre replica
[HCTR][22:50:11.277][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][22:50:11.285][ERROR][RK0][main]: coll ps creation done
[HCTR][22:50:11.285][ERROR][RK0][main]: replica 1 waits for coll ps creation barrier
[HCTR][22:50:11.294][ERROR][RK0][main]: replica 7 reaches 1000, calling init pre replica
[HCTR][22:50:11.294][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][22:50:11.302][ERROR][RK0][main]: coll ps creation done
[HCTR][22:50:11.302][ERROR][RK0][main]: replica 7 waits for coll ps creation barrier
[HCTR][22:50:11.310][ERROR][RK0][tid #140475000407808]: replica 6 reaches 1000, calling init pre replica
[HCTR][22:50:11.310][ERROR][RK0][tid #140475000407808]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][22:50:11.318][ERROR][RK0][tid #140475000407808]: coll ps creation done
[HCTR][22:50:11.318][ERROR][RK0][tid #140475000407808]: replica 6 waits for coll ps creation barrier
[HCTR][22:50:11.325][ERROR][RK0][tid #140474992015104]: replica 5 reaches 1000, calling init pre replica
[HCTR][22:50:11.326][ERROR][RK0][tid #140474992015104]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][22:50:11.330][ERROR][RK0][tid #140474992015104]: coll ps creation done
[HCTR][22:50:11.330][ERROR][RK0][tid #140474992015104]: replica 5 waits for coll ps creation barrier
[HCTR][22:50:11.340][ERROR][RK0][main]: replica 4 reaches 1000, calling init pre replica
[HCTR][22:50:11.340][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][22:50:11.348][ERROR][RK0][main]: coll ps creation done
[HCTR][22:50:11.348][ERROR][RK0][main]: replica 4 waits for coll ps creation barrier
[HCTR][22:50:11.355][ERROR][RK0][tid #140475000407808]: replica 3 reaches 1000, calling init pre replica
[HCTR][22:50:11.355][ERROR][RK0][tid #140475000407808]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][22:50:11.360][ERROR][RK0][tid #140475000407808]: coll ps creation done
[HCTR][22:50:11.360][ERROR][RK0][tid #140475000407808]: replica 3 waits for coll ps creation barrier
[HCTR][22:50:11.369][ERROR][RK0][tid #140475134625536]: replica 2 reaches 1000, calling init pre replica
[HCTR][22:50:11.369][ERROR][RK0][tid #140475134625536]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][22:50:11.374][ERROR][RK0][tid #140475134625536]: coll ps creation done
[HCTR][22:50:11.374][ERROR][RK0][tid #140475134625536]: replica 2 waits for coll ps creation barrier
[HCTR][22:50:11.374][ERROR][RK0][tid #140475000407808]: replica 0 preparing frequency
[HCTR][22:50:12.229][ERROR][RK0][tid #140475000407808]: replica 0 preparing frequency done
[HCTR][22:50:12.262][ERROR][RK0][tid #140475000407808]: replica 0 calling init per replica
[HCTR][22:50:12.262][ERROR][RK0][main]: replica 7 calling init per replica
[HCTR][22:50:12.262][ERROR][RK0][main]: replica 1 calling init per replica
[HCTR][22:50:12.262][ERROR][RK0][tid #140474992015104]: replica 5 calling init per replica
[HCTR][22:50:12.262][ERROR][RK0][tid #140475000407808]: replica 6 calling init per replica
[HCTR][22:50:12.262][ERROR][RK0][tid #140475000407808]: replica 3 calling init per replica
[HCTR][22:50:12.262][ERROR][RK0][main]: replica 4 calling init per replica
[HCTR][22:50:12.262][ERROR][RK0][tid #140475134625536]: replica 2 calling init per replica
[HCTR][22:50:12.262][ERROR][RK0][tid #140475000407808]: Calling build_v2
[HCTR][22:50:12.262][ERROR][RK0][main]: Calling build_v2
[HCTR][22:50:12.262][ERROR][RK0][main]: Calling build_v2
[HCTR][22:50:12.262][ERROR][RK0][tid #140474992015104]: Calling build_v2
[HCTR][22:50:12.262][ERROR][RK0][tid #140475000407808]: Calling build_v2
[HCTR][22:50:12.262][ERROR][RK0][tid #140475000407808]: Calling build_v2
[HCTR][22:50:12.262][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][22:50:12.262][ERROR][RK0][main]: Calling build_v2
[HCTR][22:50:12.262][ERROR][RK0][tid #140475000407808]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][22:50:12.262][ERROR][RK0][tid #140475134625536]: Calling build_v2
[HCTR][22:50:12.262][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][22:50:12.262][ERROR][RK0][tid #140474992015104]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][22:50:12.262][ERROR][RK0][tid #140475000407808]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][22:50:12.262][ERROR][RK0][tid #140475134625536]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][22:50:12.262][ERROR][RK0][tid #140475000407808]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][22:50:12.262][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[2022-12-11 22:50:12.266328: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:178] [v100x8, slow pcie
2022-12-11 22:50:12[.2022-12-11 22:50:12266370.: 266403E:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc :[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc178:] 196v100x8, slow pcie] 2022-12-11 22:50:12
assigning 0 to cpu.
266413: [E[2022-12-11 22:50:12 ./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc266469:: 2022-12-11 22:50:12178E.]  266460v100x8, slow pcie/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[: 
:2022-12-11 22:50:12E196. ] [266508/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[assigning 0 to cpu2022-12-11 22:50:12: :
.E2022-12-11 22:50:12178266524 .] : /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc266505[v100x8, slow pcieE:: [
 212E2022-12-11 22:50:12/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc]  .[:build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2665762022-12-11 22:50:122022-12-11 22:50:12196
:: ..] 178E266594266553assigning 0 to cpu]  : : [[
v100x8, slow pcie/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccEE2022-12-11 22:50:12
2022-12-11 22:50:12:[  ..212[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc266637[2022-12-11 22:50:12266604] 2022-12-11 22:50:12::: 2022-12-11 22:50:12.: build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8.196178E.266646E
266678] ]  266696:  : assigning 0 to cpuv100x8, slow pcie/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: [E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE

:E2022-12-11 22:50:12 : 213 .[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc178/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2667752022-12-11 22:50:12:] :remote time is 8.68421:: .[178v100x8, slow pcie196
212E2668122022-12-11 22:50:12] 
] ]  : [.v100x8, slow pcieassigning 0 to cpubuild asymm link desc with 8X Tesla V100-SXM2-32GB out of 8/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE[2022-12-11 22:50:12266850


: 2022-12-11 22:50:12.: 213[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.[266887E] [2022-12-11 22:50:12:2669022022-12-11 22:50:12:  remote time is 8.684212022-12-11 22:50:12.196: .E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
.266945] E266955 :266982[: assigning 0 to cpu : /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc212: 2022-12-11 22:50:12E
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE:] E. : 214build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8 267037/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc196/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] [
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: :] :cpu time is 97.05882022-12-11 22:50:12:E196[assigning 0 to cpu213
.212 ] 2022-12-11 22:50:12
] 267112] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccassigning 0 to cpu.remote time is 8.68421: build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8:
267156
E
214:  ] [E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[cpu time is 97.05882022-12-11 22:50:12 [:2022-12-11 22:50:12
./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-11 22:50:12[212.267241:.2022-12-11 22:50:12] 267243: 213267250.build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8: E] : 267284
E remote time is 8.68421E:  /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
 [E[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-11 22:50:12 2022-12-11 22:50:12:214:./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.212] 213267364:267382] cpu time is 97.0588] : 212: build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8
remote time is 8.68421E] E

 build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8 /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:[2022-12-11 22:50:12:2132022-12-11 22:50:12[.214] .2022-12-11 22:50:12267508] remote time is 8.68421267514.: cpu time is 97.0588
: 267531E
E: [  E2022-12-11 22:50:12/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc .::/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc267588214213:: ] ] 213Ecpu time is 97.0588remote time is 8.68421]  

remote time is 8.68421/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
:[214[2022-12-11 22:50:12] 2022-12-11 22:50:12.cpu time is 97.0588.267688
267697: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc::214214] ] cpu time is 97.0588cpu time is 97.0588

[2022-12-11 22:51:31.448850: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:118] solver created. now build & solve
[2022-12-11 22:51:31.488807: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:123] solver built. now solve
[2022-12-11 22:51:31.488863: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:919] num_cached_nodes = 2999999
[2022-12-11 22:51:31.608908: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:127] solver solved
[2022-12-11 22:51:31.608996: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:195] 0 solved master
[2022-12-11 22:51:31.670858: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 0 solved
[2022-12-11 22:51:31.670895: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 0 initing device 0
[2022-12-11 22:51:31.671354: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 22:51:31.672331: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 22:51:31.673114: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 22:51:31.686091: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 7 solved
[2022-12-11 22:51:31.686162: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 7 initing device 7
[2022-12-11 22:51:31.686460: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 6 solved
[[2022-12-11 22:51:312022-12-11 22:51:31..686506686523: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc::202205] ] 4 solved[worker 0 thread 6 initing device 6
2022-12-11 22:51:31
.686568[[: [2022-12-11 22:51:312022-12-11 22:51:31E2022-12-11 22:51:31.. .686576686588/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu686587: : :: EE1980E  ]  /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cceager alloc mem 381.47 MB/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc::
:202205202] ] ] 5 solvedworker 0 thread 4 initing device 41 solved


[2022-12-11 22:51:31[.2022-12-11 22:51:31686705.: 686709E:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc :/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc205:] 205worker 0 thread 5 initing device 5] 
worker 0 thread 1 initing device 1
[[2022-12-11 22:51:312022-12-11 22:51:31..686777686780: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc::202202] ] 3 solved
2 solved
[2022-12-11 22:51:31[.2022-12-11 22:51:31686867.: 686870E:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc :/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc205:] 205worker 0 thread 3 initing device 3] 
worker 0 thread 2 initing device 2
[2022-12-11 22:51:31.686961: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 22:51:31.687061: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 22:51:31.687107: [E2022-12-11 22:51:31 ./hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu687116:: 1980E]  eager alloc mem 381.47 MB/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu
:1980] eager alloc mem 381.47 MB
[2022-12-11 22:51:31.687303[: 2022-12-11 22:51:31E. 687311/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu: :E1980 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cueager alloc mem 381.47 MB:
1980] eager alloc mem 381.47 MB
[2022-12-11 22:51:31.691090: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 22:51:31.691360: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 22:51:31.691413: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 22:51:31.691470: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 22:51:31.691524: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 22:51:31.692071: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 22:51:31.692127: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 22:51:31.695565: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 22:51:31.695727: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 22:51:31.695776: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 22:51:31.695879: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 22:51:31.695937: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 22:51:31.695980: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 22:51:31.696468: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 22:51:31.749263: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 2.00 Bytes
[2022-12-11 22:51:31.749599: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1024.00 Bytes
[2022-12-11 22:51:31.754498: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-11 22:51:31.754566: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 2
[2022-12-11 22:51:31.754610: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-11 22:51:31.755362: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 11.83 MB
[2022-12-11 22:51:31.755794: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:51:31.756751: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:51:31.756853: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 22:51:31.757529: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 22:51:31.757572: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1.43 GB
[[[[[[[2022-12-11 22:51:312022-12-11 22:51:312022-12-11 22:51:312022-12-11 22:51:312022-12-11 22:51:312022-12-11 22:51:312022-12-11 22:51:31.......789554789554789554789554789554789554789554: : : : : : : EEEEEEE       /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:::::::1980198019801980198019801980] ] ] ] ] ] ] eager alloc mem 2.00 Byteseager alloc mem 2.00 Byteseager alloc mem 2.00 Byteseager alloc mem 2.00 Byteseager alloc mem 2.00 Byteseager alloc mem 2.00 Byteseager alloc mem 2.00 Bytes






[[[[[[2022-12-11 22:51:31[2022-12-11 22:51:312022-12-11 22:51:312022-12-11 22:51:312022-12-11 22:51:312022-12-11 22:51:31.2022-12-11 22:51:31.....790052.790053790052790053790055790054: 790061: : : : : E: EEEEE E     /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:::::1980:19801980198019801980] 1980] ] ] ] ] eager alloc mem 1024.00 Bytes] eager alloc mem 1024.00 Byteseager alloc mem 1024.00 Byteseager alloc mem 1024.00 Byteseager alloc mem 1024.00 Byteseager alloc mem 1024.00 Bytes
eager alloc mem 1024.00 Bytes





[2022-12-11 22:51:31.796798: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-11 22:51:31.796885: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 2
[2022-12-11 22:51:31.796930: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-11 22:51:31.796943: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[[2022-12-11 22:51:312022-12-11 22:51:31..797007797022: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::638638] ] eager release cuda mem 1024eager release cuda mem 2

[2022-12-11 22:51:31[.2022-12-11 22:51:31797094.: 797097E:  E[/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc 2022-12-11 22:51:31:/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc.638:797102] 638: eager release cuda mem 400000000] E
eager release cuda mem 2 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-11 22:51:31.797188: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] [eager release cuda mem 4000000002022-12-11 22:51:31
[.2022-12-11 22:51:31797205.: 797198E:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc :/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc638:] 638eager release cuda mem 2] 
eager release cuda mem 1024
[[2022-12-11 22:51:312022-12-11 22:51:31.[.7972662022-12-11 22:51:31797284: .: E797291E :  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccE/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc: :638/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc638] :] eager release cuda mem 1024638eager release cuda mem 400000000[
] 
2022-12-11 22:51:31eager release cuda mem 2.
797346: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024[
2022-12-11 22:51:31[.2022-12-11 22:51:31797395.: 797401E:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc :/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc[638:2022-12-11 22:51:31] 638.eager release cuda mem 2] 797428
eager release cuda mem 400000000: 
E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 2
[2022-12-11 22:51:31.797472: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000[
2022-12-11 22:51:31.797494: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-11 22:51:31.797823: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 11.83 MB
[2022-12-11 22:51:31.798946: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 11.83 MB
[2022-12-11 22:51:31.799864: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 11.83 MB
[2022-12-11 22:51:31.800504: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 11.83 MB
[2022-12-11 22:51:31.801029: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 11.83 MB
[2022-12-11 22:51:31.801534: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 11.83 MB
[2022-12-11 22:51:31.802038: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 11.83 MB
[2022-12-11 22:51:31.802616: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:51:31.803126: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:51:31.803572: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:51:31.803658: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 22:51:31.804006: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:51:31.804052: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:51:31.804105: [E2022-12-11 22:51:31 ./hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc804109:: 638E]  eager release cuda mem 625663/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu
:1980] eager alloc mem 611.00 KB
[2022-12-11 22:51:31.804158: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:51:31[.2022-12-11 22:51:31804201.: 804214E:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu :/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu1980:] 1980eager alloc mem 611.00 KB] 
eager alloc mem 25.25 KB
[2022-12-11 22:51:31.804337: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 22:51:31.804379: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1.43 GB
[2022-12-11 22:51:31.804907: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 22:51:31.804947: E [/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu2022-12-11 22:51:31:.1980804955] : eager alloc mem 1.43 GBE
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:51:31.804999: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:51:31.805052: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 22:51:31.805082[: 2022-12-11 22:51:31E.[ 8050892022-12-11 22:51:31/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu: .:E8050971980 : ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccEeager alloc mem 25.25 KB: 
638/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc] :eager release cuda mem 625663638
] eager release cuda mem 625663
[2022-12-11 22:51:31.805176: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:51:31[.2022-12-11 22:51:31805218.: 805222E:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu :/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu1980:] 1980eager alloc mem 25.25 KB] 
[eager alloc mem 25.25 KB2022-12-11 22:51:31
.805262: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 22:51:31.805727: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 22:51:31.805769: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1.43 GB
[2022-12-11 22:51:31.805793: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 22:51:31.805834: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1.43 GB
[2022-12-11 22:51:31.805911: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638[] 2022-12-11 22:51:31eager release cuda mem 25855.
[8059292022-12-11 22:51:31: .E805940 : [/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccE2022-12-11 22:51:31: .638/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc805958] :: eager release cuda mem 25855638E
]  eager release cuda mem 25855/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu
:1980] eager alloc mem 1.43 GB[
2022-12-11 22:51:31.806008[: 2022-12-11 22:51:31E. 806017/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu: :E1980 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cueager alloc mem 1.43 GB:
1980] eager alloc mem 1.43 GB
[[[[[[[2022-12-11 22:51:322022-12-11 22:51:322022-12-11 22:51:322022-12-11 22:51:322022-12-11 22:51:322022-12-11 22:51:322022-12-11 22:51:32....... 88825 88824 88824 88824 88824 88833 88842: : [: : : : : EE2022-12-11 22:51:32EEEEE  .     /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu 88900/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::: :::::19801980E19801980198019801980] ]  ] ] ] ] ] eager alloc mem 611.00 KBeager alloc mem 611.00 KB/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cueager alloc mem 611.00 KBeager alloc mem 611.00 KBeager alloc mem 611.00 KBeager alloc mem 611.00 KBeager alloc mem 611.00 KB

:




1980] eager alloc mem 611.00 KB
[[2022-12-11 22:51:322022-12-11 22:51:32.. 89893[ 89892: 2022-12-11 22:51:32: [[E.E2022-12-11 22:51:322022-12-11 22:51:32  89907 ../hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc[: [/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc 89915 89917:2022-12-11 22:51:32[E2022-12-11 22:51:32:: : 638.2022-12-11 22:51:32 .638EE]  89938./hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc 89941]   eager release cuda mem 625663:  89949:: eager release cuda mem 625663/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc
E: 638E
:: E]  638638/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc eager release cuda mem 625663/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc] ] :/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc
[:eager release cuda mem 625663eager release cuda mem 625663638[:2022-12-11 22:51:32638

] 2022-12-11 22:51:32638.] eager release cuda mem 625663.]  90099eager release cuda mem 625663
 90113eager release cuda mem 625663: [
: 
E2022-12-11 22:51:32E[[ . 2022-12-11 22:51:322022-12-11 22:51:32/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu 90178/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu..:: : 90203 902051980[E1980: : ] 2022-12-11 22:51:32 ] [EEeager alloc mem 611.00 KB./hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu[eager alloc mem 611.00 KB2022-12-11 22:51:32  
 90236:2022-12-11 22:51:32
./hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu: 1980. 90251::E]  90260: 19801980 eager alloc mem 611.00 KB: E] ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu
E eager alloc mem 611.00 KBeager alloc mem 611.00 KB: /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu

1980/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:] :1980eager alloc mem 611.00 KB1980] 
] eager alloc mem 611.00 KBeager alloc mem 611.00 KB

[2022-12-11 22:51:32. 91020: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc[:2022-12-11 22:51:32638.]  91037eager release cuda mem 625663: 
E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:51:32. 91091: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc[:2022-12-11 22:51:32638[.] 2022-12-11 22:51:32 91105[eager release cuda mem 625663.: 2022-12-11 22:51:32
[ 91112E.2022-12-11 22:51:32:   91120.E/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:  91142 :[E: [/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc19802022-12-11 22:51:32 [E2022-12-11 22:51:32:] [./hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu2022-12-11 22:51:32 .638eager alloc mem 611.00 KB2022-12-11 22:51:32 91183:./hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc 91174] 
.: 1980 91192:: eager release cuda mem 625663 91205E] : 638E
:  eager alloc mem 611.00 KBE]  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc
 eager release cuda mem 625663/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc :/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc
:/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu638:638:[] 638] 19802022-12-11 22:51:32eager release cuda mem 625663] eager release cuda mem 625663] .
eager release cuda mem 625663
eager alloc mem 611.00 KB[ 91357

2022-12-11 22:51:32: .E 91398 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE: 1980/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu] :eager alloc mem 611.00 KB1980
[] 2022-12-11 22:51:32eager alloc mem 611.00 KB.
 91449: [E[2022-12-11 22:51:32 2022-12-11 22:51:32./hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu. 91466: 91457: 1980: E] E eager alloc mem 611.00 KB /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu
/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::19801980] ] eager alloc mem 611.00 KBeager alloc mem 611.00 KB

[2022-12-11 22:51:32. 91996: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:51:32. 92059[: 2022-12-11 22:51:32E.  92069/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc: :E638 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cueager release cuda mem 625663:
1980] eager alloc mem 611.00 KB
[2022-12-11 22:51:32[.2022-12-11 22:51:32 92146.:  92157E:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc :[/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu6382022-12-11 22:51:32:] .1980eager release cuda mem 625663 92181] [
: eager alloc mem 611.00 KB2022-12-11 22:51:32E
.  92205/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc: :E638 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cceager release cuda mem 625663:
638] eager release cuda mem 625663
[2022-12-11 22:51:32. 92271: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:51:32.[ 92304[2022-12-11 22:51:32: 2022-12-11 22:51:32.E. 92307  92310: /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu: E:E 1980[ /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc] 2022-12-11 22:51:32/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:eager alloc mem 611.00 KB.:638
 923431980] : ] eager release cuda mem 625663Eeager alloc mem 611.00 KB
 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:51:32. 92435: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu[:2022-12-11 22:51:321980.]  92450eager alloc mem 611.00 KB: 
E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:51:32. 92843: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:51:32. 92914: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[[2022-12-11 22:51:322022-12-11 22:51:32.. 92959 92947: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::[6386382022-12-11 22:51:32] ] .eager release cuda mem 625663eager release cuda mem 625663 93023

: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:51:32. 93097: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc[:2022-12-11 22:51:32638.]  93111eager release cuda mem 625663: 
E[[ 2022-12-11 22:51:322022-12-11 22:51:32/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu..: 93132 931331980[: : ] 2022-12-11 22:51:32EEeager alloc mem 611.00 KB[.  
2022-12-11 22:51:32 93155/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu.: :: 93182E[6381980:  2022-12-11 22:51:32[] ] E/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu.2022-12-11 22:51:32eager release cuda mem 625663eager alloc mem 611.00 KB : 93226.

/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu1980:  93240:] E: 1980eager alloc mem 611.00 KB E] 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc eager alloc mem 611.00 KB[:/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc
2022-12-11 22:51:32638:.] 638 93336eager release cuda mem 625663] : 
eager release cuda mem 625663E
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:51:32.[ 934172022-12-11 22:51:32: .E 93423 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE: 1980/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu] :eager alloc mem 611.00 KB1980
] eager alloc mem 611.00 KB
[2022-12-11 22:51:32. 93663: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:51:32. 93734: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:51:32. 93929: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:51:32. 93997: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:51:32. 94024: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[[2022-12-11 22:51:322022-12-11 22:51:32.[. 940772022-12-11 22:51:32 94077: .: E 94092[E : E2022-12-11 22:51:32 /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc ./hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu 94130:638:: 638] 1980E] eager release cuda mem 625663]  [eager release cuda mem 625663[
eager alloc mem 611.00 KB/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc2022-12-11 22:51:32
2022-12-11 22:51:32
:..638 94214 94219] : : eager release cuda mem 625663EE
  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc[::2022-12-11 22:51:32638638.] []  94284eager release cuda mem 6256632022-12-11 22:51:32eager release cuda mem 625663: 
.
[E 943002022-12-11 22:51:32 : ./hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE 94322: : [1980/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE[2022-12-11 22:51:32] : 2022-12-11 22:51:32.eager alloc mem 611.00 KB1980/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu. 94369
] : 94377: eager alloc mem 611.00 KB1980: E
] E eager alloc mem 611.00 KB /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu
/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::19801980] ] eager alloc mem 611.00 KBeager alloc mem 611.00 KB
[
2022-12-11 22:51:32. 94480: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:51:32. 94551: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:51:32. 94750: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:51:32. 94818: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:51:32. 94980: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:51:32. 95048: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:51:32. 95164: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:[6382022-12-11 22:51:32] .eager release cuda mem 625663 95179
: [E2022-12-11 22:51:32 ./hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc 95208:[: 638[2022-12-11 22:51:32E] [2022-12-11 22:51:32. eager release cuda mem 6256632022-12-11 22:51:32. 95236/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc
. 95242: : 95256[: E638: 2022-12-11 22:51:32E ] E. /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cceager release cuda mem 625663  95300/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:
/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu: :638:[E638] 19802022-12-11 22:51:32 ] eager release cuda mem 625663] ./hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cceager release cuda mem 625663
eager alloc mem 611.00 KB[ 95344:

2022-12-11 22:51:32: 638.E]  95387 eager release cuda mem 625663: /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu[
E:2022-12-11 22:51:32[ 1980.2022-12-11 22:51:32[/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu]  95437.2022-12-11 22:51:32:eager alloc mem 611.00 KB:  95451.1980
E:  95465]  E: eager alloc mem 611.00 KB/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu E
:/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu 1980:/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc] 1980:eager alloc mem 611.00 KB] 638
eager alloc mem 611.00 KB] 
eager release cuda mem 12399996[
2022-12-11 22:51:32. 95569: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:51:32. 95610: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 12399996
[2022-12-11 22:51:32. 95798: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:51:32. 95835: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 12399996
[2022-12-11 22:51:32. 96134: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:51:32. 96177: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 12399996
[2022-12-11 22:51:32. 96242: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:51:32. 96276: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:51:32.[ 96316[2022-12-11 22:51:32: [2022-12-11 22:51:32.E2022-12-11 22:51:32. 96317 . 96319: /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc 96329: E:: E 638E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc]  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:eager release cuda mem 12399996/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638
:1980] 638] eager release cuda mem 625663] eager alloc mem 611.00 KB
eager release cuda mem 625663

[2022-12-11 22:51:32. 96476: E[ 2022-12-11 22:51:32/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc.: 96486638: ] Eeager release cuda mem 12399996 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 12399996
[2022-12-11 22:51:32. 96531: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 2999999 / 100000000 nodes ( 3.00 %~3.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 97000001 / 100000000 nodes ( 97.00 %) | 1.43 GB | 0.409474 secs 
[2022-12-11 22:51:32. 97197: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:51:32. 97236: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 12399996
[2022-12-11 22:51:32.103155: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 2999999 / 100000000 nodes ( 3.00 %~3.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 97000001 / 100000000 nodes ( 97.00 %) | 1.43 GB | 0.416591 secs 
[2022-12-11 22:51:32.108681: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 2999999 / 100000000 nodes ( 3.00 %~3.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 97000001 / 100000000 nodes ( 97.00 %) | 1.43 GB | 0.42158 secs 
[2022-12-11 22:51:32.109609: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 2999999 / 100000000 nodes ( 3.00 %~3.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 97000001 / 100000000 nodes ( 97.00 %) | 1.43 GB | 0.422499 secs 
[2022-12-11 22:51:32.110021: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 2999999 / 100000000 nodes ( 3.00 %~3.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 97000001 / 100000000 nodes ( 97.00 %) | 1.43 GB | 0.423067 secs 
[2022-12-11 22:51:32.110432: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 2999999 / 100000000 nodes ( 3.00 %~3.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 97000001 / 100000000 nodes ( 97.00 %) | 1.43 GB | 0.423136 secs 
[2022-12-11 22:51:32.110517: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 2999999 / 100000000 nodes ( 3.00 %~3.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 97000001 / 100000000 nodes ( 97.00 %) | 1.43 GB | 0.423212 secs 
[2022-12-11 22:51:32.110976: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 2999999 / 100000000 nodes ( 3.00 %~3.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 97000001 / 100000000 nodes ( 97.00 %) | 1.43 GB | 0.439628 secs 
[HCTR][22:51:32.111][ERROR][RK0][tid #140475000407808]: replica 6 calling init per replica done, doing barrier
[HCTR][22:51:32.111][ERROR][RK0][main]: replica 1 calling init per replica done, doing barrier
[HCTR][22:51:32.111][ERROR][RK0][main]: replica 7 calling init per replica done, doing barrier
[HCTR][22:51:32.111][ERROR][RK0][tid #140475000407808]: replica 0 calling init per replica done, doing barrier
[HCTR][22:51:32.111][ERROR][RK0][main]: replica 4 calling init per replica done, doing barrier
[HCTR][22:51:32.111][ERROR][RK0][tid #140475000407808]: replica 3 calling init per replica done, doing barrier
[HCTR][22:51:32.111][ERROR][RK0][tid #140474992015104]: replica 5 calling init per replica done, doing barrier
[HCTR][22:51:32.111][ERROR][RK0][tid #140475134625536]: replica 2 calling init per replica done, doing barrier
[HCTR][22:51:32.111][ERROR][RK0][tid #140475134625536]: replica 2 calling init per replica done, doing barrier done
[HCTR][22:51:32.111][ERROR][RK0][main]: replica 4 calling init per replica done, doing barrier done
[HCTR][22:51:32.111][ERROR][RK0][tid #140475000407808]: replica 3 calling init per replica done, doing barrier done
[HCTR][22:51:32.111][ERROR][RK0][tid #140475000407808]: replica 0 calling init per replica done, doing barrier done
[HCTR][22:51:32.111][ERROR][RK0][tid #140474992015104]: replica 5 calling init per replica done, doing barrier done
[HCTR][22:51:32.111][ERROR][RK0][main]: replica 1 calling init per replica done, doing barrier done
[HCTR][22:51:32.111][ERROR][RK0][main]: replica 7 calling init per replica done, doing barrier done
[HCTR][22:51:32.111][ERROR][RK0][tid #140475000407808]: replica 6 calling init per replica done, doing barrier done
[HCTR][22:51:32.111][ERROR][RK0][tid #140475134625536]: init per replica done
[HCTR][22:51:32.111][ERROR][RK0][main]: init per replica done
[HCTR][22:51:32.111][ERROR][RK0][tid #140475000407808]: init per replica done
[HCTR][22:51:32.111][ERROR][RK0][tid #140474992015104]: init per replica done
[HCTR][22:51:32.111][ERROR][RK0][main]: init per replica done
[HCTR][22:51:32.111][ERROR][RK0][main]: init per replica done
[HCTR][22:51:32.111][ERROR][RK0][tid #140475000407808]: init per replica done
[HCTR][22:51:32.113][ERROR][RK0][tid #140475000407808]: init per replica done








