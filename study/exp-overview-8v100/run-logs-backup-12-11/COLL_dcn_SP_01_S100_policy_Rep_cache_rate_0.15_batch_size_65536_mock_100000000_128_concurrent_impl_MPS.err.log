2022-12-12 04:22:53.136453: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.144734: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.151675: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.155884: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.161697: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.174836: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.180965: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.190877: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.244250: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.247641: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.248624: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.249588: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.250549: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.251495: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.252527: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.253604: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.254847: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.256599: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.257601: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.257680: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.259168: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.259291: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.260638: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.260986: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.261967: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.262562: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.263541: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.264213: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.265190: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.265854: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.267361: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.267548: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.268924: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.269148: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.270756: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.271720: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.272667: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.273629: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.274939: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.276662: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.277300: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.278655: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.279626: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.280558: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.281539: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.282251: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 04:22:53.282529: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.283588: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.284693: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.289405: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.290650: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.291867: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.292636: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.293101: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.294071: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.294714: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.295402: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.296571: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.298682: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.299758: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.300692: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.302275: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.304706: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.305178: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.307035: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.307780: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.309723: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.310500: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.310727: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.312392: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.313250: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.313518: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.313960: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.314791: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.315100: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.316362: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.316621: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.317360: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.318105: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.318372: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.319378: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.320060: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.321153: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.322605: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.324039: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.324285: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.325131: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.325307: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.326916: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.327242: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.328095: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.328179: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.329389: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.347210: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.364062: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.364659: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.364902: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.365163: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.365286: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.366718: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.367821: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.368444: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.369179: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.369283: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.369333: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.370591: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.372871: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.373495: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.373598: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.374566: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.375973: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.377157: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.377824: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.378175: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.379232: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.380798: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.381278: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.381413: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.382312: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.383810: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.384575: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.384784: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.385478: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.386570: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.387385: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.387627: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.388408: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.389608: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.390072: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.390400: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.391090: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.392342: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.392813: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.393063: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.393834: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.395059: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.395658: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.396081: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.396873: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.397969: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.398403: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.398947: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.399585: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.401389: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.401698: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.401727: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.402221: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.404066: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.404554: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.404684: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.405109: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.406686: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.407358: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.407544: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.407555: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.407780: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.409868: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.410853: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.410954: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.411033: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.411200: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.414212: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.414619: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.414772: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.414806: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.415275: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.416490: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.417675: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.418169: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.418257: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.419086: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.419670: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.420663: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.421493: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.421662: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.422496: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.422554: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.423278: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.424318: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.425315: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.426490: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.426831: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.427326: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.427561: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.428267: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.429019: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.429821: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.431218: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.431589: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.432390: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.433092: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.433822: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.434098: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.436249: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.436403: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.436397: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 04:22:53.436684: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.437733: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.438324: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.438370: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.440177: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.440405: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.440487: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.441707: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.442023: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.442209: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.444200: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.444272: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.444480: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.445908: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.446314: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.446371: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.448017: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.448272: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.448596: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.450094: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.450271: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.451837: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.451995: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.452371: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.452427: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 04:22:53.453234: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 04:22:53.453722: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.454917: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.455002: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.455424: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.456864: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 04:22:53.457114: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.458231: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.458668: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.460176: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.461792: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.461904: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.463034: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.463485: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.463895: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.465403: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.465641: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.467334: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.467448: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.467657: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.468092: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.469979: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.470933: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.472035: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.472249: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.472374: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.472721: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.474934: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.475971: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.477074: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.477169: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.479435: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.480528: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.483210: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.513257: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.514421: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.517338: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.518848: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.519315: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.522743: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.524693: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.525148: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.527933: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.529298: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.532086: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.533677: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.537258: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.540269: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.541672: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.542796: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.546775: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.547925: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.548302: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 04:22:53.559017: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.579756: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.581378: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.581697: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.585669: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.585906: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.588636: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 04:22:53.599193: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.606860: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.609265: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 04:22:53.611184: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.619815: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.706920: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:53.714146: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:54.576291: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:54.577528: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:54.578313: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:54.578783: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 04:22:54.578837: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-12 04:22:54.597478: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:54.598120: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:54.598709: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:54.599373: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:54.600358: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:54.600825: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-12 04:22:54.646326: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 04:22:54.646536: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 04:22:54.675661: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12340
2022-12-12 04:22:54.830435: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:54.831250: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:54.831777: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:54.832250: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 04:22:54.832301: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-12 04:22:54.850349: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:54.852347: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:54.853823: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:54.853934: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:54.854667: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:54.854889: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:54.855664: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:54.855720: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 04:22:54.855771: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-12 04:22:54.856378: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:54.856844: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:1/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-12 04:22:54.874053: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:54.874519: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:54.874837: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:54.875573: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:54.876122: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:54.876895: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:54.877376: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:54.877809: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 04:22:54.877865: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-12 04:22:54.878285: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:54.878758: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:2/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-12 04:22:54.881359: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:54.886385: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:54.886926: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:54.887410: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 04:22:54.887454: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-12 04:22:54.896077: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:54.896700: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:54.897220: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:54.897797: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:54.898321: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:54.898790: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:7/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-12 04:22:54.906311: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:54.907169: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:54.907691: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:54.908410: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:54.908924: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:54.909393: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:5/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-12 04:22:54.924419: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:54.925038: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:54.925571: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:54.926031: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 04:22:54.926090: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-12 04:22:54.944438: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:54.945115: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:54.945503: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:54.945529: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:54.945612: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 04:22:54.945772: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 04:22:54.945968: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 04:22:54.946125: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:54.946150: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 04:22:54.947293: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:54.947324: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:54.947534: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12347
2022-12-12 04:22:54.947651: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:54.947817: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12341
2022-12-12 04:22:54.948891: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:54.949008: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:54.949228: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:54.950394: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 04:22:54.950450: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-12 04:22:54.950607: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 04:22:54.950684: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-12 04:22:54.950753: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:6/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-12 04:22:54.955736: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 04:22:54.955888: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 04:22:54.957659: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12345
2022-12-12 04:22:54.964526: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 04:22:54.964713: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 04:22:54.967508: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12342
2022-12-12 04:22:54.968953: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:54.969615: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:54.969752: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:54.970591: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:54.970992: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:54.972020: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:54.972369: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:54.973059: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:54.973466: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:54.973930: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:4/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-12 04:22:54.974273: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:22:54.974744: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:3/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-12 04:22:54.997147: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 04:22:54.997337: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 04:22:54.999017: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12346
2022-12-12 04:22:55.019191: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 04:22:55.019376: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 04:22:55.021085: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12344
2022-12-12 04:22:55.021873: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 04:22:55.022077: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 04:22:55.023857: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12343
[HCTR][04:22:56.293][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][04:22:56.293][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][04:22:56.293][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][04:22:56.294][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][04:22:56.294][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][04:22:56.294][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][04:22:56.295][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][04:22:56.295][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 1it [00:01,  1.52s/it]warmup run: 101it [00:01, 86.69it/s]warmup run: 1it [00:01,  1.51s/it]warmup run: 202it [00:01, 187.66it/s]warmup run: 1it [00:01,  1.53s/it]warmup run: 1it [00:01,  1.53s/it]warmup run: 1it [00:01,  1.53s/it]warmup run: 100it [00:01, 85.90it/s]warmup run: 1it [00:01,  1.49s/it]warmup run: 1it [00:01,  1.49s/it]warmup run: 1it [00:01,  1.49s/it]warmup run: 303it [00:01, 298.66it/s]warmup run: 95it [00:01, 80.79it/s]warmup run: 99it [00:01, 84.43it/s]warmup run: 92it [00:01, 78.48it/s]warmup run: 198it [00:01, 183.76it/s]warmup run: 96it [00:01, 83.58it/s]warmup run: 98it [00:01, 85.37it/s]warmup run: 97it [00:01, 84.74it/s]warmup run: 405it [00:01, 414.75it/s]warmup run: 189it [00:01, 174.05it/s]warmup run: 198it [00:01, 183.02it/s]warmup run: 184it [00:01, 169.98it/s]warmup run: 298it [00:01, 293.97it/s]warmup run: 192it [00:01, 180.69it/s]warmup run: 195it [00:01, 183.51it/s]warmup run: 194it [00:01, 183.20it/s]warmup run: 506it [00:02, 525.50it/s]warmup run: 285it [00:01, 279.35it/s]warmup run: 297it [00:01, 291.58it/s]warmup run: 275it [00:01, 269.27it/s]warmup run: 398it [00:01, 407.61it/s]warmup run: 290it [00:01, 288.37it/s]warmup run: 290it [00:01, 289.56it/s]warmup run: 291it [00:01, 290.78it/s]warmup run: 608it [00:02, 629.48it/s]warmup run: 380it [00:01, 386.80it/s]warmup run: 397it [00:01, 405.28it/s]warmup run: 366it [00:01, 372.26it/s]warmup run: 498it [00:02, 517.96it/s]warmup run: 385it [00:01, 396.30it/s]warmup run: 388it [00:01, 401.95it/s]warmup run: 388it [00:01, 401.51it/s]warmup run: 707it [00:02, 712.16it/s]warmup run: 475it [00:02, 491.56it/s]warmup run: 496it [00:02, 514.37it/s]warmup run: 458it [00:02, 474.11it/s]warmup run: 601it [00:02, 625.02it/s]warmup run: 479it [00:01, 499.12it/s]warmup run: 487it [00:01, 512.50it/s]warmup run: 485it [00:01, 508.41it/s]warmup run: 806it [00:02, 780.62it/s]warmup run: 570it [00:02, 587.41it/s]warmup run: 597it [00:02, 618.36it/s]warmup run: 551it [00:02, 569.72it/s]warmup run: 703it [00:02, 715.78it/s]warmup run: 575it [00:02, 596.62it/s]warmup run: 587it [00:02, 616.02it/s]warmup run: 583it [00:02, 608.23it/s]warmup run: 904it [00:02, 831.71it/s]warmup run: 666it [00:02, 672.56it/s]warmup run: 699it [00:02, 709.77it/s]warmup run: 640it [00:02, 638.35it/s]warmup run: 804it [00:02, 787.57it/s]warmup run: 670it [00:02, 678.85it/s]warmup run: 687it [00:02, 704.45it/s]warmup run: 682it [00:02, 695.53it/s]warmup run: 1003it [00:02, 872.83it/s]warmup run: 760it [00:02, 738.77it/s]warmup run: 802it [00:02, 787.28it/s]warmup run: 729it [00:02, 699.79it/s]warmup run: 903it [00:02, 840.47it/s]warmup run: 768it [00:02, 752.51it/s]warmup run: 787it [00:02, 776.39it/s]warmup run: 780it [00:02, 766.40it/s]warmup run: 1101it [00:02, 900.71it/s]warmup run: 853it [00:02, 787.72it/s]warmup run: 904it [00:02, 846.20it/s]warmup run: 822it [00:02, 758.55it/s]warmup run: 1003it [00:02, 881.42it/s]warmup run: 865it [00:02, 808.94it/s]warmup run: 887it [00:02, 833.18it/s]warmup run: 878it [00:02, 822.33it/s]warmup run: 1202it [00:02, 930.27it/s]warmup run: 946it [00:02, 822.75it/s]warmup run: 1005it [00:02, 890.55it/s]warmup run: 914it [00:02, 800.22it/s]warmup run: 1102it [00:02, 911.00it/s]warmup run: 961it [00:02, 849.62it/s]warmup run: 986it [00:02, 873.85it/s]warmup run: 978it [00:02, 869.81it/s]warmup run: 1303it [00:02, 952.18it/s]warmup run: 1040it [00:02, 853.53it/s]warmup run: 1108it [00:02, 927.61it/s]warmup run: 1006it [00:02, 833.20it/s]warmup run: 1201it [00:02, 929.38it/s]warmup run: 1059it [00:02, 885.71it/s]warmup run: 1084it [00:02, 900.22it/s]warmup run: 1077it [00:02, 901.32it/s]warmup run: 1405it [00:02, 970.90it/s]warmup run: 1135it [00:02, 879.07it/s]warmup run: 1210it [00:02, 953.35it/s]warmup run: 1097it [00:02, 854.79it/s]warmup run: 1300it [00:02, 934.79it/s]warmup run: 1158it [00:02, 913.44it/s]warmup run: 1182it [00:02, 920.42it/s]warmup run: 1177it [00:02, 926.68it/s]warmup run: 1506it [00:03, 981.26it/s]warmup run: 1229it [00:02, 896.13it/s]warmup run: 1312it [00:02, 970.19it/s]warmup run: 1189it [00:02, 872.68it/s]warmup run: 1398it [00:02, 944.58it/s]warmup run: 1255it [00:02, 925.91it/s]warmup run: 1280it [00:02, 932.14it/s]warmup run: 1276it [00:02, 940.47it/s]warmup run: 1608it [00:03, 990.16it/s]warmup run: 1323it [00:02, 907.24it/s]warmup run: 1415it [00:02, 984.84it/s]warmup run: 1280it [00:02, 872.96it/s]warmup run: 1497it [00:03, 955.25it/s]warmup run: 1353it [00:02, 940.32it/s]warmup run: 1378it [00:02, 946.02it/s]warmup run: 1375it [00:02, 952.73it/s]warmup run: 1710it [00:03, 997.05it/s]warmup run: 1417it [00:03, 914.71it/s]warmup run: 1518it [00:03, 997.55it/s]warmup run: 1373it [00:03, 887.58it/s]warmup run: 1597it [00:03, 966.57it/s]warmup run: 1451it [00:02, 949.99it/s]warmup run: 1477it [00:03, 956.71it/s]warmup run: 1473it [00:02, 956.93it/s]warmup run: 1812it [00:03, 1001.52it/s]warmup run: 1622it [00:03, 1007.16it/s]warmup run: 1511it [00:03, 918.27it/s]warmup run: 1470it [00:03, 910.73it/s]warmup run: 1696it [00:03, 967.55it/s]warmup run: 1548it [00:03, 953.43it/s]warmup run: 1576it [00:03, 966.07it/s]warmup run: 1571it [00:03, 958.16it/s]warmup run: 1915it [00:03, 1007.85it/s]warmup run: 1605it [00:03, 924.44it/s]warmup run: 1726it [00:03, 1014.26it/s]warmup run: 1566it [00:03, 922.79it/s]warmup run: 1794it [00:03, 966.36it/s]warmup run: 1645it [00:03, 951.75it/s]warmup run: 1675it [00:03, 970.89it/s]warmup run: 1669it [00:03, 951.33it/s]warmup run: 2021it [00:03, 1021.93it/s]warmup run: 1699it [00:03, 928.55it/s]warmup run: 1830it [00:03, 1019.33it/s]warmup run: 1662it [00:03, 931.64it/s]warmup run: 1892it [00:03, 965.65it/s]warmup run: 1742it [00:03, 955.13it/s]warmup run: 1774it [00:03, 970.81it/s]warmup run: 1766it [00:03, 940.13it/s]warmup run: 2144it [00:03, 1081.49it/s]warmup run: 1793it [00:03, 931.62it/s]warmup run: 1933it [00:03, 1017.97it/s]warmup run: 1757it [00:03, 936.57it/s]warmup run: 1990it [00:03, 962.23it/s]warmup run: 1841it [00:03, 965.34it/s]warmup run: 1872it [00:03, 972.10it/s]warmup run: 1862it [00:03, 945.79it/s]warmup run: 2266it [00:03, 1122.77it/s]warmup run: 1891it [00:03, 945.60it/s]warmup run: 2040it [00:03, 1032.93it/s]warmup run: 1853it [00:03, 941.51it/s]warmup run: 2101it [00:03, 1003.71it/s]warmup run: 1943it [00:03, 979.19it/s]warmup run: 1971it [00:03, 977.23it/s]warmup run: 1961it [00:03, 957.87it/s]warmup run: 2388it [00:03, 1151.60it/s]warmup run: 1991it [00:03, 961.77it/s]warmup run: 2159it [00:03, 1079.40it/s]warmup run: 1955it [00:03, 962.83it/s]warmup run: 2214it [00:03, 1040.98it/s]warmup run: 2052it [00:03, 1010.98it/s]warmup run: 2084it [00:03, 1021.84it/s]warmup run: 2065it [00:03, 980.79it/s]warmup run: 2510it [00:03, 1171.35it/s]warmup run: 2108it [00:03, 1023.91it/s]warmup run: 2278it [00:03, 1112.32it/s]warmup run: 2063it [00:03, 995.43it/s]warmup run: 2327it [00:03, 1067.23it/s]warmup run: 2173it [00:03, 1068.44it/s]warmup run: 2204it [00:03, 1072.56it/s]warmup run: 2182it [00:03, 1035.47it/s]warmup run: 2630it [00:04, 1177.46it/s]warmup run: 2228it [00:03, 1075.52it/s]warmup run: 2397it [00:03, 1135.19it/s]warmup run: 2187it [00:03, 1066.28it/s]warmup run: 2440it [00:03, 1085.04it/s]warmup run: 2294it [00:03, 1109.81it/s]warmup run: 2323it [00:03, 1105.44it/s]warmup run: 2305it [00:03, 1090.65it/s]warmup run: 2753it [00:04, 1190.79it/s]warmup run: 2348it [00:03, 1111.62it/s]warmup run: 2519it [00:03, 1158.28it/s]warmup run: 2311it [00:03, 1115.75it/s]warmup run: 2553it [00:04, 1098.03it/s]warmup run: 2415it [00:03, 1138.31it/s]warmup run: 2442it [00:03, 1128.08it/s]warmup run: 2428it [00:03, 1130.14it/s]warmup run: 2874it [00:04, 1195.75it/s]warmup run: 2468it [00:04, 1136.67it/s]warmup run: 2641it [00:04, 1175.29it/s]warmup run: 2435it [00:04, 1150.89it/s]warmup run: 2667it [00:04, 1108.00it/s]warmup run: 2536it [00:04, 1158.22it/s]warmup run: 2561it [00:04, 1144.48it/s]warmup run: 2550it [00:04, 1156.75it/s]warmup run: 2996it [00:04, 1202.57it/s]warmup run: 2588it [00:04, 1151.89it/s]warmup run: 2762it [00:04, 1184.89it/s]warmup run: 3000it [00:04, 691.12it/s] warmup run: 2559it [00:04, 1175.33it/s]warmup run: 2779it [00:04, 1109.72it/s]warmup run: 2656it [00:04, 1168.52it/s]warmup run: 2680it [00:04, 1157.02it/s]warmup run: 2672it [00:04, 1175.59it/s]warmup run: 2709it [00:04, 1168.47it/s]warmup run: 2883it [00:04, 1189.66it/s]warmup run: 2683it [00:04, 1192.76it/s]warmup run: 2893it [00:04, 1115.91it/s]warmup run: 2774it [00:04, 1171.61it/s]warmup run: 2798it [00:04, 1161.13it/s]warmup run: 2793it [00:04, 1183.49it/s]warmup run: 3000it [00:04, 690.65it/s] warmup run: 2826it [00:04, 1149.03it/s]warmup run: 2804it [00:04, 1197.05it/s]warmup run: 3000it [00:04, 675.46it/s] warmup run: 2893it [00:04, 1175.86it/s]warmup run: 2917it [00:04, 1169.66it/s]warmup run: 2913it [00:04, 1187.16it/s]warmup run: 2941it [00:04, 1147.48it/s]warmup run: 2924it [00:04, 1180.71it/s]warmup run: 3000it [00:04, 683.06it/s] warmup run: 3000it [00:04, 665.69it/s] warmup run: 3000it [00:04, 681.59it/s] warmup run: 3000it [00:04, 682.94it/s] warmup run: 3000it [00:04, 663.38it/s] 

warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]



warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   6%|         | 165/3000 [00:00<00:01, 1649.09it/s]warmup should be done:   5%|         | 163/3000 [00:00<00:01, 1625.64it/s]warmup should be done:   5%|         | 155/3000 [00:00<00:01, 1540.99it/s]warmup should be done:   5%|         | 156/3000 [00:00<00:01, 1555.39it/s]warmup should be done:   5%|         | 162/3000 [00:00<00:01, 1614.90it/s]warmup should be done:   5%|         | 162/3000 [00:00<00:01, 1613.83it/s]warmup should be done:   5%|         | 160/3000 [00:00<00:01, 1591.05it/s]warmup should be done:   5%|         | 163/3000 [00:00<00:01, 1619.61it/s]warmup should be done:  11%|         | 333/3000 [00:00<00:01, 1662.98it/s]warmup should be done:  10%|         | 310/3000 [00:00<00:01, 1545.71it/s]warmup should be done:  11%|         | 327/3000 [00:00<00:01, 1631.10it/s]warmup should be done:  11%|         | 326/3000 [00:00<00:01, 1627.42it/s]warmup should be done:  11%|         | 327/3000 [00:00<00:01, 1631.12it/s]warmup should be done:  11%|         | 321/3000 [00:00<00:01, 1598.90it/s]warmup should be done:  10%|         | 313/3000 [00:00<00:01, 1558.92it/s]warmup should be done:  11%|         | 325/3000 [00:00<00:01, 1617.43it/s]warmup should be done:  16%|        | 491/3000 [00:00<00:01, 1634.86it/s]warmup should be done:  16%|        | 489/3000 [00:00<00:01, 1627.57it/s]warmup should be done:  17%|        | 500/3000 [00:00<00:01, 1662.54it/s]warmup should be done:  16%|        | 469/3000 [00:00<00:01, 1557.41it/s]warmup should be done:  16%|        | 487/3000 [00:00<00:01, 1617.33it/s]warmup should be done:  16%|        | 481/3000 [00:00<00:01, 1596.20it/s]warmup should be done:  16%|        | 465/3000 [00:00<00:01, 1539.78it/s]warmup should be done:  16%|        | 491/3000 [00:00<00:01, 1625.94it/s]warmup should be done:  22%|       | 656/3000 [00:00<00:01, 1637.00it/s]warmup should be done:  22%|       | 667/3000 [00:00<00:01, 1662.02it/s]warmup should be done:  22%|       | 652/3000 [00:00<00:01, 1623.57it/s]warmup should be done:  22%|       | 649/3000 [00:00<00:01, 1616.17it/s]warmup should be done:  21%|        | 625/3000 [00:00<00:01, 1551.86it/s]warmup should be done:  21%|       | 641/3000 [00:00<00:01, 1591.06it/s]warmup should be done:  21%|        | 619/3000 [00:00<00:01, 1534.73it/s]warmup should be done:  22%|       | 654/3000 [00:00<00:01, 1623.09it/s]warmup should be done:  28%|       | 834/3000 [00:00<00:01, 1662.92it/s]warmup should be done:  27%|       | 820/3000 [00:00<00:01, 1632.53it/s]warmup should be done:  27%|       | 811/3000 [00:00<00:01, 1612.66it/s]warmup should be done:  27%|       | 815/3000 [00:00<00:01, 1619.81it/s]warmup should be done:  26%|       | 773/3000 [00:00<00:01, 1535.58it/s]warmup should be done:  26%|       | 781/3000 [00:00<00:01, 1551.07it/s]warmup should be done:  27%|       | 801/3000 [00:00<00:01, 1590.15it/s]warmup should be done:  27%|       | 817/3000 [00:00<00:01, 1621.45it/s]warmup should be done:  33%|      | 1001/3000 [00:00<00:01, 1660.99it/s]warmup should be done:  33%|      | 977/3000 [00:00<00:01, 1619.01it/s]warmup should be done:  33%|      | 984/3000 [00:00<00:01, 1631.39it/s]warmup should be done:  31%|      | 940/3000 [00:00<00:01, 1563.40it/s]warmup should be done:  31%|       | 931/3000 [00:00<00:01, 1547.54it/s]warmup should be done:  32%|      | 973/3000 [00:00<00:01, 1610.22it/s]warmup should be done:  32%|      | 961/3000 [00:00<00:01, 1586.91it/s]warmup should be done:  33%|      | 980/3000 [00:00<00:01, 1616.05it/s]warmup should be done:  38%|      | 1148/3000 [00:00<00:01, 1631.54it/s]warmup should be done:  37%|      | 1101/3000 [00:00<00:01, 1576.03it/s]warmup should be done:  36%|      | 1091/3000 [00:00<00:01, 1562.26it/s]warmup should be done:  39%|      | 1168/3000 [00:00<00:01, 1657.49it/s]warmup should be done:  38%|      | 1139/3000 [00:00<00:01, 1613.77it/s]warmup should be done:  38%|      | 1135/3000 [00:00<00:01, 1604.70it/s]warmup should be done:  37%|      | 1120/3000 [00:00<00:01, 1581.34it/s]warmup should be done:  38%|      | 1142/3000 [00:00<00:01, 1610.84it/s]warmup should be done:  44%|     | 1312/3000 [00:00<00:01, 1633.07it/s]warmup should be done:  42%|     | 1252/3000 [00:00<00:01, 1577.11it/s]warmup should be done:  44%|     | 1334/3000 [00:00<00:01, 1657.62it/s]warmup should be done:  42%|     | 1262/3000 [00:00<00:01, 1584.90it/s]warmup should be done:  43%|     | 1301/3000 [00:00<00:01, 1608.97it/s]warmup should be done:  43%|     | 1296/3000 [00:00<00:01, 1602.47it/s]warmup should be done:  43%|     | 1279/3000 [00:00<00:01, 1579.98it/s]warmup should be done:  43%|     | 1304/3000 [00:00<00:01, 1611.27it/s]warmup should be done:  49%|     | 1476/3000 [00:00<00:00, 1634.46it/s]warmup should be done:  50%|     | 1500/3000 [00:00<00:00, 1658.28it/s]warmup should be done:  47%|     | 1414/3000 [00:00<00:00, 1589.14it/s]warmup should be done:  47%|     | 1423/3000 [00:00<00:00, 1592.53it/s]warmup should be done:  49%|     | 1462/3000 [00:00<00:00, 1606.41it/s]warmup should be done:  49%|     | 1457/3000 [00:00<00:00, 1601.49it/s]warmup should be done:  48%|     | 1437/3000 [00:00<00:00, 1579.58it/s]warmup should be done:  49%|     | 1466/3000 [00:00<00:00, 1612.08it/s]warmup should be done:  56%|    | 1666/3000 [00:01<00:00, 1658.64it/s]warmup should be done:  55%|    | 1640/3000 [00:01<00:00, 1634.90it/s]warmup should be done:  53%|    | 1584/3000 [00:01<00:00, 1596.39it/s]warmup should be done:  53%|    | 1576/3000 [00:01<00:00, 1595.93it/s]warmup should be done:  54%|    | 1623/3000 [00:01<00:00, 1605.77it/s]warmup should be done:  53%|    | 1595/3000 [00:01<00:00, 1578.94it/s]warmup should be done:  54%|    | 1619/3000 [00:01<00:00, 1605.83it/s]warmup should be done:  54%|    | 1628/3000 [00:01<00:00, 1611.48it/s]warmup should be done:  61%|    | 1832/3000 [00:01<00:00, 1657.38it/s]warmup should be done:  60%|    | 1804/3000 [00:01<00:00, 1635.24it/s]warmup should be done:  58%|    | 1738/3000 [00:01<00:00, 1602.17it/s]warmup should be done:  58%|    | 1746/3000 [00:01<00:00, 1600.65it/s]warmup should be done:  59%|    | 1784/3000 [00:01<00:00, 1605.47it/s]warmup should be done:  59%|    | 1780/3000 [00:01<00:00, 1606.86it/s]warmup should be done:  58%|    | 1753/3000 [00:01<00:00, 1577.90it/s]warmup should be done:  60%|    | 1793/3000 [00:01<00:00, 1622.50it/s]warmup should be done:  67%|   | 1998/3000 [00:01<00:00, 1656.79it/s]warmup should be done:  66%|   | 1968/3000 [00:01<00:00, 1635.05it/s]warmup should be done:  63%|   | 1902/3000 [00:01<00:00, 1611.37it/s]warmup should be done:  64%|   | 1908/3000 [00:01<00:00, 1603.74it/s]warmup should be done:  65%|   | 1945/3000 [00:01<00:00, 1605.35it/s]warmup should be done:  64%|   | 1911/3000 [00:01<00:00, 1577.79it/s]warmup should be done:  65%|   | 1942/3000 [00:01<00:00, 1608.79it/s]warmup should be done:  65%|   | 1957/3000 [00:01<00:00, 1626.94it/s]warmup should be done:  72%|  | 2164/3000 [00:01<00:00, 1656.93it/s]warmup should be done:  71%|   | 2132/3000 [00:01<00:00, 1635.95it/s]warmup should be done:  69%|   | 2070/3000 [00:01<00:00, 1606.54it/s]warmup should be done:  70%|   | 2106/3000 [00:01<00:00, 1605.60it/s]warmup should be done:  69%|   | 2064/3000 [00:01<00:00, 1604.14it/s]warmup should be done:  69%|   | 2069/3000 [00:01<00:00, 1576.64it/s]warmup should be done:  70%|   | 2104/3000 [00:01<00:00, 1610.04it/s]warmup should be done:  71%|   | 2120/3000 [00:01<00:00, 1625.69it/s]warmup should be done:  78%|  | 2330/3000 [00:01<00:00, 1656.51it/s]warmup should be done:  77%|  | 2296/3000 [00:01<00:00, 1635.77it/s]warmup should be done:  74%|  | 2232/3000 [00:01<00:00, 1609.04it/s]warmup should be done:  76%|  | 2268/3000 [00:01<00:00, 1609.70it/s]warmup should be done:  74%|  | 2226/3000 [00:01<00:00, 1608.55it/s]warmup should be done:  74%|  | 2227/3000 [00:01<00:00, 1577.26it/s]warmup should be done:  76%|  | 2266/3000 [00:01<00:00, 1611.10it/s]warmup should be done:  76%|  | 2283/3000 [00:01<00:00, 1624.86it/s]warmup should be done:  83%| | 2496/3000 [00:01<00:00, 1652.28it/s]warmup should be done:  82%| | 2460/3000 [00:01<00:00, 1631.83it/s]warmup should be done:  80%|  | 2393/3000 [00:01<00:00, 1607.65it/s]warmup should be done:  80%|  | 2388/3000 [00:01<00:00, 1610.39it/s]warmup should be done:  81%|  | 2429/3000 [00:01<00:00, 1605.00it/s]warmup should be done:  80%|  | 2385/3000 [00:01<00:00, 1573.49it/s]warmup should be done:  81%|  | 2428/3000 [00:01<00:00, 1608.32it/s]warmup should be done:  82%| | 2446/3000 [00:01<00:00, 1466.09it/s]warmup should be done:  89%| | 2662/3000 [00:01<00:00, 1652.18it/s]warmup should be done:  87%| | 2624/3000 [00:01<00:00, 1631.86it/s]warmup should be done:  85%| | 2554/3000 [00:01<00:00, 1606.43it/s]warmup should be done:  85%| | 2551/3000 [00:01<00:00, 1615.05it/s]warmup should be done:  86%| | 2590/3000 [00:01<00:00, 1604.09it/s]warmup should be done:  85%| | 2543/3000 [00:01<00:00, 1574.20it/s]warmup should be done:  86%| | 2590/3000 [00:01<00:00, 1609.28it/s]warmup should be done:  87%| | 2606/3000 [00:01<00:00, 1501.81it/s]warmup should be done:  94%|| 2828/3000 [00:01<00:00, 1654.47it/s]warmup should be done:  93%|| 2788/3000 [00:01<00:00, 1634.11it/s]warmup should be done:  90%| | 2715/3000 [00:01<00:00, 1603.38it/s]warmup should be done:  90%| | 2714/3000 [00:01<00:00, 1617.75it/s]warmup should be done:  92%|| 2751/3000 [00:01<00:00, 1604.54it/s]warmup should be done:  90%| | 2701/3000 [00:01<00:00, 1574.87it/s]warmup should be done:  92%|| 2752/3000 [00:01<00:00, 1610.82it/s]warmup should be done:  92%|| 2766/3000 [00:01<00:00, 1528.23it/s]warmup should be done: 100%|| 2995/3000 [00:01<00:00, 1656.97it/s]warmup should be done:  98%|| 2955/3000 [00:01<00:00, 1642.68it/s]warmup should be done:  96%|| 2877/3000 [00:01<00:00, 1606.62it/s]warmup should be done:  96%|| 2878/3000 [00:01<00:00, 1623.03it/s]warmup should be done:  95%|| 2860/3000 [00:01<00:00, 1578.15it/s]warmup should be done:  97%|| 2914/3000 [00:01<00:00, 1609.48it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1657.18it/s]warmup should be done:  97%|| 2916/3000 [00:01<00:00, 1616.87it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1635.13it/s]warmup should be done:  98%|| 2929/3000 [00:01<00:00, 1555.14it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1610.68it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1610.47it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1593.06it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1592.35it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1586.62it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1581.22it/s]





warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   5%|         | 163/3000 [00:00<00:01, 1628.88it/s]warmup should be done:   6%|         | 167/3000 [00:00<00:01, 1669.19it/s]warmup should be done:   6%|         | 165/3000 [00:00<00:01, 1649.40it/s]warmup should be done:   6%|         | 167/3000 [00:00<00:01, 1665.46it/s]warmup should be done:   6%|         | 170/3000 [00:00<00:01, 1694.73it/s]warmup should be done:   6%|         | 165/3000 [00:00<00:01, 1645.87it/s]warmup should be done:   6%|         | 167/3000 [00:00<00:01, 1664.90it/s]warmup should be done:   6%|         | 166/3000 [00:00<00:01, 1653.78it/s]warmup should be done:  11%|         | 337/3000 [00:00<00:01, 1684.38it/s]warmup should be done:  11%|         | 334/3000 [00:00<00:01, 1666.09it/s]warmup should be done:  11%|         | 330/3000 [00:00<00:01, 1646.70it/s]warmup should be done:  11%|         | 326/3000 [00:00<00:01, 1624.30it/s]warmup should be done:  11%|         | 330/3000 [00:00<00:01, 1645.00it/s]warmup should be done:  11%|        | 340/3000 [00:00<00:01, 1693.47it/s]warmup should be done:  11%|         | 333/3000 [00:00<00:01, 1659.78it/s]warmup should be done:  11%|         | 336/3000 [00:00<00:01, 1674.36it/s]warmup should be done:  17%|        | 496/3000 [00:00<00:01, 1652.35it/s]warmup should be done:  17%|        | 507/3000 [00:00<00:01, 1688.72it/s]warmup should be done:  17%|        | 502/3000 [00:00<00:01, 1668.82it/s]warmup should be done:  16%|        | 490/3000 [00:00<00:01, 1628.75it/s]warmup should be done:  17%|        | 496/3000 [00:00<00:01, 1648.55it/s]warmup should be done:  17%|        | 501/3000 [00:00<00:01, 1665.34it/s]warmup should be done:  17%|        | 504/3000 [00:00<00:01, 1674.25it/s]warmup should be done:  17%|        | 510/3000 [00:00<00:01, 1691.75it/s]warmup should be done:  22%|       | 663/3000 [00:00<00:01, 1656.84it/s]warmup should be done:  22%|       | 654/3000 [00:00<00:01, 1632.85it/s]warmup should be done:  22%|       | 670/3000 [00:00<00:01, 1672.56it/s]warmup should be done:  23%|       | 678/3000 [00:00<00:01, 1694.22it/s]warmup should be done:  22%|       | 672/3000 [00:00<00:01, 1675.83it/s]warmup should be done:  22%|       | 668/3000 [00:00<00:01, 1665.93it/s]warmup should be done:  22%|       | 663/3000 [00:00<00:01, 1653.41it/s]warmup should be done:  23%|       | 681/3000 [00:00<00:01, 1695.46it/s]warmup should be done:  28%|       | 829/3000 [00:00<00:01, 1657.82it/s]warmup should be done:  28%|       | 838/3000 [00:00<00:01, 1672.90it/s]warmup should be done:  28%|       | 835/3000 [00:00<00:01, 1666.85it/s]warmup should be done:  27%|       | 818/3000 [00:00<00:01, 1632.35it/s]warmup should be done:  28%|       | 850/3000 [00:00<00:01, 1700.24it/s]warmup should be done:  28%|       | 829/3000 [00:00<00:01, 1654.53it/s]warmup should be done:  28%|       | 852/3000 [00:00<00:01, 1698.43it/s]warmup should be done:  28%|       | 840/3000 [00:00<00:01, 1672.05it/s]warmup should be done:  33%|      | 995/3000 [00:00<00:01, 1656.75it/s]warmup should be done:  33%|      | 1003/3000 [00:00<00:01, 1668.07it/s]warmup should be done:  33%|      | 982/3000 [00:00<00:01, 1631.04it/s]warmup should be done:  34%|      | 1021/3000 [00:00<00:01, 1699.93it/s]warmup should be done:  34%|      | 1006/3000 [00:00<00:01, 1669.90it/s]warmup should be done:  33%|      | 995/3000 [00:00<00:01, 1652.03it/s]warmup should be done:  34%|      | 1022/3000 [00:00<00:01, 1695.69it/s]warmup should be done:  34%|      | 1008/3000 [00:00<00:01, 1667.42it/s]warmup should be done:  39%|      | 1161/3000 [00:00<00:01, 1657.67it/s]warmup should be done:  39%|      | 1170/3000 [00:00<00:01, 1668.04it/s]warmup should be done:  38%|      | 1146/3000 [00:00<00:01, 1633.59it/s]warmup should be done:  39%|      | 1161/3000 [00:00<00:01, 1652.88it/s]warmup should be done:  39%|      | 1173/3000 [00:00<00:01, 1666.28it/s]warmup should be done:  40%|      | 1191/3000 [00:00<00:01, 1695.06it/s]warmup should be done:  40%|      | 1192/3000 [00:00<00:01, 1691.74it/s]warmup should be done:  39%|      | 1175/3000 [00:00<00:01, 1663.98it/s]warmup should be done:  44%|     | 1327/3000 [00:00<00:01, 1656.07it/s]warmup should be done:  45%|     | 1337/3000 [00:00<00:00, 1665.21it/s]warmup should be done:  45%|     | 1340/3000 [00:00<00:00, 1667.25it/s]warmup should be done:  44%|     | 1310/3000 [00:00<00:01, 1629.85it/s]warmup should be done:  44%|     | 1327/3000 [00:00<00:01, 1649.95it/s]warmup should be done:  45%|     | 1361/3000 [00:00<00:00, 1691.85it/s]warmup should be done:  45%|     | 1363/3000 [00:00<00:00, 1695.37it/s]warmup should be done:  45%|     | 1342/3000 [00:00<00:00, 1664.32it/s]warmup should be done:  50%|     | 1493/3000 [00:00<00:00, 1656.81it/s]warmup should be done:  50%|     | 1507/3000 [00:00<00:00, 1665.83it/s]warmup should be done:  50%|     | 1504/3000 [00:00<00:00, 1663.14it/s]warmup should be done:  49%|     | 1474/3000 [00:00<00:00, 1631.66it/s]warmup should be done:  51%|     | 1533/3000 [00:00<00:00, 1696.37it/s]warmup should be done:  50%|     | 1493/3000 [00:00<00:00, 1651.01it/s]warmup should be done:  50%|     | 1509/3000 [00:00<00:00, 1663.56it/s]warmup should be done:  51%|     | 1531/3000 [00:00<00:00, 1680.01it/s]warmup should be done:  55%|    | 1659/3000 [00:01<00:00, 1657.05it/s]warmup should be done:  56%|    | 1671/3000 [00:01<00:00, 1664.85it/s]warmup should be done:  56%|    | 1675/3000 [00:01<00:00, 1668.64it/s]warmup should be done:  55%|    | 1639/3000 [00:01<00:00, 1635.05it/s]warmup should be done:  57%|    | 1704/3000 [00:01<00:00, 1697.83it/s]warmup should be done:  55%|    | 1660/3000 [00:01<00:00, 1653.84it/s]warmup should be done:  56%|    | 1676/3000 [00:01<00:00, 1664.66it/s]warmup should be done:  57%|    | 1700/3000 [00:01<00:00, 1679.54it/s]warmup should be done:  61%|    | 1825/3000 [00:01<00:00, 1655.71it/s]warmup should be done:  61%|   | 1843/3000 [00:01<00:00, 1671.49it/s]warmup should be done:  60%|    | 1803/3000 [00:01<00:00, 1635.94it/s]warmup should be done:  61%|   | 1838/3000 [00:01<00:00, 1663.03it/s]warmup should be done:  62%|   | 1875/3000 [00:01<00:00, 1700.06it/s]warmup should be done:  61%|    | 1826/3000 [00:01<00:00, 1654.63it/s]warmup should be done:  61%|   | 1843/3000 [00:01<00:00, 1665.02it/s]warmup should be done:  62%|   | 1868/3000 [00:01<00:00, 1679.01it/s]warmup should be done:  66%|   | 1991/3000 [00:01<00:00, 1650.31it/s]warmup should be done:  67%|   | 2011/3000 [00:01<00:00, 1670.32it/s]warmup should be done:  66%|   | 1967/3000 [00:01<00:00, 1634.24it/s]warmup should be done:  67%|   | 2005/3000 [00:01<00:00, 1660.60it/s]warmup should be done:  68%|   | 2046/3000 [00:01<00:00, 1699.79it/s]warmup should be done:  66%|   | 1992/3000 [00:01<00:00, 1652.72it/s]warmup should be done:  67%|   | 2010/3000 [00:01<00:00, 1661.96it/s]warmup should be done:  68%|   | 2036/3000 [00:01<00:00, 1677.18it/s]warmup should be done:  72%|  | 2157/3000 [00:01<00:00, 1648.04it/s]warmup should be done:  71%|   | 2131/3000 [00:01<00:00, 1635.87it/s]warmup should be done:  73%|  | 2179/3000 [00:01<00:00, 1668.22it/s]warmup should be done:  74%|  | 2216/3000 [00:01<00:00, 1697.43it/s]warmup should be done:  72%|  | 2172/3000 [00:01<00:00, 1657.78it/s]warmup should be done:  72%|  | 2158/3000 [00:01<00:00, 1652.02it/s]warmup should be done:  73%|  | 2177/3000 [00:01<00:00, 1657.53it/s]warmup should be done:  73%|  | 2204/3000 [00:01<00:00, 1675.52it/s]warmup should be done:  77%|  | 2323/3000 [00:01<00:00, 1649.99it/s]warmup should be done:  77%|  | 2296/3000 [00:01<00:00, 1637.18it/s]warmup should be done:  78%|  | 2347/3000 [00:01<00:00, 1669.38it/s]warmup should be done:  78%|  | 2325/3000 [00:01<00:00, 1656.73it/s]warmup should be done:  78%|  | 2339/3000 [00:01<00:00, 1658.90it/s]warmup should be done:  80%|  | 2386/3000 [00:01<00:00, 1690.15it/s]warmup should be done:  78%|  | 2344/3000 [00:01<00:00, 1658.66it/s]warmup should be done:  79%|  | 2372/3000 [00:01<00:00, 1540.79it/s]warmup should be done:  83%| | 2489/3000 [00:01<00:00, 1651.46it/s]warmup should be done:  82%| | 2460/3000 [00:01<00:00, 1635.94it/s]warmup should be done:  84%| | 2515/3000 [00:01<00:00, 1671.38it/s]warmup should be done:  83%| | 2493/3000 [00:01<00:00, 1662.97it/s]warmup should be done:  84%| | 2506/3000 [00:01<00:00, 1659.62it/s]warmup should be done:  85%| | 2556/3000 [00:01<00:00, 1686.39it/s]warmup should be done:  84%| | 2510/3000 [00:01<00:00, 1652.88it/s]warmup should be done:  85%| | 2537/3000 [00:01<00:00, 1570.16it/s]warmup should be done:  88%| | 2655/3000 [00:01<00:00, 1648.31it/s]warmup should be done:  87%| | 2624/3000 [00:01<00:00, 1633.71it/s]warmup should be done:  89%| | 2661/3000 [00:01<00:00, 1666.46it/s]warmup should be done:  89%| | 2683/3000 [00:01<00:00, 1670.66it/s]warmup should be done:  89%| | 2673/3000 [00:01<00:00, 1662.59it/s]warmup should be done:  91%| | 2725/3000 [00:01<00:00, 1684.41it/s]warmup should be done:  89%| | 2676/3000 [00:01<00:00, 1641.70it/s]warmup should be done:  90%| | 2701/3000 [00:01<00:00, 1589.30it/s]warmup should be done:  93%|| 2788/3000 [00:01<00:00, 1634.28it/s]warmup should be done:  94%|| 2820/3000 [00:01<00:00, 1646.25it/s]warmup should be done:  94%|| 2829/3000 [00:01<00:00, 1668.09it/s]warmup should be done:  95%|| 2841/3000 [00:01<00:00, 1666.16it/s]warmup should be done:  96%|| 2894/3000 [00:01<00:00, 1680.83it/s]warmup should be done:  95%|| 2841/3000 [00:01<00:00, 1638.47it/s]warmup should be done:  95%|| 2851/3000 [00:01<00:00, 1640.19it/s]warmup should be done:  95%|| 2864/3000 [00:01<00:00, 1599.68it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1691.05it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1664.59it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1664.12it/s]warmup should be done:  98%|| 2953/3000 [00:01<00:00, 1637.20it/s]warmup should be done: 100%|| 2986/3000 [00:01<00:00, 1649.59it/s]warmup should be done: 100%|| 2998/3000 [00:01<00:00, 1672.22it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1658.03it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1657.26it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1651.98it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1646.78it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1633.58it/s]
WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7ff49d782730>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7ff49d784d30>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7ff49c44d1f0>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7ff49c44d160>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7ff49c45b2b0>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7ff49d781e80>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7ff49c4500d0>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7ff49c44f1c0>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
2022-12-12 04:24:25.160251: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fefcf02d990 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 04:24:25.160320: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 04:24:25.168264: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 04:24:25.750599: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fefca830130 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 04:24:25.750661: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 04:24:25.760279: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 04:24:25.782235: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fefc302d5b0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 04:24:25.782294: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 04:24:25.791115: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 04:24:25.877316: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fefc2f92870 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 04:24:25.877378: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 04:24:25.886704: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 04:24:26.033050: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fefc3029d10 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 04:24:26.033120: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 04:24:26.038065: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fefc302d450 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 04:24:26.038116: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 04:24:26.039543: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fefc6834540 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 04:24:26.039586: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 04:24:26.042739: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 04:24:26.047313: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 04:24:26.048359: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 04:24:26.054057: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fefbf031520 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 04:24:26.054121: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 04:24:26.063032: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 04:24:32.407453: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 04:24:32.610728: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 04:24:32.683194: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 04:24:32.895088: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 04:24:32.931902: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 04:24:32.933889: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 04:24:32.953600: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 04:24:32.964849: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
[HCTR][04:25:24.247][ERROR][RK0][tid #140668785628928]: replica 5 reaches 1000, calling init pre replica
[HCTR][04:25:24.247][ERROR][RK0][tid #140668785628928]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][04:25:24.255][ERROR][RK0][tid #140668785628928]: coll ps creation done
[HCTR][04:25:24.255][ERROR][RK0][tid #140668785628928]: replica 5 waits for coll ps creation barrier
[HCTR][04:25:24.300][ERROR][RK0][main]: replica 7 reaches 1000, calling init pre replica
[HCTR][04:25:24.300][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][04:25:24.305][ERROR][RK0][main]: coll ps creation done
[HCTR][04:25:24.305][ERROR][RK0][main]: replica 7 waits for coll ps creation barrier
[HCTR][04:25:24.505][ERROR][RK0][tid #140668324259584]: replica 3 reaches 1000, calling init pre replica
[HCTR][04:25:24.505][ERROR][RK0][tid #140668324259584]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][04:25:24.513][ERROR][RK0][tid #140668324259584]: coll ps creation done
[HCTR][04:25:24.513][ERROR][RK0][tid #140668324259584]: replica 3 waits for coll ps creation barrier
[HCTR][04:25:24.514][ERROR][RK0][tid #140668324259584]: replica 4 reaches 1000, calling init pre replica
[HCTR][04:25:24.514][ERROR][RK0][tid #140668324259584]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][04:25:24.523][ERROR][RK0][tid #140668324259584]: coll ps creation done
[HCTR][04:25:24.523][ERROR][RK0][tid #140668324259584]: replica 4 waits for coll ps creation barrier
[HCTR][04:25:24.590][ERROR][RK0][tid #140668391368448]: replica 6 reaches 1000, calling init pre replica
[HCTR][04:25:24.590][ERROR][RK0][tid #140668391368448]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][04:25:24.590][ERROR][RK0][tid #140668592695040]: replica 2 reaches 1000, calling init pre replica
[HCTR][04:25:24.590][ERROR][RK0][tid #140668592695040]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][04:25:24.594][ERROR][RK0][tid #140668391368448]: coll ps creation done
[HCTR][04:25:24.594][ERROR][RK0][tid #140668391368448]: replica 6 waits for coll ps creation barrier
[HCTR][04:25:24.597][ERROR][RK0][tid #140668592695040]: coll ps creation done
[HCTR][04:25:24.597][ERROR][RK0][tid #140668592695040]: replica 2 waits for coll ps creation barrier
[HCTR][04:25:24.608][ERROR][RK0][tid #140668391368448]: replica 0 reaches 1000, calling init pre replica
[HCTR][04:25:24.608][ERROR][RK0][tid #140668391368448]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][04:25:24.615][ERROR][RK0][tid #140668391368448]: coll ps creation done
[HCTR][04:25:24.615][ERROR][RK0][tid #140668391368448]: replica 0 waits for coll ps creation barrier
[HCTR][04:25:24.721][ERROR][RK0][main]: replica 1 reaches 1000, calling init pre replica
[HCTR][04:25:24.721][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][04:25:24.730][ERROR][RK0][main]: coll ps creation done
[HCTR][04:25:24.730][ERROR][RK0][main]: replica 1 waits for coll ps creation barrier
[HCTR][04:25:24.730][ERROR][RK0][tid #140668391368448]: replica 0 preparing frequency
[HCTR][04:25:25.686][ERROR][RK0][tid #140668391368448]: replica 0 preparing frequency done
[HCTR][04:25:25.725][ERROR][RK0][tid #140668391368448]: replica 0 calling init per replica
[HCTR][04:25:25.725][ERROR][RK0][tid #140668324259584]: replica 3 calling init per replica
[HCTR][04:25:25.725][ERROR][RK0][tid #140668324259584]: replica 4 calling init per replica
[HCTR][04:25:25.725][ERROR][RK0][tid #140668592695040]: replica 2 calling init per replica
[HCTR][04:25:25.725][ERROR][RK0][tid #140668391368448]: replica 6 calling init per replica
[HCTR][04:25:25.725][ERROR][RK0][main]: replica 1 calling init per replica
[HCTR][04:25:25.725][ERROR][RK0][tid #140668391368448]: Calling build_v2
[HCTR][04:25:25.725][ERROR][RK0][main]: replica 7 calling init per replica
[HCTR][04:25:25.725][ERROR][RK0][tid #140668785628928]: replica 5 calling init per replica
[HCTR][04:25:25.725][ERROR][RK0][tid #140668391368448]: Calling build_v2
[HCTR][04:25:25.725][ERROR][RK0][tid #140668324259584]: Calling build_v2
[HCTR][04:25:25.725][ERROR][RK0][tid #140668324259584]: Calling build_v2
[HCTR][04:25:25.725][ERROR][RK0][tid #140668592695040]: Calling build_v2
[HCTR][04:25:25.725][ERROR][RK0][main]: Calling build_v2
[HCTR][04:25:25.725][ERROR][RK0][tid #140668391368448]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][04:25:25.725][ERROR][RK0][tid #140668324259584]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][04:25:25.725][ERROR][RK0][main]: Calling build_v2
[HCTR][04:25:25.725][ERROR][RK0][tid #140668391368448]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][04:25:25.725][ERROR][RK0][tid #140668785628928]: Calling build_v2
[HCTR][04:25:25.725][ERROR][RK0][tid #140668324259584]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][04:25:25.725][ERROR][RK0][tid #140668592695040]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][04:25:25.725][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][04:25:25.725][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][04:25:25.725][ERROR][RK0][tid #140668785628928]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[[[[[[2022-12-12 04:25:25[[2022-12-12 04:25:252022-12-12 04:25:25.2022-12-12 04:25:252022-12-12 04:25:25..2022-12-12 04:25:252022-12-12 04:25:25725847.2022-12-12 04:25:25.725854725849..: 725850.725849: : 725862725868E: 725870: EE: :  E: E  EE/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc E /hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc  :/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc /hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc::/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc136:/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc:136136::] 136:136] ] 136136using concurrent impl MPS] 136] using concurrent impl MPSusing concurrent impl MPS] ] 
using concurrent impl MPS] using concurrent impl MPS

using concurrent impl MPSusing concurrent impl MPS
using concurrent impl MPS



[2022-12-12 04:25:25.730081: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:178] v100x8, slow pcie
[2022-12-12 04:25:25.730121: E[ 2022-12-12 04:25:25/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.:730125196: ] Eassigning 8 to cpu 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:178] v100x8, slow pcie
[[2022-12-12 04:25:252022-12-12 04:25:25..730176730172: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[::2022-12-12 04:25:25196[178.] 2022-12-12 04:25:25] 730204assigning 8 to cpu.v100x8, slow pcie: 
730219
E:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[ :2022-12-12 04:25:25/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc212[.:] 2022-12-12 04:25:25730262178build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8.: [] 
730268E2022-12-12 04:25:25v100x8, slow pcie:  .
E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc730290[ [:: [2022-12-12 04:25:25/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 04:25:25196E2022-12-12 04:25:25.:.]  .730319178730326assigning 8 to cpu/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc730330: ] : [
:: Ev100x8, slow pcieE2022-12-12 04:25:25212E 
 [.]  /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 04:25:25[730366build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:2022-12-12 04:25:25:.2022-12-12 04:25:25: 
:178.213730415.E196] 730409] [: 730421 ] v100x8, slow pcie: remote time is 8.684212022-12-12 04:25:25E: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccassigning 8 to cpu
E
. E[:
 [730488/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc 2022-12-12 04:25:25178/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 04:25:25: :/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.] :[.E212:730561v100x8, slow pcie1782022-12-12 04:25:25730554 ] 196: 
] .: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccbuild asymm link desc with 8X Tesla V100-SXM2-32GB out of 8] Ev100x8, slow pcie730594[E:
assigning 8 to cpu 
: 2022-12-12 04:25:25 213
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[E.[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] :2022-12-12 04:25:25 7306522022-12-12 04:25:25:remote time is 8.68421214./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: .196[
] 730692:[E730696] 2022-12-12 04:25:25cpu time is 97.0588: 2122022-12-12 04:25:25 : assigning 8 to cpu.
E] ./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE
730736 build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8730765: : /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
: 196/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE:E] :[[ 213 assigning 8 to cpu1962022-12-12 04:25:252022-12-12 04:25:25/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
] ..:remote time is 8.68421:assigning 8 to cpu730848730849212
214
: : ] ] [E[Ebuild asymm link desc with 8X Tesla V100-SXM2-32GB out of 8cpu time is 97.05882022-12-12 04:25:25 2022-12-12 04:25:25 

./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc730931:[730934[:: 2122022-12-12 04:25:25: 2022-12-12 04:25:25213E] .E.]  build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8730978 730980remote time is 8.68421/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: 
:E:E[214 [212 2022-12-12 04:25:25] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 04:25:25] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.cpu time is 97.0588:.build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8:731065
212731074
213: ] : ] Ebuild asymm link desc with 8X Tesla V100-SXM2-32GB out of 8Eremote time is 8.68421[ 
 
2022-12-12 04:25:25/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.::[[7311912132142022-12-12 04:25:252022-12-12 04:25:25: ] ] ..Eremote time is 8.68421cpu time is 97.0588731232731231 

: : /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE[E: 2022-12-12 04:25:25 213/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] :731313:remote time is 8.68421214: 213
] E] cpu time is 97.0588 [remote time is 8.68421
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 04:25:25
:.214731395[] : 2022-12-12 04:25:25cpu time is 97.0588E.
 731434/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: :E214 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cccpu time is 97.0588:
214] cpu time is 97.0588
[2022-12-12 04:26:43.840442: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:118] solver created. now build & solve
[2022-12-12 04:26:43.890017: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:123] solver built. now solve
[2022-12-12 04:26:43.890130: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:919] num_cached_nodes = 15000000
[2022-12-12 04:26:44. 10925: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:127] solver solved
[2022-12-12 04:26:44. 11020: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:195] 0 solved master
[2022-12-12 04:26:44. 16262: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 0 solved
[2022-12-12 04:26:44. 16296: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 0 initing device 0
[2022-12-12 04:26:44. 16727: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 04:26:44. 17677: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 04:26:44. 18473: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 04:26:44. 31475: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 5 solved
[2022-12-12 04:26:44. 31534: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 5 initing device 5
[[2022-12-12 04:26:442022-12-12 04:26:44.. 31731 31738: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc::202202] ] 1 solved6 solved

[[2022-12-12 04:26:442022-12-12 04:26:44.. 31818 31820: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc::205205] ] worker 0 thread 1 initing device 1worker 0 thread 6 initing device 6

[2022-12-12 04:26:44. 31908: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[[2022-12-12 04:26:442022-12-12 04:26:44.. 32238 32240: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::19801980] ] eager alloc mem 381.47 MBeager alloc mem 381.47 MB

[2022-12-12 04:26:44.[ 333262022-12-12 04:26:44: .E 33359 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.ccE: 202/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc] :3 solved202
] 4 solved
[2022-12-12 04:26:44. 33461[: 2022-12-12 04:26:44E.  33468/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc: :E205 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.ccworker 0 thread 3 initing device 3:
205] worker 0 thread 4 initing device 4
[2022-12-12 04:26:44. 33920: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB[
2022-12-12 04:26:44. 33941: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 04:26:44. 34015: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 04:26:44. 34117: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 04:26:44. 34161: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 04:26:44. 37278: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 04:26:44. 37383: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB[
2022-12-12 04:26:44. 37413: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 04:26:44. 37471: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 04:26:44. 37515: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 04:26:44. 38136: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 7 solved
[[2022-12-12 04:26:442022-12-12 04:26:44.. 38167 38194: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc::205202] ] worker 0 thread 7 initing device 72 solved

[2022-12-12 04:26:44. 38293: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 2 initing device 2
[2022-12-12 04:26:44. 38642: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 04:26:44. 38685: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 04:26:44. 41551: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 04:26:44. 41611: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 04:26:44. 42942: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 04:26:44. 42988: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 04:26:44. 44774: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 04:26:44. 44820: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 04:26:44.100257: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 2.00 Bytes
[2022-12-12 04:26:44.100667: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1024.00 Bytes
[2022-12-12 04:26:44.106220: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-12 04:26:44.106327: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 2
[2022-12-12 04:26:44.106373: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 04:26:44.107198: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 57.60 MB
[2022-12-12 04:26:44.107796: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:26:44.108862: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:26:44.108955: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 04:26:44.109631: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 04:26:44.109692: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 7.15 GB
[[[[2022-12-12 04:26:442022-12-12 04:26:442022-12-12 04:26:442022-12-12 04:26:44....120466120467120466120475: : : : EEEE    /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::::1980198019801980] ] ] ] eager alloc mem 2.00 Byteseager alloc mem 2.00 Byteseager alloc mem 2.00 Byteseager alloc mem 2.00 Bytes



[[2022-12-12 04:26:442022-12-12 04:26:44.[.1208852022-12-12 04:26:44[120885: .[2022-12-12 04:26:44: E1208922022-12-12 04:26:44.E : .120898 /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE120887: /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu: : E:1980/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE 1980] : /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu] eager alloc mem 1024.00 Bytes1980/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:eager alloc mem 1024.00 Bytes
] :1980
eager alloc mem 1024.00 Bytes1980] 
] eager alloc mem 1024.00 Byteseager alloc mem 2.00 Bytes

[2022-12-12 04:26:44.121285: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1024.00 Bytes
[2022-12-12 04:26:44.127140: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-12 04:26:44.127229: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 2
[2022-12-12 04:26:44.127281: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 04:26:44.127342: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-12 04:26:44.127418: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc[:2022-12-12 04:26:44638.] 127418eager release cuda mem 2: 
E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-12 04:26:44.127474: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 04:26:44[.2022-12-12 04:26:44127498.: 127490E:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc :/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc638:] 638eager release cuda mem 2] 
eager release cuda mem 1024
[2022-12-12 04:26:44.127569: E [/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc2022-12-12 04:26:44:.638127580] : eager release cuda mem 400000000E
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 2
[[2022-12-12 04:26:442022-12-12 04:26:44..127617127635: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::638638] ] eager release cuda mem 1024eager release cuda mem 400000000

[2022-12-12 04:26:44.127710: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 2
[2022-12-12 04:26:44.127753: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 04:26:44.128177: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 57.60 MB
[[2022-12-12 04:26:442022-12-12 04:26:44..128393128393: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::19801980] ] eager alloc mem 2.00 Byteseager alloc mem 2.00 Bytes

[2022-12-12 04:26:44.128752: [E2022-12-12 04:26:44 ./hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu128760:: 1980E]  eager alloc mem 1024.00 Bytes/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu
:1980] eager alloc mem 1024.00 Bytes
[2022-12-12 04:26:44.129211: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 57.60 MB
[2022-12-12 04:26:44.130008: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 57.60 MB
[2022-12-12 04:26:44.130615: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 57.60 MB
[2022-12-12 04:26:44.131118: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 57.60 MB
[2022-12-12 04:26:44.131828: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:26:44.132816: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:26:44.132876: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:26:44.132978: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 04:26:44.133212: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB[
2022-12-12 04:26:44.133239: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:26:44.133344: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:26:44.133654: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 04:26:44.133696: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 7.15 GB
[2022-12-12 04:26:44.133838: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:26:44.133922: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 04:26:44.134253: E[ 2022-12-12 04:26:44/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc.:134266638: ] Eeager release cuda mem 625663 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:26:44.134352: [E2022-12-12 04:26:44 .[/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu1343612022-12-12 04:26:44:: .1980E134369]  : eager alloc mem 25.25 KB/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE
: 1980/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc] :eager alloc mem 25.25 KB638
] eager release cuda mem 625663
[2022-12-12 04:26:44.134484: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 04:26:44.134591: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 04:26:44.134631: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 7.15 GB
[2022-12-12 04:26:44.134678: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-12 04:26:44.134747: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 2
[[2022-12-12 04:26:442022-12-12 04:26:44..134788134776: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::638638] ] eager release cuda mem 400000000eager release cuda mem 1024

[2022-12-12 04:26:44.134870: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 2
[2022-12-12 04:26:44.134914: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 04:26:44.135051: [E2022-12-12 04:26:44 ./hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc135062:: 638E]  eager release cuda mem 25855/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc
:638] eager release cuda mem 25855
[2022-12-12 04:26:44.135119: E[ 2022-12-12 04:26:44/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu.:1351391980: ] Eeager alloc mem 7.15 GB 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu[:2022-12-12 04:26:441980.] 135166eager alloc mem 7.15 GB: 
E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 04:26:44.135217: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 7.15 GB
[2022-12-12 04:26:44.176378: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 57.60 MB
[2022-12-12 04:26:44.176887: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 57.60 MB
[2022-12-12 04:26:44.178565: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:26:44.178598: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:26:44.179597: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:26:44.179622: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:26:44.179685: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 04:26:44.179712: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 04:26:44.180356: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 04:26:44.180384: E[ 2022-12-12 04:26:44/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc.:180397638: ] Eeager release cuda mem 25855 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 7.15 GB
[2022-12-12 04:26:44.180443: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 7.15 GB
[[[[[[[[2022-12-12 04:26:452022-12-12 04:26:452022-12-12 04:26:452022-12-12 04:26:452022-12-12 04:26:452022-12-12 04:26:452022-12-12 04:26:452022-12-12 04:26:45........631737631743631737631738631742631739631750631750: : : : : : : : EEEEEEEE        /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::::::::19801980198019801980198019801980] ] ] ] ] ] ] ] eager alloc mem 611.00 KBeager alloc mem 611.00 KBeager alloc mem 611.00 KBeager alloc mem 611.00 KBeager alloc mem 611.00 KBeager alloc mem 611.00 KBeager alloc mem 611.00 KBeager alloc mem 611.00 KB







[2022-12-12 04:26:45[.[2022-12-12 04:26:45[632865[[2022-12-12 04:26:45.[2022-12-12 04:26:45: 2022-12-12 04:26:452022-12-12 04:26:45[.6328702022-12-12 04:26:45.E..2022-12-12 04:26:45632873: .632874 632876632876.: E632882: /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc: : 632892E : E:EE:  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccE 638  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc: /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc] /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc :638/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:eager release cuda mem 625663::/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc638] :638
638638:] eager release cuda mem 625663638] ] ] 638eager release cuda mem 625663
] eager release cuda mem 625663[eager release cuda mem 625663eager release cuda mem 625663] 
eager release cuda mem 625663
2022-12-12 04:26:45

eager release cuda mem 625663
.
[6331622022-12-12 04:26:45: .E633202 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE: 1980/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu] :[eager alloc mem 611.00 KB19802022-12-12 04:26:45
[] .[[2022-12-12 04:26:45eager alloc mem 611.00 KB[6332412022-12-12 04:26:452022-12-12 04:26:45.
[2022-12-12 04:26:45: ..6332482022-12-12 04:26:45.E633251633253: .633257 : : E633269: /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuEE : E:  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE 1980/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu: /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu] ::1980/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:eager alloc mem 611.00 KB19801980] :1980
] ] eager alloc mem 611.00 KB1980] eager alloc mem 611.00 KBeager alloc mem 611.00 KB
] eager alloc mem 611.00 KB

eager alloc mem 611.00 KB

[2022-12-12 04:26:45.633981: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:26:45.634016: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:26:45.634055: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:26:45.634086: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:26:45.634155: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:26:45.[6341812022-12-12 04:26:45: .[E[6341892022-12-12 04:26:45[ 2022-12-12 04:26:45: .2022-12-12 04:26:45/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc.E[634196.:634198 2022-12-12 04:26:45: 634202638: /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc.E: ] E:634235 Eeager release cuda mem 625663 638: /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc] E:/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:eager release cuda mem 625663 638:638
/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu] 638] :eager release cuda mem 625663] eager release cuda mem 6256631980[
eager release cuda mem 625663
] 2022-12-12 04:26:45
eager alloc mem 611.00 KB.[
6343932022-12-12 04:26:45: .E634424 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE: 1980/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu] [:eager alloc mem 611.00 KB[2022-12-12 04:26:451980
2022-12-12 04:26:45[.] .2022-12-12 04:26:45634459eager alloc mem 611.00 KB634463.: 
: 634475EE:   E/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu ::/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu19801980:] ] 1980eager alloc mem 611.00 KBeager alloc mem 611.00 KB] 

eager alloc mem 611.00 KB
[2022-12-12 04:26:45.634804: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:26:45.634840: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:26:45.634875: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:26:45.634909: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:26:45.635164: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:26:45.635213: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:26:45.635241[: 2022-12-12 04:26:45E. 635247/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu: :E1980 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc[eager alloc mem 611.00 KB:2022-12-12 04:26:45
638.] 635285eager release cuda mem 625663: 
E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:26:45[.2022-12-12 04:26:45[635332.2022-12-12 04:26:45: [635336.E2022-12-12 04:26:45: 635342 .E: /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc635356 E:: /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc 638E:/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc]  638:eager release cuda mem 625663/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu] 638
:eager release cuda mem 625663] 1980
eager release cuda mem 625663] 
eager alloc mem 611.00 KB
[2022-12-12 04:26:45.635517: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu[:2022-12-12 04:26:451980.[] 6355302022-12-12 04:26:45eager alloc mem 611.00 KB: .
E635543 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE: 1980/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu] :eager alloc mem 611.00 KB1980
] eager alloc mem 611.00 KB
[2022-12-12 04:26:45.635626: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:26:45.635662: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:26:45.635698: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:26:45.635732: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:26:45.636025: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:26:45.636053: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:26:45.636094: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:26:45.636121: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:26:45.636228: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:26:45.636298: [E2022-12-12 04:26:45 ./hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu636307:: 1980E]  eager alloc mem 611.00 KB[/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc
2022-12-12 04:26:45:[.6382022-12-12 04:26:45636332] .: eager release cuda mem 625663636344E
:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc :/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc638:] 638eager release cuda mem 625663] 
eager release cuda mem 625663
[2022-12-12 04:26:45[.2022-12-12 04:26:45636443.: 636447E:  E[/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu [2022-12-12 04:26:45:[/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc2022-12-12 04:26:45.19802022-12-12 04:26:45:.636466] .638636473: eager alloc mem 611.00 KB636482] : E
: eager release cuda mem 625663E E
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:1980:1980] 638] eager alloc mem 611.00 KB] [eager alloc mem 611.00 KB
eager release cuda mem 6256632022-12-12 04:26:45

.636613: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:26:45.636687: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:26:45.636847: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:26:45.636872: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:26:45.636915: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:26:45.636952: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:26:45.637071: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:26:45.637139: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:26:45.637280: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:26:45.637350: [E2022-12-12 04:26:45 ./hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu637355[:: 2022-12-12 04:26:45[1980E.2022-12-12 04:26:45]  637377.eager alloc mem 611.00 KB/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc: 637388
:E[: 638 2022-12-12 04:26:45E] /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc. eager release cuda mem 625663:637435/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc
638: :] E638eager release cuda mem 625663 ] 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cceager release cuda mem 625663:
638] [eager release cuda mem 6256632022-12-12 04:26:45
.637538: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:26:45.[6375752022-12-12 04:26:45: .[E6375802022-12-12 04:26:45 : ./hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE637595: : 1980/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE] : eager alloc mem 611.00 KB1980/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu[
] :2022-12-12 04:26:45eager alloc mem 611.00 KB1980.
] 637665eager alloc mem 611.00 KB[: 
2022-12-12 04:26:45E. 637699/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc: :E638 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cceager release cuda mem 625663:
638] eager release cuda mem 625663
[2022-12-12 04:26:45.[6377802022-12-12 04:26:45: .E637785 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE: 1980/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu] :eager alloc mem 611.00 KB1980
] eager alloc mem 611.00 KB
[2022-12-12 04:26:45.637894: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:26:45.637963: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:26:45.638166: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:26:45.638241: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:26:45.638298: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:26:45.638366: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:26:45.638408: E[ 2022-12-12 04:26:45/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc[.:2022-12-12 04:26:45638420638.: ] 638431Eeager release cuda mem 625663:  
E/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc :/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc638:] 638eager release cuda mem 625663] 
eager release cuda mem 625663
[2022-12-12 04:26:45[.2022-12-12 04:26:45638519.: [638522E2022-12-12 04:26:45:  .[E[/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc6385312022-12-12 04:26:45 2022-12-12 04:26:45:: ./hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc.638E638548:638551]  : 638: eager release cuda mem 60400000/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE] E
: eager release cuda mem 60400000 1980/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc
/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc] ::eager alloc mem 611.00 KB638638
] ] eager release cuda mem 625663eager release cuda mem 625663

[2022-12-12 04:26:45.638711[: [2022-12-12 04:26:45E2022-12-12 04:26:45. .638721/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc638722: :: E638E ]  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cceager release cuda mem 625663/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:
:638638] ] eager release cuda mem 60400000eager release cuda mem 60400000
[
2022-12-12 04:26:45.638783: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 60400000
[2022-12-12 04:26:45.638990: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:26:45.639027: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 60400000
[2022-12-12 04:26:45.639112: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:26:45.639160: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 60400000
[2022-12-12 04:26:45.639427: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:26:45[.2022-12-12 04:26:45639454.: 639468E:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu :/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc1793:] 638Collaborative GPU cache (policy: rep_cache) | local 15000000 / 100000000 nodes ( 15.00 %~15.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 85000000 / 100000000 nodes ( 85.00 %) | 7.15 GB | 1.60722 secs ] 
eager release cuda mem 60400000
[2022-12-12 04:26:45.640050: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 15000000 / 100000000 nodes ( 15.00 %~15.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 85000000 / 100000000 nodes ( 85.00 %) | 7.15 GB | 1.60815 secs 
[2022-12-12 04:26:45.640408: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 15000000 / 100000000 nodes ( 15.00 %~15.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 85000000 / 100000000 nodes ( 85.00 %) | 7.15 GB | 1.60817 secs 
[2022-12-12 04:26:45.640869: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 15000000 / 100000000 nodes ( 15.00 %~15.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 85000000 / 100000000 nodes ( 85.00 %) | 7.15 GB | 1.60695 secs 
[2022-12-12 04:26:45.641441: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 15000000 / 100000000 nodes ( 15.00 %~15.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 85000000 / 100000000 nodes ( 85.00 %) | 7.15 GB | 1.62472 secs 
[2022-12-12 04:26:45.641645: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 15000000 / 100000000 nodes ( 15.00 %~15.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 85000000 / 100000000 nodes ( 85.00 %) | 7.15 GB | 1.60301 secs 
[2022-12-12 04:26:45.642247: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 15000000 / 100000000 nodes ( 15.00 %~15.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 85000000 / 100000000 nodes ( 85.00 %) | 7.15 GB | 1.60357 secs 
[2022-12-12 04:26:45.642776: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 15000000 / 100000000 nodes ( 15.00 %~15.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 85000000 / 100000000 nodes ( 85.00 %) | 7.15 GB | 1.60885 secs 
[2022-12-12 04:26:45.642824: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 15.47 GB
[2022-12-12 04:26:47. 80275: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 15.74 GB
[2022-12-12 04:26:47. 80590: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 15.74 GB
[2022-12-12 04:26:47. 80869: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 15.74 GB
[2022-12-12 04:26:48.459316: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 16.00 GB
[2022-12-12 04:26:48.460684: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 16.00 GB
[2022-12-12 04:26:48.461717: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 16.00 GB
[2022-12-12 04:26:49.794340: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 16.21 GB
[2022-12-12 04:26:49.794490: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 16.21 GB
[2022-12-12 04:26:49.794835: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 16.21 GB
[2022-12-12 04:26:51.498819: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 16.43 GB
[2022-12-12 04:26:51.498973: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 16.43 GB
[2022-12-12 04:26:51.499318: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 16.43 GB
[2022-12-12 04:26:52.699653: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 16.89 GB
[2022-12-12 04:26:52.699796: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 16.89 GB
[2022-12-12 04:26:52.700178: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 16.89 GB
[2022-12-12 04:26:53.835382: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 17.08 GB
[2022-12-12 04:26:53.835951: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 17.08 GB
[HCTR][04:26:54.642][ERROR][RK0][tid #140668592695040]: replica 2 calling init per replica done, doing barrier
[HCTR][04:26:54.642][ERROR][RK0][tid #140668324259584]: replica 3 calling init per replica done, doing barrier
[HCTR][04:26:54.642][ERROR][RK0][main]: replica 1 calling init per replica done, doing barrier
[HCTR][04:26:54.642][ERROR][RK0][main]: replica 7 calling init per replica done, doing barrier
[HCTR][04:26:54.642][ERROR][RK0][tid #140668391368448]: replica 6 calling init per replica done, doing barrier
[HCTR][04:26:54.642][ERROR][RK0][tid #140668391368448]: replica 0 calling init per replica done, doing barrier
[HCTR][04:26:54.642][ERROR][RK0][tid #140668324259584]: replica 4 calling init per replica done, doing barrier
[HCTR][04:26:54.642][ERROR][RK0][tid #140668785628928]: replica 5 calling init per replica done, doing barrier
[HCTR][04:26:54.642][ERROR][RK0][tid #140668391368448]: replica 6 calling init per replica done, doing barrier done
[HCTR][04:26:54.642][ERROR][RK0][tid #140668592695040]: replica 2 calling init per replica done, doing barrier done
[HCTR][04:26:54.642][ERROR][RK0][tid #140668391368448]: replica 0 calling init per replica done, doing barrier done
[HCTR][04:26:54.642][ERROR][RK0][tid #140668785628928]: replica 5 calling init per replica done, doing barrier done
[HCTR][04:26:54.642][ERROR][RK0][main]: replica 1 calling init per replica done, doing barrier done
[HCTR][04:26:54.642][ERROR][RK0][main]: replica 7 calling init per replica done, doing barrier done
[HCTR][04:26:54.642][ERROR][RK0][tid #140668324259584]: replica 4 calling init per replica done, doing barrier done
[HCTR][04:26:54.642][ERROR][RK0][tid #140668324259584]: replica 3 calling init per replica done, doing barrier done
[HCTR][04:26:54.642][ERROR][RK0][tid #140668391368448]: init per replica done
[HCTR][04:26:54.642][ERROR][RK0][tid #140668592695040]: init per replica done
[HCTR][04:26:54.642][ERROR][RK0][tid #140668785628928]: init per replica done
[HCTR][04:26:54.642][ERROR][RK0][main]: init per replica done
[HCTR][04:26:54.642][ERROR][RK0][main]: init per replica done
[HCTR][04:26:54.642][ERROR][RK0][tid #140668324259584]: init per replica done
[HCTR][04:26:54.642][ERROR][RK0][tid #140668324259584]: init per replica done
[HCTR][04:26:54.645][ERROR][RK0][tid #140668391368448]: init per replica done
[HCTR][04:26:54.681][ERROR][RK0][main]: 7 allocated 3276800 at 0x7fd248238400
[HCTR][04:26:54.681][ERROR][RK0][main]: 7 allocated 6553600 at 0x7fd248558400
[HCTR][04:26:54.681][ERROR][RK0][main]: 7 allocated 3276800 at 0x7fd248b98400
[HCTR][04:26:54.681][ERROR][RK0][main]: 7 allocated 6553600 at 0x7fd248eb8400
[HCTR][04:26:54.681][ERROR][RK0][tid #140668324259584]: 3 allocated 3276800 at 0x7fd1b4238400
[HCTR][04:26:54.681][ERROR][RK0][tid #140668324259584]: 3 allocated 6553600 at 0x7fd1b4558400
[HCTR][04:26:54.681][ERROR][RK0][tid #140668324259584]: 3 allocated 3276800 at 0x7fd1b4b98400
[HCTR][04:26:54.681][ERROR][RK0][tid #140668324259584]: 3 allocated 6553600 at 0x7fd1b4eb8400
[HCTR][04:26:54.681][ERROR][RK0][tid #140668324259584]: 4 allocated 3276800 at 0x7fd2c8238400
[HCTR][04:26:54.682][ERROR][RK0][tid #140668324259584]: 4 allocated 6553600 at 0x7fd2c8558400
[HCTR][04:26:54.682][ERROR][RK0][tid #140668324259584]: 4 allocated 3276800 at 0x7fd2c8b98400
[HCTR][04:26:54.682][ERROR][RK0][main]: 6 allocated 3276800 at 0x7fd24c238400
[HCTR][04:26:54.682][ERROR][RK0][tid #140668324259584]: 4 allocated 6553600 at 0x7fd2c8eb8400
[HCTR][04:26:54.682][ERROR][RK0][main]: 6 allocated 6553600 at 0x7fd24c558400
[HCTR][04:26:54.682][ERROR][RK0][main]: 6 allocated 3276800 at 0x7fd24cb98400
[HCTR][04:26:54.682][ERROR][RK0][main]: 6 allocated 6553600 at 0x7fd24ceb8400
[HCTR][04:26:54.682][ERROR][RK0][tid #140668785628928]: 5 allocated 3276800 at 0x7fd2d0238400
[HCTR][04:26:54.682][ERROR][RK0][main]: 1 allocated 3276800 at 0x7fd244238400
[HCTR][04:26:54.682][ERROR][RK0][tid #140668785628928]: 5 allocated 6553600 at 0x7fd2d0558400
[HCTR][04:26:54.682][ERROR][RK0][main]: 1 allocated 6553600 at 0x7fd244558400
[HCTR][04:26:54.682][ERROR][RK0][tid #140668785628928]: 5 allocated 3276800 at 0x7fd2d0b98400
[HCTR][04:26:54.682][ERROR][RK0][main]: 1 allocated 3276800 at 0x7fd244b98400
[HCTR][04:26:54.682][ERROR][RK0][tid #140668785628928]: 5 allocated 6553600 at 0x7fd2d0eb8400
[HCTR][04:26:54.682][ERROR][RK0][main]: 1 allocated 6553600 at 0x7fd244eb8400
[HCTR][04:26:54.682][ERROR][RK0][main]: 2 allocated 3276800 at 0x7fd2d0238400
[HCTR][04:26:54.682][ERROR][RK0][main]: 2 allocated 6553600 at 0x7fd2d0558400
[HCTR][04:26:54.682][ERROR][RK0][main]: 2 allocated 3276800 at 0x7fd2d0b98400
[HCTR][04:26:54.682][ERROR][RK0][main]: 2 allocated 6553600 at 0x7fd2d0eb8400
[HCTR][04:26:54.684][ERROR][RK0][tid #140668391368448]: 0 allocated 3276800 at 0x7fd1e4320000
[HCTR][04:26:54.684][ERROR][RK0][tid #140668391368448]: 0 allocated 6553600 at 0x7fd1e4640000
[HCTR][04:26:54.684][ERROR][RK0][tid #140668391368448]: 0 allocated 3276800 at 0x7fd1e4c80000
[HCTR][04:26:54.684][ERROR][RK0][tid #140668391368448]: 0 allocated 6553600 at 0x7fd1e4fa0000
