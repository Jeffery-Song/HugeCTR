2022-12-12 04:58:11.005758: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.012521: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.018745: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.029836: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.035268: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.039968: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.051967: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.059712: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.104645: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.109584: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.110762: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.111919: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.113058: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.114171: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.115236: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.115956: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.116565: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.117443: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.118667: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.119627: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.120647: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.121669: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.122602: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.123658: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.124682: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.125727: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.126752: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.127771: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.128777: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.129713: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.130733: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.131770: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.133557: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.134552: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.135528: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.136469: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.137409: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.138400: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.139426: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.140447: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.145759: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 04:58:11.145858: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.146779: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.147282: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.148576: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.148890: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.150229: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.150661: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.152308: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.152703: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.154574: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.154655: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.154826: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.155896: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.156968: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.157059: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.157481: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.158691: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.160175: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.160273: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.160694: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.161642: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.162336: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.163685: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.164662: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.165749: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.165999: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.168147: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.169220: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.169364: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.169836: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.171373: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.172218: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.172752: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.173301: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.174070: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.174536: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.175232: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.175940: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.176488: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.177471: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.177771: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.178274: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.179262: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.179791: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.180767: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.181316: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.182396: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.182792: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.183511: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.183840: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.185278: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.185617: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.186317: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.186361: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.188164: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.188425: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.188991: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.189061: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.191074: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.191267: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.191669: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.205891: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.206588: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.207333: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.207796: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.208923: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.209462: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.211310: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.224442: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.234679: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.245489: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.246368: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.246610: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.246642: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.246707: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.248937: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.249288: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.250288: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.250672: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.250811: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.250811: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.250927: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.254656: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.255034: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.255379: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.255791: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.255935: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.256061: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.257197: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.260082: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.260696: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.260894: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.261209: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.261682: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.261740: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.262053: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.264686: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.266195: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.266422: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.266533: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.266723: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.266815: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.267149: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.270409: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.271883: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.271941: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.272078: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.272279: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.272485: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.272833: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.275301: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.276399: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.276738: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.276921: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.277095: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.277246: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.277533: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.280075: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.281297: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.281546: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.281670: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.281715: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.281904: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.282274: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.285048: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.286234: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.286479: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.286540: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.286583: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.286673: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.287240: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.289836: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.291424: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.291709: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.291850: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.291944: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.292600: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.293444: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.296615: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.297830: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.298010: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.298049: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.298326: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.298808: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.300912: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.301951: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 04:58:11.302229: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.302324: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.302416: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.302571: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.303187: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.305077: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.305869: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.306098: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.306130: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.306741: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.307100: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.309044: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.310210: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.310308: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.310310: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.310610: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.311158: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.311999: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.313259: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.314433: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.314586: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.314808: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.314992: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.315444: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.316448: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.317975: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.320095: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.320537: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.320676: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.320925: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.321602: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.322140: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.323773: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.325491: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.326048: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.326152: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.326661: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.326940: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.329826: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.330506: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.330761: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.330811: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.330836: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.331112: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.334798: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.335393: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.335863: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.336030: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.336147: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.336325: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.339636: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.340310: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.340818: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.341005: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.341123: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.341245: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.346391: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.346832: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.347368: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.347583: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.347632: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.347782: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.350737: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.351401: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.352085: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.352266: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.352355: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.353096: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.355528: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.355966: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.356745: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.356919: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.357728: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.360080: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.360737: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 04:58:11.360908: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.363487: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.364058: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 04:58:11.364381: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 04:58:11.365210: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.365244: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 04:58:11.370505: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 04:58:11.370859: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.371206: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 04:58:11.373889: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.374262: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.375164: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.380161: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.380509: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.383018: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.383360: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.383438: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.383474: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.384769: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.385001: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.388014: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.388156: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.388842: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.388929: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.391163: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:11.391359: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:12.581038: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:12.581679: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:12.582362: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:12.582827: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 04:58:12.582882: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-12 04:58:12.600276: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:12.600913: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:12.601435: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:12.602005: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:12.602527: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:12.603543: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-12 04:58:12.651038: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 04:58:12.651259: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 04:58:12.692523: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12340
2022-12-12 04:58:12.765871: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:12.767297: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:12.768590: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:12.769995: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 04:58:12.770054: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-12 04:58:12.788577: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:12.789882: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:12.790473: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:12.791215: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:12.793263: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:12.795047: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:1/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-12 04:58:12.862299: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:12.863683: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:12.864707: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:12.864995: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:12.866493: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:12.866680: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 04:58:12.866738: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-12 04:58:12.867878: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:12.868931: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 04:58:12.868982: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-12 04:58:12.869279: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:12.870395: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:12.871414: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:12.871817: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:12.872698: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 04:58:12.872725: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:12.872751: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-12 04:58:12.873826: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:12.874621: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:12.875450: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:12.876201: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:12.876948: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 04:58:12.877011: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-12 04:58:12.877771: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 04:58:12.877825: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-12 04:58:12.885716: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:12.885951: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:12.887495: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:12.887862: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:12.889063: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:12.889503: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:12.890894: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:12.891161: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:12.891418: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:12.892726: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:12.893090: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:12.893387: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 04:58:12.893547: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 04:58:12.893631: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:12.893932: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:12.894729: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:12.894889: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:5/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-12 04:58:12.895310: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12341
2022-12-12 04:58:12.895523: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:4/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-12 04:58:12.896132: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:12.896507: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:12.896949: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:12.898327: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:12.898510: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:12.899223: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:12.900641: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:12.900827: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:12.901394: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:12.902565: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:6/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-12 04:58:12.902821: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:12.903278: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:12.904342: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:2/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-12 04:58:12.904674: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:7/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-12 04:58:12.915795: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:12.917292: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:12.918410: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:12.919345: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 04:58:12.919420: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-12 04:58:12.937628: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:12.938302: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:12.938803: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:12.939413: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:12.939932: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:58:12.940411: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:3/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-12 04:58:12.940511: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 04:58:12.940656: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 04:58:12.940680: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 04:58:12.940782: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 04:58:12.942535: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12345
2022-12-12 04:58:12.942547: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12344
2022-12-12 04:58:12.948837: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 04:58:12.949005: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 04:58:12.950782: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12346
2022-12-12 04:58:12.953363: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 04:58:12.953511: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 04:58:12.955499: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12347
2022-12-12 04:58:12.985522: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 04:58:12.985716: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 04:58:12.987548: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12343
2022-12-12 04:58:12.991100: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 04:58:12.991267: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 04:58:12.993058: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12342
[HCTR][04:58:14.253][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][04:58:14.253][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][04:58:14.253][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][04:58:14.254][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][04:58:14.317][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][04:58:14.317][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][04:58:14.318][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][04:58:14.318][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 1it [00:01,  1.54s/it]warmup run: 1it [00:01,  1.53s/it]warmup run: 1it [00:01,  1.56s/it]warmup run: 98it [00:01, 82.69it/s]warmup run: 96it [00:01, 81.77it/s]warmup run: 1it [00:01,  1.49s/it]warmup run: 1it [00:01,  1.51s/it]warmup run: 100it [00:01, 83.56it/s]warmup run: 197it [00:01, 180.56it/s]warmup run: 192it [00:01, 177.30it/s]warmup run: 100it [00:01, 87.03it/s]warmup run: 1it [00:01,  1.52s/it]warmup run: 100it [00:01, 86.08it/s]warmup run: 201it [00:01, 182.47it/s]warmup run: 1it [00:01,  1.50s/it]warmup run: 297it [00:01, 289.59it/s]warmup run: 289it [00:01, 283.56it/s]warmup run: 200it [00:01, 188.18it/s]warmup run: 98it [00:01, 84.10it/s]warmup run: 1it [00:01,  1.50s/it]warmup run: 198it [00:01, 184.15it/s]warmup run: 295it [00:01, 282.46it/s]warmup run: 96it [00:01, 83.40it/s]warmup run: 387it [00:01, 386.73it/s]warmup run: 386it [00:01, 393.41it/s]warmup run: 300it [00:01, 299.13it/s]warmup run: 198it [00:01, 184.24it/s]warmup run: 99it [00:01, 85.79it/s]warmup run: 298it [00:01, 294.54it/s]warmup run: 196it [00:01, 184.87it/s]warmup run: 395it [00:01, 395.81it/s]warmup run: 484it [00:02, 494.95it/s]warmup run: 482it [00:02, 499.08it/s]warmup run: 400it [00:01, 413.26it/s]warmup run: 296it [00:01, 291.63it/s]warmup run: 198it [00:01, 185.63it/s]warmup run: 398it [00:01, 408.87it/s]warmup run: 296it [00:01, 296.13it/s]warmup run: 494it [00:02, 505.44it/s]warmup run: 584it [00:02, 600.25it/s]warmup run: 580it [00:02, 600.27it/s]warmup run: 500it [00:01, 524.16it/s]warmup run: 393it [00:01, 400.81it/s]warmup run: 298it [00:01, 296.31it/s]warmup run: 497it [00:02, 517.67it/s]warmup run: 396it [00:01, 410.82it/s]warmup run: 594it [00:02, 608.55it/s]warmup run: 686it [00:02, 696.59it/s]warmup run: 681it [00:02, 693.70it/s]warmup run: 603it [00:02, 631.23it/s]warmup run: 490it [00:02, 506.67it/s]warmup run: 398it [00:01, 410.75it/s]warmup run: 598it [00:02, 621.69it/s]warmup run: 497it [00:01, 523.30it/s]warmup run: 692it [00:02, 692.77it/s]warmup run: 788it [00:02, 774.99it/s]warmup run: 782it [00:02, 770.50it/s]warmup run: 705it [00:02, 720.30it/s]warmup run: 589it [00:02, 607.36it/s]warmup run: 498it [00:02, 521.36it/s]warmup run: 699it [00:02, 711.46it/s]warmup run: 599it [00:02, 628.75it/s]warmup run: 791it [00:02, 765.73it/s]warmup run: 889it [00:02, 834.32it/s]warmup run: 882it [00:02, 830.40it/s]warmup run: 807it [00:02, 793.46it/s]warmup run: 683it [00:02, 671.24it/s]warmup run: 600it [00:02, 626.73it/s]warmup run: 801it [00:02, 786.48it/s]warmup run: 701it [00:02, 719.55it/s]warmup run: 892it [00:02, 828.06it/s]warmup run: 991it [00:02, 884.30it/s]warmup run: 983it [00:02, 878.31it/s]warmup run: 909it [00:02, 850.83it/s]warmup run: 775it [00:02, 730.63it/s]warmup run: 702it [00:02, 716.80it/s]warmup run: 902it [00:02, 843.06it/s]warmup run: 804it [00:02, 795.36it/s]warmup run: 991it [00:02, 871.77it/s]warmup run: 1094it [00:02, 922.67it/s]warmup run: 1086it [00:02, 920.38it/s]warmup run: 1010it [00:02, 892.68it/s]warmup run: 878it [00:02, 807.57it/s]warmup run: 804it [00:02, 791.00it/s]warmup run: 1003it [00:02, 886.95it/s]warmup run: 906it [00:02, 852.94it/s]warmup run: 1092it [00:02, 909.32it/s]warmup run: 1196it [00:02, 947.95it/s]warmup run: 1189it [00:02, 949.20it/s]warmup run: 1110it [00:02, 919.50it/s]warmup run: 980it [00:02, 863.46it/s]warmup run: 905it [00:02, 847.16it/s]warmup run: 1105it [00:02, 922.28it/s]warmup run: 1008it [00:02, 898.21it/s]warmup run: 1191it [00:02, 923.14it/s]warmup run: 1297it [00:02, 962.17it/s]warmup run: 1290it [00:02, 966.71it/s]warmup run: 1210it [00:02, 937.75it/s]warmup run: 1081it [00:02, 903.19it/s]warmup run: 1006it [00:02, 889.17it/s]warmup run: 1206it [00:02, 944.92it/s]warmup run: 1111it [00:02, 930.75it/s]warmup run: 1289it [00:02, 937.21it/s]warmup run: 1398it [00:02, 975.73it/s]warmup run: 1391it [00:02, 978.46it/s]warmup run: 1310it [00:02, 952.27it/s]warmup run: 1181it [00:02, 930.34it/s]warmup run: 1108it [00:02, 924.16it/s]warmup run: 1308it [00:02, 963.82it/s]warmup run: 1214it [00:02, 957.65it/s]warmup run: 1387it [00:02, 942.88it/s]warmup run: 1499it [00:03, 981.03it/s]warmup run: 1492it [00:03, 984.15it/s]warmup run: 1410it [00:02, 961.60it/s]warmup run: 1280it [00:02, 944.79it/s]warmup run: 1209it [00:02, 947.51it/s]warmup run: 1409it [00:02, 975.14it/s]warmup run: 1317it [00:02, 978.52it/s]warmup run: 1602it [00:03, 993.51it/s]warmup run: 1485it [00:03, 945.47it/s]warmup run: 1593it [00:03, 987.38it/s]warmup run: 1509it [00:03, 968.33it/s]warmup run: 1379it [00:02, 956.72it/s]warmup run: 1310it [00:02, 964.76it/s]warmup run: 1511it [00:03, 986.71it/s]warmup run: 1422it [00:02, 997.86it/s]warmup run: 1705it [00:03, 1004.00it/s]warmup run: 1582it [00:03, 946.28it/s]warmup run: 1694it [00:03, 987.29it/s]warmup run: 1608it [00:03, 973.02it/s]warmup run: 1478it [00:03, 965.04it/s]warmup run: 1411it [00:02, 976.44it/s]warmup run: 1612it [00:03, 988.23it/s]warmup run: 1525it [00:03, 1005.87it/s]warmup run: 1808it [00:03, 1008.92it/s]warmup run: 1678it [00:03, 949.29it/s]warmup run: 1794it [00:03, 990.39it/s]warmup run: 1707it [00:03, 977.82it/s]warmup run: 1580it [00:03, 979.18it/s]warmup run: 1512it [00:03, 985.56it/s]warmup run: 1713it [00:03, 992.95it/s]warmup run: 1628it [00:03, 989.96it/s] warmup run: 1911it [00:03, 1012.41it/s]warmup run: 1776it [00:03, 956.75it/s]warmup run: 1894it [00:03, 992.84it/s]warmup run: 1806it [00:03, 980.25it/s]warmup run: 1682it [00:03, 988.99it/s]warmup run: 1613it [00:03, 990.86it/s]warmup run: 1814it [00:03, 997.00it/s]warmup run: 2014it [00:03, 1016.66it/s]warmup run: 1729it [00:03, 991.07it/s]warmup run: 1878it [00:03, 974.15it/s]warmup run: 1998it [00:03, 1005.70it/s]warmup run: 1905it [00:03, 975.72it/s]warmup run: 1785it [00:03, 998.37it/s]warmup run: 1714it [00:03, 993.02it/s]warmup run: 1915it [00:03, 999.15it/s]warmup run: 2127it [00:03, 1050.16it/s]warmup run: 1978it [00:03, 981.23it/s]warmup run: 1830it [00:03, 991.79it/s]warmup run: 2119it [00:03, 1066.02it/s]warmup run: 2004it [00:03, 966.22it/s]warmup run: 1888it [00:03, 1005.41it/s]warmup run: 1815it [00:03, 994.66it/s]warmup run: 2017it [00:03, 1003.82it/s]warmup run: 2248it [00:03, 1095.45it/s]warmup run: 2091it [00:03, 1023.92it/s]warmup run: 1930it [00:03, 992.82it/s]warmup run: 2242it [00:03, 1113.34it/s]warmup run: 2121it [00:03, 1025.28it/s]warmup run: 1991it [00:03, 1012.00it/s]warmup run: 1916it [00:03, 996.32it/s]warmup run: 2135it [00:03, 1054.12it/s]warmup run: 2370it [00:03, 1131.90it/s]warmup run: 2208it [00:03, 1066.53it/s]warmup run: 2036it [00:03, 1011.89it/s]warmup run: 2365it [00:03, 1145.84it/s]warmup run: 2238it [00:03, 1067.12it/s]warmup run: 2109it [00:03, 1061.50it/s]warmup run: 2018it [00:03, 1001.37it/s]warmup run: 2253it [00:03, 1090.16it/s]warmup run: 2492it [00:03, 1156.90it/s]warmup run: 2325it [00:03, 1095.45it/s]warmup run: 2158it [00:03, 1072.16it/s]warmup run: 2488it [00:03, 1168.41it/s]warmup run: 2355it [00:03, 1095.72it/s]warmup run: 2229it [00:03, 1101.47it/s]warmup run: 2135it [00:03, 1050.12it/s]warmup run: 2372it [00:03, 1119.04it/s]warmup run: 2614it [00:04, 1173.20it/s]warmup run: 2441it [00:03, 1114.43it/s]warmup run: 2280it [00:03, 1114.25it/s]warmup run: 2610it [00:04, 1182.14it/s]warmup run: 2473it [00:03, 1118.32it/s]warmup run: 2349it [00:03, 1129.67it/s]warmup run: 2253it [00:03, 1088.67it/s]warmup run: 2491it [00:03, 1138.56it/s]warmup run: 2736it [00:04, 1184.51it/s]warmup run: 2558it [00:04, 1129.38it/s]warmup run: 2402it [00:03, 1144.00it/s]warmup run: 2732it [00:04, 1190.92it/s]warmup run: 2594it [00:04, 1144.66it/s]warmup run: 2469it [00:03, 1148.93it/s]warmup run: 2370it [00:03, 1112.12it/s]warmup run: 2609it [00:04, 1150.80it/s]warmup run: 2857it [00:04, 1191.16it/s]warmup run: 2674it [00:04, 1136.41it/s]warmup run: 2523it [00:03, 1162.83it/s]warmup run: 2855it [00:04, 1201.48it/s]warmup run: 2715it [00:04, 1161.79it/s]warmup run: 2589it [00:04, 1163.24it/s]warmup run: 2489it [00:03, 1133.45it/s]warmup run: 2726it [00:04, 1156.13it/s]warmup run: 2980it [00:04, 1199.82it/s]warmup run: 2791it [00:04, 1145.01it/s]warmup run: 2643it [00:04, 1172.87it/s]warmup run: 2978it [00:04, 1208.15it/s]warmup run: 3000it [00:04, 683.78it/s] warmup run: 2836it [00:04, 1174.39it/s]warmup run: 3000it [00:04, 686.58it/s] warmup run: 2708it [00:04, 1169.48it/s]warmup run: 2610it [00:04, 1156.11it/s]warmup run: 2845it [00:04, 1165.53it/s]warmup run: 2910it [00:04, 1155.81it/s]warmup run: 2762it [00:04, 1175.67it/s]warmup run: 2956it [00:04, 1179.63it/s]warmup run: 2828it [00:04, 1177.07it/s]warmup run: 2731it [00:04, 1171.03it/s]warmup run: 2965it [00:04, 1173.01it/s]warmup run: 3000it [00:04, 687.89it/s] warmup run: 3000it [00:04, 671.77it/s] warmup run: 2881it [00:04, 1177.42it/s]warmup run: 3000it [00:04, 687.45it/s] warmup run: 2947it [00:04, 1178.70it/s]warmup run: 2849it [00:04, 1173.07it/s]warmup run: 3000it [00:04, 683.16it/s] warmup run: 3000it [00:04, 1181.14it/s]warmup run: 3000it [00:04, 693.73it/s] warmup run: 2967it [00:04, 1175.09it/s]warmup run: 3000it [00:04, 689.78it/s] 

warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]

warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]


warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   5%|         | 163/3000 [00:00<00:01, 1629.02it/s]warmup should be done:   5%|         | 162/3000 [00:00<00:01, 1619.02it/s]warmup should be done:   5%|         | 162/3000 [00:00<00:01, 1619.38it/s]warmup should be done:   6%|         | 166/3000 [00:00<00:01, 1652.26it/s]warmup should be done:   5%|         | 163/3000 [00:00<00:01, 1620.95it/s]warmup should be done:   5%|         | 160/3000 [00:00<00:01, 1592.91it/s]warmup should be done:   6%|         | 165/3000 [00:00<00:01, 1640.59it/s]warmup should be done:   5%|         | 164/3000 [00:00<00:01, 1630.78it/s]warmup should be done:  11%|         | 328/3000 [00:00<00:01, 1639.55it/s]warmup should be done:  11%|         | 332/3000 [00:00<00:01, 1653.92it/s]warmup should be done:  11%|         | 326/3000 [00:00<00:01, 1627.05it/s]warmup should be done:  11%|         | 332/3000 [00:00<00:01, 1656.92it/s]warmup should be done:  11%|         | 326/3000 [00:00<00:01, 1625.58it/s]warmup should be done:  11%|         | 324/3000 [00:00<00:01, 1617.25it/s]warmup should be done:  11%|         | 327/3000 [00:00<00:01, 1628.14it/s]warmup should be done:  11%|         | 330/3000 [00:00<00:01, 1642.58it/s]warmup should be done:  16%|        | 492/3000 [00:00<00:01, 1638.90it/s]warmup should be done:  17%|        | 498/3000 [00:00<00:01, 1658.13it/s]warmup should be done:  16%|        | 489/3000 [00:00<00:01, 1625.51it/s]warmup should be done:  16%|        | 489/3000 [00:00<00:01, 1624.93it/s]warmup should be done:  16%|        | 490/3000 [00:00<00:01, 1625.30it/s]warmup should be done:  17%|        | 498/3000 [00:00<00:01, 1650.33it/s]warmup should be done:  16%|        | 486/3000 [00:00<00:01, 1613.91it/s]warmup should be done:  16%|        | 495/3000 [00:00<00:01, 1642.01it/s]warmup should be done:  22%|       | 664/3000 [00:00<00:01, 1657.95it/s]warmup should be done:  22%|       | 657/3000 [00:00<00:01, 1639.82it/s]warmup should be done:  22%|       | 660/3000 [00:00<00:01, 1643.32it/s]warmup should be done:  22%|       | 653/3000 [00:00<00:01, 1623.03it/s]warmup should be done:  22%|       | 664/3000 [00:00<00:01, 1649.06it/s]warmup should be done:  22%|       | 652/3000 [00:00<00:01, 1619.26it/s]warmup should be done:  22%|       | 648/3000 [00:00<00:01, 1610.48it/s]warmup should be done:  22%|       | 652/3000 [00:00<00:01, 1608.47it/s]warmup should be done:  28%|       | 830/3000 [00:00<00:01, 1655.98it/s]warmup should be done:  27%|       | 822/3000 [00:00<00:01, 1640.16it/s]warmup should be done:  27%|       | 814/3000 [00:00<00:01, 1619.19it/s]warmup should be done:  28%|       | 825/3000 [00:00<00:01, 1643.01it/s]warmup should be done:  28%|       | 829/3000 [00:00<00:01, 1646.36it/s]warmup should be done:  27%|       | 816/3000 [00:00<00:01, 1619.83it/s]warmup should be done:  27%|       | 810/3000 [00:00<00:01, 1607.42it/s]warmup should be done:  27%|       | 815/3000 [00:00<00:01, 1613.91it/s]warmup should be done:  33%|      | 976/3000 [00:00<00:01, 1617.82it/s]warmup should be done:  33%|      | 996/3000 [00:00<00:01, 1648.35it/s]warmup should be done:  33%|      | 987/3000 [00:00<00:01, 1635.68it/s]warmup should be done:  33%|      | 990/3000 [00:00<00:01, 1638.00it/s]warmup should be done:  33%|      | 994/3000 [00:00<00:01, 1640.43it/s]warmup should be done:  33%|      | 978/3000 [00:00<00:01, 1611.86it/s]warmup should be done:  32%|      | 971/3000 [00:00<00:01, 1600.14it/s]warmup should be done:  33%|      | 977/3000 [00:00<00:01, 1612.09it/s]warmup should be done:  38%|      | 1151/3000 [00:00<00:01, 1635.88it/s]warmup should be done:  38%|      | 1138/3000 [00:00<00:01, 1614.75it/s]warmup should be done:  38%|      | 1155/3000 [00:00<00:01, 1638.82it/s]warmup should be done:  39%|      | 1161/3000 [00:00<00:01, 1640.73it/s]warmup should be done:  39%|      | 1159/3000 [00:00<00:01, 1639.04it/s]warmup should be done:  38%|      | 1139/3000 [00:00<00:01, 1614.46it/s]warmup should be done:  38%|      | 1140/3000 [00:00<00:01, 1610.45it/s]warmup should be done:  38%|      | 1132/3000 [00:00<00:01, 1599.37it/s]warmup should be done:  44%|     | 1315/3000 [00:00<00:01, 1636.45it/s]warmup should be done:  43%|     | 1301/3000 [00:00<00:01, 1617.47it/s]warmup should be done:  44%|     | 1320/3000 [00:00<00:01, 1640.79it/s]warmup should be done:  44%|     | 1323/3000 [00:00<00:01, 1637.38it/s]warmup should be done:  43%|     | 1301/3000 [00:00<00:01, 1614.23it/s]warmup should be done:  44%|     | 1326/3000 [00:00<00:01, 1636.42it/s]warmup should be done:  43%|     | 1302/3000 [00:00<00:01, 1608.50it/s]warmup should be done:  43%|     | 1293/3000 [00:00<00:01, 1599.64it/s]warmup should be done:  49%|     | 1479/3000 [00:00<00:00, 1637.26it/s]warmup should be done:  49%|     | 1464/3000 [00:00<00:00, 1620.23it/s]warmup should be done:  50%|     | 1485/3000 [00:00<00:00, 1640.58it/s]warmup should be done:  50%|     | 1487/3000 [00:00<00:00, 1637.49it/s]warmup should be done:  49%|     | 1463/3000 [00:00<00:00, 1615.01it/s]warmup should be done:  48%|     | 1453/3000 [00:00<00:00, 1599.19it/s]warmup should be done:  49%|     | 1463/3000 [00:00<00:00, 1607.49it/s]warmup should be done:  50%|     | 1490/3000 [00:00<00:00, 1633.31it/s]warmup should be done:  55%|    | 1643/3000 [00:01<00:00, 1636.78it/s]warmup should be done:  54%|    | 1628/3000 [00:01<00:00, 1624.76it/s]warmup should be done:  55%|    | 1651/3000 [00:01<00:00, 1637.97it/s]warmup should be done:  55%|    | 1650/3000 [00:01<00:00, 1640.47it/s]warmup should be done:  54%|    | 1625/3000 [00:01<00:00, 1615.66it/s]warmup should be done:  54%|    | 1624/3000 [00:01<00:00, 1607.79it/s]warmup should be done:  54%|    | 1613/3000 [00:01<00:00, 1598.63it/s]warmup should be done:  55%|    | 1654/3000 [00:01<00:00, 1628.57it/s]warmup should be done:  60%|    | 1807/3000 [00:01<00:00, 1637.60it/s]warmup should be done:  60%|    | 1815/3000 [00:01<00:00, 1637.99it/s]warmup should be done:  60%|    | 1787/3000 [00:01<00:00, 1616.67it/s]warmup should be done:  60%|    | 1815/3000 [00:01<00:00, 1640.68it/s]warmup should be done:  60%|    | 1785/3000 [00:01<00:00, 1607.36it/s]warmup should be done:  59%|    | 1775/3000 [00:01<00:00, 1602.48it/s]warmup should be done:  61%|    | 1817/3000 [00:01<00:00, 1626.77it/s]warmup should be done:  60%|    | 1791/3000 [00:01<00:00, 1605.66it/s]warmup should be done:  66%|   | 1971/3000 [00:01<00:00, 1637.43it/s]warmup should be done:  66%|   | 1979/3000 [00:01<00:00, 1636.93it/s]warmup should be done:  65%|   | 1949/3000 [00:01<00:00, 1615.82it/s]warmup should be done:  66%|   | 1980/3000 [00:01<00:00, 1641.30it/s]warmup should be done:  65%|   | 1946/3000 [00:01<00:00, 1606.50it/s]warmup should be done:  65%|   | 1938/3000 [00:01<00:00, 1610.65it/s]warmup should be done:  66%|   | 1980/3000 [00:01<00:00, 1625.44it/s]warmup should be done:  65%|   | 1952/3000 [00:01<00:00, 1603.21it/s]warmup should be done:  71%|   | 2135/3000 [00:01<00:00, 1636.07it/s]warmup should be done:  71%|  | 2143/3000 [00:01<00:00, 1636.16it/s]warmup should be done:  72%|  | 2145/3000 [00:01<00:00, 1640.32it/s]warmup should be done:  70%|   | 2109/3000 [00:01<00:00, 1611.23it/s]warmup should be done:  70%|   | 2102/3000 [00:01<00:00, 1616.83it/s]warmup should be done:  71%|  | 2143/3000 [00:01<00:00, 1624.20it/s]warmup should be done:  70%|   | 2114/3000 [00:01<00:00, 1605.23it/s]warmup should be done:  70%|   | 2111/3000 [00:01<00:00, 1589.86it/s]warmup should be done:  77%|  | 2299/3000 [00:01<00:00, 1631.63it/s]warmup should be done:  77%|  | 2307/3000 [00:01<00:00, 1633.24it/s]warmup should be done:  75%|  | 2264/3000 [00:01<00:00, 1617.61it/s]warmup should be done:  76%|  | 2271/3000 [00:01<00:00, 1608.10it/s]warmup should be done:  77%|  | 2310/3000 [00:01<00:00, 1635.49it/s]warmup should be done:  77%|  | 2306/3000 [00:01<00:00, 1618.92it/s]warmup should be done:  76%|  | 2275/3000 [00:01<00:00, 1600.30it/s]warmup should be done:  76%|  | 2271/3000 [00:01<00:00, 1568.33it/s]warmup should be done:  82%| | 2463/3000 [00:01<00:00, 1631.53it/s]warmup should be done:  81%|  | 2427/3000 [00:01<00:00, 1621.17it/s]warmup should be done:  82%| | 2471/3000 [00:01<00:00, 1629.81it/s]warmup should be done:  82%| | 2474/3000 [00:01<00:00, 1635.60it/s]warmup should be done:  82%| | 2469/3000 [00:01<00:00, 1619.85it/s]warmup should be done:  81%|  | 2436/3000 [00:01<00:00, 1598.50it/s]warmup should be done:  81%|  | 2432/3000 [00:01<00:00, 1587.31it/s]warmup should be done:  81%|  | 2432/3000 [00:01<00:00, 1579.76it/s]warmup should be done:  88%| | 2627/3000 [00:01<00:00, 1632.39it/s]warmup should be done:  86%| | 2590/3000 [00:01<00:00, 1623.42it/s]warmup should be done:  88%| | 2634/3000 [00:01<00:00, 1625.56it/s]warmup should be done:  88%| | 2638/3000 [00:01<00:00, 1636.79it/s]warmup should be done:  88%| | 2632/3000 [00:01<00:00, 1620.68it/s]warmup should be done:  87%| | 2598/3000 [00:01<00:00, 1602.75it/s]warmup should be done:  86%| | 2591/3000 [00:01<00:00, 1574.53it/s]warmup should be done:  86%| | 2594/3000 [00:01<00:00, 1589.34it/s]warmup should be done:  93%|| 2791/3000 [00:01<00:00, 1633.00it/s]warmup should be done:  92%|| 2753/3000 [00:01<00:00, 1620.76it/s]warmup should be done:  93%|| 2802/3000 [00:01<00:00, 1637.59it/s]warmup should be done:  93%|| 2797/3000 [00:01<00:00, 1621.53it/s]warmup should be done:  93%|| 2795/3000 [00:01<00:00, 1620.89it/s]warmup should be done:  92%|| 2759/3000 [00:01<00:00, 1604.40it/s]warmup should be done:  92%|| 2749/3000 [00:01<00:00, 1572.26it/s]warmup should be done:  92%|| 2756/3000 [00:01<00:00, 1595.87it/s]warmup should be done:  99%|| 2957/3000 [00:01<00:00, 1638.21it/s]warmup should be done:  97%|| 2916/3000 [00:01<00:00, 1620.87it/s]warmup should be done:  99%|| 2967/3000 [00:01<00:00, 1641.11it/s]warmup should be done:  99%|| 2961/3000 [00:01<00:00, 1624.02it/s]warmup should be done:  99%|| 2959/3000 [00:01<00:00, 1625.46it/s]warmup should be done:  97%|| 2924/3000 [00:01<00:00, 1616.66it/s]warmup should be done:  97%|| 2910/3000 [00:01<00:00, 1581.75it/s]warmup should be done:  97%|| 2916/3000 [00:01<00:00, 1579.47it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1639.44it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1636.19it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1634.73it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1632.29it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1611.94it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1608.91it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1601.15it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1600.46it/s]





warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   5%|         | 163/3000 [00:00<00:01, 1628.82it/s]warmup should be done:   6%|         | 168/3000 [00:00<00:01, 1678.65it/s]warmup should be done:   6%|         | 166/3000 [00:00<00:01, 1658.21it/s]warmup should be done:   6%|         | 168/3000 [00:00<00:01, 1675.50it/s]warmup should be done:   6%|         | 166/3000 [00:00<00:01, 1655.41it/s]warmup should be done:   6%|         | 168/3000 [00:00<00:01, 1675.04it/s]warmup should be done:   6%|         | 168/3000 [00:00<00:01, 1671.69it/s]warmup should be done:   6%|         | 166/3000 [00:00<00:01, 1650.71it/s]warmup should be done:  11%|         | 329/3000 [00:00<00:01, 1646.69it/s]warmup should be done:  11%|         | 332/3000 [00:00<00:01, 1657.94it/s]warmup should be done:  11%|        | 338/3000 [00:00<00:01, 1685.95it/s]warmup should be done:  11%|         | 337/3000 [00:00<00:01, 1679.05it/s]warmup should be done:  11%|         | 334/3000 [00:00<00:01, 1665.02it/s]warmup should be done:  11%|         | 332/3000 [00:00<00:01, 1651.83it/s]warmup should be done:  11%|         | 337/3000 [00:00<00:01, 1677.26it/s]warmup should be done:  11%|         | 336/3000 [00:00<00:01, 1668.07it/s]warmup should be done:  17%|        | 498/3000 [00:00<00:01, 1662.00it/s]warmup should be done:  17%|        | 500/3000 [00:00<00:01, 1663.55it/s]warmup should be done:  17%|        | 501/3000 [00:00<00:01, 1666.08it/s]warmup should be done:  17%|        | 498/3000 [00:00<00:01, 1655.34it/s]warmup should be done:  17%|        | 508/3000 [00:00<00:01, 1688.15it/s]warmup should be done:  17%|        | 505/3000 [00:00<00:01, 1675.84it/s]warmup should be done:  17%|        | 505/3000 [00:00<00:01, 1676.47it/s]warmup should be done:  17%|        | 505/3000 [00:00<00:01, 1673.49it/s]warmup should be done:  22%|       | 666/3000 [00:00<00:01, 1667.73it/s]warmup should be done:  23%|       | 677/3000 [00:00<00:01, 1688.43it/s]warmup should be done:  22%|       | 667/3000 [00:00<00:01, 1662.83it/s]warmup should be done:  22%|       | 665/3000 [00:00<00:01, 1658.69it/s]warmup should be done:  22%|       | 668/3000 [00:00<00:01, 1663.60it/s]warmup should be done:  22%|       | 674/3000 [00:00<00:01, 1677.57it/s]warmup should be done:  22%|       | 674/3000 [00:00<00:01, 1677.88it/s]warmup should be done:  22%|       | 673/3000 [00:00<00:01, 1670.09it/s]warmup should be done:  28%|       | 834/3000 [00:00<00:01, 1669.09it/s]warmup should be done:  28%|       | 834/3000 [00:00<00:01, 1664.71it/s]warmup should be done:  28%|       | 832/3000 [00:00<00:01, 1659.90it/s]warmup should be done:  28%|       | 835/3000 [00:00<00:01, 1663.89it/s]warmup should be done:  28%|       | 843/3000 [00:00<00:01, 1680.56it/s]warmup should be done:  28%|       | 843/3000 [00:00<00:01, 1679.42it/s]warmup should be done:  28%|       | 841/3000 [00:00<00:01, 1668.71it/s]warmup should be done:  28%|       | 846/3000 [00:00<00:01, 1673.91it/s]warmup should be done:  33%|      | 1002/3000 [00:00<00:01, 1671.37it/s]warmup should be done:  33%|      | 1001/3000 [00:00<00:01, 1670.00it/s]warmup should be done:  33%|      | 1002/3000 [00:00<00:01, 1667.03it/s]warmup should be done:  33%|      | 1003/3000 [00:00<00:01, 1666.87it/s]warmup should be done:  34%|      | 1013/3000 [00:00<00:01, 1684.54it/s]warmup should be done:  34%|      | 1012/3000 [00:00<00:01, 1680.47it/s]warmup should be done:  34%|      | 1009/3000 [00:00<00:01, 1669.27it/s]warmup should be done:  34%|      | 1016/3000 [00:00<00:01, 1681.06it/s]warmup should be done:  39%|      | 1171/3000 [00:00<00:01, 1675.94it/s]warmup should be done:  39%|      | 1169/3000 [00:00<00:01, 1667.19it/s]warmup should be done:  39%|      | 1171/3000 [00:00<00:01, 1676.59it/s]warmup should be done:  39%|      | 1170/3000 [00:00<00:01, 1666.72it/s]warmup should be done:  39%|      | 1182/3000 [00:00<00:01, 1684.89it/s]warmup should be done:  39%|      | 1181/3000 [00:00<00:01, 1681.48it/s]warmup should be done:  40%|      | 1185/3000 [00:00<00:01, 1682.60it/s]warmup should be done:  39%|      | 1177/3000 [00:00<00:01, 1669.71it/s]warmup should be done:  45%|     | 1341/3000 [00:00<00:00, 1681.99it/s]warmup should be done:  45%|     | 1337/3000 [00:00<00:00, 1667.13it/s]warmup should be done:  45%|     | 1337/3000 [00:00<00:00, 1668.09it/s]warmup should be done:  45%|     | 1341/3000 [00:00<00:00, 1681.25it/s]warmup should be done:  45%|     | 1351/3000 [00:00<00:00, 1685.50it/s]warmup should be done:  45%|     | 1350/3000 [00:00<00:00, 1682.65it/s]warmup should be done:  45%|     | 1355/3000 [00:00<00:00, 1685.88it/s]warmup should be done:  45%|     | 1345/3000 [00:00<00:00, 1670.32it/s]warmup should be done:  50%|     | 1504/3000 [00:00<00:00, 1667.93it/s]warmup should be done:  50%|     | 1504/3000 [00:00<00:00, 1667.72it/s]warmup should be done:  50%|     | 1510/3000 [00:00<00:00, 1680.23it/s]warmup should be done:  50%|     | 1510/3000 [00:00<00:00, 1683.54it/s]warmup should be done:  51%|     | 1520/3000 [00:00<00:00, 1684.17it/s]warmup should be done:  51%|     | 1519/3000 [00:00<00:00, 1682.52it/s]warmup should be done:  51%|     | 1524/3000 [00:00<00:00, 1686.69it/s]warmup should be done:  50%|     | 1513/3000 [00:00<00:00, 1670.45it/s]warmup should be done:  56%|    | 1672/3000 [00:01<00:00, 1669.87it/s]warmup should be done:  56%|    | 1671/3000 [00:01<00:00, 1665.52it/s]warmup should be done:  56%|    | 1680/3000 [00:01<00:00, 1686.32it/s]warmup should be done:  56%|    | 1679/3000 [00:01<00:00, 1678.02it/s]warmup should be done:  56%|    | 1694/3000 [00:01<00:00, 1688.31it/s]warmup should be done:  56%|    | 1681/3000 [00:01<00:00, 1671.24it/s]warmup should be done:  56%|    | 1689/3000 [00:01<00:00, 1674.46it/s]warmup should be done:  56%|    | 1688/3000 [00:01<00:00, 1651.44it/s]warmup should be done:  61%|   | 1839/3000 [00:01<00:00, 1666.47it/s]warmup should be done:  62%|   | 1849/3000 [00:01<00:00, 1685.93it/s]warmup should be done:  61%|   | 1838/3000 [00:01<00:00, 1662.86it/s]warmup should be done:  62%|   | 1847/3000 [00:01<00:00, 1669.46it/s]warmup should be done:  62%|   | 1849/3000 [00:01<00:00, 1671.78it/s]warmup should be done:  62%|   | 1863/3000 [00:01<00:00, 1683.19it/s]warmup should be done:  62%|   | 1857/3000 [00:01<00:00, 1653.41it/s]warmup should be done:  62%|   | 1854/3000 [00:01<00:00, 1641.31it/s]warmup should be done:  67%|   | 2018/3000 [00:01<00:00, 1683.56it/s]warmup should be done:  67%|   | 2005/3000 [00:01<00:00, 1656.43it/s]warmup should be done:  67%|   | 2006/3000 [00:01<00:00, 1654.96it/s]warmup should be done:  67%|   | 2017/3000 [00:01<00:00, 1671.29it/s]warmup should be done:  67%|   | 2014/3000 [00:01<00:00, 1661.27it/s]warmup should be done:  68%|   | 2032/3000 [00:01<00:00, 1676.95it/s]warmup should be done:  67%|   | 2023/3000 [00:01<00:00, 1641.09it/s]warmup should be done:  67%|   | 2019/3000 [00:01<00:00, 1621.19it/s]warmup should be done:  73%|  | 2187/3000 [00:01<00:00, 1680.82it/s]warmup should be done:  72%|  | 2172/3000 [00:01<00:00, 1655.72it/s]warmup should be done:  72%|  | 2172/3000 [00:01<00:00, 1658.36it/s]warmup should be done:  73%|  | 2186/3000 [00:01<00:00, 1675.42it/s]warmup should be done:  73%|  | 2200/3000 [00:01<00:00, 1672.56it/s]warmup should be done:  73%|  | 2188/3000 [00:01<00:00, 1641.50it/s]warmup should be done:  73%|  | 2181/3000 [00:01<00:00, 1629.82it/s]warmup should be done:  73%|  | 2183/3000 [00:01<00:00, 1626.29it/s]warmup should be done:  79%|  | 2356/3000 [00:01<00:00, 1680.98it/s]warmup should be done:  78%|  | 2339/3000 [00:01<00:00, 1659.47it/s]warmup should be done:  78%|  | 2340/3000 [00:01<00:00, 1663.44it/s]warmup should be done:  79%|  | 2356/3000 [00:01<00:00, 1682.47it/s]warmup should be done:  79%|  | 2368/3000 [00:01<00:00, 1671.01it/s]warmup should be done:  78%|  | 2354/3000 [00:01<00:00, 1645.16it/s]warmup should be done:  78%|  | 2349/3000 [00:01<00:00, 1643.07it/s]warmup should be done:  78%|  | 2350/3000 [00:01<00:00, 1636.53it/s]warmup should be done:  84%| | 2525/3000 [00:01<00:00, 1683.47it/s]warmup should be done:  84%| | 2509/3000 [00:01<00:00, 1668.40it/s]warmup should be done:  84%| | 2505/3000 [00:01<00:00, 1649.78it/s]warmup should be done:  85%| | 2536/3000 [00:01<00:00, 1670.82it/s]warmup should be done:  84%| | 2525/3000 [00:01<00:00, 1653.18it/s]warmup should be done:  84%| | 2521/3000 [00:01<00:00, 1652.16it/s]warmup should be done:  84%| | 2517/3000 [00:01<00:00, 1653.98it/s]warmup should be done:  84%| | 2518/3000 [00:01<00:00, 1649.17it/s]warmup should be done:  90%| | 2694/3000 [00:01<00:00, 1684.93it/s]warmup should be done:  89%| | 2676/3000 [00:01<00:00, 1664.46it/s]warmup should be done:  89%| | 2672/3000 [00:01<00:00, 1653.71it/s]warmup should be done:  90%| | 2704/3000 [00:01<00:00, 1671.61it/s]warmup should be done:  90%| | 2685/3000 [00:01<00:00, 1660.22it/s]warmup should be done:  90%| | 2691/3000 [00:01<00:00, 1642.73it/s]warmup should be done:  90%| | 2687/3000 [00:01<00:00, 1636.49it/s]warmup should be done:  90%| | 2686/3000 [00:01<00:00, 1656.73it/s]warmup should be done:  95%|| 2863/3000 [00:01<00:00, 1683.12it/s]warmup should be done:  95%|| 2839/3000 [00:01<00:00, 1656.84it/s]warmup should be done:  96%|| 2873/3000 [00:01<00:00, 1675.65it/s]warmup should be done:  95%|| 2843/3000 [00:01<00:00, 1647.87it/s]warmup should be done:  95%|| 2853/3000 [00:01<00:00, 1664.65it/s]warmup should be done:  95%|| 2851/3000 [00:01<00:00, 1633.25it/s]warmup should be done:  95%|| 2854/3000 [00:01<00:00, 1661.49it/s]warmup should be done:  95%|| 2856/3000 [00:01<00:00, 1554.93it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1679.65it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1678.01it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1663.78it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1661.62it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1661.04it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1660.97it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1655.15it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1637.80it/s]2022-12-12 04:59:50.167807: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fa25b795d90 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 04:59:50.167868: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 04:59:50.766440: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f847802e830 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 04:59:50.766506: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 04:59:51.006842: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f8470031cc0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 04:59:51.006914: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 04:59:51.149171: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f84a402a490 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 04:59:51.149233: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 04:59:51.221687: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f840802d8c0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 04:59:51.221756: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 04:59:51.246892: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fa24b834ec0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 04:59:51.246961: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 04:59:51.299150: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f847802a490 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 04:59:51.299222: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 04:59:51.301935: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fa25f830740 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 04:59:51.301976: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 04:59:52.503524: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 04:59:53.004706: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 04:59:53.231024: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 04:59:53.514543: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 04:59:53.525011: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 04:59:53.550259: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 04:59:53.633354: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 04:59:53.641601: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 04:59:55.538315: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 04:59:55.861713: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 04:59:56.091887: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 04:59:56.394218: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 04:59:56.447242: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 04:59:56.476978: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 04:59:56.484324: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 04:59:56.542852: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
[HCTR][05:00:21.087][ERROR][RK0][tid #140335439148800]: replica 5 reaches 1000, calling init pre replica
[HCTR][05:00:21.087][ERROR][RK0][tid #140335439148800]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][05:00:21.098][ERROR][RK0][tid #140335439148800]: coll ps creation done
[HCTR][05:00:21.098][ERROR][RK0][tid #140335439148800]: replica 5 waits for coll ps creation barrier
[HCTR][05:00:21.114][ERROR][RK0][tid #140336437389056]: replica 1 reaches 1000, calling init pre replica
[HCTR][05:00:21.114][ERROR][RK0][tid #140336437389056]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][05:00:21.116][ERROR][RK0][main]: replica 0 reaches 1000, calling init pre replica
[HCTR][05:00:21.116][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][05:00:21.119][ERROR][RK0][tid #140336437389056]: coll ps creation done
[HCTR][05:00:21.119][ERROR][RK0][tid #140336437389056]: replica 1 waits for coll ps creation barrier
[HCTR][05:00:21.123][ERROR][RK0][main]: coll ps creation done
[HCTR][05:00:21.123][ERROR][RK0][main]: replica 0 waits for coll ps creation barrier
[HCTR][05:00:21.156][ERROR][RK0][main]: replica 4 reaches 1000, calling init pre replica
[HCTR][05:00:21.156][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][05:00:21.162][ERROR][RK0][main]: coll ps creation done
[HCTR][05:00:21.162][ERROR][RK0][main]: replica 4 waits for coll ps creation barrier
[HCTR][05:00:21.177][ERROR][RK0][tid #140335439148800]: replica 7 reaches 1000, calling init pre replica
[HCTR][05:00:21.178][ERROR][RK0][tid #140335439148800]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][05:00:21.186][ERROR][RK0][tid #140335439148800]: coll ps creation done
[HCTR][05:00:21.186][ERROR][RK0][tid #140335439148800]: replica 7 waits for coll ps creation barrier
[HCTR][05:00:21.214][ERROR][RK0][main]: replica 6 reaches 1000, calling init pre replica
[HCTR][05:00:21.214][ERROR][RK0][tid #140335439148800]: replica 2 reaches 1000, calling init pre replica
[HCTR][05:00:21.214][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][05:00:21.214][ERROR][RK0][tid #140335439148800]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][05:00:21.219][ERROR][RK0][main]: coll ps creation done
[HCTR][05:00:21.219][ERROR][RK0][main]: replica 6 waits for coll ps creation barrier
[HCTR][05:00:21.220][ERROR][RK0][main]: replica 3 reaches 1000, calling init pre replica
[HCTR][05:00:21.220][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][05:00:21.222][ERROR][RK0][tid #140335439148800]: coll ps creation done
[HCTR][05:00:21.222][ERROR][RK0][tid #140335439148800]: replica 2 waits for coll ps creation barrier
[HCTR][05:00:21.227][ERROR][RK0][main]: coll ps creation done
[HCTR][05:00:21.227][ERROR][RK0][main]: replica 3 waits for coll ps creation barrier
[HCTR][05:00:21.227][ERROR][RK0][main]: replica 0 preparing frequency
[HCTR][05:00:22.067][ERROR][RK0][main]: replica 0 preparing frequency done
[HCTR][05:00:22.109][ERROR][RK0][main]: replica 0 calling init per replica
[HCTR][05:00:22.109][ERROR][RK0][tid #140335439148800]: replica 5 calling init per replica
[HCTR][05:00:22.109][ERROR][RK0][tid #140335439148800]: replica 7 calling init per replica
[HCTR][05:00:22.109][ERROR][RK0][tid #140336437389056]: replica 1 calling init per replica
[HCTR][05:00:22.109][ERROR][RK0][tid #140335439148800]: replica 2 calling init per replica
[HCTR][05:00:22.109][ERROR][RK0][main]: replica 6 calling init per replica
[HCTR][05:00:22.109][ERROR][RK0][main]: replica 3 calling init per replica
[HCTR][05:00:22.109][ERROR][RK0][main]: replica 4 calling init per replica
[HCTR][05:00:22.109][ERROR][RK0][main]: Calling build_v2
[HCTR][05:00:22.109][ERROR][RK0][main]: Calling build_v2
[HCTR][05:00:22.109][ERROR][RK0][tid #140335439148800]: Calling build_v2
[HCTR][05:00:22.109][ERROR][RK0][tid #140335439148800]: Calling build_v2
[HCTR][05:00:22.109][ERROR][RK0][tid #140336437389056]: Calling build_v2
[HCTR][05:00:22.109][ERROR][RK0][tid #140335439148800]: Calling build_v2
[HCTR][05:00:22.109][ERROR][RK0][main]: Calling build_v2
[HCTR][05:00:22.109][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][05:00:22.109][ERROR][RK0][tid #140335439148800]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][05:00:22.109][ERROR][RK0][main]: Calling build_v2
[HCTR][05:00:22.109][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][05:00:22.109][ERROR][RK0][tid #140335439148800]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][05:00:22.109][ERROR][RK0][tid #140335439148800]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][05:00:22.109][ERROR][RK0][tid #140336437389056]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][05:00:22.109][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][05:00:22.109][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[2022-12-12 05:00:22.113519: E[ /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:178] v100x8, slow pcie2022-12-12 05:00:22
.113567: E [[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 05:00:22:.2022-12-12 05:00:22178113627.] : 113613[v100x8, slow pcieE: 
 E2022-12-12 05:00:22/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc .[:/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc113657[2022-12-12 05:00:22196:: .2022-12-12 05:00:22] 178E113703.assigning 0 to cpu] [ : 113708
v100x8, slow pcie/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE: 2022-12-12 05:00:22
: E.178/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[ [113758] [:2022-12-12 05:00:22/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: v100x8, slow pcie2022-12-12 05:00:222022-12-12 05:00:22[196.:E
..] 1138261782022-12-12 05:00:22 113805113836[assigning 0 to cpu: ] ./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: : 2022-12-12 05:00:22
Ev100x8, slow pcie113848:EE. 
: 178  113921/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE] [[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: : v100x8, slow pcie2022-12-12 05:00:222022-12-12 05:00:22::E196/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
..178212 ] :114021114026] [] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccassigning 0 to cpu178: : v100x8, slow pcie2022-12-12 05:00:22build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8:
] EE
.
196v100x8, slow pcie  114099] [
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: [assigning 0 to cpu2022-12-12 05:00:22:2022-12-12 05:00:22[:E2022-12-12 05:00:22
.196.2022-12-12 05:00:22212 .114168] 114177.] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[114184: assigning 0 to cpu: 114196build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8:2022-12-12 05:00:22: E
E: 
196.E  E] 114259 /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[ [assigning 0 to cpu: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc::2022-12-12 05:00:22/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 05:00:22
E:196212.:. 213] ] 114347196114374/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] remote time is 8.68421assigning 0 to cpu
build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8: ] [[: :

Eassigning 0 to cpu2022-12-12 05:00:222022-12-12 05:00:22E212 
.[. ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[1144782022-12-12 05:00:22114450/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccbuild asymm link desc with 8X Tesla V100-SXM2-32GB out of 8:2022-12-12 05:00:22: .: :
213.E114515[E212] 114527 [: 2022-12-12 05:00:22 ] remote time is 8.68421: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 05:00:22E./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccbuild asymm link desc with 8X Tesla V100-SXM2-32GB out of 8
E:. 114582:
 214[114604/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: 212/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] 2022-12-12 05:00:22: [:E] :cpu time is 97.0588.E2022-12-12 05:00:22213 build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8212
114678 .] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
] : /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc114708remote time is 8.68421:build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8E:[: 
212
 2132022-12-12 05:00:22E[] [/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] . 2022-12-12 05:00:22build asymm link desc with 8X Tesla V100-SXM2-32GB out of 82022-12-12 05:00:22:remote time is 8.68421114827/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.
.214
: :114874114865] [E213[: : cpu time is 97.05882022-12-12 05:00:22 ] 2022-12-12 05:00:22EE
./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccremote time is 8.68421.  114939:
114945/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: 213: [::E] E2022-12-12 05:00:22213214 remote time is 8.68421 .] ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc115064remote time is 8.68421cpu time is 97.0588::: [

213214E2022-12-12 05:00:22] []  .remote time is 8.684212022-12-12 05:00:22cpu time is 97.0588/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc115159
.
:: 115194[214E: 2022-12-12 05:00:22]  E.cpu time is 97.0588/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc 115253
:/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: 214:E] 214 cpu time is 97.0588] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
cpu time is 97.0588:
214] cpu time is 97.0588
[2022-12-12 05:01:39.972446: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:118] solver created. now build & solve
[2022-12-12 05:01:40. 12347: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:123] solver built. now solve
[2022-12-12 05:01:40. 12410: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:919] num_cached_nodes = 20000000
[2022-12-12 05:01:40.124147: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:127] solver solved
[2022-12-12 05:01:40.124233: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:195] 0 solved master
[2022-12-12 05:01:40.124267: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 0 solved
[2022-12-12 05:01:40.124297: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 0 initing device 0
[2022-12-12 05:01:40.124748: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 05:01:40.125596: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 05:01:40.126249: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 05:01:40.139494: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 1 solved
[2022-12-12 05:01:40.139565: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 1 initing device 1
[2022-12-12 05:01:40.139842: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 4 solved
[2022-12-12 05:01:40.139901: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 4 initing device 4
[[2022-12-12 05:01:402022-12-12 05:01:40..139903139906: : [EE2022-12-12 05:01:40  ./hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc139969::: 202202E] ]  3 solved6 solved/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu

:1980] [[eager alloc mem 381.47 MB2022-12-12 05:01:402022-12-12 05:01:40
..140070140067: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc::205205] ] worker 0 thread 6 initing device 6worker 0 thread 3 initing device 3

[2022-12-12 05:01:40.140296: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[[2022-12-12 05:01:402022-12-12 05:01:40..140537140537: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::19801980] ] eager alloc mem 381.47 MBeager alloc mem 381.47 MB

[2022-12-12 05:01:40.142043: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202[] 2022-12-12 05:01:407 solved.
142080: E [/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc2022-12-12 05:01:40:.202142131] : 2 solvedE
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] [worker 0 thread 7 initing device 72022-12-12 05:01:40
.142178: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 2 initing device 2
[2022-12-12 05:01:40.142550: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 05:01:40.142585: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 05:01:40.142638: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 05:01:40.142860: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 05:01:40.143891: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 05:01:40.144279: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 5 solved
[2022-12-12 05:01:40.144354: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 5 initing device 5
[2022-12-12 05:01:40.144398: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 05:01:40.144822: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 05:01:40.147010: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 05:01:40.147153: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 05:01:40.147261: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 05:01:40.147382: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 05:01:40.147980: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 05:01:40.148480: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 05:01:40.149517: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 05:01:40.151561: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 05:01:40.151839: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 05:01:40.153265: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 05:01:40.206650: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 2.00 Bytes
[2022-12-12 05:01:40.207023: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1024.00 Bytes
[2022-12-12 05:01:40.212298: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-12 05:01:40.212370: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 2
[2022-12-12 05:01:40.212415: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 05:01:40.213314: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 76.68 MB
[2022-12-12 05:01:40.214100: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 05:01:40.215191: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 05:01:40.215279: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 05:01:40.215945: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 05:01:40.215997: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 9.54 GB
[2022-12-12 05:01:40.230290: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 2.00 Bytes
[2022-12-12 05:01:40.230633: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1024.00 Bytes
[2022-12-12 05:01:40.234383: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 2.00 Bytes
[2022-12-12 05:01:40.234716: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1024.00 Bytes
[2022-12-12 05:01:40.235765: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-12 05:01:40.235830: E[ 2022-12-12 05:01:40/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc.:235819638: ] Eeager release cuda mem 2 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 2.00 Bytes
[2022-12-12 05:01:40.235896: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 05:01:40.235903: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 2.00 Bytes
[2022-12-12 05:01:40.236133: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1024.00 Bytes
[2022-12-12 05:01:40.236221: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1024.00 Bytes
[2022-12-12 05:01:40.236803: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 76.68 MB
[2022-12-12 05:01:40.237337: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 2.00 Bytes
[[2022-12-12 05:01:402022-12-12 05:01:40..237400237400: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::19801980] ] eager alloc mem 2.00 Byteseager alloc mem 2.00 Bytes

[2022-12-12 05:01:40.237487: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 05:01:40.237672: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1024.00 Bytes
[[2022-12-12 05:01:402022-12-12 05:01:40..237734237736: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::19801980] ] eager alloc mem 1024.00 Byteseager alloc mem 1024.00 Bytes

[2022-12-12 05:01:40.238543: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 05:01:40.238631: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 05:01:40.239299: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 05:01:40.239340: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 9.54 GB
[2022-12-12 05:01:40.239636: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-12 05:01:40.239697: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 2
[2022-12-12 05:01:40.239738: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 05:01:40.241177: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-12 05:01:40.241240: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 2
[2022-12-12 05:01:40.241280: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 05:01:40.241298: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-12 05:01:40.241370: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 2
[2022-12-12 05:01:40.241410: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 05:01:40.252354: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 76.68 MB
[2022-12-12 05:01:40.253153: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 76.68 MB
[2022-12-12 05:01:40.253661: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 76.68 MB
[2022-12-12 05:01:40.254226: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-12 05:01:40.254294: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 2
[2022-12-12 05:01:40.254334[: 2022-12-12 05:01:40E. 254328/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc: :E638 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cceager release cuda mem 400000000:
638] eager release cuda mem 1024
[2022-12-12 05:01:40.[2543862022-12-12 05:01:40: .E254408 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccE: 638/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc] :eager release cuda mem 1024638
] eager release cuda mem 2
[2022-12-12 05:01:40[.2022-12-12 05:01:40254473.: 254477E:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc :/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc638:] 638eager release cuda mem 2] 
eager release cuda mem 400000000
[2022-12-12 05:01:40.254536: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 05:01:40.254963: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 05:01:40.255064: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 05:01:40.255344: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 05:01:40.255926: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 76.68 MB
[2022-12-12 05:01:40.256014: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 05:01:40.256102: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 05:01:40.256135: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 05:01:40.256218: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 05:01:40.256400: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 05:01:40.256494: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 05:01:40.256519: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 76.68 MB
[2022-12-12 05:01:40.256775: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 05:01:40.256816: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 9.54 GB
[2022-12-12 05:01:40.256887: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 05:01:40.256927: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 9.54 GB
[2022-12-12 05:01:40.257030: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 76.68 MB
[2022-12-12 05:01:40.257167: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 05:01:40.257235: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 9.54 GB
[2022-12-12 05:01:40.290742: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 05:01:40.290896: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 05:01:40.290936: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 05:01:40.291766: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 05:01:40.291854: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 05:01:40.291917: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 05:01:40.291957: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 05:01:40.292003: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 05:01:40.292040: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 05:01:40.292408: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 05:01:40.292453: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 9.54 GB
[2022-12-12 05:01:40.292558: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[[2022-12-12 05:01:402022-12-12 05:01:40..292595292598: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::6381980] ] eager release cuda mem 25855eager alloc mem 9.54 GB

[2022-12-12 05:01:40.292659: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 9.54 GB
[[[[[[[[2022-12-12 05:01:422022-12-12 05:01:422022-12-12 05:01:422022-12-12 05:01:422022-12-12 05:01:422022-12-12 05:01:422022-12-12 05:01:422022-12-12 05:01:42........260776260775260774260774260774260774260774260783: : : : : : : : EEEEEEEE        /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::::::::19801980198019801980198019801980] ] ] ] ] ] ] ] eager alloc mem 611.00 KBeager alloc mem 611.00 KBeager alloc mem 611.00 KBeager alloc mem 611.00 KBeager alloc mem 611.00 KBeager alloc mem 611.00 KBeager alloc mem 611.00 KBeager alloc mem 611.00 KB







[2022-12-12 05:01:42[.2022-12-12 05:01:42261901.[[: [[[261908[2022-12-12 05:01:422022-12-12 05:01:42E2022-12-12 05:01:422022-12-12 05:01:422022-12-12 05:01:42: 2022-12-12 05:01:42.. ...E.261914261915/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc261915261916261919 261922: : :: : : /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc: EE638EEE:E  ]    638 /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cceager release cuda mem 625663/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc] /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::
:::eager release cuda mem 625663:638638638638638
638] ] ] ] ] ] eager release cuda mem 625663eager release cuda mem 625663eager release cuda mem 625663eager release cuda mem 625663eager release cuda mem 625663eager release cuda mem 625663
[




2022-12-12 05:01:42[.2022-12-12 05:01:42262190.: 262221E:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu :/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu1980[:] 2022-12-12 05:01:421980eager alloc mem 611.00 KB[[[.[[] 
2022-12-12 05:01:422022-12-12 05:01:422022-12-12 05:01:422622652022-12-12 05:01:422022-12-12 05:01:42eager alloc mem 611.00 KB...: ..
262277262277262283E262284262286: : :  : : EEE/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuEE   :  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu1980/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:::] ::198019801980eager alloc mem 611.00 KB19801980] ] ] 
] ] eager alloc mem 611.00 KBeager alloc mem 611.00 KBeager alloc mem 611.00 KBeager alloc mem 611.00 KBeager alloc mem 611.00 KB




[2022-12-12 05:01:42.263046: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 05:01:42.263084: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 05:01:42.263119: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 05:01:42.263161: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:[19802022-12-12 05:01:42] .eager alloc mem 611.00 KB263175[[
[: [2022-12-12 05:01:42[2022-12-12 05:01:422022-12-12 05:01:42E2022-12-12 05:01:42.2022-12-12 05:01:42.. .263193.263195263198/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc263199: 263203: : :: E: EE638E E  ]  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cceager release cuda mem 625663/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::
:638:638638638] 638] ] ] eager release cuda mem 625663
] eager release cuda mem 625663eager release cuda mem 625663eager release cuda mem 625663eager release cuda mem 625663



[2022-12-12 05:01:42.263359: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 05:01:42.263386: E[[ 2022-12-12 05:01:42[[2022-12-12 05:01:42/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu.2022-12-12 05:01:422022-12-12 05:01:42.:263396..2633971980: 263400263400: ] E: : Eeager alloc mem 611.00 KB EE 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980::1980] 19801980] eager alloc mem 611.00 KB] ] eager alloc mem 611.00 KB
eager alloc mem 611.00 KBeager alloc mem 611.00 KB


[2022-12-12 05:01:42.263880: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 05:01:42.263939: [E2022-12-12 05:01:42 ./hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc263949:: 638E]  eager release cuda mem 625663/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu
:1980] eager alloc mem 611.00 KB
[2022-12-12 05:01:42.264023: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 05:01:42.264109: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 05:01:42.264176: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:[19802022-12-12 05:01:42] .eager alloc mem 611.00 KB264191
: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663[
2022-12-12 05:01:42.[264226[[2022-12-12 05:01:42: 2022-12-12 05:01:422022-12-12 05:01:42.E..264233 264235264236: /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc: : E:EE[ 638  2022-12-12 05:01:42/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc] /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc.:eager release cuda mem 625663::264276638
638638: ] ] ] Eeager release cuda mem 625663
eager release cuda mem 625663eager release cuda mem 625663 

/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 05:01:42.264366: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] [eager alloc mem 611.00 KB2022-12-12 05:01:42
[[.2022-12-12 05:01:422022-12-12 05:01:42264385..: 264391264392E: :  EE/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu  :/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu1980::] 19801980eager alloc mem 611.00 KB] ] 
eager alloc mem 611.00 KBeager alloc mem 611.00 KB

[2022-12-12 05:01:42.264716: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 05:01:42.264769: E [/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc2022-12-12 05:01:42:.638264784] : eager release cuda mem 625663E
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 05:01:42.264850: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 05:01:42.264938: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 05:01:42.265005: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 05:01:42.265085: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 05:01:42.265119: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 05:01:42.265151: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 05:01:42[.[2022-12-12 05:01:42[2651832022-12-12 05:01:42.2022-12-12 05:01:42: .265187.E265189: 265191 : E: /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccE E: /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu 638/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc] :1980:eager release cuda mem 625663638] 638
] eager alloc mem 611.00 KB] eager release cuda mem 625663
eager release cuda mem 625663

[2022-12-12 05:01:42.265313: E [/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu[2022-12-12 05:01:422022-12-12 05:01:42:..1980265327265325] : : eager alloc mem 611.00 KBEE
  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::19801980] ] eager alloc mem 611.00 KBeager alloc mem 611.00 KB

[2022-12-12 05:01:42.265543: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 05:01:42.265597: E [/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc2022-12-12 05:01:42:.638265611] : eager release cuda mem 625663E
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 05:01:42.265676: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 05:01:42.265751: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 05:01:42.265817: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 05:01:42.265894: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 05:01:42.265960: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 05:01:42.266001: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 05:01:42.266067: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB[
2022-12-12 05:01:42.266086: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 05:01:42.266111: [E2022-12-12 05:01:42 ./hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc266120:: 638E]  eager release cuda mem 625663/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc
:[6382022-12-12 05:01:42] .eager release cuda mem 625663266155
: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 05:01:42.266197: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:[19802022-12-12 05:01:42] .eager alloc mem 611.00 KB266213
: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 05:01:42.266370: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 05:01:42.266426: E [/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc2022-12-12 05:01:42:.638266439] : eager release cuda mem 625663E
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 05:01:42.266505: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 05:01:42.266572: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 05:01:42.266639: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 05:01:42.266705: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 05:01:42.266770: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 05:01:42.266816: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 05:01:42.266881: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 05:01:42.266911: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 05:01:42.266953: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:[6382022-12-12 05:01:42] .[eager release cuda mem 6256632669682022-12-12 05:01:42
: .E266978 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccE: 638/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu] :eager release cuda mem 6256631980
] eager alloc mem 611.00 KB
[2022-12-12 05:01:42.267032: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 05:01:42.267062: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 05:01:42.267199: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 05:01:42.267237: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] [eager release cuda mem 804000002022-12-12 05:01:42
.267252: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 05:01:42.267293: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 80400000
[2022-12-12 05:01:42.267383: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 05:01:42.267427: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 80400000
[2022-12-12 05:01:42.267515: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 05:01:42.267552: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 80400000
[2022-12-12 05:01:42.267626: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 05:01:42.267662: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 80400000
[2022-12-12 05:01:42.267751: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 05:01:42.[2677792022-12-12 05:01:42: .E267787 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccE: [638/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc2022-12-12 05:01:42] :.eager release cuda mem 625663638267809
] : eager release cuda mem 80400000E
 [/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc2022-12-12 05:01:42:.638267845] : eager release cuda mem 625663E
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 80400000
[2022-12-12 05:01:42.267880: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 80400000
[2022-12-12 05:01:42.267951: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 20000000 / 100000000 nodes ( 20.00 %~20.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 80000000 / 100000000 nodes ( 80.00 %) | 9.54 GB | 2.12541 secs 
[2022-12-12 05:01:42.268351: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 20000000 / 100000000 nodes ( 20.00 %~20.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 80000000 / 100000000 nodes ( 80.00 %) | 9.54 GB | 2.12782 secs 
[2022-12-12 05:01:42.269111: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 20000000 / 100000000 nodes ( 20.00 %~20.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 80000000 / 100000000 nodes ( 80.00 %) | 9.54 GB | 2.14437 secs 
[2022-12-12 05:01:42.270001: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 20000000 / 100000000 nodes ( 20.00 %~20.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 80000000 / 100000000 nodes ( 80.00 %) | 9.54 GB | 2.12971 secs 
[2022-12-12 05:01:42.270169: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 20000000 / 100000000 nodes ( 20.00 %~20.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 80000000 / 100000000 nodes ( 80.00 %) | 9.54 GB | 2.12535 secs 
[2022-12-12 05:01:42.270659: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 20000000 / 100000000 nodes ( 20.00 %~20.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 80000000 / 100000000 nodes ( 80.00 %) | 9.54 GB | 2.1307 secs 
[2022-12-12 05:01:42.270823: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 20000000 / 100000000 nodes ( 20.00 %~20.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 80000000 / 100000000 nodes ( 80.00 %) | 9.54 GB | 2.12824 secs 
[2022-12-12 05:01:42.270985: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 20000000 / 100000000 nodes ( 20.00 %~20.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 80000000 / 100000000 nodes ( 80.00 %) | 9.54 GB | 2.13046 secs 
[HCTR][05:01:42.271][ERROR][RK0][tid #140335439148800]: replica 2 calling init per replica done, doing barrier
[HCTR][05:01:42.271][ERROR][RK0][main]: replica 3 calling init per replica done, doing barrier
[HCTR][05:01:42.271][ERROR][RK0][tid #140336437389056]: replica 1 calling init per replica done, doing barrier
[HCTR][05:01:42.271][ERROR][RK0][tid #140335439148800]: replica 7 calling init per replica done, doing barrier
[HCTR][05:01:42.271][ERROR][RK0][main]: replica 0 calling init per replica done, doing barrier
[HCTR][05:01:42.271][ERROR][RK0][tid #140335439148800]: replica 5 calling init per replica done, doing barrier
[HCTR][05:01:42.271][ERROR][RK0][main]: replica 6 calling init per replica done, doing barrier
[HCTR][05:01:42.271][ERROR][RK0][main]: replica 4 calling init per replica done, doing barrier
[HCTR][05:01:42.271][ERROR][RK0][main]: replica 4 calling init per replica done, doing barrier done
[HCTR][05:01:42.271][ERROR][RK0][main]: replica 6 calling init per replica done, doing barrier done
[HCTR][05:01:42.271][ERROR][RK0][tid #140335439148800]: replica 7 calling init per replica done, doing barrier done
[HCTR][05:01:42.271][ERROR][RK0][tid #140335439148800]: replica 5 calling init per replica done, doing barrier done
[HCTR][05:01:42.271][ERROR][RK0][tid #140335439148800]: replica 2 calling init per replica done, doing barrier done
[HCTR][05:01:42.271][ERROR][RK0][main]: replica 3 calling init per replica done, doing barrier done
[HCTR][05:01:42.271][ERROR][RK0][tid #140336437389056]: replica 1 calling init per replica done, doing barrier done
[HCTR][05:01:42.271][ERROR][RK0][main]: replica 0 calling init per replica done, doing barrier done
[HCTR][05:01:42.271][ERROR][RK0][main]: init per replica done
[HCTR][05:01:42.271][ERROR][RK0][main]: init per replica done
[HCTR][05:01:42.271][ERROR][RK0][main]: init per replica done
[HCTR][05:01:42.271][ERROR][RK0][tid #140336437389056]: init per replica done
[HCTR][05:01:42.271][ERROR][RK0][tid #140335439148800]: init per replica done
[HCTR][05:01:42.271][ERROR][RK0][tid #140335439148800]: init per replica done
[HCTR][05:01:42.271][ERROR][RK0][tid #140335439148800]: init per replica done
[HCTR][05:01:42.273][ERROR][RK0][main]: init per replica done








