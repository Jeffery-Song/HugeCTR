2022-12-11 23:49:07.915603: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:07.923571: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:07.930869: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:07.935524: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:07.940685: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:07.953917: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:07.961160: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:07.973846: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.023782: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.030018: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.033056: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.034118: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.035068: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.036058: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.037115: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.038110: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.039054: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.040093: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.041401: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.043177: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.044324: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.044345: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.045748: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.045964: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.047067: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.047518: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.048522: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.049096: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.050478: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.050825: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.051927: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.052491: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.053635: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.054014: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.055423: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.055868: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.057395: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.058478: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.059459: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.060472: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.065931: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 23:49:08.066063: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.067259: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.068316: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.069355: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.070556: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.071592: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.072610: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.073663: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.075001: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.076091: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.077285: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.079897: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.082386: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.082399: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.085707: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.085800: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.086195: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.088963: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.089179: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.089721: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.089986: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.092277: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.092550: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.093130: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.093672: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.094158: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.095560: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.095785: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.097431: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.097969: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.098355: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.099923: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.100149: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.100984: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.101438: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.101784: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.102429: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.103407: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.103893: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.104894: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.105332: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.105880: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.106258: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.107888: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.108405: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.108725: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.111491: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.111685: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.114866: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.115229: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.115484: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.115559: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.117944: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.117992: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.118235: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.118529: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.151719: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.154187: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.154625: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.154773: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.155386: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.156786: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.157695: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.158175: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.158635: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.159679: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.161311: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.161363: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.161805: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.162049: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.164179: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.165301: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.165324: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.165876: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.166019: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.167752: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.168817: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.168902: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.170051: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.170139: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.171786: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.172950: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.173084: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.174015: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.174056: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.175694: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.176660: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.176799: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.177508: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.177544: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.179398: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.180060: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.180248: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.181130: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.181213: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.182707: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.183518: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.183781: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.184469: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.184521: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.186079: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.186970: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.187479: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.188207: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.188304: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.190074: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.190823: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.191299: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.191981: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.192027: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.194300: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.194339: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.194659: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.195441: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.195479: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.197625: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.197898: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.198012: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.198206: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.198985: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.199026: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.200805: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.202018: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.202113: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.202275: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.202436: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.203291: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.203467: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.205612: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.206633: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.206844: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.206886: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.207039: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.208134: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.208260: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.210150: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.211590: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.211762: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.211811: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.212715: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.212754: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.213700: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.214815: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.216116: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.216340: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.216361: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.217322: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.217357: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.218433: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.219443: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.221399: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.221629: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.222057: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.223244: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.223552: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.223923: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.224967: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.226788: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.227229: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.227933: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.228154: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.228551: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.228794: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.229781: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.231255: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.232404: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.232684: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.233110: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.233554: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.234205: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.235362: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.236223: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 23:49:08.236960: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.237134: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.237533: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.237868: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.238487: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.239356: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.240972: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.241109: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.241515: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.242453: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.242646: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.243514: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.244994: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.245042: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.245506: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.245765: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.246555: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.246904: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.247723: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.249685: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.250020: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.250168: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.251265: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.252208: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.253570: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 23:49:08.253699: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.254128: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.254255: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.254692: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 23:49:08.255210: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.256016: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.257510: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.258698: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.259288: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.261070: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 23:49:08.261476: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.262114: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.262199: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.263721: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.264238: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.264332: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 23:49:08.265503: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.266027: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.267078: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.268947: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.268976: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.269560: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.269648: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.270801: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.272814: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.273262: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.274011: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.274072: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.277675: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.278214: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.278852: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.278989: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.282460: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.282977: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.283948: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.317559: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.318208: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.322880: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.323504: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.334410: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.335018: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.341272: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.345504: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 23:49:08.349208: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 23:49:08.354620: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.357895: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.383025: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.383057: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.387970: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:08.388123: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:09.378947: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:09.379866: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:09.380414: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:09.380881: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 23:49:09.380936: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-11 23:49:09.399842: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:09.400487: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:09.401195: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:09.401966: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:09.402655: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:09.403123: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-11 23:49:09.450491: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 23:49:09.450706: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 23:49:09.499851: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12340
2022-12-11 23:49:09.614194: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:09.614814: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:09.615359: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:09.615824: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 23:49:09.615879: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-11 23:49:09.633783: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:09.634574: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:09.635080: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:09.635907: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:09.636447: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:09.636921: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:1/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-11 23:49:09.647589: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:09.648189: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:09.648758: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:09.649415: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 23:49:09.649469: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-11 23:49:09.666979: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:09.667640: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:09.668147: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:09.668727: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:09.669538: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:09.670034: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:2/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-11 23:49:09.670148: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:09.670739: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:09.671284: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:09.671775: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 23:49:09.671822: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-11 23:49:09.688819: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:09.689455: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:09.689981: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:09.690048: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:09.691053: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 23:49:09.691148: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-11 23:49:09.691281: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:09.691967: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:09.692544: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:09.693050: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:09.693592: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:09.693707: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:3/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-11 23:49:09.694291: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:09.694813: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:09.695314: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 23:49:09.695361: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-11 23:49:09.706737: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:09.707384: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:09.707927: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:09.708439: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 23:49:09.708497: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-11 23:49:09.708659: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:09.708868: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 23:49:09.709031: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 23:49:09.709519: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:09.710046: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:09.710630: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:09.710774: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12341
2022-12-11 23:49:09.710834: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:09.711658: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:09.711916: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:09.712627: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:7/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-11 23:49:09.712957: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:09.713457: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 23:49:09.713505: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-11 23:49:09.713877: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:09.714489: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:09.714993: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:09.715573: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:09.716075: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:09.716541: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:4/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-11 23:49:09.727320: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:09.727984: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:09.728507: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:09.729082: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:09.729606: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:09.730077: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:6/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-11 23:49:09.731428: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:09.732022: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:09.732538: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:09.733104: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:09.733624: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:49:09.734091: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:5/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-11 23:49:09.741416: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 23:49:09.741627: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 23:49:09.742708: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12343
2022-12-11 23:49:09.757357: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 23:49:09.757556: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 23:49:09.759362: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12347
2022-12-11 23:49:09.762328: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 23:49:09.762520: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 23:49:09.764334: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12344
2022-12-11 23:49:09.768344: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 23:49:09.768515: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 23:49:09.770910: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12342
2022-12-11 23:49:09.775696: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 23:49:09.775867: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 23:49:09.776748: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12346
2022-12-11 23:49:09.779263: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 23:49:09.779415: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 23:49:09.782027: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12345
[HCTR][23:49:11.021][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][23:49:11.021][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][23:49:11.024][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][23:49:11.034][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][23:49:11.034][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][23:49:11.035][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][23:49:11.078][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][23:49:11.078][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 1it [00:01,  1.56s/it]warmup run: 1it [00:01,  1.53s/it]warmup run: 93it [00:01, 77.78it/s]warmup run: 1it [00:01,  1.50s/it]warmup run: 97it [00:01, 82.73it/s]warmup run: 1it [00:01,  1.51s/it]warmup run: 186it [00:01, 168.93it/s]warmup run: 1it [00:01,  1.50s/it]warmup run: 102it [00:01, 88.14it/s]warmup run: 1it [00:01,  1.58s/it]warmup run: 193it [00:01, 178.21it/s]warmup run: 99it [00:01, 85.05it/s]warmup run: 279it [00:01, 269.76it/s]warmup run: 95it [00:01, 82.19it/s]warmup run: 203it [00:01, 189.66it/s]warmup run: 98it [00:01, 80.93it/s]warmup run: 290it [00:01, 284.40it/s]warmup run: 196it [00:01, 181.95it/s]warmup run: 374it [00:01, 377.97it/s]warmup run: 195it [00:01, 183.23it/s]warmup run: 1it [00:01,  1.47s/it]warmup run: 1it [00:01,  1.47s/it]warmup run: 305it [00:01, 302.45it/s]warmup run: 200it [00:01, 180.23it/s]warmup run: 385it [00:01, 391.38it/s]warmup run: 293it [00:01, 288.52it/s]warmup run: 469it [00:02, 483.25it/s]warmup run: 292it [00:01, 289.92it/s]warmup run: 97it [00:01, 85.40it/s]warmup run: 99it [00:01, 87.15it/s]warmup run: 406it [00:01, 416.85it/s]warmup run: 302it [00:01, 290.40it/s]warmup run: 392it [00:01, 401.85it/s]warmup run: 482it [00:02, 498.64it/s]warmup run: 567it [00:02, 585.90it/s]warmup run: 386it [00:01, 395.11it/s]warmup run: 195it [00:01, 185.51it/s]warmup run: 199it [00:01, 189.26it/s]warmup run: 507it [00:02, 527.92it/s]warmup run: 400it [00:01, 398.86it/s]warmup run: 492it [00:02, 513.35it/s]warmup run: 584it [00:02, 607.08it/s]warmup run: 665it [00:02, 676.25it/s]warmup run: 480it [00:02, 496.99it/s]warmup run: 294it [00:01, 296.36it/s]warmup run: 300it [00:01, 302.09it/s]warmup run: 610it [00:02, 634.21it/s]warmup run: 503it [00:02, 514.58it/s]warmup run: 686it [00:02, 702.21it/s]warmup run: 592it [00:02, 615.19it/s]warmup run: 762it [00:02, 748.73it/s]warmup run: 581it [00:02, 605.11it/s]warmup run: 396it [00:01, 414.75it/s]warmup run: 399it [00:01, 415.14it/s]warmup run: 714it [00:02, 728.17it/s]warmup run: 605it [00:02, 618.91it/s]warmup run: 691it [00:02, 701.79it/s]warmup run: 790it [00:02, 784.40it/s]warmup run: 857it [00:02, 796.68it/s]warmup run: 685it [00:02, 704.80it/s]warmup run: 496it [00:01, 525.44it/s]warmup run: 490it [00:01, 504.30it/s]warmup run: 814it [00:02, 791.95it/s]warmup run: 707it [00:02, 710.26it/s]warmup run: 791it [00:02, 774.48it/s]warmup run: 890it [00:02, 838.61it/s]warmup run: 953it [00:02, 840.59it/s]warmup run: 787it [00:02, 782.63it/s]warmup run: 596it [00:02, 625.90it/s]warmup run: 580it [00:02, 567.45it/s]warmup run: 914it [00:02, 828.16it/s]warmup run: 806it [00:02, 778.77it/s]warmup run: 891it [00:02, 832.35it/s]warmup run: 989it [00:02, 879.36it/s]warmup run: 1053it [00:02, 882.30it/s]warmup run: 890it [00:02, 845.34it/s]warmup run: 696it [00:02, 712.97it/s]warmup run: 678it [00:02, 659.94it/s]warmup run: 1015it [00:02, 875.92it/s]warmup run: 906it [00:02, 835.46it/s]warmup run: 990it [00:02, 875.15it/s]warmup run: 1088it [00:02, 902.75it/s]warmup run: 1151it [00:02, 909.59it/s]warmup run: 992it [00:02, 891.13it/s]warmup run: 794it [00:02, 779.68it/s]warmup run: 779it [00:02, 744.97it/s]warmup run: 1118it [00:02, 916.13it/s]warmup run: 1005it [00:02, 870.32it/s]warmup run: 1090it [00:02, 909.00it/s]warmup run: 1186it [00:02, 917.39it/s]warmup run: 1251it [00:02, 935.59it/s]warmup run: 1094it [00:02, 925.88it/s]warmup run: 893it [00:02, 834.28it/s]warmup run: 876it [00:02, 801.24it/s]warmup run: 1218it [00:02, 930.85it/s]warmup run: 1104it [00:02, 894.55it/s]warmup run: 1190it [00:02, 933.05it/s]warmup run: 1286it [00:02, 940.55it/s]warmup run: 1356it [00:02, 967.78it/s]warmup run: 993it [00:02, 878.78it/s]warmup run: 1195it [00:02, 893.82it/s]warmup run: 974it [00:02, 849.26it/s]warmup run: 1317it [00:02, 942.24it/s]warmup run: 1202it [00:02, 913.66it/s]warmup run: 1386it [00:02, 957.68it/s]warmup run: 1289it [00:02, 932.14it/s]warmup run: 1461it [00:03, 990.75it/s]warmup run: 1095it [00:02, 918.10it/s]warmup run: 1295it [00:02, 921.24it/s]warmup run: 1072it [00:02, 883.53it/s]warmup run: 1416it [00:02, 952.56it/s]warmup run: 1299it [00:02, 926.66it/s]warmup run: 1486it [00:03, 967.82it/s]warmup run: 1388it [00:02, 947.60it/s]warmup run: 1564it [00:03, 999.97it/s]warmup run: 1195it [00:02, 941.04it/s]warmup run: 1392it [00:02, 934.46it/s]warmup run: 1173it [00:02, 917.12it/s]warmup run: 1514it [00:03, 959.21it/s]warmup run: 1396it [00:02, 929.76it/s]warmup run: 1487it [00:03, 959.58it/s]warmup run: 1585it [00:03, 971.51it/s]warmup run: 1666it [00:03, 1004.82it/s]warmup run: 1296it [00:02, 958.75it/s]warmup run: 1489it [00:03, 943.63it/s]warmup run: 1275it [00:02, 944.86it/s]warmup run: 1612it [00:03, 961.29it/s]warmup run: 1492it [00:03, 928.36it/s]warmup run: 1586it [00:03, 967.35it/s]warmup run: 1685it [00:03, 977.58it/s]warmup run: 1768it [00:03, 1003.90it/s]warmup run: 1397it [00:02, 971.90it/s]warmup run: 1588it [00:03, 956.85it/s]warmup run: 1376it [00:02, 961.89it/s]warmup run: 1713it [00:03, 972.64it/s]warmup run: 1588it [00:03, 935.81it/s]warmup run: 1686it [00:03, 974.77it/s]warmup run: 1787it [00:03, 987.30it/s]warmup run: 1870it [00:03, 991.74it/s] warmup run: 1498it [00:02, 981.62it/s]warmup run: 1690it [00:03, 973.38it/s]warmup run: 1477it [00:03, 974.96it/s]warmup run: 1816it [00:03, 986.82it/s]warmup run: 1685it [00:03, 944.06it/s]warmup run: 1785it [00:03, 977.89it/s]warmup run: 1887it [00:03, 989.43it/s]warmup run: 1970it [00:03, 991.57it/s]warmup run: 1599it [00:03, 987.46it/s]warmup run: 1794it [00:03, 990.98it/s]warmup run: 1577it [00:03, 977.76it/s]warmup run: 1917it [00:03, 990.46it/s]warmup run: 1781it [00:03, 946.58it/s]warmup run: 1884it [00:03, 979.21it/s]warmup run: 1987it [00:03, 987.02it/s]warmup run: 2086it [00:03, 1040.32it/s]warmup run: 1700it [00:03, 989.79it/s]warmup run: 1894it [00:03, 977.97it/s]warmup run: 1677it [00:03, 981.35it/s]warmup run: 2019it [00:03, 997.15it/s]warmup run: 1878it [00:03, 951.35it/s]warmup run: 1984it [00:03, 982.55it/s]warmup run: 2099it [00:03, 1025.48it/s]warmup run: 2208it [00:03, 1093.56it/s]warmup run: 1800it [00:03, 986.11it/s]warmup run: 1998it [00:03, 993.47it/s]warmup run: 1777it [00:03, 986.08it/s]warmup run: 2141it [00:03, 1061.16it/s]warmup run: 1975it [00:03, 955.46it/s]warmup run: 2097it [00:03, 1024.76it/s]warmup run: 2216it [00:03, 1068.12it/s]warmup run: 2330it [00:03, 1130.73it/s]warmup run: 1900it [00:03, 989.61it/s]warmup run: 2118it [00:03, 1052.38it/s]warmup run: 1877it [00:03, 990.16it/s]warmup run: 2263it [00:03, 1106.20it/s]warmup run: 2087it [00:03, 1003.78it/s]warmup run: 2212it [00:03, 1062.00it/s]warmup run: 2332it [00:03, 1094.21it/s]warmup run: 2453it [00:03, 1158.41it/s]warmup run: 2001it [00:03, 992.80it/s]warmup run: 2238it [00:03, 1095.09it/s]warmup run: 1977it [00:03, 983.13it/s]warmup run: 2385it [00:03, 1139.35it/s]warmup run: 2206it [00:03, 1057.87it/s]warmup run: 2326it [00:03, 1084.68it/s]warmup run: 2446it [00:03, 1107.77it/s]warmup run: 2575it [00:04, 1176.16it/s]warmup run: 2122it [00:03, 1055.38it/s]warmup run: 2358it [00:03, 1125.32it/s]warmup run: 2076it [00:03, 967.95it/s]warmup run: 2507it [00:03, 1162.58it/s]warmup run: 2325it [00:03, 1096.99it/s]warmup run: 2440it [00:03, 1100.52it/s]warmup run: 2561it [00:04, 1117.47it/s]warmup run: 2695it [00:04, 1183.24it/s]warmup run: 2243it [00:03, 1100.31it/s]warmup run: 2479it [00:03, 1148.39it/s]warmup run: 2195it [00:03, 1033.01it/s]warmup run: 2628it [00:04, 1175.77it/s]warmup run: 2444it [00:04, 1123.79it/s]warmup run: 2559it [00:04, 1126.33it/s]warmup run: 2678it [00:04, 1131.22it/s]warmup run: 2817it [00:04, 1191.63it/s]warmup run: 2364it [00:03, 1130.71it/s]warmup run: 2600it [00:04, 1166.17it/s]warmup run: 2316it [00:03, 1083.08it/s]warmup run: 2750it [00:04, 1188.37it/s]warmup run: 2559it [00:04, 1131.30it/s]warmup run: 2676it [00:04, 1139.11it/s]warmup run: 2797it [00:04, 1146.24it/s]warmup run: 2939it [00:04, 1199.31it/s]warmup run: 2484it [00:03, 1150.85it/s]warmup run: 2720it [00:04, 1175.09it/s]warmup run: 2436it [00:03, 1117.36it/s]warmup run: 2872it [00:04, 1196.51it/s]warmup run: 3000it [00:04, 675.80it/s] warmup run: 2676it [00:04, 1141.44it/s]warmup run: 2793it [00:04, 1146.45it/s]warmup run: 2916it [00:04, 1156.84it/s]warmup run: 2604it [00:03, 1163.07it/s]warmup run: 2840it [00:04, 1181.32it/s]warmup run: 2556it [00:04, 1140.36it/s]warmup run: 2995it [00:04, 1204.53it/s]warmup run: 3000it [00:04, 690.22it/s] warmup run: 3000it [00:04, 677.55it/s] warmup run: 2791it [00:04, 1138.05it/s]warmup run: 2912it [00:04, 1156.80it/s]warmup run: 2723it [00:04, 1168.26it/s]warmup run: 2960it [00:04, 1185.99it/s]warmup run: 2675it [00:04, 1153.17it/s]warmup run: 3000it [00:04, 684.21it/s] warmup run: 3000it [00:04, 679.61it/s] warmup run: 2909it [00:04, 1150.47it/s]warmup run: 2843it [00:04, 1177.44it/s]warmup run: 2794it [00:04, 1163.09it/s]warmup run: 3000it [00:04, 668.05it/s] warmup run: 2964it [00:04, 1186.16it/s]warmup run: 2914it [00:04, 1171.63it/s]warmup run: 3000it [00:04, 693.46it/s] warmup run: 3000it [00:04, 682.39it/s] 
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]





warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   6%|▌         | 165/3000 [00:00<00:01, 1649.77it/s]warmup should be done:   5%|▌         | 158/3000 [00:00<00:01, 1579.37it/s]warmup should be done:   6%|▌         | 166/3000 [00:00<00:01, 1654.32it/s]warmup should be done:   5%|▌         | 160/3000 [00:00<00:01, 1597.27it/s]warmup should be done:   6%|▌         | 165/3000 [00:00<00:01, 1646.48it/s]warmup should be done:   6%|▌         | 165/3000 [00:00<00:01, 1640.68it/s]warmup should be done:   5%|▌         | 162/3000 [00:00<00:01, 1610.59it/s]warmup should be done:   6%|▌         | 166/3000 [00:00<00:01, 1650.81it/s]warmup should be done:  11%|█         | 332/3000 [00:00<00:01, 1661.02it/s]warmup should be done:  11%|█         | 332/3000 [00:00<00:01, 1655.93it/s]warmup should be done:  11%|█         | 324/3000 [00:00<00:01, 1622.61it/s]warmup should be done:  11%|█         | 325/3000 [00:00<00:01, 1620.99it/s]warmup should be done:  11%|█         | 322/3000 [00:00<00:01, 1605.90it/s]warmup should be done:  11%|█         | 333/3000 [00:00<00:01, 1660.47it/s]warmup should be done:  11%|█         | 331/3000 [00:00<00:01, 1648.49it/s]warmup should be done:  11%|█         | 333/3000 [00:00<00:01, 1660.70it/s]warmup should be done:  17%|█▋        | 499/3000 [00:00<00:01, 1660.54it/s]warmup should be done:  17%|█▋        | 498/3000 [00:00<00:01, 1654.29it/s]warmup should be done:  16%|█▋        | 489/3000 [00:00<00:01, 1632.63it/s]warmup should be done:  17%|█▋        | 500/3000 [00:00<00:01, 1664.58it/s]warmup should be done:  16%|█▋        | 488/3000 [00:00<00:01, 1620.85it/s]warmup should be done:  17%|█▋        | 500/3000 [00:00<00:01, 1659.00it/s]warmup should be done:  16%|█▌        | 483/3000 [00:00<00:01, 1597.15it/s]warmup should be done:  17%|█▋        | 496/3000 [00:00<00:01, 1638.82it/s]warmup should be done:  22%|██▏       | 666/3000 [00:00<00:01, 1660.97it/s]warmup should be done:  22%|██▏       | 651/3000 [00:00<00:01, 1623.90it/s]warmup should be done:  22%|██▏       | 664/3000 [00:00<00:01, 1652.62it/s]warmup should be done:  22%|██▏       | 653/3000 [00:00<00:01, 1629.30it/s]warmup should be done:  22%|██▏       | 667/3000 [00:00<00:01, 1661.84it/s]warmup should be done:  22%|██▏       | 667/3000 [00:00<00:01, 1659.53it/s]warmup should be done:  21%|██▏       | 643/3000 [00:00<00:01, 1594.98it/s]warmup should be done:  22%|██▏       | 660/3000 [00:00<00:01, 1616.46it/s]warmup should be done:  27%|██▋       | 814/3000 [00:00<00:01, 1624.96it/s]warmup should be done:  28%|██▊       | 833/3000 [00:00<00:01, 1659.61it/s]warmup should be done:  28%|██▊       | 830/3000 [00:00<00:01, 1649.95it/s]warmup should be done:  28%|██▊       | 835/3000 [00:00<00:01, 1665.12it/s]warmup should be done:  27%|██▋       | 816/3000 [00:00<00:01, 1625.90it/s]warmup should be done:  27%|██▋       | 803/3000 [00:00<00:01, 1594.85it/s]warmup should be done:  28%|██▊       | 833/3000 [00:00<00:01, 1648.13it/s]warmup should be done:  28%|██▊       | 825/3000 [00:00<00:01, 1625.30it/s]warmup should be done:  33%|███▎      | 977/3000 [00:00<00:01, 1619.52it/s]warmup should be done:  33%|███▎      | 999/3000 [00:00<00:01, 1654.80it/s]warmup should be done:  33%|███▎      | 1002/3000 [00:00<00:01, 1661.85it/s]warmup should be done:  33%|███▎      | 995/3000 [00:00<00:01, 1644.04it/s]warmup should be done:  32%|███▏      | 963/3000 [00:00<00:01, 1592.73it/s]warmup should be done:  33%|███▎      | 979/3000 [00:00<00:01, 1617.53it/s]warmup should be done:  33%|███▎      | 998/3000 [00:00<00:01, 1637.40it/s]warmup should be done:  33%|███▎      | 988/3000 [00:00<00:01, 1626.42it/s]warmup should be done:  39%|███▉      | 1165/3000 [00:00<00:01, 1655.14it/s]warmup should be done:  38%|███▊      | 1140/3000 [00:00<00:01, 1620.75it/s]warmup should be done:  39%|███▉      | 1169/3000 [00:00<00:01, 1663.76it/s]warmup should be done:  39%|███▊      | 1160/3000 [00:00<00:01, 1644.18it/s]warmup should be done:  38%|███▊      | 1141/3000 [00:00<00:01, 1617.67it/s]warmup should be done:  37%|███▋      | 1123/3000 [00:00<00:01, 1588.97it/s]warmup should be done:  38%|███▊      | 1152/3000 [00:00<00:01, 1630.42it/s]warmup should be done:  39%|███▊      | 1162/3000 [00:00<00:01, 1635.41it/s]warmup should be done:  45%|████▍     | 1336/3000 [00:00<00:00, 1665.62it/s]warmup should be done:  44%|████▍     | 1331/3000 [00:00<00:01, 1654.15it/s]warmup should be done:  43%|████▎     | 1303/3000 [00:00<00:01, 1620.23it/s]warmup should be done:  44%|████▍     | 1325/3000 [00:00<00:01, 1643.22it/s]warmup should be done:  43%|████▎     | 1303/3000 [00:00<00:01, 1615.92it/s]warmup should be done:  43%|████▎     | 1283/3000 [00:00<00:01, 1590.84it/s]warmup should be done:  44%|████▍     | 1316/3000 [00:00<00:01, 1632.68it/s]warmup should be done:  44%|████▍     | 1326/3000 [00:00<00:01, 1633.32it/s]warmup should be done:  50%|█████     | 1503/3000 [00:00<00:00, 1665.38it/s]warmup should be done:  49%|████▉     | 1466/3000 [00:00<00:00, 1622.96it/s]warmup should be done:  50%|████▉     | 1497/3000 [00:00<00:00, 1653.36it/s]warmup should be done:  50%|████▉     | 1490/3000 [00:00<00:00, 1642.16it/s]warmup should be done:  49%|████▉     | 1465/3000 [00:00<00:00, 1616.38it/s]warmup should be done:  49%|████▉     | 1480/3000 [00:00<00:00, 1633.72it/s]warmup should be done:  50%|████▉     | 1490/3000 [00:00<00:00, 1629.69it/s]warmup should be done:  48%|████▊     | 1443/3000 [00:00<00:01, 1545.10it/s]warmup should be done:  56%|█████▌    | 1670/3000 [00:01<00:00, 1661.94it/s]warmup should be done:  55%|█████▌    | 1663/3000 [00:01<00:00, 1646.32it/s]warmup should be done:  54%|█████▍    | 1629/3000 [00:01<00:00, 1610.39it/s]warmup should be done:  55%|█████▌    | 1655/3000 [00:01<00:00, 1633.04it/s]warmup should be done:  54%|█████▍    | 1627/3000 [00:01<00:00, 1604.97it/s]warmup should be done:  55%|█████▍    | 1644/3000 [00:01<00:00, 1628.59it/s]warmup should be done:  55%|█████▌    | 1653/3000 [00:01<00:00, 1621.67it/s]warmup should be done:  53%|█████▎    | 1599/3000 [00:01<00:00, 1547.93it/s]warmup should be done:  61%|██████    | 1828/3000 [00:01<00:00, 1640.34it/s]warmup should be done:  60%|█████▉    | 1791/3000 [00:01<00:00, 1606.84it/s]warmup should be done:  60%|██████    | 1807/3000 [00:01<00:00, 1628.45it/s]warmup should be done:  60%|█████▉    | 1788/3000 [00:01<00:00, 1596.37it/s]warmup should be done:  61%|██████    | 1817/3000 [00:01<00:00, 1624.93it/s]warmup should be done:  61%|██████    | 1819/3000 [00:01<00:00, 1613.35it/s]warmup should be done:  59%|█████▊    | 1756/3000 [00:01<00:00, 1552.80it/s]warmup should be done:  61%|██████    | 1837/3000 [00:01<00:00, 1530.22it/s]warmup should be done:  66%|██████▋   | 1995/3000 [00:01<00:00, 1646.51it/s]warmup should be done:  66%|██████▌   | 1970/3000 [00:01<00:00, 1628.05it/s]warmup should be done:  65%|██████▌   | 1952/3000 [00:01<00:00, 1604.47it/s]warmup should be done:  65%|██████▍   | 1948/3000 [00:01<00:00, 1592.83it/s]warmup should be done:  66%|██████▌   | 1981/3000 [00:01<00:00, 1627.48it/s]warmup should be done:  66%|██████▌   | 1982/3000 [00:01<00:00, 1615.73it/s]warmup should be done:  64%|██████▎   | 1912/3000 [00:01<00:00, 1552.73it/s]warmup should be done:  67%|██████▋   | 1999/3000 [00:01<00:00, 1553.90it/s]warmup should be done:  72%|███████▏  | 2162/3000 [00:01<00:00, 1650.93it/s]warmup should be done:  71%|███████   | 2133/3000 [00:01<00:00, 1627.95it/s]warmup should be done:  70%|███████   | 2113/3000 [00:01<00:00, 1603.51it/s]warmup should be done:  70%|███████   | 2110/3000 [00:01<00:00, 1600.62it/s]warmup should be done:  72%|███████▏  | 2145/3000 [00:01<00:00, 1628.56it/s]warmup should be done:  71%|███████▏  | 2144/3000 [00:01<00:00, 1613.62it/s]warmup should be done:  69%|██████▉   | 2070/3000 [00:01<00:00, 1558.59it/s]warmup should be done:  72%|███████▏  | 2161/3000 [00:01<00:00, 1572.71it/s]warmup should be done:  78%|███████▊  | 2328/3000 [00:01<00:00, 1651.20it/s]warmup should be done:  77%|███████▋  | 2296/3000 [00:01<00:00, 1626.46it/s]warmup should be done:  76%|███████▌  | 2274/3000 [00:01<00:00, 1601.01it/s]warmup should be done:  76%|███████▌  | 2272/3000 [00:01<00:00, 1604.18it/s]warmup should be done:  77%|███████▋  | 2308/3000 [00:01<00:00, 1628.92it/s]warmup should be done:  77%|███████▋  | 2306/3000 [00:01<00:00, 1610.98it/s]warmup should be done:  74%|███████▍  | 2226/3000 [00:01<00:00, 1557.31it/s]warmup should be done:  77%|███████▋  | 2322/3000 [00:01<00:00, 1582.53it/s]warmup should be done:  82%|████████▏ | 2460/3000 [00:01<00:00, 1629.71it/s]warmup should be done:  83%|████████▎ | 2494/3000 [00:01<00:00, 1651.05it/s]warmup should be done:  81%|████████  | 2435/3000 [00:01<00:00, 1602.56it/s]warmup should be done:  82%|████████▏ | 2472/3000 [00:01<00:00, 1631.85it/s]warmup should be done:  81%|████████  | 2435/3000 [00:01<00:00, 1609.01it/s]warmup should be done:  82%|████████▏ | 2468/3000 [00:01<00:00, 1610.17it/s]warmup should be done:  80%|███████▉  | 2388/3000 [00:01<00:00, 1573.58it/s]warmup should be done:  83%|████████▎ | 2489/3000 [00:01<00:00, 1607.74it/s]warmup should be done:  87%|████████▋ | 2624/3000 [00:01<00:00, 1630.93it/s]warmup should be done:  89%|████████▊ | 2660/3000 [00:01<00:00, 1647.68it/s]warmup should be done:  87%|████████▋ | 2596/3000 [00:01<00:00, 1603.35it/s]warmup should be done:  88%|████████▊ | 2636/3000 [00:01<00:00, 1633.31it/s]warmup should be done:  87%|████████▋ | 2599/3000 [00:01<00:00, 1617.30it/s]warmup should be done:  88%|████████▊ | 2630/3000 [00:01<00:00, 1609.80it/s]warmup should be done:  85%|████████▍ | 2547/3000 [00:01<00:00, 1578.34it/s]warmup should be done:  88%|████████▊ | 2655/3000 [00:01<00:00, 1622.42it/s]warmup should be done:  94%|█████████▍| 2825/3000 [00:01<00:00, 1647.33it/s]warmup should be done:  92%|█████████▏| 2757/3000 [00:01<00:00, 1605.05it/s]warmup should be done:  93%|█████████▎| 2800/3000 [00:01<00:00, 1634.25it/s]warmup should be done:  92%|█████████▏| 2763/3000 [00:01<00:00, 1623.04it/s]warmup should be done:  93%|█████████▎| 2788/3000 [00:01<00:00, 1616.27it/s]warmup should be done:  93%|█████████▎| 2791/3000 [00:01<00:00, 1609.21it/s]warmup should be done:  90%|█████████ | 2709/3000 [00:01<00:00, 1589.64it/s]warmup should be done:  94%|█████████▍| 2819/3000 [00:01<00:00, 1625.33it/s]warmup should be done: 100%|█████████▉| 2991/3000 [00:01<00:00, 1649.58it/s]warmup should be done:  97%|█████████▋| 2920/3000 [00:01<00:00, 1610.82it/s]warmup should be done:  98%|█████████▊| 2928/3000 [00:01<00:00, 1631.00it/s]warmup should be done:  99%|█████████▉| 2966/3000 [00:01<00:00, 1639.56it/s]warmup should be done:  98%|█████████▊| 2950/3000 [00:01<00:00, 1615.40it/s]warmup should be done:  98%|█████████▊| 2954/3000 [00:01<00:00, 1615.04it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1651.04it/s]warmup should be done:  96%|█████████▌| 2872/3000 [00:01<00:00, 1600.01it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1636.36it/s]warmup should be done:  99%|█████████▉| 2984/3000 [00:01<00:00, 1631.38it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1626.41it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1626.21it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1626.08it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1616.62it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1612.26it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1580.96it/s]


warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]



warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   6%|▌         | 168/3000 [00:00<00:01, 1677.68it/s]warmup should be done:   6%|▌         | 169/3000 [00:00<00:01, 1687.94it/s]warmup should be done:   6%|▌         | 165/3000 [00:00<00:01, 1646.03it/s]warmup should be done:   6%|▌         | 168/3000 [00:00<00:01, 1674.08it/s]warmup should be done:   6%|▌         | 166/3000 [00:00<00:01, 1655.64it/s]warmup should be done:   6%|▌         | 167/3000 [00:00<00:01, 1662.55it/s]warmup should be done:   6%|▌         | 170/3000 [00:00<00:01, 1693.49it/s]warmup should be done:   6%|▌         | 168/3000 [00:00<00:01, 1670.83it/s]warmup should be done:  11%|█▏        | 341/3000 [00:00<00:01, 1702.16it/s]warmup should be done:  11%|█▏        | 338/3000 [00:00<00:01, 1686.58it/s]warmup should be done:  11%|█         | 333/3000 [00:00<00:01, 1661.45it/s]warmup should be done:  11%|█         | 332/3000 [00:00<00:01, 1656.79it/s]warmup should be done:  11%|█         | 336/3000 [00:00<00:01, 1675.86it/s]warmup should be done:  11%|█▏        | 341/3000 [00:00<00:01, 1702.07it/s]warmup should be done:  11%|█         | 337/3000 [00:00<00:01, 1680.16it/s]warmup should be done:  11%|█         | 334/3000 [00:00<00:01, 1660.84it/s]warmup should be done:  17%|█▋        | 508/3000 [00:00<00:01, 1690.75it/s]warmup should be done:  17%|█▋        | 506/3000 [00:00<00:01, 1685.31it/s]warmup should be done:  17%|█▋        | 513/3000 [00:00<00:01, 1706.77it/s]warmup should be done:  17%|█▋        | 503/3000 [00:00<00:01, 1675.46it/s]warmup should be done:  17%|█▋        | 513/3000 [00:00<00:01, 1706.04it/s]warmup should be done:  17%|█▋        | 506/3000 [00:00<00:01, 1677.62it/s]warmup should be done:  17%|█▋        | 501/3000 [00:00<00:01, 1658.88it/s]warmup should be done:  17%|█▋        | 498/3000 [00:00<00:01, 1647.33it/s]warmup should be done:  23%|██▎       | 677/3000 [00:00<00:01, 1694.20it/s]warmup should be done:  23%|██▎       | 680/3000 [00:00<00:01, 1699.60it/s]warmup should be done:  23%|██▎       | 684/3000 [00:00<00:01, 1707.24it/s]warmup should be done:  23%|██▎       | 685/3000 [00:00<00:01, 1709.24it/s]warmup should be done:  22%|██▏       | 672/3000 [00:00<00:01, 1678.10it/s]warmup should be done:  22%|██▏       | 668/3000 [00:00<00:01, 1661.12it/s]warmup should be done:  22%|██▏       | 665/3000 [00:00<00:01, 1653.64it/s]warmup should be done:  22%|██▏       | 674/3000 [00:00<00:01, 1668.40it/s]warmup should be done:  28%|██▊       | 848/3000 [00:00<00:01, 1699.10it/s]warmup should be done:  28%|██▊       | 852/3000 [00:00<00:01, 1706.30it/s]warmup should be done:  29%|██▊       | 856/3000 [00:00<00:01, 1709.64it/s]warmup should be done:  29%|██▊       | 856/3000 [00:00<00:01, 1707.68it/s]warmup should be done:  28%|██▊       | 835/3000 [00:00<00:01, 1663.04it/s]warmup should be done:  28%|██▊       | 840/3000 [00:00<00:01, 1672.97it/s]warmup should be done:  28%|██▊       | 832/3000 [00:00<00:01, 1655.92it/s]warmup should be done:  28%|██▊       | 841/3000 [00:00<00:01, 1662.40it/s]warmup should be done:  34%|███▍      | 1024/3000 [00:00<00:01, 1710.88it/s]warmup should be done:  34%|███▍      | 1021/3000 [00:00<00:01, 1706.81it/s]warmup should be done:  34%|███▍      | 1028/3000 [00:00<00:01, 1710.68it/s]warmup should be done:  34%|███▍      | 1027/3000 [00:00<00:01, 1706.51it/s]warmup should be done:  34%|███▎      | 1009/3000 [00:00<00:01, 1678.49it/s]warmup should be done:  33%|███▎      | 1002/3000 [00:00<00:01, 1664.00it/s]warmup should be done:  33%|███▎      | 999/3000 [00:00<00:01, 1659.53it/s]warmup should be done:  34%|███▎      | 1008/3000 [00:00<00:01, 1663.74it/s]warmup should be done:  40%|███▉      | 1196/3000 [00:00<00:01, 1711.03it/s]warmup should be done:  40%|███▉      | 1194/3000 [00:00<00:01, 1711.34it/s]warmup should be done:  40%|███▉      | 1198/3000 [00:00<00:01, 1705.70it/s]warmup should be done:  39%|███▉      | 1178/3000 [00:00<00:01, 1681.52it/s]warmup should be done:  39%|███▉      | 1169/3000 [00:00<00:01, 1663.39it/s]warmup should be done:  39%|███▉      | 1166/3000 [00:00<00:01, 1662.70it/s]warmup should be done:  40%|████      | 1200/3000 [00:00<00:01, 1704.72it/s]warmup should be done:  39%|███▉      | 1175/3000 [00:00<00:01, 1663.23it/s]warmup should be done:  46%|████▌     | 1369/3000 [00:00<00:00, 1715.22it/s]warmup should be done:  46%|████▌     | 1367/3000 [00:00<00:00, 1715.75it/s]warmup should be done:  46%|████▌     | 1370/3000 [00:00<00:00, 1708.05it/s]warmup should be done:  45%|████▍     | 1348/3000 [00:00<00:00, 1684.52it/s]warmup should be done:  45%|████▍     | 1336/3000 [00:00<00:00, 1664.85it/s]warmup should be done:  46%|████▌     | 1372/3000 [00:00<00:00, 1707.26it/s]warmup should be done:  44%|████▍     | 1333/3000 [00:00<00:01, 1660.87it/s]warmup should be done:  45%|████▍     | 1342/3000 [00:00<00:00, 1662.23it/s]warmup should be done:  51%|█████▏    | 1539/3000 [00:00<00:00, 1716.59it/s]warmup should be done:  51%|█████▏    | 1541/3000 [00:00<00:00, 1715.31it/s]warmup should be done:  51%|█████▏    | 1541/3000 [00:00<00:00, 1707.57it/s]warmup should be done:  51%|█████     | 1517/3000 [00:00<00:00, 1684.98it/s]warmup should be done:  50%|█████     | 1503/3000 [00:00<00:00, 1665.20it/s]warmup should be done:  51%|█████▏    | 1543/3000 [00:00<00:00, 1707.09it/s]warmup should be done:  50%|█████     | 1500/3000 [00:00<00:00, 1660.87it/s]warmup should be done:  50%|█████     | 1509/3000 [00:00<00:00, 1661.49it/s]warmup should be done:  57%|█████▋    | 1713/3000 [00:01<00:00, 1714.54it/s]warmup should be done:  56%|█████▌    | 1670/3000 [00:01<00:00, 1666.32it/s]warmup should be done:  57%|█████▋    | 1713/3000 [00:01<00:00, 1708.43it/s]warmup should be done:  56%|█████▌    | 1687/3000 [00:01<00:00, 1686.63it/s]warmup should be done:  57%|█████▋    | 1711/3000 [00:01<00:00, 1707.89it/s]warmup should be done:  57%|█████▋    | 1714/3000 [00:01<00:00, 1707.58it/s]warmup should be done:  56%|█████▌    | 1667/3000 [00:01<00:00, 1663.61it/s]warmup should be done:  56%|█████▌    | 1676/3000 [00:01<00:00, 1662.36it/s]warmup should be done:  61%|██████    | 1837/3000 [00:01<00:00, 1666.96it/s]warmup should be done:  63%|██████▎   | 1884/3000 [00:01<00:00, 1708.79it/s]warmup should be done:  62%|██████▏   | 1856/3000 [00:01<00:00, 1686.97it/s]warmup should be done:  63%|██████▎   | 1885/3000 [00:01<00:00, 1709.02it/s]warmup should be done:  63%|██████▎   | 1886/3000 [00:01<00:00, 1708.84it/s]warmup should be done:  61%|██████    | 1835/3000 [00:01<00:00, 1665.67it/s]warmup should be done:  63%|██████▎   | 1882/3000 [00:01<00:00, 1703.20it/s]warmup should be done:  61%|██████▏   | 1843/3000 [00:01<00:00, 1662.37it/s]warmup should be done:  67%|██████▋   | 2004/3000 [00:01<00:00, 1665.37it/s]warmup should be done:  68%|██████▊   | 2025/3000 [00:01<00:00, 1687.20it/s]warmup should be done:  68%|██████▊   | 2055/3000 [00:01<00:00, 1705.43it/s]warmup should be done:  69%|██████▊   | 2058/3000 [00:01<00:00, 1709.68it/s]warmup should be done:  69%|██████▊   | 2056/3000 [00:01<00:00, 1702.90it/s]warmup should be done:  67%|██████▋   | 2002/3000 [00:01<00:00, 1663.94it/s]warmup should be done:  68%|██████▊   | 2053/3000 [00:01<00:00, 1700.96it/s]warmup should be done:  67%|██████▋   | 2010/3000 [00:01<00:00, 1660.71it/s]warmup should be done:  72%|███████▏  | 2171/3000 [00:01<00:00, 1665.67it/s]warmup should be done:  74%|███████▍  | 2226/3000 [00:01<00:00, 1706.10it/s]warmup should be done:  73%|███████▎  | 2194/3000 [00:01<00:00, 1685.12it/s]warmup should be done:  74%|███████▍  | 2229/3000 [00:01<00:00, 1709.65it/s]warmup should be done:  72%|███████▏  | 2169/3000 [00:01<00:00, 1662.93it/s]warmup should be done:  74%|███████▍  | 2224/3000 [00:01<00:00, 1697.95it/s]warmup should be done:  74%|███████▍  | 2227/3000 [00:01<00:00, 1695.38it/s]warmup should be done:  73%|███████▎  | 2178/3000 [00:01<00:00, 1666.10it/s]warmup should be done:  80%|███████▉  | 2397/3000 [00:01<00:00, 1705.85it/s]warmup should be done:  78%|███████▊  | 2339/3000 [00:01<00:00, 1667.06it/s]warmup should be done:  79%|███████▉  | 2363/3000 [00:01<00:00, 1685.05it/s]warmup should be done:  80%|████████  | 2400/3000 [00:01<00:00, 1708.93it/s]warmup should be done:  78%|███████▊  | 2336/3000 [00:01<00:00, 1664.58it/s]warmup should be done:  78%|███████▊  | 2347/3000 [00:01<00:00, 1673.09it/s]warmup should be done:  80%|███████▉  | 2394/3000 [00:01<00:00, 1694.57it/s]warmup should be done:  80%|███████▉  | 2397/3000 [00:01<00:00, 1690.17it/s]warmup should be done:  84%|████████▎ | 2506/3000 [00:01<00:00, 1665.77it/s]warmup should be done:  86%|████████▌ | 2569/3000 [00:01<00:00, 1707.21it/s]warmup should be done:  84%|████████▍ | 2533/3000 [00:01<00:00, 1686.64it/s]warmup should be done:  86%|████████▌ | 2572/3000 [00:01<00:00, 1710.44it/s]warmup should be done:  83%|████████▎ | 2504/3000 [00:01<00:00, 1666.35it/s]warmup should be done:  84%|████████▍ | 2517/3000 [00:01<00:00, 1678.44it/s]warmup should be done:  85%|████████▌ | 2564/3000 [00:01<00:00, 1690.85it/s]warmup should be done:  86%|████████▌ | 2567/3000 [00:01<00:00, 1688.67it/s]warmup should be done:  89%|████████▉ | 2673/3000 [00:01<00:00, 1665.88it/s]warmup should be done:  91%|█████████▏| 2741/3000 [00:01<00:00, 1709.61it/s]warmup should be done:  90%|█████████ | 2703/3000 [00:01<00:00, 1687.80it/s]warmup should be done:  91%|█████████▏| 2744/3000 [00:01<00:00, 1710.97it/s]warmup should be done:  89%|████████▉ | 2671/3000 [00:01<00:00, 1664.24it/s]warmup should be done:  90%|████████▉ | 2686/3000 [00:01<00:00, 1679.60it/s]warmup should be done:  91%|█████████ | 2736/3000 [00:01<00:00, 1688.94it/s]warmup should be done:  91%|█████████ | 2734/3000 [00:01<00:00, 1686.28it/s]warmup should be done:  95%|█████████▍| 2840/3000 [00:01<00:00, 1666.42it/s]warmup should be done:  97%|█████████▋| 2913/3000 [00:01<00:00, 1709.92it/s]warmup should be done:  96%|█████████▌| 2872/3000 [00:01<00:00, 1684.98it/s]warmup should be done:  97%|█████████▋| 2916/3000 [00:01<00:00, 1709.87it/s]warmup should be done:  95%|█████████▍| 2838/3000 [00:01<00:00, 1662.44it/s]warmup should be done:  95%|█████████▌| 2854/3000 [00:01<00:00, 1679.08it/s]warmup should be done:  97%|█████████▋| 2905/3000 [00:01<00:00, 1686.74it/s]warmup should be done:  97%|█████████▋| 2903/3000 [00:01<00:00, 1680.20it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1708.06it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1707.08it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1697.94it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1693.50it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1682.81it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1670.61it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1665.37it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1661.56it/s]2022-12-11 23:50:46.695652: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fbadf831210 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 23:50:46.695718: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 23:50:46.719636: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f9eec030a20 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 23:50:46.719699: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 23:50:46.765043: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fbae3f93160 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 23:50:46.765115: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 23:50:46.889617: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fbae782c310 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 23:50:46.889692: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 23:50:47.270197: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fbadf831180 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 23:50:47.270267: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 23:50:47.308114: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fbadf831440 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 23:50:47.308182: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 23:50:47.315018: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fbadf82c670 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 23:50:47.315067: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 23:50:47.337210: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f9eec02f9f0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 23:50:47.337275: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 23:50:48.994646: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 23:50:49.021278: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 23:50:49.041250: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 23:50:49.190877: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 23:50:49.561611: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 23:50:49.575455: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 23:50:49.595645: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 23:50:49.600483: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 23:50:51.893212: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 23:50:51.911094: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 23:50:51.925951: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 23:50:52.130625: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 23:50:52.406139: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 23:50:52.457217: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 23:50:52.461844: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 23:50:52.506776: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
[HCTR][23:51:27.707][ERROR][RK0][main]: replica 4 reaches 1000, calling init pre replica
[HCTR][23:51:27.707][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][23:51:27.716][ERROR][RK0][main]: coll ps creation done
[HCTR][23:51:27.716][ERROR][RK0][main]: replica 4 waits for coll ps creation barrier
[HCTR][23:51:27.724][ERROR][RK0][main]: replica 5 reaches 1000, calling init pre replica
[HCTR][23:51:27.724][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][23:51:27.724][ERROR][RK0][tid #140440976213760]: replica 6 reaches 1000, calling init pre replica
[HCTR][23:51:27.725][ERROR][RK0][tid #140440976213760]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][23:51:27.729][ERROR][RK0][tid #140440976213760]: coll ps creation done
[HCTR][23:51:27.729][ERROR][RK0][tid #140440976213760]: replica 6 waits for coll ps creation barrier
[HCTR][23:51:27.732][ERROR][RK0][main]: coll ps creation done
[HCTR][23:51:27.732][ERROR][RK0][main]: replica 5 waits for coll ps creation barrier
[HCTR][23:51:27.738][ERROR][RK0][main]: replica 7 reaches 1000, calling init pre replica
[HCTR][23:51:27.738][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][23:51:27.747][ERROR][RK0][main]: coll ps creation done
[HCTR][23:51:27.747][ERROR][RK0][main]: replica 7 waits for coll ps creation barrier
[HCTR][23:51:27.779][ERROR][RK0][main]: replica 2 reaches 1000, calling init pre replica
[HCTR][23:51:27.779][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][23:51:27.784][ERROR][RK0][main]: coll ps creation done
[HCTR][23:51:27.784][ERROR][RK0][main]: replica 2 waits for coll ps creation barrier
[HCTR][23:51:27.849][ERROR][RK0][main]: replica 3 reaches 1000, calling init pre replica
[HCTR][23:51:27.849][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][23:51:27.857][ERROR][RK0][main]: coll ps creation done
[HCTR][23:51:27.857][ERROR][RK0][main]: replica 3 waits for coll ps creation barrier
[HCTR][23:51:27.872][ERROR][RK0][main]: replica 1 reaches 1000, calling init pre replica
[HCTR][23:51:27.872][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][23:51:27.876][ERROR][RK0][main]: coll ps creation done
[HCTR][23:51:27.876][ERROR][RK0][main]: replica 1 waits for coll ps creation barrier
[HCTR][23:51:27.899][ERROR][RK0][main]: replica 0 reaches 1000, calling init pre replica
[HCTR][23:51:27.899][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][23:51:27.904][ERROR][RK0][main]: coll ps creation done
[HCTR][23:51:27.904][ERROR][RK0][main]: replica 0 waits for coll ps creation barrier
[HCTR][23:51:27.904][ERROR][RK0][main]: replica 0 preparing frequency
[HCTR][23:51:28.968][ERROR][RK0][main]: replica 0 preparing frequency done
[HCTR][23:51:29.002][ERROR][RK0][main]: replica 0 calling init per replica
[HCTR][23:51:29.002][ERROR][RK0][main]: replica 5 calling init per replica
[HCTR][23:51:29.002][ERROR][RK0][main]: Calling build_v2
[HCTR][23:51:29.002][ERROR][RK0][main]: replica 7 calling init per replica
[HCTR][23:51:29.002][ERROR][RK0][tid #140440976213760]: replica 6 calling init per replica
[HCTR][23:51:29.002][ERROR][RK0][main]: replica 3 calling init per replica
[HCTR][23:51:29.002][ERROR][RK0][main]: replica 1 calling init per replica
[HCTR][23:51:29.002][ERROR][RK0][main]: replica 2 calling init per replica
[HCTR][23:51:29.002][ERROR][RK0][main]: replica 4 calling init per replica
[HCTR][23:51:29.002][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][23:51:29.002][ERROR][RK0][main]: Calling build_v2
[HCTR][23:51:29.002][ERROR][RK0][main]: Calling build_v2
[HCTR][23:51:29.002][ERROR][RK0][tid #140440976213760]: Calling build_v2
[HCTR][23:51:29.002][ERROR][RK0][main]: Calling build_v2
[HCTR][23:51:29.002][ERROR][RK0][main]: Calling build_v2
[HCTR][23:51:29.002][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][23:51:29.002][ERROR][RK0][main]: Calling build_v2
[HCTR][23:51:29.002][ERROR][RK0][main]: Calling build_v2
[HCTR][23:51:29.002][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][23:51:29.002][ERROR][RK0][tid #140440976213760]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][23:51:29.002][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][23:51:29.002][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][23:51:29.002][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][23:51:29.002][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[2022-12-11 23:51:29[.  6780: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:2022-12-11 23:51:29178.]   6826v100x8, slow pcie: 
E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:178[] 2022-12-11 23:51:29v100x8, slow pcie.
  6886: [[E2022-12-11 23:51:29 ./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc  6907:2022-12-11 23:51:29: 196.E]   6884 assigning 0 to cpu: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
E: [196/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] :assigning 0 to cpu2022-12-11 23:51:29178
.] [  6938v100x8, slow pcie[: 
2022-12-11 23:51:292022-12-11 23:51:29E[.[.[   69752022-12-11 23:51:29  70112022-12-11 23:51:292022-12-11 23:51:29/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: .: [..:E  7039E  7022  70422022-12-11 23:51:29178 : [ : : .] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE2022-12-11 23:51:29E  7067v100x8, slow pcie: : . : 
178/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc212/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc  7117/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE] :] [:: : v100x8, slow pcie212build asymm link desc with 8X Tesla V100-SXM2-32GB out of 82022-12-11 23:51:29178E196/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
] 
.]  ] :build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8  7259[v100x8, slow pcie/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccassigning 0 to cpu178[
: 2022-12-11 23:51:29
:
] 2022-12-11 23:51:29E.178[[v100x8, slow pcie.   7340] 2022-12-11 23:51:292022-12-11 23:51:29
  7357/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: v100x8, slow pcie..[: :E[
  7400  74002022-12-11 23:51:29E196 2022-12-11 23:51:29: : [. ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.EE2022-12-11 23:51:29  7451/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccassigning 0 to cpu:  7460  .: :
196: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc  7503E213] E:::  ] assigning 0 to cpu 213196E[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccremote time is 8.68421
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] ]  2022-12-11 23:51:29:
:remote time is 8.68421assigning 0 to cpu/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.212196
[
:[  7634] ] 2022-12-11 23:51:29[1962022-12-11 23:51:29: build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8assigning 0 to cpu.2022-12-11 23:51:29] .E
[
  7703.assigning 0 to cpu  7711 2022-12-11 23:51:29:   7732
[: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.E: 2022-12-11 23:51:29E:  7771 E. 212[: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc   7807/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] 2022-12-11 23:51:29E:/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: :build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8. [214:E212
  7858/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-11 23:51:29] 214 ] : :.cpu time is 97.0588] [/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccbuild asymm link desc with 8X Tesla V100-SXM2-32GB out of 8E212  7897
cpu time is 97.05882022-12-11 23:51:29:
 ] : 
.213/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccbuild asymm link desc with 8X Tesla V100-SXM2-32GB out of 8E[  7952] :
 2022-12-11 23:51:29: remote time is 8.68421212/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.E[
] :  8028 2022-12-11 23:51:29build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8212: [/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.
] E2022-12-11 23:51:29:  8072build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8 .[213: 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc  81042022-12-11 23:51:29] E:: .remote time is 8.68421 213[E  8142
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] 2022-12-11 23:51:29 : :remote time is 8.68421[./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE213
2022-12-11 23:51:29  8183: ] .[: 214/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccremote time is 8.68421  82212022-12-11 23:51:29E] :
: .  8260 cpu time is 97.0588213E: [/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
]  E2022-12-11 23:51:29:remote time is 8.68421/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc .
213:/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc  8322] [214:: remote time is 8.684212022-12-11 23:51:29] 214E
.cpu time is 97.0588]    8377
cpu time is 97.0588/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: 
[:E2022-12-11 23:51:29214 .] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc  8414cpu time is 97.0588:: 
214E]  cpu time is 97.0588
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:214] cpu time is 97.0588
[2022-12-11 23:52:47.955779: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:118] solver created. now build & solve
[2022-12-11 23:52:47.995897: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:123] solver built. now solve
block 0 storage is 00010001
	access is	0	0	0	0	4	4	4	4	
block 1 storage is 00100010
	access is	1	1	1	1	5	5	5	5	
block 2 storage is 01000100
	access is	2	2	2	2	6	6	6	6	
block 3 storage is 10001000
	access is	3	3	3	3	7	7	7	7	
block 4 storage is 00000000
	access is	8	8	8	8	8	8	8	8	
[2022-12-11 23:52:48.124822: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:127] solver solved
[2022-12-11 23:52:48.124884: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:195] 0 solved master
[2022-12-11 23:52:48.124921: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 0 solved
[2022-12-11 23:52:48.124952: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 0 initing device 0
[2022-12-11 23:52:48.125418: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-11 23:52:48.125461: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 23:52:48.127453: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 23:52:48.128220: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 23:52:48.140132: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 6 solved
[2022-12-11 23:52:48.140191: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 6 initing device 6
[2022-12-11 23:52:48.140614: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-11 23:52:48.140658: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 23:52:48.140901: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] [2 solved2022-12-11 23:52:48
.140954: E[ 2022-12-11 23:52:48/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc.:140994202: ] E5 solved 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] [worker 0 thread 2 initing device 22022-12-11 23:52:48
.141053: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 5 initing device 5
[[2022-12-11 23:52:482022-12-11 23:52:48..141194141196: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc::202202] ] 7 solved1 solved

[2022-12-11 23:52:48[.2022-12-11 23:52:48141319.: 141323E:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc :/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc205:] 205worker 0 thread 7 initing device 7] 
worker 0 thread 1 initing device 1
[2022-12-11 23:52:48.[1414852022-12-11 23:52:48[: .2022-12-11 23:52:48E141488. : 141501/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE: : E1815/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu ] :/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuBuilding Coll Cache with ... num gpu device is 81980:
] 1815eager alloc mem 381.47 MB] 
Building Coll Cache with ... num gpu device is 8[
2022-12-11 23:52:48.141636: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 23:52:48.141670: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 23:52:48.141794: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-11 23:52:48.141825: E[ 2022-12-11 23:52:48/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu.:1418371815: ] EBuilding Coll Cache with ... num gpu device is 8 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 23:52:48.141910: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 23:52:48.143668: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 3 solved
[2022-12-11 23:52:48.143720: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 3 initing device 3
[2022-12-11 23:52:48.143756: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 4 solved
[2022-12-11 23:52:48.143810: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 4 initing device 4
[2022-12-11 23:52:48.144134: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-11 23:52:48.144179: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 23:52:48.144242: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-11 23:52:48.144285: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 23:52:48.145710: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 23:52:48.145778: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 23:52:48.145839: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 23:52:48.145958: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 23:52:48.146024: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 23:52:48.148572: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 23:52:48.148730: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 23:52:48.150059: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 23:52:48.150114: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 23:52:48.150217: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 23:52:48.150283: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 23:52:48.152352: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 23:52:48.152506: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 23:52:48.205365: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.00 Bytes
[2022-12-11 23:52:48.210621: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5
[2022-12-11 23:52:48.210755: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-11 23:52:48.211638: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 15.64 MB
[2022-12-11 23:52:48.212388: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:52:48.213386: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:52:48.213432: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 15.26 MB
[2022-12-11 23:52:48.221132: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 23:52:48.221920: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 23:52:48.221966: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1.91 GB
[2022-12-11 23:52:48.228217: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.00 Bytes
[2022-12-11 23:52:48.231654: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.00 Bytes
[[2022-12-11 23:52:482022-12-11 23:52:48..231731231731: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::19801980] ] eager alloc mem 5.00 Byteseager alloc mem 5.00 Bytes

[[[2022-12-11 23:52:482022-12-11 23:52:482022-12-11 23:52:48...237806237805237824: : : EEE   /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:::198019801980] ] ] eager alloc mem 5.00 Byteseager alloc mem 5.00 Byteseager alloc mem 5.00 Bytes


[2022-12-11 23:52:48.241079: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5
[[2022-12-11 23:52:482022-12-11 23:52:48..241151241166: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::638638] ] eager release cuda mem 5eager release cuda mem 400000000

[[2022-12-11 23:52:482022-12-11 23:52:48..241235241252: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::638638] ] eager release cuda mem 5eager release cuda mem 400000000

[2022-12-11 23:52:48[.2022-12-11 23:52:48241316.: 241336E:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc :/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc638:] 638eager release cuda mem 5] 
eager release cuda mem 400000000
[2022-12-11 23:52:48.241415: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-11 23:52:48.245286: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 15.64 MB
[2022-12-11 23:52:48.245999: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 15.64 MB
[2022-12-11 23:52:48.246227: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5
[2022-12-11 23:52:48.246311: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-11 23:52:48.246366: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5
[2022-12-11 23:52:48.246450: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-11 23:52:48.246468: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5
[2022-12-11 23:52:48.246547: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-11 23:52:48.250641: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 15.64 MB
[2022-12-11 23:52:48.258796: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 15.64 MB
[2022-12-11 23:52:48.261101: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 15.64 MB
[2022-12-11 23:52:48.261615: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 15.64 MB
[2022-12-11 23:52:48.262139: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 15.64 MB
[2022-12-11 23:52:48.262546: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:52:48.262584: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:52:48.262759: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:52:48.262876: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:52:48.263326: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:52:48.263406: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:52:48.263476: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:[6382022-12-11 23:52:48] .eager release cuda mem 625663263494
: [E2022-12-11 23:52:48 .[/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc2635022022-12-11 23:52:48:: .638E263532]  : eager release cuda mem 625663/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuW
: 1980/hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc] :[eager alloc mem 611.00 KB432022-12-11 23:52:48
] .WORKER[0] alloc host memory 15.26 MB263615
: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 15.26 MB
[2022-12-11 23:52:48.263666: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:52:48.263712: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 15.26 MB
[2022-12-11 23:52:48.263850: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:52:48.263895: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 15.26 MB
[2022-12-11 23:52:48.264281: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:52:48.264330: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 15.26 MB
[2022-12-11 23:52:48.264377: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:52:48.264424: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 15.26 MB
[2022-12-11 23:52:48.264584: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:52:48.264630: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 15.26 MB
[2022-12-11 23:52:48.274823: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 23:52:48.274943: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 23:52:48.275022: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[[2022-12-11 23:52:482022-12-11 23:52:48..275413275427: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::1980638] ] eager alloc mem 25.25 KBeager release cuda mem 25855

[[2022-12-11 23:52:482022-12-11 23:52:48..275537275542: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::6381980] ] eager release cuda mem 25855eager alloc mem 1.91 GB

[2022-12-11 23:52:48.275604: E [/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu2022-12-11 23:52:48:.1980275614] : eager alloc mem 1.91 GBE
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 23:52:48.275668: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1.91 GB
[2022-12-11 23:52:48.275850: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 23:52:48.276080: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 23:52:48.276125: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1.91 GB
[2022-12-11 23:52:48.276454: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 23:52:48.276496: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1.91 GB
[2022-12-11 23:52:48.277069: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 23:52:48.277222: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 23:52:48.277668: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 23:52:48.277710: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1.91 GB
[2022-12-11 23:52:48.277821: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 23:52:48.277863: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1.91 GB
[[[[[[[[2022-12-11 23:52:482022-12-11 23:52:482022-12-11 23:52:482022-12-11 23:52:482022-12-11 23:52:482022-12-11 23:52:482022-12-11 23:52:482022-12-11 23:52:48........994327994328994327994327994328994327994327994327: : : : : : : : EEEEEEEE        /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::::::::19261926192619261926192619261926] ] ] ] ] ] ] ] Device 3 init p2p of link 2Device 1 init p2p of link 7Device 2 init p2p of link 1Device 7 init p2p of link 4Device 0 init p2p of link 3Device 4 init p2p of link 5Device 6 init p2p of link 0Device 5 init p2p of link 6







[[[[2022-12-11 23:52:48[[[[2022-12-11 23:52:482022-12-11 23:52:482022-12-11 23:52:48.2022-12-11 23:52:482022-12-11 23:52:482022-12-11 23:52:482022-12-11 23:52:48...994846....994848994850994846: 994847994846994846994853: : : E: : : : EEE EEEE   /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu    /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:::1980::::198019801980] 1980198019801980] ] ] eager alloc mem 611.00 KB] ] ] ] eager alloc mem 611.00 KBeager alloc mem 611.00 KBeager alloc mem 611.00 KB
eager alloc mem 611.00 KBeager alloc mem 611.00 KBeager alloc mem 611.00 KBeager alloc mem 611.00 KB






[2022-12-11 23:52:48.995970: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc[:[2022-12-11 23:52:48[6382022-12-11 23:52:48[[.2022-12-11 23:52:48[[] .2022-12-11 23:52:482022-12-11 23:52:48995985.2022-12-11 23:52:482022-12-11 23:52:48eager release cuda mem 625663995987..: 995990..
: 995999996000E: 996004996005E: :  E: :  EE/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc EE/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc  :/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc  :/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc638:/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc638::] 638::] 638638eager release cuda mem 625663] 638638eager release cuda mem 625663] ] 
eager release cuda mem 625663] ] 
eager release cuda mem 625663eager release cuda mem 625663
eager release cuda mem 625663eager release cuda mem 625663



[2022-12-11 23:52:49.  9502: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 6 init p2p of link 5
[2022-12-11 23:52:49.  9644: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:52:49. 10152: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 2
[2022-12-11 23:52:49. 10298: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:52:49. 10453: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:52:49. 11122: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:52:49. 11336: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 4 init p2p of link 7
[2022-12-11 23:52:49. 11497: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:52:49. 11563: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 7 init p2p of link 1
[2022-12-11 23:52:49. 11723: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:52:49. 11991: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 5 init p2p of link 4
[2022-12-11 23:52:49. 12131: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:52:49. 12222: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 3 init p2p of link 0
[2022-12-11 23:52:49. 12299: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:52:49. 12373: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:52:49. 12451: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 2 init p2p of link 3
[2022-12-11 23:52:49. 12494: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:52:49. 12611: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:52:49. 12691: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 0 init p2p of link 6
[2022-12-11 23:52:49. 12847: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:52:49. 12933: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:52:49. 13186: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:52:49. 13422: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:52:49. 13624: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:52:49. 24162: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 6 init p2p of link 4
[2022-12-11 23:52:49. 24277: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:52:49. 24397: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 7 init p2p of link 6
[2022-12-11 23:52:49. 24508: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:52:49. 25078: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:52:49. 25113: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 3
[2022-12-11 23:52:49. 25234: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:52:49. 25273: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 0 init p2p of link 1
[2022-12-11 23:52:49. 25338: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:52:49. 25398: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:52:49. 25699: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 4 init p2p of link 2
[2022-12-11 23:52:49. 25824: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:52:49. 26032: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:52:49. 26236: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:52:49. 26326: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 5 init p2p of link 7
[2022-12-11 23:52:49. 26440: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:52:49. 26514: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 3 init p2p of link 5
[2022-12-11 23:52:49. 26600: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:52:49. 26634: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:52:49. 26694: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 2 init p2p of link 0
[2022-12-11 23:52:49. 26810: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:52:49. 27239: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:52:49. 27401: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:52:49. 27618: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:52:49. 40786: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 4 init p2p of link 6
[2022-12-11 23:52:49. 40914: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:52:49. 41362: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 3 init p2p of link 1
[2022-12-11 23:52:49. 41477: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:52:49. 41734: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:52:49. 42193: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 6 init p2p of link 7
[2022-12-11 23:52:49. 42295: E[ 2022-12-11 23:52:49/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc.: 42309638: ] Eeager release cuda mem 625663 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:52:49. 42847: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 7 init p2p of link 5
[2022-12-11 23:52:49. 42962: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:52:49. 43166: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:52:49. 43296: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 0
[2022-12-11 23:52:49. 43410: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:52:49. 43689: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 0 init p2p of link 2
[2022-12-11 23:52:49. 43777: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:52:49. 43806: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:52:49. 44213: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:52:49. 44354: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 5 init p2p of link 3
[2022-12-11 23:52:49. 44468: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:52:49. 44625: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:52:49. 44964: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 2 init p2p of link 4
[2022-12-11 23:52:49. 45081: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:52:49. 45231: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:52:49. 45846: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:52:49. 57288: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 16399996
[2022-12-11 23:52:49. 57626: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 16399996
[2022-12-11 23:52:49. 58109: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 16399996
[2022-12-11 23:52:49. 58411: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 16399996
[2022-12-11 23:52:49. 59076: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 3999999 / 100000000 nodes ( 4.00 %~4.00 %) | remote 11999997 / 100000000 nodes ( 12.00 %) | cpu 84000004 / 100000000 nodes ( 84.00 %) | 1.91 GB | 0.914796 secs 
[2022-12-11 23:52:49. 59263: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 3999999 / 100000000 nodes ( 4.00 %~4.00 %) | remote 11999997 / 100000000 nodes ( 12.00 %) | cpu 84000004 / 100000000 nodes ( 84.00 %) | 1.91 GB | 0.91509 secs 
[2022-12-11 23:52:49. 59359: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 3999999 / 100000000 nodes ( 4.00 %~4.00 %) | remote 11999997 / 100000000 nodes ( 12.00 %) | cpu 84000004 / 100000000 nodes ( 84.00 %) | 1.91 GB | 0.917731 secs 
[2022-12-11 23:52:49. 59497: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 3999999 / 100000000 nodes ( 4.00 %~4.00 %) | remote 11999997 / 100000000 nodes ( 12.00 %) | cpu 84000004 / 100000000 nodes ( 84.00 %) | 1.91 GB | 0.917839 secs 
[2022-12-11 23:52:49. 59644: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 16399996
[2022-12-11 23:52:49. 60043: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 3999999 / 100000000 nodes ( 4.00 %~4.00 %) | remote 11999997 / 100000000 nodes ( 12.00 %) | cpu 84000004 / 100000000 nodes ( 84.00 %) | 1.91 GB | 0.919393 secs 
[2022-12-11 23:52:49. 60221: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 16399996
[2022-12-11 23:52:49. 60611: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 3999999 / 100000000 nodes ( 4.00 %~4.00 %) | remote 11999997 / 100000000 nodes ( 12.00 %) | cpu 84000004 / 100000000 nodes ( 84.00 %) | 1.91 GB | 0.918781 secs 
[2022-12-11 23:52:49. 60662: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 16399996
[2022-12-11 23:52:49. 61053: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 16399996
[2022-12-11 23:52:49. 61107: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 3999999 / 100000000 nodes ( 4.00 %~4.00 %) | remote 11999997 / 100000000 nodes ( 12.00 %) | cpu 84000004 / 100000000 nodes ( 84.00 %) | 1.91 GB | 0.919213 secs 
[2022-12-11 23:52:49. 61774: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 3999999 / 100000000 nodes ( 4.00 %~4.00 %) | remote 11999997 / 100000000 nodes ( 12.00 %) | cpu 84000004 / 100000000 nodes ( 84.00 %) | 1.91 GB | 0.936319 secs 
[HCTR][23:52:49.061][ERROR][RK0][main]: replica 4 calling init per replica done, doing barrier
[HCTR][23:52:49.061][ERROR][RK0][main]: replica 2 calling init per replica done, doing barrier
[HCTR][23:52:49.061][ERROR][RK0][main]: replica 1 calling init per replica done, doing barrier
[HCTR][23:52:49.061][ERROR][RK0][main]: replica 3 calling init per replica done, doing barrier
[HCTR][23:52:49.061][ERROR][RK0][main]: replica 5 calling init per replica done, doing barrier
[HCTR][23:52:49.061][ERROR][RK0][main]: replica 7 calling init per replica done, doing barrier
[HCTR][23:52:49.061][ERROR][RK0][tid #140440976213760]: replica 6 calling init per replica done, doing barrier
[HCTR][23:52:49.061][ERROR][RK0][main]: replica 0 calling init per replica done, doing barrier
[HCTR][23:52:49.061][ERROR][RK0][main]: replica 0 calling init per replica done, doing barrier done
[HCTR][23:52:49.061][ERROR][RK0][tid #140440976213760]: replica 6 calling init per replica done, doing barrier done
[HCTR][23:52:49.061][ERROR][RK0][main]: replica 4 calling init per replica done, doing barrier done
[HCTR][23:52:49.061][ERROR][RK0][main]: replica 1 calling init per replica done, doing barrier done
[HCTR][23:52:49.061][ERROR][RK0][main]: replica 2 calling init per replica done, doing barrier done
[HCTR][23:52:49.061][ERROR][RK0][main]: replica 3 calling init per replica done, doing barrier done
[HCTR][23:52:49.061][ERROR][RK0][tid #140440976213760]: init per replica done
[HCTR][23:52:49.061][ERROR][RK0][main]: replica 5 calling init per replica done, doing barrier done
[HCTR][23:52:49.061][ERROR][RK0][main]: replica 7 calling init per replica done, doing barrier done
[HCTR][23:52:49.061][ERROR][RK0][main]: init per replica done
[HCTR][23:52:49.061][ERROR][RK0][main]: init per replica done
[HCTR][23:52:49.061][ERROR][RK0][main]: init per replica done
[HCTR][23:52:49.061][ERROR][RK0][main]: init per replica done
[HCTR][23:52:49.061][ERROR][RK0][main]: init per replica done
[HCTR][23:52:49.061][ERROR][RK0][main]: init per replica done
[HCTR][23:52:49.064][ERROR][RK0][main]: init per replica done








