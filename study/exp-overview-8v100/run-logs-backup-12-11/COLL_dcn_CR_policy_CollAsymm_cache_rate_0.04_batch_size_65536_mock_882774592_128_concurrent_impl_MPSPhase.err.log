2022-12-11 20:03:44.852646: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:44.860007: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:44.867739: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:44.872976: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:44.884370: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:44.892947: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:44.897082: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:44.916680: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:44.972938: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:44.974482: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:44.974835: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:44.975824: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:44.976758: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:44.977175: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:44.978813: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:44.978895: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:44.980520: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:44.980562: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:44.982191: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:44.982243: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:44.983753: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:44.983864: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:44.985622: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:44.985896: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:44.987788: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:44.988391: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:44.989264: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:44.989798: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:44.991361: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:44.991477: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:44.992998: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:44.993062: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:44.994487: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:44.994694: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:44.996013: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:44.996433: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:44.997436: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:44.998102: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:44.999502: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:44.999727: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.000865: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.001478: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.003047: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.004026: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.004967: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.006243: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.007893: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.008498: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.009126: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.010050: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.011432: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.012506: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.013495: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.014510: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.014662: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 20:03:45.015589: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.016677: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.020149: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.021792: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.022216: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.024066: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.024189: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.024255: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.024426: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.026541: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.026723: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.026966: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.027020: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.029466: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.029577: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.029608: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.029817: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.030297: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.032134: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.032175: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.032514: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.033136: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.034919: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.034957: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.035362: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.036021: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.037660: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.037794: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.038220: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.038828: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.038909: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.040796: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.041467: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.041967: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.042132: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.044119: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.044454: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.044779: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.046297: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.046597: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.047063: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.048407: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.048703: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.049392: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.050553: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.050882: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.052247: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.052370: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.053393: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.053587: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.062373: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.063320: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.063485: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.064499: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.065211: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.065415: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.066611: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.067415: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.085596: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.087281: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.089874: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.095937: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.102194: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.104067: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.105632: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.106375: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.106552: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.106602: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.106771: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.106778: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.109373: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.110883: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.111474: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.111511: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.111609: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.111663: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.111730: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.114526: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.117006: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.117659: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.117713: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.117821: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.117904: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.117959: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.120431: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.122249: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.122951: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.123046: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.123254: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.123267: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.123304: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.126133: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.127528: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.128263: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.128315: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.128462: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.128510: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.128554: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.131310: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.132787: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.133377: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.133419: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.133503: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.133599: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.134529: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.135767: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.137477: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.138390: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.138454: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.138486: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.138578: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.139515: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.140910: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.142378: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.143061: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.143269: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.143368: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.143418: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.144570: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.145850: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.147698: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.147794: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.147848: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.147943: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.148264: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.149242: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.150348: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.152486: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.152722: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.152746: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.152767: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.153069: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.154068: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.154953: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.157140: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.157300: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.157391: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.157403: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.157632: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.158943: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.159953: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.162236: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.162291: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.162475: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.162489: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.162738: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.163837: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.164768: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.166976: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.167019: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.167208: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.167316: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.167497: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.168350: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.170313: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.171452: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.171546: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.171864: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.171923: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.172111: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.174890: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.175664: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.175810: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.176022: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.176084: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.176490: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.177436: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 20:03:45.179034: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.180671: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.180951: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.181205: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.181373: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.181653: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.183159: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.184741: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.185215: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.185359: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.185683: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.186776: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.187290: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.188195: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.188377: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 20:03:45.188749: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.188805: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.189100: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.190679: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.191377: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.192327: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.192689: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.192958: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.193323: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.194874: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.195481: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.196436: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.197220: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.197439: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.197939: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.197948: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.199267: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.200679: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.201705: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.201827: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.202201: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.202209: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.205055: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.205706: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.205777: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.206083: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.206381: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.207748: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 20:03:45.208787: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.209644: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.209750: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.210042: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.216995: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 20:03:45.217368: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.217554: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 20:03:45.217611: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 20:03:45.217618: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 20:03:45.220011: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.222517: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.226225: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.226776: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.227191: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.227382: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.229709: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.230799: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.244653: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.245043: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.248145: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.249078: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.250069: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:45.250376: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:46.385948: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:46.386562: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:46.387102: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:46.387851: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 20:03:46.387915: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-11 20:03:46.405580: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:46.406234: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:46.406748: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:46.407828: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:46.408642: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:46.409189: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-11 20:03:46.454347: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 20:03:46.454550: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 20:03:46.495752: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12340
2022-12-11 20:03:46.591486: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:46.592325: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:46.592870: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:46.593342: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 20:03:46.593397: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-11 20:03:46.603543: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:46.604218: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:46.604766: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:46.605245: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 20:03:46.605308: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-11 20:03:46.611155: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:46.611788: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:46.612296: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:46.612874: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:46.613380: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:46.613854: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:1/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-11 20:03:46.622980: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:46.623641: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:46.624161: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:46.624737: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:46.625258: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:46.626522: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:2/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-11 20:03:46.657036: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:46.657639: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:46.658170: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:46.658638: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 20:03:46.658690: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-11 20:03:46.665522: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:46.665522: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:46.666653: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:46.666735: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:46.667757: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:46.667929: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:46.668763: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 20:03:46.668819: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-11 20:03:46.668839: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 20:03:46.668887: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-11 20:03:46.675617: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:46.676250: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:46.676960: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:46.677525: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:46.678139: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:46.678719: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:7/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-11 20:03:46.686596: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:46.686596: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:46.687919: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:46.687954: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:46.688670: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 20:03:46.688853: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 20:03:46.688903: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:46.688942: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:46.689894: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12341
2022-12-11 20:03:46.689977: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:46.690004: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:46.691028: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:46.691152: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:46.691898: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:5/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-11 20:03:46.692089: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:6/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-11 20:03:46.693422: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:46.694023: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:46.694549: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:46.695217: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 20:03:46.695288: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-11 20:03:46.701527: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:46.702125: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:46.702646: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:46.703140: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 20:03:46.703190: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-11 20:03:46.708326: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 20:03:46.708522: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 20:03:46.709532: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12342
2022-12-11 20:03:46.712050: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:46.712681: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:46.713198: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:46.713783: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:46.714296: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:46.714771: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:4/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-11 20:03:46.719889: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:46.720515: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:46.721029: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:46.721595: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:46.722135: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:03:46.722609: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:3/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-11 20:03:46.724243: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 20:03:46.724430: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 20:03:46.725581: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12347
2022-12-11 20:03:46.736946: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 20:03:46.737105: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 20:03:46.738053: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12346
2022-12-11 20:03:46.738731: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 20:03:46.738931: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 20:03:46.740924: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12345
2022-12-11 20:03:46.759621: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 20:03:46.759815: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 20:03:46.760787: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12344
2022-12-11 20:03:46.767260: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 20:03:46.767445: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 20:03:46.768254: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12343
[HCTR][20:03:48.024][ERROR][RK0][main]: using mock embedding with 882774585 * 128 elements
[HCTR][20:03:48.025][ERROR][RK0][main]: using mock embedding with 882774585 * 128 elements
[HCTR][20:03:48.027][ERROR][RK0][main]: using mock embedding with 882774585 * 128 elements
[HCTR][20:03:48.027][ERROR][RK0][main]: using mock embedding with 882774585 * 128 elements
[HCTR][20:03:48.027][ERROR][RK0][main]: using mock embedding with 882774585 * 128 elements
[HCTR][20:03:48.027][ERROR][RK0][main]: using mock embedding with 882774585 * 128 elements
[HCTR][20:03:48.063][ERROR][RK0][main]: using mock embedding with 882774585 * 128 elements
[HCTR][20:03:48.084][ERROR][RK0][main]: using mock embedding with 882774585 * 128 elements
warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 1it [00:00,  2.65it/s]warmup run: 1it [00:00,  2.55it/s]warmup run: 1it [00:00,  2.49it/s]warmup run: 60it [00:00, 163.30it/s]warmup run: 56it [00:00, 147.49it/s]warmup run: 59it [00:00, 153.82it/s]warmup run: 106it [00:00, 246.13it/s]warmup run: 106it [00:00, 244.11it/s]warmup run: 107it [00:00, 241.68it/s]warmup run: 152it [00:00, 306.70it/s]warmup run: 161it [00:00, 329.70it/s]warmup run: 154it [00:00, 304.53it/s]warmup run: 197it [00:00, 347.73it/s]warmup run: 218it [00:00, 397.06it/s]warmup run: 202it [00:00, 352.54it/s]warmup run: 246it [00:00, 388.34it/s]warmup run: 275it [00:00, 446.00it/s]warmup run: 253it [00:00, 397.59it/s]warmup run: 296it [00:00, 420.75it/s]warmup run: 332it [00:00, 481.33it/s]warmup run: 308it [00:01, 440.89it/s]warmup run: 347it [00:01, 446.51it/s]warmup run: 389it [00:01, 506.54it/s]warmup run: 364it [00:01, 474.20it/s]warmup run: 402it [00:01, 476.86it/s]warmup run: 446it [00:01, 524.70it/s]warmup run: 419it [00:01, 493.90it/s]warmup run: 458it [00:01, 499.20it/s]warmup run: 502it [00:01, 529.21it/s]warmup run: 472it [00:01, 503.04it/s]warmup run: 1it [00:01,  1.36s/it]warmup run: 1it [00:01,  1.39s/it]warmup run: 512it [00:01, 510.51it/s]warmup run: 1it [00:01,  1.39s/it]warmup run: 1it [00:01,  1.39s/it]warmup run: 1it [00:01,  1.40s/it]warmup run: 557it [00:01, 533.20it/s]warmup run: 525it [00:01, 510.14it/s]warmup run: 95it [00:01, 89.72it/s]warmup run: 96it [00:01, 89.14it/s]warmup run: 566it [00:01, 518.06it/s]warmup run: 94it [00:01, 87.02it/s]warmup run: 92it [00:01, 85.12it/s]warmup run: 90it [00:01, 83.19it/s]warmup run: 613it [00:01, 540.49it/s]warmup run: 578it [00:01, 506.57it/s]warmup run: 188it [00:01, 189.88it/s]warmup run: 194it [00:01, 193.60it/s]warmup run: 193it [00:01, 192.71it/s]warmup run: 179it [00:01, 176.87it/s]warmup run: 182it [00:01, 181.04it/s]warmup run: 619it [00:01, 515.26it/s]warmup run: 670it [00:01, 546.74it/s]warmup run: 630it [00:01, 494.90it/s]warmup run: 280it [00:01, 295.66it/s]warmup run: 287it [00:01, 298.94it/s]warmup run: 287it [00:01, 299.42it/s]warmup run: 271it [00:01, 283.23it/s]warmup run: 278it [00:01, 292.05it/s]warmup run: 727it [00:01, 551.80it/s]warmup run: 672it [00:01, 502.71it/s]warmup run: 681it [00:01, 487.96it/s]warmup run: 364it [00:01, 388.51it/s]warmup run: 385it [00:01, 414.23it/s]warmup run: 384it [00:01, 413.12it/s]warmup run: 366it [00:01, 396.10it/s]warmup run: 373it [00:01, 403.06it/s]warmup run: 724it [00:01, 507.32it/s]warmup run: 783it [00:01, 548.50it/s]warmup run: 731it [00:01, 482.93it/s]warmup run: 456it [00:01, 492.46it/s]warmup run: 485it [00:01, 527.71it/s]warmup run: 480it [00:01, 519.10it/s]warmup run: 460it [00:01, 501.69it/s]warmup run: 468it [00:01, 508.99it/s]warmup run: 779it [00:01, 519.62it/s]warmup run: 839it [00:01, 521.92it/s]warmup run: 780it [00:01, 479.04it/s]warmup run: 548it [00:01, 584.94it/s]warmup run: 586it [00:01, 632.45it/s]warmup run: 575it [00:01, 612.88it/s]warmup run: 553it [00:01, 594.37it/s]warmup run: 561it [00:02, 599.31it/s]warmup run: 841it [00:02, 548.14it/s]warmup run: 892it [00:02, 510.83it/s]warmup run: 829it [00:02, 477.52it/s]warmup run: 639it [00:02, 662.24it/s]warmup run: 687it [00:02, 721.80it/s]warmup run: 670it [00:02, 692.21it/s]warmup run: 647it [00:02, 676.01it/s]warmup run: 655it [00:02, 679.58it/s]warmup run: 902it [00:02, 565.28it/s]warmup run: 945it [00:02, 515.17it/s]warmup run: 884it [00:02, 497.03it/s]warmup run: 732it [00:02, 728.67it/s]warmup run: 789it [00:02, 796.22it/s]warmup run: 766it [00:02, 759.82it/s]warmup run: 741it [00:02, 741.53it/s]warmup run: 746it [00:02, 736.76it/s]warmup run: 959it [00:02, 558.47it/s]warmup run: 997it [00:02, 513.33it/s]warmup run: 941it [00:02, 518.13it/s]warmup run: 826it [00:02, 783.73it/s]warmup run: 890it [00:02, 851.76it/s]warmup run: 839it [00:02, 803.45it/s]warmup run: 860it [00:02, 802.04it/s]warmup run: 839it [00:02, 786.93it/s]warmup run: 1015it [00:02, 543.73it/s]warmup run: 1049it [00:02, 505.90it/s]warmup run: 993it [00:02, 513.30it/s]warmup run: 920it [00:02, 826.63it/s]warmup run: 989it [00:02, 888.05it/s]warmup run: 938it [00:02, 852.51it/s]warmup run: 935it [00:02, 833.70it/s]warmup run: 954it [00:02, 832.23it/s]warmup run: 1070it [00:02, 529.35it/s]warmup run: 1100it [00:02, 504.65it/s]warmup run: 1047it [00:02, 519.94it/s]warmup run: 1014it [00:02, 857.04it/s]warmup run: 1091it [00:02, 922.71it/s]warmup run: 1035it [00:02, 883.55it/s]warmup run: 1032it [00:02, 871.01it/s]warmup run: 1047it [00:02, 824.49it/s]warmup run: 1124it [00:02, 525.39it/s]warmup run: 1151it [00:02, 501.57it/s]warmup run: 1101it [00:02, 524.66it/s]warmup run: 1109it [00:02, 881.57it/s]warmup run: 1133it [00:02, 908.94it/s]warmup run: 1132it [00:02, 905.84it/s]warmup run: 1191it [00:02, 924.90it/s]warmup run: 1139it [00:02, 850.11it/s]warmup run: 1180it [00:02, 534.90it/s]warmup run: 1203it [00:02, 504.07it/s]warmup run: 1155it [00:02, 526.52it/s]warmup run: 1203it [00:02, 896.24it/s]warmup run: 1291it [00:02, 944.93it/s]warmup run: 1231it [00:02, 927.85it/s]warmup run: 1235it [00:02, 879.01it/s]warmup run: 1229it [00:02, 828.73it/s]warmup run: 1237it [00:02, 543.56it/s]warmup run: 1210it [00:02, 532.59it/s]warmup run: 1254it [00:02, 499.36it/s]warmup run: 1296it [00:02, 897.15it/s]warmup run: 1328it [00:02, 936.94it/s]warmup run: 1327it [00:02, 861.04it/s]warmup run: 1294it [00:02, 549.62it/s]warmup run: 1390it [00:02, 859.26it/s]warmup run: 1304it [00:02, 495.53it/s]warmup run: 1264it [00:02, 518.51it/s]warmup run: 1317it [00:02, 721.85it/s]warmup run: 1350it [00:02, 552.63it/s]warmup run: 1389it [00:02, 744.59it/s]warmup run: 1425it [00:02, 814.24it/s]warmup run: 1354it [00:02, 489.59it/s]warmup run: 1319it [00:02, 527.06it/s]warmup run: 1416it [00:03, 725.42it/s]warmup run: 1480it [00:03, 745.64it/s]warmup run: 1395it [00:03, 669.83it/s]warmup run: 1406it [00:03, 554.38it/s]warmup run: 1403it [00:03, 488.28it/s]warmup run: 1372it [00:03, 518.14it/s]warmup run: 1470it [00:03, 668.87it/s]warmup run: 1512it [00:03, 722.23it/s]warmup run: 1494it [00:03, 670.09it/s]warmup run: 1462it [00:03, 555.80it/s]warmup run: 1560it [00:03, 685.94it/s]warmup run: 1453it [00:03, 489.45it/s]warmup run: 1467it [00:03, 636.49it/s]warmup run: 1424it [00:03, 512.36it/s]warmup run: 1543it [00:03, 621.96it/s]warmup run: 1519it [00:03, 557.91it/s]warmup run: 1589it [00:03, 658.05it/s]warmup run: 1503it [00:03, 491.18it/s]warmup run: 1633it [00:03, 648.94it/s]warmup run: 1565it [00:03, 612.73it/s]warmup run: 1476it [00:03, 505.47it/s]warmup run: 1534it [00:03, 616.59it/s]warmup run: 1575it [00:03, 558.02it/s]warmup run: 1609it [00:03, 592.03it/s]warmup run: 1553it [00:03, 491.64it/s]warmup run: 1527it [00:03, 503.52it/s]warmup run: 1659it [00:03, 616.44it/s]warmup run: 1701it [00:03, 623.94it/s]warmup run: 1598it [00:03, 601.60it/s]warmup run: 1630it [00:03, 574.35it/s]warmup run: 1631it [00:03, 557.84it/s]warmup run: 1603it [00:03, 493.53it/s]warmup run: 1671it [00:03, 582.55it/s]warmup run: 1579it [00:03, 507.07it/s]warmup run: 1660it [00:03, 590.80it/s]warmup run: 1766it [00:03, 606.16it/s]warmup run: 1724it [00:03, 583.92it/s]warmup run: 1690it [00:03, 550.95it/s]warmup run: 1688it [00:03, 560.11it/s]warmup run: 1653it [00:03, 494.06it/s]warmup run: 1633it [00:03, 516.43it/s]warmup run: 1731it [00:03, 572.14it/s]warmup run: 1720it [00:03, 581.83it/s]warmup run: 1828it [00:03, 594.10it/s]warmup run: 1745it [00:03, 559.35it/s]warmup run: 1747it [00:03, 536.99it/s]warmup run: 1785it [00:03, 540.55it/s]warmup run: 1703it [00:03, 494.46it/s]warmup run: 1689it [00:03, 528.58it/s]warmup run: 1790it [00:03, 566.89it/s]warmup run: 1779it [00:03, 566.30it/s]warmup run: 1889it [00:03, 585.76it/s]warmup run: 1802it [00:03, 561.58it/s]warmup run: 1802it [00:03, 526.01it/s]warmup run: 1841it [00:03, 533.45it/s]warmup run: 1754it [00:03, 497.55it/s]warmup run: 1744it [00:03, 534.51it/s]warmup run: 1848it [00:03, 566.51it/s]warmup run: 1948it [00:03, 579.25it/s]warmup run: 1859it [00:03, 563.41it/s]warmup run: 1836it [00:03, 538.49it/s]warmup run: 1856it [00:03, 527.90it/s]warmup run: 1804it [00:03, 492.44it/s]warmup run: 1896it [00:03, 525.41it/s]warmup run: 1798it [00:03, 531.35it/s]warmup run: 1906it [00:03, 565.07it/s]warmup run: 2007it [00:03, 575.93it/s]warmup run: 1916it [00:03, 561.26it/s]warmup run: 1891it [00:03, 536.62it/s]warmup run: 1911it [00:03, 533.49it/s]warmup run: 1854it [00:03, 493.16it/s]warmup run: 1950it [00:04, 504.75it/s]warmup run: 1852it [00:04, 515.64it/s]warmup run: 1963it [00:04, 563.08it/s]warmup run: 2065it [00:04, 573.65it/s]warmup run: 1973it [00:04, 563.45it/s]warmup run: 1955it [00:04, 563.69it/s]warmup run: 1966it [00:04, 536.00it/s]warmup run: 1907it [00:04, 502.94it/s]warmup run: 2001it [00:04, 493.52it/s]warmup run: 2020it [00:04, 560.43it/s]warmup run: 1904it [00:04, 501.19it/s]warmup run: 2123it [00:04, 570.46it/s]warmup run: 2030it [00:04, 564.92it/s]warmup run: 2016it [00:04, 576.60it/s]warmup run: 2021it [00:04, 539.92it/s]warmup run: 1961it [00:04, 513.24it/s]warmup run: 2077it [00:04, 559.91it/s]warmup run: 2051it [00:04, 488.82it/s]warmup run: 1955it [00:04, 494.91it/s]warmup run: 2181it [00:04, 567.22it/s]warmup run: 2087it [00:04, 566.02it/s]warmup run: 2074it [00:04, 575.19it/s]warmup run: 2078it [00:04, 548.33it/s]warmup run: 2016it [00:04, 523.05it/s]warmup run: 2134it [00:04, 561.18it/s]warmup run: 2104it [00:04, 498.41it/s]warmup run: 2005it [00:04, 484.51it/s]warmup run: 2238it [00:04, 564.43it/s]warmup run: 2144it [00:04, 563.92it/s]warmup run: 2132it [00:04, 574.22it/s]warmup run: 2135it [00:04, 552.37it/s]warmup run: 2071it [00:04, 529.63it/s]warmup run: 2191it [00:04, 561.73it/s]warmup run: 2157it [00:04, 505.84it/s]warmup run: 2054it [00:04, 476.12it/s]warmup run: 2201it [00:04, 565.64it/s]warmup run: 2295it [00:04, 559.12it/s]warmup run: 2190it [00:04, 572.90it/s]warmup run: 2191it [00:04, 546.02it/s]warmup run: 2127it [00:04, 536.38it/s]warmup run: 2211it [00:04, 513.56it/s]warmup run: 2248it [00:04, 549.72it/s]warmup run: 2102it [00:04, 477.07it/s]warmup run: 2258it [00:04, 565.62it/s]warmup run: 2352it [00:04, 560.41it/s]warmup run: 2248it [00:04, 572.50it/s]warmup run: 2181it [00:04, 527.91it/s]warmup run: 2246it [00:04, 511.65it/s]warmup run: 2268it [00:04, 529.78it/s]warmup run: 2304it [00:04, 531.82it/s]warmup run: 2156it [00:04, 494.82it/s]warmup run: 2315it [00:04, 566.11it/s]warmup run: 2409it [00:04, 561.19it/s]warmup run: 2306it [00:04, 568.26it/s]warmup run: 2234it [00:04, 516.56it/s]warmup run: 2298it [00:04, 501.50it/s]warmup run: 2326it [00:04, 542.01it/s]warmup run: 2211it [00:04, 509.94it/s]warmup run: 2358it [00:04, 526.06it/s]warmup run: 2372it [00:04, 567.05it/s]warmup run: 2466it [00:04, 562.20it/s]warmup run: 2364it [00:04, 569.15it/s]warmup run: 2287it [00:04, 517.76it/s]warmup run: 2351it [00:04, 509.03it/s]warmup run: 2383it [00:04, 548.72it/s]warmup run: 2265it [00:04, 516.29it/s]warmup run: 2411it [00:04, 516.96it/s]warmup run: 2429it [00:04, 567.57it/s]warmup run: 2523it [00:04, 562.67it/s]warmup run: 2422it [00:04, 570.54it/s]warmup run: 2340it [00:04, 518.08it/s]warmup run: 2403it [00:04, 508.71it/s]warmup run: 2440it [00:04, 554.67it/s]warmup run: 2318it [00:04, 518.43it/s]warmup run: 2464it [00:04, 520.22it/s]warmup run: 2486it [00:04, 567.46it/s]warmup run: 2580it [00:04, 563.58it/s]warmup run: 2480it [00:04, 569.88it/s]warmup run: 2392it [00:05, 505.98it/s]warmup run: 2497it [00:05, 557.79it/s]warmup run: 2455it [00:05, 492.95it/s]warmup run: 2372it [00:05, 522.51it/s]warmup run: 2517it [00:05, 520.85it/s]warmup run: 2543it [00:05, 565.49it/s]warmup run: 2637it [00:05, 562.46it/s]warmup run: 2538it [00:05, 565.78it/s]warmup run: 2443it [00:05, 500.59it/s]warmup run: 2554it [00:05, 560.42it/s]warmup run: 2507it [00:05, 498.95it/s]warmup run: 2425it [00:05, 523.00it/s]warmup run: 2600it [00:05, 565.51it/s]warmup run: 2570it [00:05, 505.82it/s]warmup run: 2595it [00:05, 566.40it/s]warmup run: 2495it [00:05, 504.00it/s]warmup run: 2611it [00:05, 555.90it/s]warmup run: 2559it [00:05, 502.45it/s]warmup run: 2480it [00:05, 528.87it/s]warmup run: 2623it [00:05, 511.64it/s]warmup run: 2652it [00:05, 563.37it/s]warmup run: 2550it [00:05, 516.86it/s]warmup run: 2615it [00:05, 516.65it/s]warmup run: 2667it [00:05, 536.08it/s]warmup run: 2537it [00:05, 540.57it/s]warmup run: 2694it [00:05, 359.60it/s]warmup run: 2607it [00:05, 531.48it/s]warmup run: 2671it [00:05, 528.67it/s]warmup run: 2594it [00:05, 548.10it/s]warmup run: 2657it [00:05, 360.86it/s]warmup run: 2750it [00:05, 401.61it/s]warmup run: 2713it [00:05, 403.31it/s]warmup run: 2805it [00:05, 434.34it/s]warmup run: 2675it [00:05, 321.61it/s]warmup run: 2709it [00:05, 359.61it/s]warmup run: 2721it [00:05, 338.59it/s]warmup run: 2769it [00:05, 439.28it/s]warmup run: 2859it [00:05, 460.07it/s]warmup run: 2726it [00:05, 359.68it/s]warmup run: 2765it [00:05, 401.96it/s]warmup run: 2661it [00:05, 347.31it/s]warmup run: 2724it [00:05, 343.09it/s]warmup run: 2771it [00:05, 371.50it/s]warmup run: 2826it [00:05, 469.87it/s]warmup run: 2914it [00:05, 481.82it/s]warmup run: 2778it [00:05, 394.72it/s]warmup run: 2822it [00:05, 439.81it/s]warmup run: 2649it [00:05, 320.16it/s]warmup run: 2716it [00:05, 390.10it/s]warmup run: 2779it [00:05, 387.01it/s]warmup run: 2822it [00:05, 401.37it/s]warmup run: 2881it [00:05, 488.87it/s]warmup run: 2969it [00:05, 498.10it/s]warmup run: 2834it [00:05, 433.60it/s]warmup run: 2879it [00:05, 471.07it/s]warmup run: 2706it [00:05, 369.18it/s]warmup run: 2773it [00:05, 430.62it/s]warmup run: 2832it [00:05, 420.43it/s]warmup run: 3000it [00:05, 505.93it/s]warmup run: 2874it [00:05, 428.59it/s]warmup run: 2937it [00:05, 507.59it/s]warmup run: 2890it [00:05, 465.79it/s]warmup run: 2936it [00:05, 495.57it/s]warmup run: 2757it [00:05, 398.80it/s]warmup run: 2829it [00:06, 462.28it/s]warmup run: 2885it [00:06, 447.75it/s]warmup run: 2923it [00:06, 443.16it/s]warmup run: 2994it [00:06, 524.89it/s]warmup run: 3000it [00:06, 494.70it/s]warmup run: 2993it [00:06, 514.34it/s]warmup run: 2941it [00:06, 475.13it/s]warmup run: 2810it [00:06, 428.91it/s]warmup run: 3000it [00:06, 491.94it/s]warmup run: 2881it [00:06, 476.47it/s]warmup run: 2938it [00:06, 468.73it/s]warmup run: 2978it [00:06, 471.56it/s]warmup run: 2997it [00:06, 497.26it/s]warmup run: 3000it [00:06, 484.48it/s]warmup run: 2860it [00:06, 436.73it/s]warmup run: 3000it [00:06, 483.65it/s]warmup run: 2933it [00:06, 483.21it/s]warmup run: 2989it [00:06, 474.46it/s]warmup run: 3000it [00:06, 479.89it/s]warmup run: 2913it [00:06, 459.95it/s]warmup run: 2991it [00:06, 508.56it/s]warmup run: 3000it [00:06, 473.61it/s]warmup run: 2964it [00:06, 471.50it/s]warmup run: 3000it [00:06, 462.72it/s]

warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]


warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]

warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   5%|         | 164/3000 [00:00<00:01, 1637.88it/s]warmup should be done:   6%|         | 168/3000 [00:00<00:01, 1678.25it/s]warmup should be done:   6%|         | 166/3000 [00:00<00:01, 1659.38it/s]warmup should be done:   6%|         | 168/3000 [00:00<00:01, 1674.15it/s]warmup should be done:   5%|         | 159/3000 [00:00<00:01, 1583.36it/s]warmup should be done:   6%|         | 169/3000 [00:00<00:01, 1680.71it/s]warmup should be done:   6%|         | 166/3000 [00:00<00:01, 1653.17it/s]warmup should be done:   6%|         | 167/3000 [00:00<00:01, 1661.58it/s]warmup should be done:  11%|         | 329/3000 [00:00<00:01, 1640.37it/s]warmup should be done:  11%|         | 337/3000 [00:00<00:01, 1680.79it/s]warmup should be done:  11%|         | 334/3000 [00:00<00:01, 1666.58it/s]warmup should be done:  11%|         | 335/3000 [00:00<00:01, 1669.48it/s]warmup should be done:  11%|         | 324/3000 [00:00<00:01, 1618.22it/s]warmup should be done:  11%|        | 338/3000 [00:00<00:01, 1684.55it/s]warmup should be done:  11%|        | 339/3000 [00:00<00:01, 1686.80it/s]warmup should be done:  11%|         | 332/3000 [00:00<00:01, 1649.71it/s]warmup should be done:  16%|        | 487/3000 [00:00<00:01, 1623.42it/s]warmup should be done:  17%|        | 504/3000 [00:00<00:01, 1677.60it/s]warmup should be done:  17%|        | 497/3000 [00:00<00:01, 1647.81it/s]warmup should be done:  17%|        | 507/3000 [00:00<00:01, 1679.45it/s]warmup should be done:  16%|        | 494/3000 [00:00<00:01, 1633.85it/s]warmup should be done:  17%|        | 508/3000 [00:00<00:01, 1680.71it/s]warmup should be done:  17%|        | 506/3000 [00:00<00:01, 1673.04it/s]warmup should be done:  17%|        | 501/3000 [00:00<00:01, 1655.95it/s]warmup should be done:  22%|       | 674/3000 [00:00<00:01, 1682.63it/s]warmup should be done:  22%|       | 675/3000 [00:00<00:01, 1676.19it/s]warmup should be done:  22%|       | 658/3000 [00:00<00:01, 1632.87it/s]warmup should be done:  22%|       | 662/3000 [00:00<00:01, 1642.54it/s]warmup should be done:  23%|       | 677/3000 [00:00<00:01, 1681.15it/s]warmup should be done:  22%|       | 674/3000 [00:00<00:01, 1672.96it/s]warmup should be done:  22%|       | 669/3000 [00:00<00:01, 1662.22it/s]warmup should be done:  22%|       | 650/3000 [00:00<00:01, 1613.67it/s]warmup should be done:  28%|       | 844/3000 [00:00<00:01, 1686.18it/s]warmup should be done:  28%|       | 836/3000 [00:00<00:01, 1664.67it/s]warmup should be done:  28%|       | 827/3000 [00:00<00:01, 1641.01it/s]warmup should be done:  28%|       | 846/3000 [00:00<00:01, 1678.74it/s]warmup should be done:  27%|       | 822/3000 [00:00<00:01, 1628.59it/s]warmup should be done:  28%|       | 842/3000 [00:00<00:01, 1667.96it/s]warmup should be done:  28%|       | 843/3000 [00:00<00:01, 1668.57it/s]warmup should be done:  27%|       | 812/3000 [00:00<00:01, 1585.82it/s]warmup should be done:  34%|      | 1013/3000 [00:00<00:01, 1682.52it/s]warmup should be done:  33%|      | 1004/3000 [00:00<00:01, 1668.38it/s]warmup should be done:  34%|      | 1014/3000 [00:00<00:01, 1678.11it/s]warmup should be done:  33%|      | 986/3000 [00:00<00:01, 1630.26it/s]warmup should be done:  34%|      | 1009/3000 [00:00<00:01, 1664.41it/s]warmup should be done:  33%|      | 992/3000 [00:00<00:01, 1636.15it/s]warmup should be done:  34%|      | 1010/3000 [00:00<00:01, 1663.11it/s]warmup should be done:  32%|      | 972/3000 [00:00<00:01, 1588.62it/s]warmup should be done:  39%|      | 1174/3000 [00:00<00:01, 1676.12it/s]warmup should be done:  39%|      | 1182/3000 [00:00<00:01, 1676.85it/s]warmup should be done:  38%|      | 1150/3000 [00:00<00:01, 1632.13it/s]warmup should be done:  39%|      | 1182/3000 [00:00<00:01, 1673.39it/s]warmup should be done:  39%|      | 1156/3000 [00:00<00:01, 1633.70it/s]warmup should be done:  39%|      | 1177/3000 [00:00<00:01, 1660.29it/s]warmup should be done:  39%|      | 1176/3000 [00:00<00:01, 1658.69it/s]warmup should be done:  38%|      | 1131/3000 [00:00<00:01, 1577.03it/s]warmup should be done:  45%|     | 1344/3000 [00:00<00:00, 1681.19it/s]warmup should be done:  45%|     | 1350/3000 [00:00<00:00, 1676.77it/s]warmup should be done:  44%|     | 1314/3000 [00:00<00:01, 1632.55it/s]warmup should be done:  45%|     | 1350/3000 [00:00<00:00, 1673.14it/s]warmup should be done:  45%|     | 1342/3000 [00:00<00:00, 1658.39it/s]warmup should be done:  44%|     | 1320/3000 [00:00<00:01, 1630.84it/s]warmup should be done:  45%|     | 1344/3000 [00:00<00:00, 1658.45it/s]warmup should be done:  43%|     | 1294/3000 [00:00<00:01, 1591.93it/s]warmup should be done:  51%|     | 1518/3000 [00:00<00:00, 1676.64it/s]warmup should be done:  50%|     | 1514/3000 [00:00<00:00, 1685.10it/s]warmup should be done:  51%|     | 1518/3000 [00:00<00:00, 1673.02it/s]warmup should be done:  49%|     | 1478/3000 [00:00<00:00, 1627.24it/s]warmup should be done:  50%|     | 1510/3000 [00:00<00:00, 1662.37it/s]warmup should be done:  50%|     | 1512/3000 [00:00<00:00, 1663.37it/s]warmup should be done:  49%|     | 1484/3000 [00:00<00:00, 1626.81it/s]warmup should be done:  48%|     | 1454/3000 [00:00<00:00, 1591.89it/s]warmup should be done:  56%|    | 1683/3000 [00:01<00:00, 1684.91it/s]warmup should be done:  56%|    | 1686/3000 [00:01<00:00, 1673.62it/s]warmup should be done:  56%|    | 1678/3000 [00:01<00:00, 1667.07it/s]warmup should be done:  55%|    | 1641/3000 [00:01<00:00, 1623.90it/s]warmup should be done:  55%|    | 1648/3000 [00:01<00:00, 1628.90it/s]warmup should be done:  56%|    | 1679/3000 [00:01<00:00, 1661.06it/s]warmup should be done:  56%|    | 1686/3000 [00:01<00:00, 1658.24it/s]warmup should be done:  54%|    | 1615/3000 [00:01<00:00, 1595.48it/s]warmup should be done:  62%|   | 1852/3000 [00:01<00:00, 1684.61it/s]warmup should be done:  62%|   | 1854/3000 [00:01<00:00, 1673.41it/s]warmup should be done:  62%|   | 1846/3000 [00:01<00:00, 1670.77it/s]warmup should be done:  60%|    | 1804/3000 [00:01<00:00, 1623.64it/s]warmup should be done:  60%|    | 1812/3000 [00:01<00:00, 1630.04it/s]warmup should be done:  62%|   | 1846/3000 [00:01<00:00, 1659.43it/s]warmup should be done:  62%|   | 1853/3000 [00:01<00:00, 1659.89it/s]warmup should be done:  59%|    | 1775/3000 [00:01<00:00, 1578.93it/s]warmup should be done:  67%|   | 2022/3000 [00:01<00:00, 1674.51it/s]warmup should be done:  67%|   | 2021/3000 [00:01<00:00, 1679.05it/s]warmup should be done:  67%|   | 2014/3000 [00:01<00:00, 1673.38it/s]warmup should be done:  66%|   | 1968/3000 [00:01<00:00, 1626.09it/s]warmup should be done:  66%|   | 1977/3000 [00:01<00:00, 1634.21it/s]warmup should be done:  67%|   | 2012/3000 [00:01<00:00, 1659.31it/s]warmup should be done:  67%|   | 2020/3000 [00:01<00:00, 1662.59it/s]warmup should be done:  65%|   | 1936/3000 [00:01<00:00, 1587.61it/s]warmup should be done:  73%|  | 2190/3000 [00:01<00:00, 1675.23it/s]warmup should be done:  73%|  | 2189/3000 [00:01<00:00, 1676.24it/s]warmup should be done:  73%|  | 2183/3000 [00:01<00:00, 1676.18it/s]warmup should be done:  71%|   | 2132/3000 [00:01<00:00, 1629.18it/s]warmup should be done:  71%|  | 2142/3000 [00:01<00:00, 1637.31it/s]warmup should be done:  73%|  | 2179/3000 [00:01<00:00, 1659.83it/s]warmup should be done:  73%|  | 2188/3000 [00:01<00:00, 1666.41it/s]warmup should be done:  70%|   | 2095/3000 [00:01<00:00, 1585.57it/s]warmup should be done:  79%|  | 2358/3000 [00:01<00:00, 1676.16it/s]warmup should be done:  79%|  | 2357/3000 [00:01<00:00, 1674.48it/s]warmup should be done:  78%|  | 2351/3000 [00:01<00:00, 1673.39it/s]warmup should be done:  77%|  | 2296/3000 [00:01<00:00, 1630.23it/s]warmup should be done:  77%|  | 2309/3000 [00:01<00:00, 1644.85it/s]warmup should be done:  78%|  | 2345/3000 [00:01<00:00, 1658.31it/s]warmup should be done:  79%|  | 2356/3000 [00:01<00:00, 1668.55it/s]warmup should be done:  75%|  | 2259/3000 [00:01<00:00, 1600.93it/s]warmup should be done:  84%| | 2526/3000 [00:01<00:00, 1676.42it/s]warmup should be done:  84%| | 2525/3000 [00:01<00:00, 1673.51it/s]warmup should be done:  82%| | 2460/3000 [00:01<00:00, 1632.62it/s]warmup should be done:  84%| | 2511/3000 [00:01<00:00, 1658.66it/s]warmup should be done:  83%| | 2476/3000 [00:01<00:00, 1651.12it/s]warmup should be done:  84%| | 2519/3000 [00:01<00:00, 1664.00it/s]warmup should be done:  84%| | 2524/3000 [00:01<00:00, 1670.18it/s]warmup should be done:  81%|  | 2425/3000 [00:01<00:00, 1617.05it/s]warmup should be done:  90%| | 2694/3000 [00:01<00:00, 1676.60it/s]warmup should be done:  90%| | 2693/3000 [00:01<00:00, 1673.64it/s]warmup should be done:  87%| | 2624/3000 [00:01<00:00, 1634.28it/s]warmup should be done:  88%| | 2643/3000 [00:01<00:00, 1656.13it/s]warmup should be done:  89%| | 2678/3000 [00:01<00:00, 1659.82it/s]warmup should be done:  90%| | 2692/3000 [00:01<00:00, 1672.77it/s]warmup should be done:  90%| | 2686/3000 [00:01<00:00, 1659.58it/s]warmup should be done:  86%| | 2591/3000 [00:01<00:00, 1627.48it/s]warmup should be done:  95%|| 2863/3000 [00:01<00:00, 1678.78it/s]warmup should be done:  95%|| 2861/3000 [00:01<00:00, 1674.80it/s]warmup should be done:  93%|| 2788/3000 [00:01<00:00, 1635.86it/s]warmup should be done:  95%|| 2845/3000 [00:01<00:00, 1662.40it/s]warmup should be done:  94%|| 2811/3000 [00:01<00:00, 1660.77it/s]warmup should be done:  95%|| 2860/3000 [00:01<00:00, 1674.86it/s]warmup should be done:  95%|| 2852/3000 [00:01<00:00, 1657.93it/s]warmup should be done:  92%|| 2755/3000 [00:01<00:00, 1628.93it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1677.53it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1674.47it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1671.81it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1665.42it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1663.78it/s]warmup should be done:  98%|| 2953/3000 [00:01<00:00, 1639.55it/s]warmup should be done:  99%|| 2979/3000 [00:01<00:00, 1663.63it/s]warmup should be done:  97%|| 2918/3000 [00:01<00:00, 1624.90it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1644.86it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1632.19it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1605.09it/s]






warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   6%|         | 169/3000 [00:00<00:01, 1689.14it/s]warmup should be done:   6%|         | 173/3000 [00:00<00:01, 1726.46it/s]warmup should be done:   6%|         | 172/3000 [00:00<00:01, 1716.64it/s]warmup should be done:   6%|         | 170/3000 [00:00<00:01, 1695.64it/s]warmup should be done:   6%|         | 172/3000 [00:00<00:01, 1714.75it/s]warmup should be done:   6%|         | 170/3000 [00:00<00:01, 1694.56it/s]warmup should be done:   6%|         | 172/3000 [00:00<00:01, 1712.90it/s]warmup should be done:   6%|         | 172/3000 [00:00<00:01, 1713.30it/s]warmup should be done:  11%|        | 338/3000 [00:00<00:01, 1688.50it/s]warmup should be done:  11%|        | 340/3000 [00:00<00:01, 1696.09it/s]warmup should be done:  11%|        | 344/3000 [00:00<00:01, 1716.22it/s]warmup should be done:  11%|        | 341/3000 [00:00<00:01, 1701.53it/s]warmup should be done:  11%|        | 344/3000 [00:00<00:01, 1714.72it/s]warmup should be done:  12%|        | 346/3000 [00:00<00:01, 1724.08it/s]warmup should be done:  12%|        | 345/3000 [00:00<00:01, 1718.64it/s]warmup should be done:  11%|        | 344/3000 [00:00<00:01, 1709.54it/s]warmup should be done:  17%|        | 510/3000 [00:00<00:01, 1697.22it/s]warmup should be done:  17%|        | 507/3000 [00:00<00:01, 1684.62it/s]warmup should be done:  17%|        | 516/3000 [00:00<00:01, 1714.53it/s]warmup should be done:  17%|        | 512/3000 [00:00<00:01, 1702.43it/s]warmup should be done:  17%|        | 517/3000 [00:00<00:01, 1718.72it/s]warmup should be done:  17%|        | 517/3000 [00:00<00:01, 1718.53it/s]warmup should be done:  17%|        | 519/3000 [00:00<00:01, 1723.27it/s]warmup should be done:  17%|        | 515/3000 [00:00<00:01, 1704.81it/s]warmup should be done:  23%|       | 681/3000 [00:00<00:01, 1699.01it/s]warmup should be done:  23%|       | 689/3000 [00:00<00:01, 1719.80it/s]warmup should be done:  23%|       | 689/3000 [00:00<00:01, 1718.16it/s]warmup should be done:  23%|       | 677/3000 [00:00<00:01, 1688.15it/s]warmup should be done:  23%|       | 683/3000 [00:00<00:01, 1701.16it/s]warmup should be done:  23%|       | 689/3000 [00:00<00:01, 1711.80it/s]warmup should be done:  23%|       | 686/3000 [00:00<00:01, 1705.19it/s]warmup should be done:  23%|       | 692/3000 [00:00<00:01, 1699.85it/s]warmup should be done:  28%|       | 847/3000 [00:00<00:01, 1691.50it/s]warmup should be done:  29%|       | 863/3000 [00:00<00:01, 1724.81it/s]warmup should be done:  28%|       | 852/3000 [00:00<00:01, 1699.77it/s]warmup should be done:  29%|       | 861/3000 [00:00<00:01, 1714.55it/s]warmup should be done:  29%|       | 862/3000 [00:00<00:01, 1717.78it/s]warmup should be done:  29%|       | 858/3000 [00:00<00:01, 1710.03it/s]warmup should be done:  28%|       | 854/3000 [00:00<00:01, 1698.46it/s]warmup should be done:  29%|       | 863/3000 [00:00<00:01, 1698.08it/s]warmup should be done:  34%|      | 1017/3000 [00:00<00:01, 1691.13it/s]warmup should be done:  34%|      | 1022/3000 [00:00<00:01, 1697.89it/s]warmup should be done:  35%|      | 1036/3000 [00:00<00:01, 1723.09it/s]warmup should be done:  34%|      | 1033/3000 [00:00<00:01, 1721.21it/s]warmup should be done:  35%|      | 1036/3000 [00:00<00:01, 1722.69it/s]warmup should be done:  34%|      | 1025/3000 [00:00<00:01, 1700.68it/s]warmup should be done:  34%|      | 1033/3000 [00:00<00:01, 1710.26it/s]warmup should be done:  34%|      | 1033/3000 [00:00<00:01, 1694.37it/s]warmup should be done:  40%|      | 1188/3000 [00:00<00:01, 1695.41it/s]warmup should be done:  40%|      | 1192/3000 [00:00<00:01, 1694.58it/s]warmup should be done:  40%|      | 1209/3000 [00:00<00:01, 1723.16it/s]warmup should be done:  40%|      | 1196/3000 [00:00<00:01, 1701.61it/s]warmup should be done:  40%|      | 1206/3000 [00:00<00:01, 1719.58it/s]warmup should be done:  40%|      | 1209/3000 [00:00<00:01, 1713.34it/s]warmup should be done:  40%|      | 1205/3000 [00:00<00:01, 1703.66it/s]warmup should be done:  40%|      | 1203/3000 [00:00<00:01, 1694.65it/s]warmup should be done:  45%|     | 1360/3000 [00:00<00:00, 1701.78it/s]warmup should be done:  46%|     | 1368/3000 [00:00<00:00, 1707.41it/s]warmup should be done:  46%|     | 1383/3000 [00:00<00:00, 1726.57it/s]warmup should be done:  46%|     | 1379/3000 [00:00<00:00, 1722.13it/s]warmup should be done:  46%|     | 1381/3000 [00:00<00:00, 1712.70it/s]warmup should be done:  45%|     | 1362/3000 [00:00<00:00, 1684.83it/s]warmup should be done:  46%|     | 1376/3000 [00:00<00:00, 1696.00it/s]warmup should be done:  46%|     | 1375/3000 [00:00<00:00, 1701.58it/s]warmup should be done:  51%|     | 1531/3000 [00:00<00:00, 1704.04it/s]warmup should be done:  51%|    | 1540/3000 [00:00<00:00, 1708.59it/s]warmup should be done:  52%|    | 1557/3000 [00:00<00:00, 1727.92it/s]warmup should be done:  52%|    | 1552/3000 [00:00<00:00, 1722.06it/s]warmup should be done:  51%|     | 1532/3000 [00:00<00:00, 1686.96it/s]warmup should be done:  52%|    | 1553/3000 [00:00<00:00, 1708.90it/s]warmup should be done:  52%|    | 1546/3000 [00:00<00:00, 1696.15it/s]warmup should be done:  52%|    | 1546/3000 [00:00<00:00, 1702.89it/s]warmup should be done:  57%|    | 1703/3000 [00:01<00:00, 1707.33it/s]warmup should be done:  57%|    | 1711/3000 [00:01<00:00, 1707.82it/s]warmup should be done:  58%|    | 1730/3000 [00:01<00:00, 1726.65it/s]warmup should be done:  57%|    | 1725/3000 [00:01<00:00, 1718.95it/s]warmup should be done:  57%|    | 1702/3000 [00:01<00:00, 1689.95it/s]warmup should be done:  57%|    | 1724/3000 [00:01<00:00, 1706.17it/s]warmup should be done:  57%|    | 1716/3000 [00:01<00:00, 1697.13it/s]warmup should be done:  57%|    | 1719/3000 [00:01<00:00, 1710.30it/s]warmup should be done:  62%|   | 1875/3000 [00:01<00:00, 1709.95it/s]warmup should be done:  63%|   | 1882/3000 [00:01<00:00, 1708.45it/s]warmup should be done:  63%|   | 1904/3000 [00:01<00:00, 1728.66it/s]warmup should be done:  63%|   | 1897/3000 [00:01<00:00, 1717.72it/s]warmup should be done:  62%|   | 1872/3000 [00:01<00:00, 1692.20it/s]warmup should be done:  63%|   | 1896/3000 [00:01<00:00, 1707.92it/s]warmup should be done:  63%|   | 1887/3000 [00:01<00:00, 1699.71it/s]warmup should be done:  63%|   | 1893/3000 [00:01<00:00, 1717.91it/s]warmup should be done:  68%|   | 2047/3000 [00:01<00:00, 1712.18it/s]warmup should be done:  68%|   | 2054/3000 [00:01<00:00, 1709.23it/s]warmup should be done:  69%|   | 2078/3000 [00:01<00:00, 1731.60it/s]warmup should be done:  69%|   | 2070/3000 [00:01<00:00, 1718.58it/s]warmup should be done:  68%|   | 2042/3000 [00:01<00:00, 1690.34it/s]warmup should be done:  69%|   | 2058/3000 [00:01<00:00, 1702.26it/s]warmup should be done:  69%|   | 2069/3000 [00:01<00:00, 1713.51it/s]warmup should be done:  69%|   | 2067/3000 [00:01<00:00, 1722.72it/s]warmup should be done:  74%|  | 2219/3000 [00:01<00:00, 1712.89it/s]warmup should be done:  74%|  | 2225/3000 [00:01<00:00, 1708.18it/s]warmup should be done:  75%|  | 2252/3000 [00:01<00:00, 1731.73it/s]warmup should be done:  75%|  | 2242/3000 [00:01<00:00, 1718.59it/s]warmup should be done:  74%|  | 2212/3000 [00:01<00:00, 1691.57it/s]warmup should be done:  75%|  | 2242/3000 [00:01<00:00, 1717.50it/s]warmup should be done:  74%|  | 2229/3000 [00:01<00:00, 1702.48it/s]warmup should be done:  75%|  | 2241/3000 [00:01<00:00, 1725.92it/s]warmup should be done:  80%|  | 2391/3000 [00:01<00:00, 1713.28it/s]warmup should be done:  80%|  | 2396/3000 [00:01<00:00, 1705.85it/s]warmup should be done:  81%|  | 2426/3000 [00:01<00:00, 1729.14it/s]warmup should be done:  80%|  | 2414/3000 [00:01<00:00, 1716.67it/s]warmup should be done:  79%|  | 2382/3000 [00:01<00:00, 1690.24it/s]warmup should be done:  80%|  | 2415/3000 [00:01<00:00, 1719.76it/s]warmup should be done:  80%|  | 2400/3000 [00:01<00:00, 1701.81it/s]warmup should be done:  80%|  | 2415/3000 [00:01<00:00, 1727.88it/s]warmup should be done:  85%| | 2563/3000 [00:01<00:00, 1714.34it/s]warmup should be done:  86%| | 2567/3000 [00:01<00:00, 1706.91it/s]warmup should be done:  87%| | 2599/3000 [00:01<00:00, 1727.07it/s]warmup should be done:  86%| | 2586/3000 [00:01<00:00, 1715.62it/s]warmup should be done:  86%| | 2588/3000 [00:01<00:00, 1721.03it/s]warmup should be done:  86%| | 2571/3000 [00:01<00:00, 1703.83it/s]warmup should be done:  85%| | 2552/3000 [00:01<00:00, 1690.29it/s]warmup should be done:  86%| | 2588/3000 [00:01<00:00, 1727.91it/s]warmup should be done:  91%| | 2735/3000 [00:01<00:00, 1715.21it/s]warmup should be done:  91%|| 2738/3000 [00:01<00:00, 1707.66it/s]warmup should be done:  92%|| 2758/3000 [00:01<00:00, 1715.85it/s]warmup should be done:  92%|| 2773/3000 [00:01<00:00, 1728.19it/s]warmup should be done:  92%|| 2761/3000 [00:01<00:00, 1723.42it/s]warmup should be done:  91%|| 2742/3000 [00:01<00:00, 1705.30it/s]warmup should be done:  91%| | 2722/3000 [00:01<00:00, 1692.70it/s]warmup should be done:  92%|| 2761/3000 [00:01<00:00, 1724.11it/s]warmup should be done:  97%|| 2910/3000 [00:01<00:00, 1708.62it/s]warmup should be done:  98%|| 2930/3000 [00:01<00:00, 1715.27it/s]warmup should be done:  98%|| 2946/3000 [00:01<00:00, 1721.52it/s]warmup should be done:  97%|| 2913/3000 [00:01<00:00, 1704.89it/s]warmup should be done:  98%|| 2934/3000 [00:01<00:00, 1719.13it/s]warmup should be done:  97%|| 2907/3000 [00:01<00:00, 1685.76it/s]warmup should be done:  96%|| 2892/3000 [00:01<00:00, 1673.15it/s]warmup should be done:  98%|| 2934/3000 [00:01<00:00, 1698.83it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1723.52it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1716.02it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1715.77it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1708.24it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1706.03it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1704.73it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1697.56it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1688.59it/s]
WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f2a0d9c3e50>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f2a0d905130>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f2a0d9050d0>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f2a0d9061f0>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f2a0d9cadf0>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f2a0d9052e0>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f2a0d9c6fd0>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f2a0d9c4790>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
2022-12-11 20:04:52.020178: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f2542830da0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 20:04:52.020243: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 20:04:52.029718: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 20:04:52.040796: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f25368294d0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 20:04:52.040851: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 20:04:52.049412: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 20:04:52.066438: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f253af94410 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 20:04:52.066491: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 20:04:52.075510: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 20:04:52.620953: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f253e8314d0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 20:04:52.621018: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 20:04:52.631433: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 20:04:52.676101: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f253eb867f0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 20:04:52.676168: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 20:04:52.679490: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f253a82a3e0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 20:04:52.679549: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 20:04:52.680346: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f2546b80e00 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 20:04:52.680391: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 20:04:52.686431: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 20:04:52.687082: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f2546b81f80 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 20:04:52.687122: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 20:04:52.689740: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 20:04:52.690968: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 20:04:52.696780: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 20:04:54.807339: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 20:04:54.809998: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 20:04:54.810086: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 20:04:55.012449: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 20:04:55.081323: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 20:04:55.093494: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 20:04:55.113034: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 20:04:55.130118: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
[HCTR][20:05:16.603][ERROR][RK0][tid #139798928926464]: replica 4 reaches 1000, calling init pre replica
[HCTR][20:05:16.604][ERROR][RK0][tid #139798928926464]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][20:05:16.614][ERROR][RK0][tid #139798928926464]: coll ps creation done
[HCTR][20:05:16.614][ERROR][RK0][tid #139798928926464]: replica 4 waits for coll ps creation barrier
[HCTR][20:05:16.678][ERROR][RK0][tid #139798467557120]: replica 5 reaches 1000, calling init pre replica
[HCTR][20:05:16.678][ERROR][RK0][tid #139798467557120]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][20:05:16.683][ERROR][RK0][tid #139798467557120]: coll ps creation done
[HCTR][20:05:16.683][ERROR][RK0][tid #139798467557120]: replica 5 waits for coll ps creation barrier
[HCTR][20:05:16.790][ERROR][RK0][tid #139799616800512]: replica 3 reaches 1000, calling init pre replica
[HCTR][20:05:16.791][ERROR][RK0][tid #139799616800512]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][20:05:16.804][ERROR][RK0][tid #139799616800512]: coll ps creation done
[HCTR][20:05:16.804][ERROR][RK0][tid #139799616800512]: replica 3 waits for coll ps creation barrier
[HCTR][20:05:16.805][ERROR][RK0][main]: replica 1 reaches 1000, calling init pre replica
[HCTR][20:05:16.805][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][20:05:16.813][ERROR][RK0][main]: coll ps creation done
[HCTR][20:05:16.813][ERROR][RK0][main]: replica 1 waits for coll ps creation barrier
[HCTR][20:05:16.823][ERROR][RK0][main]: replica 6 reaches 1000, calling init pre replica
[HCTR][20:05:16.823][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][20:05:16.832][ERROR][RK0][main]: coll ps creation done
[HCTR][20:05:16.832][ERROR][RK0][main]: replica 6 waits for coll ps creation barrier
[HCTR][20:05:16.855][ERROR][RK0][main]: replica 2 reaches 1000, calling init pre replica
[HCTR][20:05:16.855][ERROR][RK0][tid #139798668883712]: replica 7 reaches 1000, calling init pre replica
[HCTR][20:05:16.855][ERROR][RK0][tid #139798668883712]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][20:05:16.855][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][20:05:16.863][ERROR][RK0][main]: coll ps creation done
[HCTR][20:05:16.863][ERROR][RK0][main]: replica 2 waits for coll ps creation barrier
[HCTR][20:05:16.864][ERROR][RK0][tid #139798668883712]: coll ps creation done
[HCTR][20:05:16.864][ERROR][RK0][tid #139798668883712]: replica 7 waits for coll ps creation barrier
[HCTR][20:05:16.900][ERROR][RK0][main]: replica 0 reaches 1000, calling init pre replica
[HCTR][20:05:16.901][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][20:05:16.908][ERROR][RK0][main]: coll ps creation done
[HCTR][20:05:16.908][ERROR][RK0][main]: replica 0 waits for coll ps creation barrier
[HCTR][20:05:16.908][ERROR][RK0][main]: replica 0 preparing frequency
[HCTR][20:05:24.092][ERROR][RK0][main]: replica 0 preparing frequency done
[HCTR][20:05:24.127][ERROR][RK0][main]: replica 0 calling init per replica
[HCTR][20:05:24.127][ERROR][RK0][main]: replica 1 calling init per replica
[HCTR][20:05:24.127][ERROR][RK0][main]: replica 2 calling init per replica
[HCTR][20:05:24.127][ERROR][RK0][tid #139798668883712]: replica 7 calling init per replica
[HCTR][20:05:24.127][ERROR][RK0][main]: replica 6 calling init per replica
[HCTR][20:05:24.127][ERROR][RK0][tid #139799616800512]: replica 3 calling init per replica
[HCTR][20:05:24.127][ERROR][RK0][tid #139798467557120]: replica 5 calling init per replica
[HCTR][20:05:24.127][ERROR][RK0][tid #139798928926464]: replica 4 calling init per replica
[HCTR][20:05:24.127][ERROR][RK0][main]: Calling build_v2
[HCTR][20:05:24.127][ERROR][RK0][main]: Calling build_v2
[HCTR][20:05:24.127][ERROR][RK0][main]: Calling build_v2
[HCTR][20:05:24.127][ERROR][RK0][tid #139798668883712]: Calling build_v2
[HCTR][20:05:24.127][ERROR][RK0][main]: Calling build_v2
[HCTR][20:05:24.127][ERROR][RK0][tid #139799616800512]: Calling build_v2
[HCTR][20:05:24.127][ERROR][RK0][tid #139798467557120]: Calling build_v2
[HCTR][20:05:24.127][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][20:05:24.127][ERROR][RK0][tid #139798928926464]: Calling build_v2
[HCTR][20:05:24.127][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][20:05:24.127][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][20:05:24.127][ERROR][RK0][tid #139798668883712]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][20:05:24.127][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][20:05:24.127][ERROR][RK0][tid #139799616800512]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][20:05:24.127][ERROR][RK0][tid #139798467557120]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][20:05:24.127][ERROR][RK0][tid #139798928926464]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[[[[[[[[2022-12-11 20:05:242022-12-11 20:05:242022-12-11 20:05:242022-12-11 20:05:24.2022-12-11 20:05:242022-12-11 20:05:24.2022-12-11 20:05:24...2022-12-11 20:05:24127604..127604127617127604127615.: 127619127617: : : : 127617E: : EEEE:  EE    E/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc  /hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc :/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc::::/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc136::136136136136:] 136136] ] ] ] 136using concurrent impl MPSPhase] ] using concurrent impl MPSPhaseusing concurrent impl MPSPhaseusing concurrent impl MPSPhaseusing concurrent impl MPSPhase] 
using concurrent impl MPSPhaseusing concurrent impl MPSPhase



using concurrent impl MPSPhase


[2022-12-11 20:05:24.132043: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:178] v100x8, slow pcie
[2022-12-11 20:05:24.132082: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:[1962022-12-11 20:05:24] .assigning 8 to cpu132091
: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:178] v100x8, slow pcie
[2022-12-11 20:05:24.132136: [E[2022-12-11 20:05:24 2022-12-11 20:05:24./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.132139:132147: 196: E] E assigning 8 to cpu /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc::178212[] ] 2022-12-11 20:05:24v100x8, slow pciebuild asymm link desc with 8X Tesla V100-SXM2-32GB out of 8.

[1321912022-12-11 20:05:24[: .[2022-12-11 20:05:24[E1322262022-12-11 20:05:24.2022-12-11 20:05:24 : .132237./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE132239: 132251[: : E: 2022-12-11 20:05:24178/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE E.] : /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[ 132290v100x8, slow pcie212/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:2022-12-11 20:05:24/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: 
] :196.[:Ebuild asymm link desc with 8X Tesla V100-SXM2-32GB out of 8178[] 1323352022-12-11 20:05:24213 
] 2022-12-11 20:05:24assigning 8 to cpu: .] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccv100x8, slow pcie.
[E132378remote time is 8.68421:
1324002022-12-11 20:05:24 : 
178[: [./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE] [2022-12-11 20:05:24E2022-12-11 20:05:24132463: v100x8, slow pcie2022-12-11 20:05:24. .: 178/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
.132508/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc132514E] :132534: [::  v100x8, slow pcie178: E2022-12-11 20:05:24196E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
] E .]  :v100x8, slow pcie [/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc132607assigning 8 to cpu/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc213
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-11 20:05:24:: 
:] :[.212E196remote time is 8.684212142022-12-11 20:05:24132677]  [] 
] .: [build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-11 20:05:24assigning 8 to cpucpu time is 97.0588132740E2022-12-11 20:05:24
:.

:  .196132779[E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc132814] : 2022-12-11 20:05:24 :[: assigning 8 to cpuE./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc1962022-12-11 20:05:24E
 132859:] . /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: 196assigning 8 to cpu132888/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:E] 
[: :212 assigning 8 to cpu2022-12-11 20:05:24E214] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
. ] build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8:132978/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[cpu time is 97.0588
213: :2022-12-11 20:05:24
[] E212[.2022-12-11 20:05:24remote time is 8.68421 ] 2022-12-11 20:05:24133043.
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccbuild asymm link desc with 8X Tesla V100-SXM2-32GB out of 8.: 133070:[
133089E: 2122022-12-11 20:05:24:  E] [.E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc build asymm link desc with 8X Tesla V100-SXM2-32GB out of 82022-12-11 20:05:24133162 :/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
.: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc212:133192E:] [212:  213build asymm link desc with 8X Tesla V100-SXM2-32GB out of 82022-12-11 20:05:24] E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] 
.build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8 :remote time is 8.68421133252
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[214
: :2022-12-11 20:05:24[] E[213.2022-12-11 20:05:24cpu time is 97.0588 2022-12-11 20:05:24] 133311.
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.remote time is 8.68421: 133331:133343
E: 213:  E] [E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc remote time is 8.684212022-12-11 20:05:24 :/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc213:133427:[] 213: 2142022-12-11 20:05:24remote time is 8.68421] E] .
remote time is 8.68421 cpu time is 97.0588133473
[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
: 2022-12-11 20:05:24[:E.2022-12-11 20:05:24214 133525.] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: 133541cpu time is 97.0588:E: 
214 E] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc cpu time is 97.0588:/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
214:] 214cpu time is 97.0588] 
cpu time is 97.0588
[2022-12-11 20:07:07.773462: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:118] solver created. now build & solve
[2022-12-11 20:07:08.155111: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 3.29 GB
[2022-12-11 20:07:08.155232: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 3.29 GB
[2022-12-11 20:07:08.156339: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:74] mapping nid to rank...
[2022-12-11 20:07:08.857215: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:91] counting slots...
[2022-12-11 20:07:10.437752: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:105] Final num slot is 2
[2022-12-11 20:07:10.437900: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:109] counting blocks...
[2022-12-11 20:08:47.408533: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:118] Final num block is 1001
[2022-12-11 20:08:47.408623: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:123] counting freq and density...
[2022-12-11 20:09:00.955382: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:154] averaging freq and density...
[2022-12-11 20:09:00.955480: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:155] 1001
[2022-12-11 20:09:00.974818: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:123] solver built. now solve
[2022-12-11 20:09:00.974898: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:354] constructing optimal solver, device=8, stream=1
1001 blocks, 8 devices
[2022-12-11 20:09:01.278268: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:527] Add Var...
[2022-12-11 20:09:01.310549: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:545] Capacity...
[2022-12-11 20:09:01.311940: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:548] Connect CPU...
[2022-12-11 20:09:01.333244: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:550] Connect Access To Storage...
[2022-12-11 20:09:01.920443: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:554] Time...
[2022-12-11 20:09:01.922725: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:516] dst 0, total sm is 80
[2022-12-11 20:09:01.925793: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:516] dst 1, total sm is 80
[2022-12-11 20:09:01.928688: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:516] dst 2, total sm is 80
[2022-12-11 20:09:01.931577: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:516] dst 3, total sm is 80
[2022-12-11 20:09:01.934442: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:516] dst 4, total sm is 80
[2022-12-11 20:09:01.937346: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:516] dst 5, total sm is 80
[2022-12-11 20:09:01.940246: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:516] dst 6, total sm is 80
[2022-12-11 20:09:01.943149: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:516] dst 7, total sm is 80
[2022-12-11 20:09:16.874608: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:569] Coll Cache init block placement array
[2022-12-11 20:09:16.882341: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:645] Coll Cache init block placement array done
[2022-12-11 20:09:16.884305: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:647] Coll Cache model reset done
[2022-12-11 20:09:16.929757: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:127] solver solved
[2022-12-11 20:09:16.929858: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:195] 0 solved master
[2022-12-11 20:09:16.929890: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 0 solved
[2022-12-11 20:09:16.929918: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 0 initing device 0
[2022-12-11 20:09:16.930458: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-11 20:09:16.930513: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 3.29 GB
[2022-12-11 20:09:16.934671: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 3.29 GB
[2022-12-11 20:09:16.938591: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 3.29 GB
[2022-12-11 20:09:17. 62946: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 1 solved
[2022-12-11 20:09:17. 63034: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 1 initing device 1
[2022-12-11 20:09:17. 63460: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-11 20:09:17. 63515: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 3.29 GB
[2022-12-11 20:09:17. 65252: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 4 solved
[2022-12-11 20:09:17. 65317: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 4 initing device 4
[2022-12-11 20:09:17. 65697: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-11 20:09:17. 65765: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 3.29 GB
[2022-12-11 20:09:17. 66087: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 3 solved
[2022-12-11 20:09:17. 66158: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 3 initing device 3
[2022-12-11 20:09:17. 66547: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-11 20:09:17. 66602: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 3.29 GB
[2022-12-11 20:09:17. 68153: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 2 solved
[2022-12-11 20:09:17. 68215: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 2 initing device 2
[2022-12-11 20:09:17. 68606: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-11 20:09:17. 68654: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 3.29 GB
[2022-12-11 20:09:17. 69642: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 5 solved
[2022-12-11 20:09:17. 69703: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 5 initing device 5
[2022-12-11 20:09:17. 70028: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 6 solved
[2022-12-11 20:09:17. 70083: E[ 2022-12-11 20:09:17/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc.: 70090205: ] Eworker 0 thread 6 initing device 6 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-11 20:09:17. 70174: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 3.29 GB
[2022-12-11 20:09:17. 70392: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 7 solved
[2022-12-11 20:09:17. 70448: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 7 initing device 7
[2022-12-11 20:09:17. 70494: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-11 20:09:17. 70541: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 3.29 GB
[2022-12-11 20:09:17. 70843: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-11 20:09:17. 70893: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 3.29 GB
[2022-12-11 20:09:17. 89994: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 3.29 GB
[2022-12-11 20:09:17. 90320: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 3.29 GB
[2022-12-11 20:09:17. 90484: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 3.29 GB
[2022-12-11 20:09:17. 90582: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 3.29 GB
[2022-12-11 20:09:17. 94439: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 3.29 GB
[2022-12-11 20:09:17. 94518: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 3.29 GB
[2022-12-11 20:09:17. 94626: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 3.29 GB
[2022-12-11 20:09:17.117186: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 3.29 GB
[2022-12-11 20:09:17.117448: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 3.29 GB
[2022-12-11 20:09:17.117526: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 3.29 GB
[2022-12-11 20:09:17.117624: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 3.29 GB
[2022-12-11 20:09:17.117736: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 3.29 GB
[2022-12-11 20:09:17.117808: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 3.29 GB
[2022-12-11 20:09:17.121656: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 3.29 GB
[2022-12-11 20:09:17.605423: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1001.00 Bytes
[2022-12-11 20:09:17.605634: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1855] using empty feat=27
[2022-12-11 20:09:17.623294: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1001
[2022-12-11 20:09:17.623442: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 3531098340
[2022-12-11 20:09:17.627715: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 138.07 MB
[2022-12-11 20:09:17.628535: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 20:09:17.635860: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 20:09:17.636257: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 134.70 MB
[2022-12-11 20:09:17.734361: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 20:09:17.738972: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 20:09:17.739033: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 16.88 GB
[[[[2022-12-11 20:09:172022-12-11 20:09:172022-12-11 20:09:172022-12-11 20:09:17....830203830203830199830203: : : : EEEE    /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::::1980198019801980] ] ] ] eager alloc mem 1001.00 Byteseager alloc mem 1001.00 Byteseager alloc mem 1001.00 Byteseager alloc mem 1001.00 Bytes



[[[[2022-12-11 20:09:172022-12-11 20:09:172022-12-11 20:09:172022-12-11 20:09:17....830469830469830470830471: : : : WWWW    /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::::1855185518551855] ] ] ] using empty feat=27using empty feat=27using empty feat=27using empty feat=27



[2022-12-11 20:09:17.847260: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1001.00 Bytes
[[2022-12-11 20:09:172022-12-11 20:09:17..847338847338: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::19801980] ] eager alloc mem 1001.00 Byteseager alloc mem 1001.00 Bytes

[2022-12-11 20:09:17.847435: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1855] using empty feat=27
[2022-12-11 20:09:17.[8475152022-12-11 20:09:17: .W847521 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuW: 1855/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu] :using empty feat=271855
] using empty feat=27
[2022-12-11 20:09:17.848575: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1001
[2022-12-11 20:09:17.848663: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc[:2022-12-11 20:09:17638.] 848676eager release cuda mem 3531098340: 
E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1001
[[2022-12-11 20:09:172022-12-11 20:09:17..848768848753: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::638638] ] eager release cuda mem 3531098340eager release cuda mem 1001

[2022-12-11 20:09:17.848841: [E2022-12-11 20:09:17 ./hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc848866:: 638E]  eager release cuda mem 1001/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc
:638] eager release cuda mem 3531098340
[2022-12-11 20:09:17.848933: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 3531098340
[2022-12-11 20:09:17.853164: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 138.07 MB
[2022-12-11 20:09:17.857365: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 138.07 MB
[2022-12-11 20:09:17.862199: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 138.07 MB
[2022-12-11 20:09:17.866542: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 138.07 MB
[2022-12-11 20:09:17.868116: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 20:09:17.868225: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 20:09:17.868329: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 20:09:17.868376: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 20:09:17.875794: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 20:09:17.875913: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 20:09:17.876023: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 20:09:17.876053: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 20:09:17.876112: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 134.70 MB
[2022-12-11 20:09:17.876364: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 134.70 MB
[2022-12-11 20:09:17.876565: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 134.70 MB
[2022-12-11 20:09:17.876634: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 134.70 MB
[2022-12-11 20:09:17.881459: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1001
[2022-12-11 20:09:17.881548: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 3531098340[
2022-12-11 20:09:17.881549: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1001
[[2022-12-11 20:09:172022-12-11 20:09:17..881634881647: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::638638] ] eager release cuda mem 1001eager release cuda mem 3531098340

[2022-12-11 20:09:17.881743: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 3531098340
[2022-12-11 20:09:17.885872: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 138.07 MB
[2022-12-11 20:09:17.890228: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 138.07 MB
[2022-12-11 20:09:17.894157: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 138.07 MB
[2022-12-11 20:09:17.895003: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 20:09:17.895256: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] [eager alloc mem 5.26 MB2022-12-11 20:09:17
.895282: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 20:09:17.902199: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 20:09:17.902485: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 131.44 MB
[2022-12-11 20:09:17.902729[: 2022-12-11 20:09:17E. 902743/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc: :E638 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cceager release cuda mem 5518079:
638] eager release cuda mem 5518079
[2022-12-11 20:09:17.903179: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 131.44 MB
[2022-12-11 20:09:17.903240: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 134.70 MB
[2022-12-11 20:09:17.964519: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 20:09:17.966557: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 20:09:17.968314: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 20:09:17.969069: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 20:09:17.969114: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 16.88 GB
[2022-12-11 20:09:17.969268: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 20:09:17.971052: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 20:09:17.971096: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 16.88 GB
[2022-12-11 20:09:17.972824: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 20:09:17.972869: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 16.88 GB
[2022-12-11 20:09:17.973783: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 20:09:17.973843: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 16.88 GB
[2022-12-11 20:09:17.987413: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 20:09:17.991284: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 20:09:17.991923: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 20:09:17.991967: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 16.47 GB
[2022-12-11 20:09:17.993453: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 20:09:17.995778: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 20:09:17.995824: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 16.47 GB
[2022-12-11 20:09:17.997928: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 20:09:17.997972: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 16.88 GB
[[[[[[[[2022-12-11 20:09:212022-12-11 20:09:212022-12-11 20:09:212022-12-11 20:09:212022-12-11 20:09:212022-12-11 20:09:212022-12-11 20:09:212022-12-11 20:09:21........201688201693201688201689201688201689201693201690: : : : : : : : EEEEEEEE        /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::::::::19261926192619261926192619261926] ] ] ] ] ] ] ] Device 4 init p2p of link 5Device 7 init p2p of link 4Device 5 init p2p of link 6Device 0 init p2p of link 3Device 6 init p2p of link 0Device 3 init p2p of link 2Device 1 init p2p of link 7Device 2 init p2p of link 1







[[[2022-12-11 20:09:21[[2022-12-11 20:09:21[[2022-12-11 20:09:21.2022-12-11 20:09:212022-12-11 20:09:21.2022-12-11 20:09:212022-12-11 20:09:21.202283..202283.[.202283: 202289202290: 2022922022-12-11 20:09:21202294: E: : E: .: E EE E202337E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu :  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu :] ::1980::/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu1980eager alloc mem 5.26 MB19801980] 19801980:] 
] ] eager alloc mem 5.26 MB] ] 1980eager alloc mem 5.26 MBeager alloc mem 5.26 MBeager alloc mem 5.26 MB
eager alloc mem 5.26 MBeager alloc mem 5.26 MB] 




eager alloc mem 5.26 MB
[2022-12-11 20:09:21.212012: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[[[2022-12-11 20:09:212022-12-11 20:09:212022-12-11 20:09:21.[..2120642022-12-11 20:09:21212064212067: .: : E212084EE :   /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccE/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc: ::638/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc638638] :] ] eager release cuda mem 5518079638eager release cuda mem 5518079eager release cuda mem 5518079
] 

eager release cuda mem 5518079
[[2022-12-11 20:09:212022-12-11 20:09:21..212245212251[: : 2022-12-11 20:09:21EE.  212288/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc: ::E638638 ] ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cceager release cuda mem 5518079eager release cuda mem 5518079:

638] eager release cuda mem 5518079
[2022-12-11 20:09:21.231111: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 6 init p2p of link 5
[2022-12-11 20:09:21.231256: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 20:09:21.235295: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 2
[2022-12-11 20:09:21.235484: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 20:09:21.239278: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 0 init p2p of link 6
[2022-12-11 20:09:21.239431: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 20:09:21.239779: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 4 init p2p of link 7
[2022-12-11 20:09:21.239945: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 20:09:21.240089: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 3 init p2p of link 0
[2022-12-11 20:09:21.240244: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 20:09:21.240376: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 2 init p2p of link 3
[2022-12-11 20:09:21.240539: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 20:09:21.241108: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[[2022-12-11 20:09:212022-12-11 20:09:21..241234241243: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::19261926] ] Device 7 init p2p of link 1Device 5 init p2p of link 4

[2022-12-11 20:09:21.241414: [E2022-12-11 20:09:21 ./hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu241424:: 1980E]  eager alloc mem 5.26 MB/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu
:1980] eager alloc mem 5.26 MB
[2022-12-11 20:09:21.242474: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 20:09:21.246495: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 20:09:21.247404: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 20:09:21.247562: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 20:09:21.247721: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 20:09:21.249300: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 20:09:21.249542: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 20:09:21.253961: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 6 init p2p of link 4
[2022-12-11 20:09:21.254083: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 20:09:21.262334: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 3
[2022-12-11 20:09:21.262497: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 20:09:21.263598: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 0 init p2p of link 1
[2022-12-11 20:09:21.263732: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 20:09:21.265213: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 2 init p2p of link 0
[2022-12-11 20:09:21.265339: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 20:09:21.266135: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 7 init p2p of link 6
[2022-12-11 20:09:21.266259: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 20:09:21.266450: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 20:09:21.269330: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 4 init p2p of link 2
[2022-12-11 20:09:21.269460: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 20:09:21.269983: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 5 init p2p of link 7
[2022-12-11 20:09:21.270105: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 20:09:21.270729: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 3 init p2p of link 5
[2022-12-11 20:09:21.270861: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 20:09:21.271081: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 20:09:21.271328: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 20:09:21.272002: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 20:09:21.273085: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 20:09:21.276766: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 20:09:21.277148: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 20:09:21.277612: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 20:09:21.286748: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 6 init p2p of link 7
[2022-12-11 20:09:21.286864: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 20:09:21.295529: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 0
[2022-12-11 20:09:21.295685: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 20:09:21.298089: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 20:09:21.298651: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 0 init p2p of link 2
[2022-12-11 20:09:21.298780: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 20:09:21.301550: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 4 init p2p of link 6
[2022-12-11 20:09:21.301672: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 20:09:21.302524: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 20:09:21.304238: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 3 init p2p of link 1
[2022-12-11 20:09:21.304357: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 20:09:21.305154: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 7 init p2p of link 5
[2022-12-11 20:09:21.305289: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 20:09:21.305304: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 2 init p2p of link 4
[2022-12-11 20:09:21.305427: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 20:09:21.305882: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 20:09:21.308274: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 20:09:21.310077: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 5 init p2p of link 3
[2022-12-11 20:09:21.310196: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 20:09:21.312732: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 20:09:21.313692: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 20:09:21.313804: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 20:09:21.317489: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 20:09:21.318188: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 144775028
[2022-12-11 20:09:21.321940: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 34456688 / 882774585 nodes ( 3.90 %~4.00 %) | remote 95339592 / 882774585 nodes ( 10.80 %) | cpu 752978305 / 882774585 nodes ( 85.30 %) | 16.47 GB | 4.25141 secs 
[2022-12-11 20:09:21.327422: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] [eager release cuda mem 1447750282022-12-11 20:09:21
.327443: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 144775028
[2022-12-11 20:09:21.328610: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 35310960 / 882774585 nodes ( 4.00 %~4.00 %) | remote 94485320 / 882774585 nodes ( 10.70 %) | cpu 752978305 / 882774585 nodes ( 85.30 %) | 16.88 GB | 4.26286 secs 
[2022-12-11 20:09:21.328990: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 35310960 / 882774585 nodes ( 4.00 %~4.00 %) | remote 94485320 / 882774585 nodes ( 10.70 %) | cpu 752978305 / 882774585 nodes ( 85.30 %) | 16.88 GB | 4.26549 secs 
[2022-12-11 20:09:21.329099: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 144775028
[2022-12-11 20:09:21.330867: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 35310960 / 882774585 nodes ( 4.00 %~4.00 %) | remote 94485320 / 882774585 nodes ( 10.70 %) | cpu 752978305 / 882774585 nodes ( 85.30 %) | 16.88 GB | 4.40037 secs 
[2022-12-11 20:09:21.331629: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2213] before create ctx, mem is 28.27 GB
[2022-12-11 20:09:21.336220: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 144775028
[2022-12-11 20:09:21.336537: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 144775028
[2022-12-11 20:09:21.337853: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 35310960 / 882774585 nodes ( 4.00 %~4.00 %) | remote 94485320 / 882774585 nodes ( 10.70 %) | cpu 752978305 / 882774585 nodes ( 85.30 %) | 16.88 GB | 4.27127 secs 
[2022-12-11 20:09:21.338341: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 35310960 / 882774585 nodes ( 4.00 %~4.00 %) | remote 94485320 / 882774585 nodes ( 10.70 %) | cpu 752978305 / 882774585 nodes ( 85.30 %) | 16.88 GB | 4.26818 secs 
[2022-12-11 20:09:21.339733: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 144775028
[2022-12-11 20:09:21.339825: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 144775028
[2022-12-11 20:09:21.369162: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 35310960 / 882774585 nodes ( 4.00 %~4.00 %) | remote 94485320 / 882774585 nodes ( 10.70 %) | cpu 752978305 / 882774585 nodes ( 85.30 %) | 16.88 GB | 4.29828 secs 
[2022-12-11 20:09:21.369392: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 34456688 / 882774585 nodes ( 3.90 %~4.00 %) | remote 95339592 / 882774585 nodes ( 10.80 %) | cpu 752978305 / 882774585 nodes ( 85.30 %) | 16.47 GB | 4.30075 secs 
[2022-12-11 20:09:22.696980: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2219] after create ctx, mem is 28.53 GB
[2022-12-11 20:09:22.698288: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2226] after create stream, mem is 28.53 GB
[2022-12-11 20:09:22.702824: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2213] before create ctx, mem is 28.53 GB
[2022-12-11 20:09:24.146926: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2219] after create ctx, mem is 28.80 GB
[2022-12-11 20:09:24.147049: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2226] after create stream, mem is 28.80 GB
[2022-12-11 20:09:24.148122: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2213] before create ctx, mem is 28.80 GB
[2022-12-11 20:09:25.135268: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2219] after create ctx, mem is 29.01 GB
[2022-12-11 20:09:25.135407: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2226] after create stream, mem is 29.01 GB
[2022-12-11 20:09:25.135715: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2213] before create ctx, mem is 29.01 GB
[2022-12-11 20:09:26.535348: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2219] after create ctx, mem is 29.23 GB
[2022-12-11 20:09:26.536049: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2226] after create stream, mem is 29.23 GB
[2022-12-11 20:09:26.536914: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2243] before create stream, mem is 29.23 GB
[2022-12-11 20:09:26.538156: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2249] after create stream, mem is 29.23 GB
[2022-12-11 20:09:26.540450: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2213] before create ctx, mem is 29.23 GB
[2022-12-11 20:09:28.216732: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2219] after create ctx, mem is 29.42 GB
[2022-12-11 20:09:28.217352: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2226] after create stream, mem is 29.42 GB
[HCTR][20:09:29.548][ERROR][RK0][tid #139799616800512]: replica 3 calling init per replica done, doing barrier
[HCTR][20:09:29.548][ERROR][RK0][tid #139798668883712]: replica 7 calling init per replica done, doing barrier
[HCTR][20:09:29.548][ERROR][RK0][tid #139798467557120]: replica 5 calling init per replica done, doing barrier
[HCTR][20:09:29.548][ERROR][RK0][tid #139798928926464]: replica 4 calling init per replica done, doing barrier
[HCTR][20:09:29.548][ERROR][RK0][main]: replica 1 calling init per replica done, doing barrier
[HCTR][20:09:29.548][ERROR][RK0][main]: replica 0 calling init per replica done, doing barrier
[HCTR][20:09:29.548][ERROR][RK0][main]: replica 2 calling init per replica done, doing barrier
[HCTR][20:09:29.548][ERROR][RK0][main]: replica 6 calling init per replica done, doing barrier
[HCTR][20:09:29.548][ERROR][RK0][tid #139799616800512]: replica 3 calling init per replica done, doing barrier done
[HCTR][20:09:29.548][ERROR][RK0][tid #139798668883712]: replica 7 calling init per replica done, doing barrier done
[HCTR][20:09:29.548][ERROR][RK0][main]: replica 6 calling init per replica done, doing barrier done
[HCTR][20:09:29.548][ERROR][RK0][main]: replica 1 calling init per replica done, doing barrier done
[HCTR][20:09:29.548][ERROR][RK0][main]: replica 0 calling init per replica done, doing barrier done
[HCTR][20:09:29.548][ERROR][RK0][main]: replica 2 calling init per replica done, doing barrier done
[HCTR][20:09:29.548][ERROR][RK0][tid #139798467557120]: replica 5 calling init per replica done, doing barrier done
[HCTR][20:09:29.548][ERROR][RK0][tid #139798928926464]: replica 4 calling init per replica done, doing barrier done
[HCTR][20:09:29.548][ERROR][RK0][tid #139799616800512]: init per replica done
[HCTR][20:09:29.548][ERROR][RK0][tid #139798668883712]: init per replica done
[HCTR][20:09:29.548][ERROR][RK0][main]: init per replica done
[HCTR][20:09:29.548][ERROR][RK0][main]: init per replica done
[HCTR][20:09:29.548][ERROR][RK0][main]: init per replica done
[HCTR][20:09:29.548][ERROR][RK0][tid #139798467557120]: init per replica done
[HCTR][20:09:29.548][ERROR][RK0][tid #139798928926464]: init per replica done
[HCTR][20:09:29.569][ERROR][RK0][main]: init per replica done
