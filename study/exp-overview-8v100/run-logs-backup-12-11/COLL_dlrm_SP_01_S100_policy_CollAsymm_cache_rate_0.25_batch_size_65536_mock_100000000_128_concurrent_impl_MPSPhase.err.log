2022-12-12 06:35:41.570612: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.576476: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.582279: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.594927: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.601049: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.606644: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.609646: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.621509: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.674324: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.674640: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.676695: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.676802: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.678314: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.678460: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.679966: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.680175: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.681511: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.682010: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.683076: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.683686: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.684542: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.685454: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.686178: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.687094: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.687876: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.689194: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.690096: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.691037: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.691945: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.692966: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.693985: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.695019: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.696924: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.698085: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.699083: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.700143: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.701184: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.702215: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.703249: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.704265: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.707780: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.708890: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.709655: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 06:35:41.709961: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.710921: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.711920: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.712905: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.713905: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.715103: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.718765: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.719267: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.721096: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.721519: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.722428: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.723737: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.723891: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.724284: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.724894: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.726323: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.727167: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.727783: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.729277: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.730224: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.730793: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.730831: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.732360: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.733372: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.733389: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.734175: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.734182: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.734694: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.735793: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.736769: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.736985: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.737723: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.737824: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.738484: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.739359: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.740188: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.740755: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.741367: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.741423: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.742269: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.743550: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.744248: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.744722: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.744813: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.745524: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.746507: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.747349: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.748036: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.748514: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.749116: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.750550: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.750938: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.751259: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.752580: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.752922: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.752957: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.760474: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.770077: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.770434: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.770767: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.771838: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.772672: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.774416: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.787829: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.791703: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.793523: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.809617: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.811394: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.811444: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.811489: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.811566: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.811927: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.813758: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.815004: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.815101: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.815291: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.815425: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.815783: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.816212: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.818538: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.819592: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.819724: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.820010: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.820098: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.820477: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.821862: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.823252: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.824416: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.824614: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.824738: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.824869: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.825471: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.825827: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.827400: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.829168: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.829398: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.829524: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.829657: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.829965: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.830321: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.831849: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.834058: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.834077: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.834162: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.834414: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.834455: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.834813: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.836473: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.837849: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.838217: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.838397: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.838583: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.838746: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.839050: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.840497: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.842234: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.842276: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.842425: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.842733: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.842953: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.843252: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.844515: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.846276: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.846321: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.846584: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.846792: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.847142: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.847316: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.848844: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.850374: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.850545: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.850784: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.851090: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.852233: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.852347: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.853865: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.855251: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.855516: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.855736: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.856031: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.856455: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.856635: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.858099: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.859578: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.859947: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.860078: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.860267: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.860735: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.860951: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.862067: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.863649: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.863907: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.864081: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.864827: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.864852: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.865094: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.866265: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.867952: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.868038: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.868301: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.868962: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.869200: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.870106: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.871551: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.871764: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.872024: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.872667: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.872844: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.873302: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 06:35:41.873890: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.875850: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.876288: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.876522: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.876748: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.876935: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.877059: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.879656: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.880103: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.880311: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.880809: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.880944: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.881548: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.882261: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.883745: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.884328: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.885070: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.885295: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.885475: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.885930: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.886794: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.887756: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.888602: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.889083: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.889372: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.889643: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.890011: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.890819: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.892085: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.892658: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.893189: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.893621: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.893871: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.894300: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.896439: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.897430: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.897707: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.898038: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.898413: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.900787: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.901113: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 06:35:41.901158: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.901402: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.901820: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.902022: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.904424: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.904891: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.905089: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.905571: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.905714: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.908704: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.909568: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.909631: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.911632: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 06:35:41.911753: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.912328: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 06:35:41.912587: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 06:35:41.912651: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.914903: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.915779: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 06:35:41.917277: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 06:35:41.920440: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.921486: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.921543: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.923101: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.924932: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.926170: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.937276: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.937506: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.939770: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.940437: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.940518: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.942198: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.942316: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.944899: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:41.945069: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:43.085964: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:43.086588: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:43.087106: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:43.087589: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 06:35:43.087646: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-12 06:35:43.105791: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:43.106633: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:43.107219: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:43.107814: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:43.108578: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:43.109056: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-12 06:35:43.154038: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 06:35:43.154248: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 06:35:43.203674: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12340
2022-12-12 06:35:43.297287: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:43.297913: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:43.298887: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:43.299372: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 06:35:43.299430: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-12 06:35:43.318511: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:43.319156: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:43.319823: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:43.320411: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:43.321156: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:43.321638: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:1/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-12 06:35:43.358946: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:43.360048: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:43.361207: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:43.362140: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 06:35:43.362197: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-12 06:35:43.380349: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:43.380810: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:43.382247: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:43.382488: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:43.383901: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:43.384333: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:43.385178: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:43.385978: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:43.386284: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 06:35:43.386334: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-12 06:35:43.387412: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:43.388134: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:43.389249: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:43.389641: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:2/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-12 06:35:43.390738: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 06:35:43.390794: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-12 06:35:43.393488: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:43.394651: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:43.395790: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:43.395982: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:43.397080: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 06:35:43.397245: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 06:35:43.397296: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 06:35:43.397364: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-12 06:35:43.397622: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:43.398860: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:43.399141: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12341
2022-12-12 06:35:43.399884: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 06:35:43.399929: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-12 06:35:43.404289: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:43.405291: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:43.406181: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:43.407280: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:43.408338: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:43.408523: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:43.409264: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:43.411393: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:5/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-12 06:35:43.411844: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:43.412236: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:43.414382: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:43.414651: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:43.415626: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:43.415911: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 06:35:43.415960: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-12 06:35:43.416348: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:43.417006: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:43.417025: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:43.417380: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:43.418560: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:43.418682: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:43.418865: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:4/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-12 06:35:43.419714: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:43.419735: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:43.421595: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:43.422045: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:43.423909: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:7/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-12 06:35:43.424277: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:43.425120: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:6/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-12 06:35:43.434242: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:43.434879: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:43.435411: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:43.435990: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:43.436525: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:35:43.436990: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:3/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-12 06:35:43.456321: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 06:35:43.456540: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 06:35:43.458257: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12345
2022-12-12 06:35:43.463930: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 06:35:43.464128: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 06:35:43.466282: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12344
2022-12-12 06:35:43.469238: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 06:35:43.469393: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 06:35:43.469583: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 06:35:43.469719: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 06:35:43.471284: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12347
2022-12-12 06:35:43.471588: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12346
2022-12-12 06:35:43.482978: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 06:35:43.483173: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 06:35:43.484998: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12343
2022-12-12 06:35:43.501234: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 06:35:43.501423: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 06:35:43.503224: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12342
[HCTR][06:35:44.772][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][06:35:44.772][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][06:35:44.772][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][06:35:44.772][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][06:35:44.772][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][06:35:44.772][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][06:35:44.772][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][06:35:44.773][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 1it [00:01,  1.56s/it]warmup run: 101it [00:01, 84.20it/s]warmup run: 202it [00:01, 182.91it/s]warmup run: 1it [00:01,  1.56s/it]warmup run: 1it [00:01,  1.56s/it]warmup run: 1it [00:01,  1.56s/it]warmup run: 303it [00:01, 292.13it/s]warmup run: 1it [00:01,  1.56s/it]warmup run: 1it [00:01,  1.56s/it]warmup run: 1it [00:01,  1.56s/it]warmup run: 1it [00:01,  1.55s/it]warmup run: 98it [00:01, 82.07it/s]warmup run: 99it [00:01, 82.69it/s]warmup run: 99it [00:01, 82.55it/s]warmup run: 99it [00:01, 82.56it/s]warmup run: 403it [00:01, 404.07it/s]warmup run: 94it [00:01, 78.67it/s]warmup run: 91it [00:01, 75.98it/s]warmup run: 100it [00:01, 84.09it/s]warmup run: 199it [00:01, 181.33it/s]warmup run: 198it [00:01, 179.52it/s]warmup run: 199it [00:01, 180.38it/s]warmup run: 198it [00:01, 179.37it/s]warmup run: 501it [00:02, 509.78it/s]warmup run: 188it [00:01, 170.90it/s]warmup run: 186it [00:01, 169.22it/s]warmup run: 201it [00:01, 183.53it/s]warmup run: 300it [00:01, 290.99it/s]warmup run: 298it [00:01, 287.77it/s]warmup run: 298it [00:01, 287.08it/s]warmup run: 298it [00:01, 287.66it/s]warmup run: 602it [00:02, 612.96it/s]warmup run: 283it [00:01, 274.11it/s]warmup run: 282it [00:01, 273.64it/s]warmup run: 301it [00:01, 291.88it/s]warmup run: 401it [00:01, 405.34it/s]warmup run: 397it [00:01, 399.16it/s]warmup run: 395it [00:01, 395.09it/s]warmup run: 699it [00:02, 694.56it/s]warmup run: 398it [00:01, 400.18it/s]warmup run: 380it [00:01, 384.44it/s]warmup run: 371it [00:01, 371.43it/s]warmup run: 400it [00:01, 403.01it/s]warmup run: 503it [00:02, 518.33it/s]warmup run: 495it [00:02, 506.12it/s]warmup run: 494it [00:02, 504.35it/s]warmup run: 497it [00:02, 509.08it/s]warmup run: 797it [00:02, 763.46it/s]warmup run: 474it [00:02, 486.69it/s]warmup run: 468it [00:02, 481.83it/s]warmup run: 499it [00:02, 511.42it/s]warmup run: 604it [00:02, 620.66it/s]warmup run: 595it [00:02, 608.18it/s]warmup run: 598it [00:02, 612.97it/s]warmup run: 594it [00:02, 606.40it/s]warmup run: 894it [00:02, 816.39it/s]warmup run: 572it [00:02, 588.69it/s]warmup run: 568it [00:02, 589.77it/s]warmup run: 597it [00:02, 608.27it/s]warmup run: 701it [00:02, 698.05it/s]warmup run: 695it [00:02, 697.25it/s]warmup run: 692it [00:02, 691.96it/s]warmup run: 699it [00:02, 703.00it/s]warmup run: 991it [00:02, 853.21it/s]warmup run: 668it [00:02, 673.38it/s]warmup run: 669it [00:02, 685.50it/s]warmup run: 695it [00:02, 693.10it/s]warmup run: 798it [00:02, 759.51it/s]warmup run: 792it [00:02, 761.51it/s]warmup run: 791it [00:02, 764.66it/s]warmup run: 797it [00:02, 770.46it/s]warmup run: 1090it [00:02, 888.94it/s]warmup run: 770it [00:02, 757.00it/s]warmup run: 770it [00:02, 764.34it/s]warmup run: 795it [00:02, 767.48it/s]warmup run: 894it [00:02, 808.04it/s]warmup run: 889it [00:02, 811.03it/s]warmup run: 897it [00:02, 830.00it/s]warmup run: 893it [00:02, 830.19it/s]warmup run: 1189it [00:02, 915.55it/s]warmup run: 871it [00:02, 822.74it/s]warmup run: 871it [00:02, 827.17it/s]warmup run: 894it [00:02, 825.06it/s]warmup run: 990it [00:02, 844.80it/s]warmup run: 985it [00:02, 847.45it/s]warmup run: 995it [00:02, 879.74it/s]warmup run: 997it [00:02, 874.00it/s]warmup run: 1287it [00:02, 931.38it/s]warmup run: 973it [00:02, 873.93it/s]warmup run: 971it [00:02, 871.93it/s]warmup run: 993it [00:02, 869.35it/s]warmup run: 1085it [00:02, 873.26it/s]warmup run: 1081it [00:02, 870.99it/s]warmup run: 1098it [00:02, 919.27it/s]warmup run: 1096it [00:02, 898.67it/s]warmup run: 1385it [00:02, 936.29it/s]warmup run: 1074it [00:02, 909.71it/s]warmup run: 1070it [00:02, 898.34it/s]warmup run: 1092it [00:02, 902.24it/s]warmup run: 1181it [00:02, 897.54it/s]warmup run: 1176it [00:02, 888.95it/s]warmup run: 1198it [00:02, 937.76it/s]warmup run: 1196it [00:02, 924.63it/s]warmup run: 1483it [00:03, 948.91it/s]warmup run: 1176it [00:02, 938.72it/s]warmup run: 1168it [00:02, 909.10it/s]warmup run: 1191it [00:02, 922.38it/s]warmup run: 1277it [00:02, 905.22it/s]warmup run: 1271it [00:02, 898.61it/s]warmup run: 1295it [00:02, 939.45it/s]warmup run: 1298it [00:02, 947.13it/s]warmup run: 1583it [00:03, 963.83it/s]warmup run: 1278it [00:02, 961.83it/s]warmup run: 1267it [00:02, 929.70it/s]warmup run: 1294it [00:02, 951.55it/s]warmup run: 1374it [00:02, 922.48it/s]warmup run: 1365it [00:02, 908.17it/s]warmup run: 1397it [00:02, 958.18it/s]warmup run: 1393it [00:02, 939.74it/s]warmup run: 1681it [00:03, 957.98it/s]warmup run: 1380it [00:02, 976.12it/s]warmup run: 1368it [00:02, 950.75it/s]warmup run: 1397it [00:02, 972.01it/s]warmup run: 1471it [00:03, 933.89it/s]warmup run: 1459it [00:03, 916.10it/s]warmup run: 1497it [00:03, 969.35it/s]warmup run: 1779it [00:03, 963.93it/s]warmup run: 1490it [00:03, 932.42it/s]warmup run: 1481it [00:03, 981.26it/s]warmup run: 1471it [00:03, 973.48it/s]warmup run: 1501it [00:03, 989.33it/s]warmup run: 1570it [00:03, 950.10it/s]warmup run: 1553it [00:03, 921.04it/s]warmup run: 1597it [00:03, 976.30it/s]warmup run: 1877it [00:03, 965.12it/s]warmup run: 1582it [00:03, 977.83it/s]warmup run: 1586it [00:03, 923.27it/s]warmup run: 1574it [00:03, 989.37it/s]warmup run: 1602it [00:03, 987.28it/s]warmup run: 1670it [00:03, 963.62it/s]warmup run: 1648it [00:03, 927.86it/s]warmup run: 1698it [00:03, 985.38it/s]warmup run: 1976it [00:03, 971.23it/s]warmup run: 1682it [00:03, 969.75it/s]warmup run: 1680it [00:03, 916.36it/s]warmup run: 1677it [00:03, 1000.50it/s]warmup run: 1703it [00:03, 986.10it/s]warmup run: 1770it [00:03, 972.49it/s]warmup run: 1743it [00:03, 932.41it/s]warmup run: 1799it [00:03, 990.79it/s]warmup run: 2089it [00:03, 1018.40it/s]warmup run: 1780it [00:03, 1007.98it/s]warmup run: 1773it [00:03, 912.78it/s]warmup run: 1780it [00:03, 961.92it/s]warmup run: 1803it [00:03, 988.49it/s]warmup run: 1870it [00:03, 978.82it/s]warmup run: 1837it [00:03, 934.39it/s]warmup run: 1901it [00:03, 997.38it/s]warmup run: 2207it [00:03, 1066.44it/s]warmup run: 1884it [00:03, 1014.91it/s]warmup run: 1865it [00:03, 905.73it/s]warmup run: 1877it [00:03, 954.77it/s]warmup run: 1905it [00:03, 994.84it/s]warmup run: 1971it [00:03, 987.41it/s]warmup run: 1937it [00:03, 953.30it/s]warmup run: 2002it [00:03, 998.65it/s]warmup run: 2326it [00:03, 1102.07it/s]warmup run: 1988it [00:03, 1020.05it/s]warmup run: 1957it [00:03, 905.79it/s]warmup run: 1973it [00:03, 952.24it/s]warmup run: 2008it [00:03, 1004.33it/s]warmup run: 2087it [00:03, 1037.58it/s]warmup run: 2044it [00:03, 986.12it/s]warmup run: 2122it [00:03, 1057.02it/s]warmup run: 2443it [00:03, 1120.02it/s]warmup run: 2107it [00:03, 1068.21it/s]warmup run: 2087it [00:03, 1007.01it/s]warmup run: 2065it [00:03, 954.63it/s]warmup run: 2132it [00:03, 1071.94it/s]warmup run: 2210it [00:03, 1092.83it/s]warmup run: 2165it [00:03, 1050.84it/s]warmup run: 2242it [00:03, 1098.91it/s]warmup run: 2562it [00:04, 1139.71it/s]warmup run: 2229it [00:03, 1112.39it/s]warmup run: 2207it [00:03, 1062.13it/s]warmup run: 2185it [00:03, 1025.09it/s]warmup run: 2256it [00:03, 1120.13it/s]warmup run: 2333it [00:03, 1132.39it/s]warmup run: 2286it [00:03, 1096.10it/s]warmup run: 2362it [00:03, 1127.62it/s]warmup run: 2682it [00:04, 1155.03it/s]warmup run: 2351it [00:03, 1143.88it/s]warmup run: 2327it [00:03, 1101.51it/s]warmup run: 2305it [00:03, 1074.56it/s]warmup run: 2380it [00:03, 1154.35it/s]warmup run: 2456it [00:03, 1160.33it/s]warmup run: 2406it [00:03, 1126.40it/s]warmup run: 2482it [00:03, 1146.94it/s]warmup run: 2803it [00:04, 1169.97it/s]warmup run: 2473it [00:03, 1166.18it/s]warmup run: 2424it [00:04, 1108.58it/s]warmup run: 2448it [00:03, 1131.55it/s]warmup run: 2504it [00:03, 1178.31it/s]warmup run: 2579it [00:04, 1180.05it/s]warmup run: 2526it [00:04, 1147.79it/s]warmup run: 2602it [00:04, 1160.98it/s]warmup run: 2924it [00:04, 1181.17it/s]warmup run: 2595it [00:04, 1181.36it/s]warmup run: 2543it [00:04, 1131.63it/s]warmup run: 2569it [00:04, 1153.13it/s]warmup run: 2628it [00:04, 1194.07it/s]warmup run: 3000it [00:04, 673.18it/s] warmup run: 2700it [00:04, 1186.75it/s]warmup run: 2647it [00:04, 1164.55it/s]warmup run: 2721it [00:04, 1166.74it/s]warmup run: 2716it [00:04, 1187.19it/s]warmup run: 2660it [00:04, 1142.56it/s]warmup run: 2688it [00:04, 1161.26it/s]warmup run: 2750it [00:04, 1199.68it/s]warmup run: 2821it [00:04, 1193.30it/s]warmup run: 2769it [00:04, 1179.51it/s]warmup run: 2841it [00:04, 1175.57it/s]warmup run: 2838it [00:04, 1196.02it/s]warmup run: 2780it [00:04, 1157.25it/s]warmup run: 2808it [00:04, 1171.85it/s]warmup run: 2874it [00:04, 1209.11it/s]warmup run: 2943it [00:04, 1198.50it/s]warmup run: 2892it [00:04, 1192.76it/s]warmup run: 2961it [00:04, 1180.83it/s]warmup run: 2960it [00:04, 1202.08it/s]warmup run: 2899it [00:04, 1166.37it/s]warmup run: 2928it [00:04, 1180.18it/s]warmup run: 2998it [00:04, 1216.33it/s]warmup run: 3000it [00:04, 676.46it/s] warmup run: 3000it [00:04, 677.88it/s] warmup run: 3000it [00:04, 684.78it/s] warmup run: 3000it [00:04, 678.08it/s] warmup run: 3000it [00:04, 668.81it/s] warmup run: 3000it [00:04, 673.00it/s] warmup run: 3000it [00:04, 666.34it/s] 

warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]




warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   5%|▌         | 164/3000 [00:00<00:01, 1638.86it/s]warmup should be done:   5%|▌         | 160/3000 [00:00<00:01, 1596.14it/s]warmup should be done:   5%|▌         | 161/3000 [00:00<00:01, 1607.79it/s]warmup should be done:   5%|▌         | 164/3000 [00:00<00:01, 1636.45it/s]warmup should be done:   6%|▌         | 166/3000 [00:00<00:01, 1655.94it/s]warmup should be done:   5%|▌         | 161/3000 [00:00<00:01, 1604.76it/s]warmup should be done:   6%|▌         | 166/3000 [00:00<00:01, 1652.66it/s]warmup should be done:   6%|▌         | 167/3000 [00:00<00:01, 1660.28it/s]warmup should be done:  11%|█         | 320/3000 [00:00<00:01, 1598.36it/s]warmup should be done:  11%|█         | 333/3000 [00:00<00:01, 1662.32it/s]warmup should be done:  11%|█         | 329/3000 [00:00<00:01, 1640.05it/s]warmup should be done:  11%|█         | 329/3000 [00:00<00:01, 1639.80it/s]warmup should be done:  11%|█         | 334/3000 [00:00<00:01, 1665.17it/s]warmup should be done:  11%|█         | 336/3000 [00:00<00:01, 1673.70it/s]warmup should be done:  11%|█         | 322/3000 [00:00<00:01, 1601.10it/s]warmup should be done:  11%|█         | 322/3000 [00:00<00:01, 1603.02it/s]warmup should be done:  17%|█▋        | 500/3000 [00:00<00:01, 1662.49it/s]warmup should be done:  16%|█▋        | 493/3000 [00:00<00:01, 1636.82it/s]warmup should be done:  16%|█▌        | 485/3000 [00:00<00:01, 1612.45it/s]warmup should be done:  16%|█▌        | 485/3000 [00:00<00:01, 1613.39it/s]warmup should be done:  16%|█▌        | 480/3000 [00:00<00:01, 1590.22it/s]warmup should be done:  17%|█▋        | 501/3000 [00:00<00:01, 1662.86it/s]warmup should be done:  17%|█▋        | 504/3000 [00:00<00:01, 1672.94it/s]warmup should be done:  16%|█▋        | 494/3000 [00:00<00:01, 1635.72it/s]warmup should be done:  22%|██▏       | 648/3000 [00:00<00:01, 1619.22it/s]warmup should be done:  22%|██▏       | 667/3000 [00:00<00:01, 1661.84it/s]warmup should be done:  22%|██▏       | 648/3000 [00:00<00:01, 1617.44it/s]warmup should be done:  22%|██▏       | 672/3000 [00:00<00:01, 1674.72it/s]warmup should be done:  22%|██▏       | 657/3000 [00:00<00:01, 1634.15it/s]warmup should be done:  22%|██▏       | 668/3000 [00:00<00:01, 1661.78it/s]warmup should be done:  22%|██▏       | 658/3000 [00:00<00:01, 1633.33it/s]warmup should be done:  21%|██▏       | 640/3000 [00:00<00:01, 1586.25it/s]warmup should be done:  27%|██▋       | 810/3000 [00:00<00:01, 1617.50it/s]warmup should be done:  27%|██▋       | 811/3000 [00:00<00:01, 1621.01it/s]warmup should be done:  28%|██▊       | 840/3000 [00:00<00:01, 1674.12it/s]warmup should be done:  28%|██▊       | 834/3000 [00:00<00:01, 1661.33it/s]warmup should be done:  28%|██▊       | 835/3000 [00:00<00:01, 1659.39it/s]warmup should be done:  27%|██▋       | 799/3000 [00:00<00:01, 1584.28it/s]warmup should be done:  27%|██▋       | 822/3000 [00:00<00:01, 1631.11it/s]warmup should be done:  27%|██▋       | 821/3000 [00:00<00:01, 1619.68it/s]warmup should be done:  32%|███▏      | 972/3000 [00:00<00:01, 1617.27it/s]warmup should be done:  34%|███▎      | 1008/3000 [00:00<00:01, 1669.64it/s]warmup should be done:  32%|███▏      | 974/3000 [00:00<00:01, 1616.16it/s]warmup should be done:  33%|███▎      | 1001/3000 [00:00<00:01, 1657.35it/s]warmup should be done:  32%|███▏      | 959/3000 [00:00<00:01, 1587.95it/s]warmup should be done:  33%|███▎      | 1001/3000 [00:00<00:01, 1654.00it/s]warmup should be done:  33%|███▎      | 983/3000 [00:00<00:01, 1617.13it/s]warmup should be done:  33%|███▎      | 986/3000 [00:00<00:01, 1624.19it/s]warmup should be done:  39%|███▉      | 1175/3000 [00:00<00:01, 1669.02it/s]warmup should be done:  37%|███▋      | 1118/3000 [00:00<00:01, 1588.46it/s]warmup should be done:  39%|███▉      | 1167/3000 [00:00<00:01, 1657.21it/s]warmup should be done:  38%|███▊      | 1134/3000 [00:00<00:01, 1608.75it/s]warmup should be done:  38%|███▊      | 1136/3000 [00:00<00:01, 1611.35it/s]warmup should be done:  39%|███▉      | 1167/3000 [00:00<00:01, 1652.98it/s]warmup should be done:  38%|███▊      | 1146/3000 [00:00<00:01, 1620.04it/s]warmup should be done:  38%|███▊      | 1149/3000 [00:00<00:01, 1617.41it/s]warmup should be done:  45%|████▍     | 1342/3000 [00:00<00:00, 1667.93it/s]warmup should be done:  43%|████▎     | 1278/3000 [00:00<00:01, 1590.02it/s]warmup should be done:  44%|████▍     | 1334/3000 [00:00<00:01, 1658.68it/s]warmup should be done:  43%|████▎     | 1298/3000 [00:00<00:01, 1611.59it/s]warmup should be done:  44%|████▍     | 1333/3000 [00:00<00:01, 1651.87it/s]warmup should be done:  44%|████▎     | 1309/3000 [00:00<00:01, 1621.56it/s]warmup should be done:  43%|████▎     | 1295/3000 [00:00<00:01, 1596.57it/s]warmup should be done:  44%|████▎     | 1311/3000 [00:00<00:01, 1606.91it/s]warmup should be done:  50%|█████     | 1509/3000 [00:00<00:00, 1666.29it/s]warmup should be done:  48%|████▊     | 1438/3000 [00:00<00:00, 1591.59it/s]warmup should be done:  50%|█████     | 1500/3000 [00:00<00:00, 1656.86it/s]warmup should be done:  49%|████▊     | 1461/3000 [00:00<00:00, 1614.91it/s]warmup should be done:  50%|████▉     | 1499/3000 [00:00<00:00, 1652.26it/s]warmup should be done:  49%|████▉     | 1472/3000 [00:00<00:00, 1623.33it/s]warmup should be done:  49%|████▊     | 1457/3000 [00:00<00:00, 1602.85it/s]warmup should be done:  49%|████▉     | 1472/3000 [00:00<00:00, 1600.86it/s]warmup should be done:  56%|█████▌    | 1676/3000 [00:01<00:00, 1665.60it/s]warmup should be done:  56%|█████▌    | 1666/3000 [00:01<00:00, 1657.75it/s]warmup should be done:  53%|█████▎    | 1598/3000 [00:01<00:00, 1591.54it/s]warmup should be done:  56%|█████▌    | 1665/3000 [00:01<00:00, 1653.64it/s]warmup should be done:  55%|█████▍    | 1636/3000 [00:01<00:00, 1626.15it/s]warmup should be done:  54%|█████▍    | 1623/3000 [00:01<00:00, 1608.35it/s]warmup should be done:  54%|█████▍    | 1618/3000 [00:01<00:00, 1600.28it/s]warmup should be done:  54%|█████▍    | 1633/3000 [00:01<00:00, 1596.54it/s]warmup should be done:  61%|██████    | 1832/3000 [00:01<00:00, 1657.62it/s]warmup should be done:  61%|██████▏   | 1843/3000 [00:01<00:00, 1664.99it/s]warmup should be done:  59%|█████▊    | 1758/3000 [00:01<00:00, 1592.46it/s]warmup should be done:  61%|██████    | 1831/3000 [00:01<00:00, 1654.35it/s]warmup should be done:  60%|██████    | 1800/3000 [00:01<00:00, 1628.03it/s]warmup should be done:  59%|█████▉    | 1784/3000 [00:01<00:00, 1590.25it/s]warmup should be done:  59%|█████▉    | 1779/3000 [00:01<00:00, 1584.76it/s]warmup should be done:  60%|█████▉    | 1793/3000 [00:01<00:00, 1593.32it/s]warmup should be done:  67%|██████▋   | 1998/3000 [00:01<00:00, 1655.46it/s]warmup should be done:  64%|██████▍   | 1918/3000 [00:01<00:00, 1590.48it/s]warmup should be done:  65%|██████▌   | 1963/3000 [00:01<00:00, 1628.13it/s]warmup should be done:  67%|██████▋   | 1997/3000 [00:01<00:00, 1643.34it/s]warmup should be done:  67%|██████▋   | 2010/3000 [00:01<00:00, 1645.16it/s]warmup should be done:  65%|██████▍   | 1938/3000 [00:01<00:00, 1580.43it/s]warmup should be done:  65%|██████▍   | 1944/3000 [00:01<00:00, 1584.71it/s]warmup should be done:  65%|██████▌   | 1953/3000 [00:01<00:00, 1593.53it/s]warmup should be done:  72%|███████▏  | 2165/3000 [00:01<00:00, 1656.98it/s]warmup should be done:  69%|██████▉   | 2078/3000 [00:01<00:00, 1592.05it/s]warmup should be done:  71%|███████   | 2126/3000 [00:01<00:00, 1628.03it/s]warmup should be done:  72%|███████▏  | 2162/3000 [00:01<00:00, 1636.72it/s]warmup should be done:  72%|███████▎  | 2175/3000 [00:01<00:00, 1639.78it/s]warmup should be done:  70%|██████▉   | 2099/3000 [00:01<00:00, 1587.84it/s]warmup should be done:  70%|███████   | 2105/3000 [00:01<00:00, 1590.93it/s]warmup should be done:  70%|███████   | 2115/3000 [00:01<00:00, 1601.10it/s]warmup should be done:  78%|███████▊  | 2331/3000 [00:01<00:00, 1656.02it/s]warmup should be done:  75%|███████▍  | 2238/3000 [00:01<00:00, 1591.67it/s]warmup should be done:  76%|███████▋  | 2289/3000 [00:01<00:00, 1626.09it/s]warmup should be done:  78%|███████▊  | 2340/3000 [00:01<00:00, 1635.87it/s]warmup should be done:  78%|███████▊  | 2326/3000 [00:01<00:00, 1626.91it/s]warmup should be done:  75%|███████▌  | 2259/3000 [00:01<00:00, 1589.16it/s]warmup should be done:  76%|███████▌  | 2265/3000 [00:01<00:00, 1588.88it/s]warmup should be done:  76%|███████▌  | 2276/3000 [00:01<00:00, 1593.73it/s]warmup should be done:  83%|████████▎ | 2498/3000 [00:01<00:00, 1657.92it/s]warmup should be done:  80%|███████▉  | 2398/3000 [00:01<00:00, 1592.00it/s]warmup should be done:  82%|████████▏ | 2452/3000 [00:01<00:00, 1624.99it/s]warmup should be done:  83%|████████▎ | 2504/3000 [00:01<00:00, 1632.77it/s]warmup should be done:  81%|████████  | 2420/3000 [00:01<00:00, 1592.52it/s]warmup should be done:  81%|████████  | 2426/3000 [00:01<00:00, 1594.45it/s]warmup should be done:  83%|████████▎ | 2489/3000 [00:01<00:00, 1619.73it/s]warmup should be done:  81%|████████  | 2436/3000 [00:01<00:00, 1592.68it/s]warmup should be done:  89%|████████▉ | 2664/3000 [00:01<00:00, 1654.27it/s]warmup should be done:  85%|████████▌ | 2558/3000 [00:01<00:00, 1590.81it/s]warmup should be done:  87%|████████▋ | 2616/3000 [00:01<00:00, 1626.55it/s]warmup should be done:  89%|████████▉ | 2668/3000 [00:01<00:00, 1630.84it/s]warmup should be done:  86%|████████▌ | 2583/3000 [00:01<00:00, 1601.17it/s]warmup should be done:  86%|████████▋ | 2589/3000 [00:01<00:00, 1603.47it/s]warmup should be done:  88%|████████▊ | 2651/3000 [00:01<00:00, 1618.05it/s]warmup should be done:  87%|████████▋ | 2596/3000 [00:01<00:00, 1590.89it/s]warmup should be done:  93%|█████████▎| 2779/3000 [00:01<00:00, 1625.87it/s]warmup should be done:  91%|█████████ | 2718/3000 [00:01<00:00, 1589.56it/s]warmup should be done:  94%|█████████▍| 2830/3000 [00:01<00:00, 1646.33it/s]warmup should be done:  94%|█████████▍| 2832/3000 [00:01<00:00, 1630.81it/s]warmup should be done:  92%|█████████▏| 2751/3000 [00:01<00:00, 1607.88it/s]warmup should be done:  91%|█████████▏| 2744/3000 [00:01<00:00, 1601.54it/s]warmup should be done:  94%|█████████▍| 2814/3000 [00:01<00:00, 1619.44it/s]warmup should be done:  92%|█████████▏| 2756/3000 [00:01<00:00, 1584.26it/s]warmup should be done:  98%|█████████▊| 2944/3000 [00:01<00:00, 1630.87it/s]warmup should be done:  96%|█████████▌| 2879/3000 [00:01<00:00, 1592.92it/s]warmup should be done: 100%|█████████▉| 2996/3000 [00:01<00:00, 1648.51it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1655.18it/s]warmup should be done: 100%|█████████▉| 2996/3000 [00:01<00:00, 1633.19it/s]warmup should be done:  97%|█████████▋| 2915/3000 [00:01<00:00, 1616.72it/s]warmup should be done:  97%|█████████▋| 2907/3000 [00:01<00:00, 1609.93it/s]warmup should be done:  99%|█████████▉| 2978/3000 [00:01<00:00, 1625.47it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1651.09it/s]warmup should be done:  97%|█████████▋| 2916/3000 [00:01<00:00, 1588.00it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1640.60it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1627.03it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1607.26it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1602.36it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1600.61it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1591.38it/s]






warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   6%|▌         | 167/3000 [00:00<00:01, 1669.08it/s]warmup should be done:   5%|▌         | 164/3000 [00:00<00:01, 1637.92it/s]warmup should be done:   6%|▌         | 167/3000 [00:00<00:01, 1666.49it/s]warmup should be done:   6%|▌         | 166/3000 [00:00<00:01, 1654.72it/s]warmup should be done:   5%|▌         | 164/3000 [00:00<00:01, 1634.41it/s]warmup should be done:   5%|▌         | 162/3000 [00:00<00:01, 1614.14it/s]warmup should be done:   6%|▌         | 168/3000 [00:00<00:01, 1672.08it/s]warmup should be done:   6%|▌         | 167/3000 [00:00<00:01, 1663.22it/s]warmup should be done:  11%|█         | 329/3000 [00:00<00:01, 1644.60it/s]warmup should be done:  11%|█         | 334/3000 [00:00<00:01, 1666.65it/s]warmup should be done:  11%|█         | 333/3000 [00:00<00:01, 1662.99it/s]warmup should be done:  11%|█         | 328/3000 [00:00<00:01, 1635.71it/s]warmup should be done:  11%|█         | 335/3000 [00:00<00:01, 1671.02it/s]warmup should be done:  11%|█         | 337/3000 [00:00<00:01, 1677.01it/s]warmup should be done:  11%|█         | 334/3000 [00:00<00:01, 1647.68it/s]warmup should be done:  11%|█         | 324/3000 [00:00<00:01, 1592.57it/s]warmup should be done:  17%|█▋        | 501/3000 [00:00<00:01, 1667.62it/s]warmup should be done:  16%|█▋        | 495/3000 [00:00<00:01, 1647.77it/s]warmup should be done:  17%|█▋        | 501/3000 [00:00<00:01, 1668.21it/s]warmup should be done:  16%|█▋        | 493/3000 [00:00<00:01, 1640.03it/s]warmup should be done:  17%|█▋        | 504/3000 [00:00<00:01, 1676.05it/s]warmup should be done:  17%|█▋        | 506/3000 [00:00<00:01, 1680.33it/s]warmup should be done:  17%|█▋        | 503/3000 [00:00<00:01, 1662.59it/s]warmup should be done:  16%|█▌        | 484/3000 [00:00<00:01, 1588.69it/s]warmup should be done:  22%|██▏       | 660/3000 [00:00<00:01, 1648.56it/s]warmup should be done:  22%|██▏       | 669/3000 [00:00<00:01, 1670.18it/s]warmup should be done:  22%|██▏       | 669/3000 [00:00<00:01, 1670.07it/s]warmup should be done:  22%|██▏       | 658/3000 [00:00<00:01, 1642.04it/s]warmup should be done:  22%|██▏       | 673/3000 [00:00<00:01, 1677.67it/s]warmup should be done:  22%|██▎       | 675/3000 [00:00<00:01, 1679.92it/s]warmup should be done:  22%|██▏       | 671/3000 [00:00<00:01, 1667.04it/s]warmup should be done:  22%|██▏       | 645/3000 [00:00<00:01, 1594.40it/s]warmup should be done:  28%|██▊       | 825/3000 [00:00<00:01, 1646.56it/s]warmup should be done:  28%|██▊       | 837/3000 [00:00<00:01, 1670.95it/s]warmup should be done:  27%|██▋       | 823/3000 [00:00<00:01, 1641.61it/s]warmup should be done:  28%|██▊       | 841/3000 [00:00<00:01, 1677.21it/s]warmup should be done:  28%|██▊       | 837/3000 [00:00<00:01, 1668.67it/s]warmup should be done:  28%|██▊       | 844/3000 [00:00<00:01, 1680.67it/s]warmup should be done:  28%|██▊       | 839/3000 [00:00<00:01, 1668.75it/s]warmup should be done:  27%|██▋       | 808/3000 [00:00<00:01, 1606.10it/s]warmup should be done:  33%|███▎      | 991/3000 [00:00<00:01, 1648.91it/s]warmup should be done:  34%|███▎      | 1005/3000 [00:00<00:01, 1670.87it/s]warmup should be done:  33%|███▎      | 988/3000 [00:00<00:01, 1642.93it/s]warmup should be done:  34%|███▎      | 1009/3000 [00:00<00:01, 1676.87it/s]warmup should be done:  34%|███▎      | 1006/3000 [00:00<00:01, 1672.62it/s]warmup should be done:  34%|███▍      | 1013/3000 [00:00<00:01, 1681.93it/s]warmup should be done:  34%|███▎      | 1007/3000 [00:00<00:01, 1671.81it/s]warmup should be done:  32%|███▏      | 972/3000 [00:00<00:01, 1616.00it/s]warmup should be done:  39%|███▊      | 1159/3000 [00:00<00:01, 1656.97it/s]warmup should be done:  39%|███▉      | 1177/3000 [00:00<00:01, 1677.45it/s]warmup should be done:  38%|███▊      | 1153/3000 [00:00<00:01, 1644.06it/s]warmup should be done:  39%|███▉      | 1173/3000 [00:00<00:01, 1668.71it/s]warmup should be done:  39%|███▉      | 1177/3000 [00:00<00:01, 1681.76it/s]warmup should be done:  39%|███▉      | 1182/3000 [00:00<00:01, 1680.29it/s]warmup should be done:  39%|███▉      | 1175/3000 [00:00<00:01, 1673.67it/s]warmup should be done:  38%|███▊      | 1138/3000 [00:00<00:01, 1628.70it/s]warmup should be done:  44%|████▍     | 1326/3000 [00:00<00:01, 1660.78it/s]warmup should be done:  45%|████▍     | 1345/3000 [00:00<00:00, 1677.67it/s]warmup should be done:  44%|████▍     | 1318/3000 [00:00<00:01, 1641.20it/s]warmup should be done:  45%|████▍     | 1341/3000 [00:00<00:00, 1669.77it/s]warmup should be done:  45%|████▍     | 1348/3000 [00:00<00:00, 1688.64it/s]warmup should be done:  45%|████▌     | 1351/3000 [00:00<00:00, 1681.09it/s]warmup should be done:  43%|████▎     | 1304/3000 [00:00<00:01, 1636.22it/s]warmup should be done:  45%|████▍     | 1343/3000 [00:00<00:01, 1654.69it/s]warmup should be done:  50%|████▉     | 1494/3000 [00:00<00:00, 1666.43it/s]warmup should be done:  50%|█████     | 1513/3000 [00:00<00:00, 1676.78it/s]warmup should be done:  51%|█████     | 1518/3000 [00:00<00:00, 1691.35it/s]warmup should be done:  50%|█████     | 1509/3000 [00:00<00:00, 1669.95it/s]warmup should be done:  51%|█████     | 1520/3000 [00:00<00:00, 1680.14it/s]warmup should be done:  49%|████▉     | 1483/3000 [00:00<00:00, 1631.93it/s]warmup should be done:  49%|████▉     | 1472/3000 [00:00<00:00, 1647.06it/s]warmup should be done:  50%|█████     | 1511/3000 [00:00<00:00, 1661.49it/s]warmup should be done:  55%|█████▌    | 1662/3000 [00:01<00:00, 1670.39it/s]warmup should be done:  56%|█████▌    | 1681/3000 [00:01<00:00, 1677.44it/s]warmup should be done:  56%|█████▌    | 1677/3000 [00:01<00:00, 1671.04it/s]warmup should be done:  56%|█████▋    | 1689/3000 [00:01<00:00, 1694.21it/s]warmup should be done:  56%|█████▋    | 1689/3000 [00:01<00:00, 1680.41it/s]warmup should be done:  55%|█████▍    | 1649/3000 [00:01<00:00, 1638.06it/s]warmup should be done:  55%|█████▍    | 1639/3000 [00:01<00:00, 1651.85it/s]warmup should be done:  56%|█████▌    | 1680/3000 [00:01<00:00, 1668.42it/s]warmup should be done:  61%|██████    | 1831/3000 [00:01<00:00, 1674.19it/s]warmup should be done:  62%|██████▏   | 1849/3000 [00:01<00:00, 1677.74it/s]warmup should be done:  62%|██████▏   | 1845/3000 [00:01<00:00, 1672.21it/s]warmup should be done:  62%|██████▏   | 1860/3000 [00:01<00:00, 1696.79it/s]warmup should be done:  62%|██████▏   | 1858/3000 [00:01<00:00, 1680.62it/s]warmup should be done:  60%|██████    | 1815/3000 [00:01<00:00, 1642.34it/s]warmup should be done:  60%|██████    | 1808/3000 [00:01<00:00, 1660.66it/s]warmup should be done:  62%|██████▏   | 1849/3000 [00:01<00:00, 1673.78it/s]warmup should be done:  67%|██████▋   | 2017/3000 [00:01<00:00, 1676.82it/s]warmup should be done:  67%|██████▋   | 1999/3000 [00:01<00:00, 1672.44it/s]warmup should be done:  67%|██████▋   | 2013/3000 [00:01<00:00, 1673.03it/s]warmup should be done:  68%|██████▊   | 2031/3000 [00:01<00:00, 1698.34it/s]warmup should be done:  68%|██████▊   | 2027/3000 [00:01<00:00, 1681.30it/s]warmup should be done:  66%|██████▌   | 1980/3000 [00:01<00:00, 1641.18it/s]warmup should be done:  66%|██████▌   | 1977/3000 [00:01<00:00, 1667.91it/s]warmup should be done:  67%|██████▋   | 2017/3000 [00:01<00:00, 1674.87it/s]warmup should be done:  72%|███████▏  | 2167/3000 [00:01<00:00, 1672.91it/s]warmup should be done:  73%|███████▎  | 2185/3000 [00:01<00:00, 1674.19it/s]warmup should be done:  73%|███████▎  | 2201/3000 [00:01<00:00, 1698.06it/s]warmup should be done:  73%|███████▎  | 2181/3000 [00:01<00:00, 1671.34it/s]warmup should be done:  73%|███████▎  | 2196/3000 [00:01<00:00, 1680.77it/s]warmup should be done:  72%|███████▏  | 2145/3000 [00:01<00:00, 1641.75it/s]warmup should be done:  72%|███████▏  | 2145/3000 [00:01<00:00, 1671.02it/s]warmup should be done:  73%|███████▎  | 2185/3000 [00:01<00:00, 1674.70it/s]warmup should be done:  78%|███████▊  | 2336/3000 [00:01<00:00, 1675.27it/s]warmup should be done:  79%|███████▉  | 2371/3000 [00:01<00:00, 1698.48it/s]warmup should be done:  79%|███████▉  | 2365/3000 [00:01<00:00, 1681.69it/s]warmup should be done:  77%|███████▋  | 2313/3000 [00:01<00:00, 1672.56it/s]warmup should be done:  77%|███████▋  | 2311/3000 [00:01<00:00, 1644.60it/s]warmup should be done:  78%|███████▊  | 2353/3000 [00:01<00:00, 1650.90it/s]warmup should be done:  78%|███████▊  | 2354/3000 [00:01<00:00, 1677.51it/s]warmup should be done:  78%|███████▊  | 2349/3000 [00:01<00:00, 1649.65it/s]warmup should be done:  83%|████████▎ | 2504/3000 [00:01<00:00, 1676.11it/s]warmup should be done:  85%|████████▍ | 2542/3000 [00:01<00:00, 1699.01it/s]warmup should be done:  85%|████████▍ | 2537/3000 [00:01<00:00, 1691.68it/s]warmup should be done:  83%|████████▎ | 2476/3000 [00:01<00:00, 1645.53it/s]warmup should be done:  83%|████████▎ | 2483/3000 [00:01<00:00, 1677.95it/s]warmup should be done:  84%|████████▍ | 2520/3000 [00:01<00:00, 1654.70it/s]warmup should be done:  84%|████████▍ | 2523/3000 [00:01<00:00, 1678.76it/s]warmup should be done:  84%|████████▍ | 2516/3000 [00:01<00:00, 1653.88it/s]warmup should be done:  89%|████████▉ | 2672/3000 [00:01<00:00, 1673.76it/s]warmup should be done:  90%|█████████ | 2713/3000 [00:01<00:00, 1700.30it/s]warmup should be done:  90%|█████████ | 2707/3000 [00:01<00:00, 1689.53it/s]warmup should be done:  88%|████████▊ | 2652/3000 [00:01<00:00, 1680.06it/s]warmup should be done:  90%|████████▉ | 2687/3000 [00:01<00:00, 1656.99it/s]warmup should be done:  88%|████████▊ | 2641/3000 [00:01<00:00, 1638.67it/s]warmup should be done:  90%|████████▉ | 2691/3000 [00:01<00:00, 1678.55it/s]warmup should be done:  89%|████████▉ | 2683/3000 [00:01<00:00, 1656.05it/s]warmup should be done:  95%|█████████▍| 2840/3000 [00:01<00:00, 1672.56it/s]warmup should be done:  96%|█████████▌| 2884/3000 [00:01<00:00, 1698.77it/s]warmup should be done:  96%|█████████▌| 2878/3000 [00:01<00:00, 1695.46it/s]warmup should be done:  94%|█████████▍| 2821/3000 [00:01<00:00, 1682.46it/s]warmup should be done:  94%|█████████▎| 2806/3000 [00:01<00:00, 1640.76it/s]warmup should be done:  95%|█████████▌| 2859/3000 [00:01<00:00, 1676.52it/s]warmup should be done:  95%|█████████▌| 2853/3000 [00:01<00:00, 1646.86it/s]warmup should be done:  95%|█████████▍| 2849/3000 [00:01<00:00, 1646.51it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1689.59it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1685.56it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1671.47it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1665.94it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1665.60it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1662.28it/s]warmup should be done: 100%|█████████▉| 2991/3000 [00:01<00:00, 1685.14it/s]warmup should be done:  99%|█████████▉| 2972/3000 [00:01<00:00, 1644.30it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1652.83it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1641.16it/s]2022-12-12 06:37:19.601765: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f42578332f0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 06:37:19.601830: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 06:37:20.607414: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f425b830290 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 06:37:20.607470: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 06:37:20.619886: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f230c02e830 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 06:37:20.619941: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 06:37:20.640981: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f23d8029a70 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 06:37:20.641045: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 06:37:21.195735: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f23dc02dcb0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 06:37:21.195807: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 06:37:21.221814: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f426382c8e0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 06:37:21.221883: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 06:37:21.225811: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f425b795c70 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 06:37:21.225875: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 06:37:21.257699: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f425f8341b0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 06:37:21.257794: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 06:37:21.806587: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 06:37:22.910685: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 06:37:22.918407: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 06:37:22.955257: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 06:37:23.468075: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 06:37:23.497565: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 06:37:23.590752: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 06:37:23.649176: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 06:37:24.736991: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 06:37:25.826387: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 06:37:25.878558: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 06:37:25.878558: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 06:37:26.318168: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 06:37:26.381822: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 06:37:26.534833: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 06:37:26.650855: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
[HCTR][06:37:50.371][ERROR][RK0][tid #139923373938432]: replica 4 reaches 1000, calling init pre replica
[HCTR][06:37:50.371][ERROR][RK0][tid #139923373938432]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][06:37:50.380][ERROR][RK0][tid #139923373938432]: coll ps creation done
[HCTR][06:37:50.380][ERROR][RK0][tid #139923373938432]: replica 4 waits for coll ps creation barrier
[HCTR][06:37:50.388][ERROR][RK0][tid #139923306829568]: replica 3 reaches 1000, calling init pre replica
[HCTR][06:37:50.388][ERROR][RK0][tid #139923306829568]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][06:37:50.393][ERROR][RK0][tid #139923306829568]: coll ps creation done
[HCTR][06:37:50.393][ERROR][RK0][tid #139923306829568]: replica 3 waits for coll ps creation barrier
[HCTR][06:37:50.395][ERROR][RK0][main]: replica 6 reaches 1000, calling init pre replica
[HCTR][06:37:50.395][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][06:37:50.396][ERROR][RK0][tid #139923373938432]: replica 1 reaches 1000, calling init pre replica
[HCTR][06:37:50.396][ERROR][RK0][tid #139923373938432]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][06:37:50.396][ERROR][RK0][main]: replica 0 reaches 1000, calling init pre replica
[HCTR][06:37:50.397][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][06:37:50.401][ERROR][RK0][tid #139923373938432]: coll ps creation done
[HCTR][06:37:50.401][ERROR][RK0][tid #139923373938432]: replica 1 waits for coll ps creation barrier
[HCTR][06:37:50.403][ERROR][RK0][main]: coll ps creation done
[HCTR][06:37:50.403][ERROR][RK0][main]: replica 6 waits for coll ps creation barrier
[HCTR][06:37:50.404][ERROR][RK0][main]: coll ps creation done
[HCTR][06:37:50.404][ERROR][RK0][main]: replica 0 waits for coll ps creation barrier
[HCTR][06:37:50.433][ERROR][RK0][main]: replica 7 reaches 1000, calling init pre replica
[HCTR][06:37:50.434][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][06:37:50.438][ERROR][RK0][main]: coll ps creation done
[HCTR][06:37:50.438][ERROR][RK0][main]: replica 7 waits for coll ps creation barrier
[HCTR][06:37:50.450][ERROR][RK0][tid #139923701090048]: replica 5 reaches 1000, calling init pre replica
[HCTR][06:37:50.450][ERROR][RK0][tid #139923701090048]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][06:37:50.457][ERROR][RK0][tid #139923701090048]: coll ps creation done
[HCTR][06:37:50.457][ERROR][RK0][tid #139923701090048]: replica 5 waits for coll ps creation barrier
[HCTR][06:37:50.484][ERROR][RK0][tid #139923373938432]: replica 2 reaches 1000, calling init pre replica
[HCTR][06:37:50.484][ERROR][RK0][tid #139923373938432]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][06:37:50.488][ERROR][RK0][tid #139923373938432]: coll ps creation done
[HCTR][06:37:50.488][ERROR][RK0][tid #139923373938432]: replica 2 waits for coll ps creation barrier
[HCTR][06:37:50.488][ERROR][RK0][main]: replica 0 preparing frequency
[HCTR][06:37:51.306][ERROR][RK0][main]: replica 0 preparing frequency done
[HCTR][06:37:51.352][ERROR][RK0][main]: replica 0 calling init per replica
[HCTR][06:37:51.352][ERROR][RK0][tid #139923701090048]: replica 5 calling init per replica
[HCTR][06:37:51.352][ERROR][RK0][tid #139923373938432]: replica 4 calling init per replica
[HCTR][06:37:51.352][ERROR][RK0][tid #139923373938432]: replica 1 calling init per replica
[HCTR][06:37:51.352][ERROR][RK0][tid #139923373938432]: replica 2 calling init per replica
[HCTR][06:37:51.352][ERROR][RK0][main]: replica 7 calling init per replica
[HCTR][06:37:51.352][ERROR][RK0][main]: replica 6 calling init per replica
[HCTR][06:37:51.352][ERROR][RK0][tid #139923306829568]: replica 3 calling init per replica
[HCTR][06:37:51.352][ERROR][RK0][main]: Calling build_v2
[HCTR][06:37:51.352][ERROR][RK0][tid #139923701090048]: Calling build_v2
[HCTR][06:37:51.352][ERROR][RK0][tid #139923373938432]: Calling build_v2
[HCTR][06:37:51.352][ERROR][RK0][tid #139923373938432]: Calling build_v2
[HCTR][06:37:51.352][ERROR][RK0][tid #139923373938432]: Calling build_v2
[HCTR][06:37:51.352][ERROR][RK0][main]: Calling build_v2
[HCTR][06:37:51.352][ERROR][RK0][main]: Calling build_v2
[HCTR][06:37:51.352][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][06:37:51.352][ERROR][RK0][tid #139923306829568]: Calling build_v2
[HCTR][06:37:51.352][ERROR][RK0][tid #139923701090048]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][06:37:51.352][ERROR][RK0][tid #139923373938432]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][06:37:51.352][ERROR][RK0][tid #139923373938432]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][06:37:51.352][ERROR][RK0][tid #139923373938432]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][06:37:51.352][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][06:37:51.352][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][06:37:51.352][ERROR][RK0][tid #139923306829568]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[[[[[[[[2022-12-12 06:37:512022-12-12 06:37:512022-12-12 06:37:512022-12-12 06:37:512022-12-12 06:37:512022-12-12 06:37:51.2022-12-12 06:37:512022-12-12 06:37:51.....352240..352240352245352245352245352245: 352245352245: : : : : E: : EEEEE EE     /hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc  /hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc:/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc:::::136::136136136136136] 136136] ] ] ] ] using concurrent impl MPSPhase] ] using concurrent impl MPSPhaseusing concurrent impl MPSPhaseusing concurrent impl MPSPhaseusing concurrent impl MPSPhaseusing concurrent impl MPSPhase
using concurrent impl MPSPhaseusing concurrent impl MPSPhase






[2022-12-12 06:37:51.356582: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:178] v100x8, slow pcie
[2022-12-12 06:37:51.356621: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:[1962022-12-12 06:37:51] .assigning 8 to cpu356630
: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:178] v100x8, slow pcie
[[2022-12-12 06:37:512022-12-12 06:37:51.[.3566742022-12-12 06:37:51356675: .: E356675E :  /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: :212/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc196] :] build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8178assigning 8 to cpu
] 
v100x8, slow pcie
[[2022-12-12 06:37:51[[2022-12-12 06:37:51.2022-12-12 06:37:512022-12-12 06:37:51.356756.[.356738: 3567602022-12-12 06:37:51356768: E: .: E E356775E[ /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc :  2022-12-12 06:37:51/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.:[196: :3568211782022-12-12 06:37:51] 213/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc212: ] [.assigning 8 to cpu] :] Ev100x8, slow pcie2022-12-12 06:37:51356866
remote time is 8.68421178build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8 
.: 
[] 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc356911E[2022-12-12 06:37:51[v100x8, slow pcie::  2022-12-12 06:37:51.[2022-12-12 06:37:51
178E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.3570102022-12-12 06:37:51.]  [:357009: .357019v100x8, slow pcie/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 06:37:51178: E357036: 
:.] E : E178357070[v100x8, slow pcie /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE ] : 2022-12-12 06:37:51
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccv100x8, slow pcieE.:212[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:
 357129196] 2022-12-12 06:37:51:214/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: [] build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8.213] :E2022-12-12 06:37:51assigning 8 to cpu
357179] cpu time is 97.0588196[ .
: remote time is 8.68421
] 2022-12-12 06:37:51/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc357217E
assigning 8 to cpu.::  
[[357282196E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 06:37:512022-12-12 06:37:51: ]  :..Eassigning 8 to cpu/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc196357334[357336 
:] : 2022-12-12 06:37:51: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc196assigning 8 to cpuE.E:] 
 [357384 213assigning 8 to cpu/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 06:37:51: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] 
:.E :remote time is 8.68421[212357458/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc214
2022-12-12 06:37:51] : [:] .build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8[E2022-12-12 06:37:51212cpu time is 97.0588357512
2022-12-12 06:37:51 .] 
: ./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc357563build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8[E357562:: 
2022-12-12 06:37:51 : 212E./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE] [ 357615: build asymm link desc with 8X Tesla V100-SXM2-32GB out of 82022-12-12 06:37:51/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: 212/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
.:E] :357673212 build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8[214: ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
2022-12-12 06:37:51] Ebuild asymm link desc with 8X Tesla V100-SXM2-32GB out of 8:.cpu time is 97.0588 [
213357748
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 06:37:51] : [:.remote time is 8.68421E2022-12-12 06:37:51213357797
 .] : /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc357831remote time is 8.68421[E:: 
2022-12-12 06:37:51 213E./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[]  357877:2022-12-12 06:37:51remote time is 8.68421/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: 213.
:E] 357909213[ remote time is 8.68421: ] 2022-12-12 06:37:51/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
Eremote time is 8.68421.: 
357960[214/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: 2022-12-12 06:37:51[] :E.2022-12-12 06:37:51cpu time is 97.0588214 358000.
] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: 358015cpu time is 97.0588:E: 
214 E] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc cpu time is 97.0588:/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
214:] 214cpu time is 97.0588] 
cpu time is 97.0588
[2022-12-12 06:39:08.332190: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:118] solver created. now build & solve
[2022-12-12 06:39:08.372244: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 381.47 MB
[2022-12-12 06:39:08.372330: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 381.47 MB
[2022-12-12 06:39:08.373307: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:74] mapping nid to rank...
[2022-12-12 06:39:08.448447: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:91] counting slots...
[2022-12-12 06:39:08.837515: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:105] Final num slot is 49
[2022-12-12 06:39:08.837613: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:109] counting blocks...
[2022-12-12 06:39:15.795424: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:118] Final num block is 1024
[2022-12-12 06:39:15.795521: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:123] counting freq and density...
[2022-12-12 06:39:17.477751: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:154] averaging freq and density...
[2022-12-12 06:39:17.477860: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:155] 1024
[2022-12-12 06:39:17.480668: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:123] solver built. now solve
[2022-12-12 06:39:17.480725: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:354] constructing optimal solver, device=8, stream=1
1024 blocks, 8 devices
[2022-12-12 06:39:17.755355: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:527] Add Var...
[2022-12-12 06:39:17.783771: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:545] Capacity...
[2022-12-12 06:39:17.785149: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:548] Connect CPU...
[2022-12-12 06:39:17.805349: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:550] Connect Access To Storage...
[2022-12-12 06:39:18.324065: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:554] Time...
[2022-12-12 06:39:18.326305: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:516] dst 0, total sm is 80
[2022-12-12 06:39:18.329345: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:516] dst 1, total sm is 80
[2022-12-12 06:39:18.332276: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:516] dst 2, total sm is 80
[2022-12-12 06:39:18.335180: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:516] dst 3, total sm is 80
[2022-12-12 06:39:18.338054: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:516] dst 4, total sm is 80
[2022-12-12 06:39:18.340923: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:516] dst 5, total sm is 80
[2022-12-12 06:39:18.343810: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:516] dst 6, total sm is 80
[2022-12-12 06:39:18.346666: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:516] dst 7, total sm is 80
[2022-12-12 06:40:04.951928: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:569] Coll Cache init block placement array
[2022-12-12 06:40:04.961390: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:645] Coll Cache init block placement array done
[2022-12-12 06:40:04.962526: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:647] Coll Cache model reset done
[2022-12-12 06:40:05.  8616: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:127] solver solved
[2022-12-12 06:40:05.  8725: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:195] 0 solved master
[2022-12-12 06:40:05.  8760: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 0 solved
[2022-12-12 06:40:05.  8791: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 0 initing device 0
[2022-12-12 06:40:05.  9338: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-12 06:40:05.  9391: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 06:40:05. 10294: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 06:40:05. 10964: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 06:40:05. 24104: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 7 solved
[2022-12-12 06:40:05. 24191: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 7 initing device 7
[2022-12-12 06:40:05. 24381: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202[] 2022-12-12 06:40:054 solved.
 24416[: 2022-12-12 06:40:05[E.2022-12-12 06:40:05  24426./hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:  24454:E: 202 E] /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc 2 solved:/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc
202:] [2055 solved2022-12-12 06:40:05] 
.worker 0 thread 4 initing device 4 24568
: [E2022-12-12 06:40:05 ./hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc 24603:: 205E]  [worker 0 thread 2 initing device 22022-12-12 06:40:05/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc
.:[ 246322052022-12-12 06:40:05: ] .Eworker 0 thread 5 initing device 5 24674 
: /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.ccE: 202/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu] :18151 solved] 
Building Coll Cache with ... num gpu device is 8
[2022-12-12 06:40:05. 24762: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 1 initing device 1[
2022-12-12 06:40:05. 24783: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 06:40:05. 25014: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-12 06:40:05. 25058: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 06:40:05. 25077: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-12 06:40:05. 25121: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 06:40:05. 25154: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-12 06:40:05. 25193: [E2022-12-12 06:40:05 ./hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu 25204:: 1815E]  Building Coll Cache with ... num gpu device is 8/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu
:1980] eager alloc mem 381.47 MB
[2022-12-12 06:40:05. 25273: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 06:40:05. 25859: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 3 solved
[2022-12-12 06:40:05. 25922: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 3 initing device 3
[2022-12-12 06:40:05. 25992: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 6 solved
[2022-12-12 06:40:05. 26047: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 6 initing device 6
[2022-12-12 06:40:05. 26350: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-12 06:40:05. 26395: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 06:40:05. 26475: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-12 06:40:05. 26518: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 06:40:05. 29256: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 06:40:05. 29311: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 06:40:05. 29365: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 06:40:05. 29600: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 06:40:05. 29667: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 06:40:05. 30609: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 06:40:05. 31113: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 06:40:05. 33619: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 06:40:05. 33682: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 06:40:05. 33799: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 06:40:05. 33921: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 06:40:05. 33974: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 06:40:05. 34467: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 06:40:05. 34980: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 06:40:05. 92442: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1024.00 Bytes
[2022-12-12 06:40:05. 97862: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-12 06:40:05. 98005: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 06:40:05. 98872: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 95.75 MB
[2022-12-12 06:40:05. 99584: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:40:05.100730: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:40:05.100780: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 95.24 MB
[2022-12-12 06:40:05.113020: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1024.00 Bytes
[2022-12-12 06:40:05.113145: E [[/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu2022-12-12 06:40:052022-12-12 06:40:05:..1980113187113188] : : eager alloc mem 1024.00 BytesEE
  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::19801980] ] eager alloc mem 1024.00 Byteseager alloc mem 1024.00 Bytes

[2022-12-12 06:40:05.117431[: 2022-12-12 06:40:05E. 117456/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu: :E1980 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cueager alloc mem 1024.00 Bytes:
1980] eager alloc mem 1024.00 Bytes
[2022-12-12 06:40:05.119002: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-12 06:40:05.119089: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000[
2022-12-12 06:40:05.119091: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-12 06:40:05.119176: [E2022-12-12 06:40:05 ./hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc119202:: 638E]  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cceager release cuda mem 1024:
638] eager release cuda mem 400000000
[[2022-12-12 06:40:052022-12-12 06:40:05..119274119260: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::638638] ] eager release cuda mem 400000000eager release cuda mem 1024

[2022-12-12 06:40:05.119367: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 06:40:05.119408: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1024.00 Bytes
[2022-12-12 06:40:05.126816: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 95.75 MB
[2022-12-12 06:40:05.127359: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 95.75 MB
[2022-12-12 06:40:05.127879: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 95.75 MB
[2022-12-12 06:40:05.128388: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 95.75 MB
[2022-12-12 06:40:05.129979: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:40:05.130019: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:40:05.130066: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:40:05.130107: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:40:05.130131: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-12 06:40:05.130203: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 06:40:05.130236: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-12 06:40:05.130308: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000[
2022-12-12 06:40:05.130318: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-12 06:40:05.130390: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 06:40:05.131001: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 95.75 MB
[2022-12-12 06:40:05.131070: E [/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc2022-12-12 06:40:05:.638131085] : eager release cuda mem 625663E
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc[:2022-12-12 06:40:05638.] 131146eager release cuda mem 625663: 
[E2022-12-12 06:40:05 ./hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc131166[:[: 2022-12-12 06:40:056382022-12-12 06:40:05W.] . 131189eager release cuda mem 625663131192/hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc: 
: :EW43  ] [/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.ccWORKER[0] alloc host memory 95.28 MB2022-12-12 06:40:05::
.63843131267] ] : eager release cuda mem 625663WORKER[0] alloc host memory 95.17 MBW

 /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 95.08 MB
[2022-12-12 06:40:05.131341: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 95.35 MB
[2022-12-12 06:40:05.131907: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 95.75 MB
[2022-12-12 06:40:05.132492: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 95.75 MB
[2022-12-12 06:40:05.132960: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:40:05.133385: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:40:05.133433: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:40:05.134038: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:40:05.134083: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 95.20 MB
[2022-12-12 06:40:05.134471[: 2022-12-12 06:40:05E. 134482/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc: :E638 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cceager release cuda mem 625663:
638] eager release cuda mem 625663
[2022-12-12 06:40:05.134574[: 2022-12-12 06:40:05W. 134581/hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc: :W43 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.ccWORKER[0] alloc host memory 95.20 MB:
43] WORKER[0] alloc host memory 95.25 MB
[2022-12-12 06:40:05.165430: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 06:40:05.166065: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 06:40:05.166110: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 11.91 GB
[2022-12-12 06:40:05.193519: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 06:40:05.194017: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 06:40:05.194133: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 06:40:05.194176: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 11.90 GB
[2022-12-12 06:40:05.194366: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 06:40:05.194622: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 06:40:05.194671: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 11.89 GB
[2022-12-12 06:40:05.194974: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 06:40:05.195016: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 11.92 GB
[2022-12-12 06:40:05.195541: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 06:40:05.196154: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 06:40:05.196195: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 11.91 GB
[2022-12-12 06:40:05.196910: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 06:40:05.196991: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 06:40:05.197463: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 06:40:05.197506: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 06:40:05.197549: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 11.90 GB
[2022-12-12 06:40:05.197596: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 06:40:05.197637: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 11.90 GB
[2022-12-12 06:40:05.198068: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 06:40:05.198111: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 11.91 GB
[[[[[[[[2022-12-12 06:40:092022-12-12 06:40:092022-12-12 06:40:092022-12-12 06:40:092022-12-12 06:40:092022-12-12 06:40:092022-12-12 06:40:092022-12-12 06:40:09........  3105  3105  3105  3105  3105  3105  3105  3105: : : : : : : : EEEEEEEE        /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::::::::19261926192619261926192619261926] ] ] ] ] ] ] ] Device 0 init p2p of link 3Device 1 init p2p of link 7Device 5 init p2p of link 6Device 4 init p2p of link 5Device 2 init p2p of link 1Device 7 init p2p of link 4Device 6 init p2p of link 0Device 3 init p2p of link 2







[[[[[[[2022-12-12 06:40:092022-12-12 06:40:092022-12-12 06:40:092022-12-12 06:40:092022-12-12 06:40:092022-12-12 06:40:092022-12-12 06:40:09[.......2022-12-12 06:40:09  3694  3694  3694  3703  3694  3694  3695.: : : : : : :   3717EEEEEEE:        E/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu :::::::/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu1980198019801980198019801980:] ] ] ] ] ] ] 1980eager alloc mem 611.00 KBeager alloc mem 611.00 KBeager alloc mem 611.00 KBeager alloc mem 611.00 KBeager alloc mem 611.00 KBeager alloc mem 611.00 KBeager alloc mem 611.00 KB] 






eager alloc mem 611.00 KB
[[[2022-12-12 06:40:092022-12-12 06:40:092022-12-12 06:40:09.[..  48202022-12-12 06:40:09  4821  4823: .[: : E  48312022-12-12 06:40:09E[[E[ : . 2022-12-12 06:40:092022-12-12 06:40:09 2022-12-12 06:40:09/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccE  4856/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc../hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc.: : :  4873  4873:  4878638/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccE638: : 638: ] : ] EE] Eeager release cuda mem 625663638/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cceager release cuda mem 625663  eager release cuda mem 625663 
] :
/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc
/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cceager release cuda mem 625663638:::
] 638638638eager release cuda mem 625663] ] ] 
eager release cuda mem 625663eager release cuda mem 625663eager release cuda mem 625663


[2022-12-12 06:40:09. 23118: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 6 init p2p of link 5
[2022-12-12 06:40:09. 23286: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:40:09. 23937: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 2
[2022-12-12 06:40:09. 24102: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:40:09. 24231: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:40:09. 24820: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 4 init p2p of link 7
[2022-12-12 06:40:09. 24978: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:40:09. 25040: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638[] 2022-12-12 06:40:09eager release cuda mem 625663.
 25046: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 5 init p2p of link 4
[2022-12-12 06:40:09. 25214: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:40:09. 25754: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 7 init p2p of link 1
[2022-12-12 06:40:09. 25910: E[ 2022-12-12 06:40:09/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc.: 25922638: ] Eeager release cuda mem 625663 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:40:09. 26005: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 0 init p2p of link 6
[2022-12-12 06:40:09. 26132: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:40:09. 26160: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:40:09. 26347: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 2 init p2p of link 3
[2022-12-12 06:40:09. 26522: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:40:09. 26566: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 3 init p2p of link 0
[2022-12-12 06:40:09. 26727: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:40:09. 26823: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:40:09. 27022: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:40:09. 27443: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:40:09. 27648: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:40:09. 41505: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 6 init p2p of link 4
[2022-12-12 06:40:09. 41628: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:40:09. 42569: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:40:09. 42628: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 0 init p2p of link 1
[2022-12-12 06:40:09. 42751: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:40:09. 43234: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 3
[2022-12-12 06:40:09. 43358: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:40:09. 43652: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 7 init p2p of link 6
[2022-12-12 06:40:09. 43696: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:40:09[.2022-12-12 06:40:09 43773.:  43767E:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu :/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu1980:] 1926eager alloc mem 611.00 KB] 
Device 4 init p2p of link 2
[2022-12-12 06:40:09. 43906: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:40:09. 44272: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:40:09. 44691: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:40:09. 44821[: 2022-12-12 06:40:09E.  44819/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc: :E638 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cueager release cuda mem 625663:
1926] Device 5 init p2p of link 7
[2022-12-12 06:40:09. 44985: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980[] 2022-12-12 06:40:09eager alloc mem 611.00 KB.
 44992: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 3 init p2p of link 5
[2022-12-12 06:40:09. 45144: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:40:09. 45181: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 2 init p2p of link 0
[2022-12-12 06:40:09. 45301: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:40:09. 45898: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:40:09. 45984: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:40:09. 46211: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:40:09. 65627: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 6 init p2p of link 7
[2022-12-12 06:40:09. 65745: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:40:09. 66443: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 3 init p2p of link 1
[2022-12-12 06:40:09. 66568: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:40:09. 66650: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:40:09. 67507: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:40:09. 70099: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 7 init p2p of link 5
[2022-12-12 06:40:09. 70213: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:40:09. 70478: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 0
[2022-12-12 06:40:09. 70530: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 4 init p2p of link 6
[2022-12-12 06:40:09. 70599: [E2022-12-12 06:40:09 ./hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu 70597:: 1980E]  eager alloc mem 611.00 KB/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu
:1926[] 2022-12-12 06:40:09Device 0 init p2p of link 2.
 70644: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:40:09. 70737: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:40:09. 71157: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:40:09. 71255: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 5 init p2p of link 3
[2022-12-12 06:40:09. 71369: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:40:09. 71523: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638[] 2022-12-12 06:40:09eager release cuda mem 625663.
 71543: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:40:09. 71665: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:40:09. 72242: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:40:09. 72387: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 2 init p2p of link 4
[2022-12-12 06:40:09. 72511: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:40:09. 73341: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:40:09. 81784: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 100400000
[2022-12-12 06:40:09. 84247: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 24995898 / 100000000 nodes ( 25.00 %~25.00 %) | remote 70772198 / 100000000 nodes ( 70.77 %) | cpu 4231904 / 100000000 nodes ( 4.23 %) | 11.92 GB | 4.05773 secs 
[2022-12-12 06:40:09. 84447: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 100400000
[2022-12-12 06:40:09. 84885: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 24955649 / 100000000 nodes ( 24.96 %~25.00 %) | remote 70812447 / 100000000 nodes ( 70.81 %) | cpu 4231904 / 100000000 nodes ( 4.23 %) | 11.90 GB | 4.0585 secs 
[2022-12-12 06:40:09. 90163: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 100400000
[2022-12-12 06:40:09. 90830: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 24956307 / 100000000 nodes ( 24.96 %~25.00 %) | remote 70811789 / 100000000 nodes ( 70.81 %) | cpu 4231904 / 100000000 nodes ( 4.23 %) | 11.90 GB | 4.06572 secs 
[2022-12-12 06:40:09. 93758: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 100400000
[2022-12-12 06:40:09. 94297: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 100400000
[2022-12-12 06:40:09. 94877: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 100400000
[2022-12-12 06:40:09. 94958: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 100400000
[2022-12-12 06:40:09. 96670: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 100400000
[2022-12-12 06:40:09.116594: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 24975952 / 100000000 nodes ( 24.98 %~25.00 %) | remote 70792144 / 100000000 nodes ( 70.79 %) | cpu 4231904 / 100000000 nodes ( 4.23 %) | 11.91 GB | 4.0914 secs 
[2022-12-12 06:40:09.116883: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 24969340 / 100000000 nodes ( 24.97 %~25.00 %) | remote 70798756 / 100000000 nodes ( 70.80 %) | cpu 4231904 / 100000000 nodes ( 4.23 %) | 11.91 GB | 4.09162 secs 
[2022-12-12 06:40:09.117070: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 24947574 / 100000000 nodes ( 24.95 %~25.00 %) | remote 70820522 / 100000000 nodes ( 70.82 %) | cpu 4231904 / 100000000 nodes ( 4.23 %) | 11.90 GB | 4.09202 secs 
[2022-12-12 06:40:09.117259: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 24925468 / 100000000 nodes ( 24.93 %~25.00 %) | remote 70842628 / 100000000 nodes ( 70.84 %) | cpu 4231904 / 100000000 nodes ( 4.23 %) | 11.89 GB | 4.09249 secs 
[2022-12-12 06:40:09.117491: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 24965435 / 100000000 nodes ( 24.97 %~25.00 %) | remote 70802661 / 100000000 nodes ( 70.80 %) | cpu 4231904 / 100000000 nodes ( 4.23 %) | 11.91 GB | 4.10811 secs 
[2022-12-12 06:40:09.140602: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2213] before create ctx, mem is 18.55 GB
[2022-12-12 06:40:10.555301: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2219] after create ctx, mem is 18.81 GB
[2022-12-12 06:40:10.556195: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2226] after create stream, mem is 18.81 GB
[2022-12-12 06:40:10.557530: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2213] before create ctx, mem is 18.81 GB
[2022-12-12 06:40:11.883102: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2219] after create ctx, mem is 19.08 GB
[2022-12-12 06:40:11.883490: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2226] after create stream, mem is 19.08 GB
[2022-12-12 06:40:11.884100: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2213] before create ctx, mem is 19.08 GB
[2022-12-12 06:40:13. 66725: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2219] after create ctx, mem is 19.29 GB
[2022-12-12 06:40:13. 67092: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2226] after create stream, mem is 19.29 GB
[2022-12-12 06:40:13. 67553: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2213] before create ctx, mem is 19.29 GB
[2022-12-12 06:40:14.579561: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2219] after create ctx, mem is 19.50 GB
[2022-12-12 06:40:14.579790: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2226] after create stream, mem is 19.50 GB
[2022-12-12 06:40:14.581040: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2243] before create stream, mem is 19.50 GB
[2022-12-12 06:40:14.581433: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2249] after create stream, mem is 19.50 GB
[2022-12-12 06:40:14.581838: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2213] before create ctx, mem is 19.50 GB
[2022-12-12 06:40:16. 22770: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2219] after create ctx, mem is 19.70 GB
[2022-12-12 06:40:16. 22925: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2226] after create stream, mem is 19.70 GB
[HCTR][06:40:16.525][ERROR][RK0][main]: replica 0 calling init per replica done, doing barrier
[HCTR][06:40:16.525][ERROR][RK0][main]: replica 7 calling init per replica done, doing barrier
[HCTR][06:40:16.525][ERROR][RK0][main]: replica 6 calling init per replica done, doing barrier
[HCTR][06:40:16.525][ERROR][RK0][tid #139923373938432]: replica 1 calling init per replica done, doing barrier
[HCTR][06:40:16.525][ERROR][RK0][tid #139923373938432]: replica 2 calling init per replica done, doing barrier
[HCTR][06:40:16.525][ERROR][RK0][tid #139923701090048]: replica 5 calling init per replica done, doing barrier
[HCTR][06:40:16.525][ERROR][RK0][tid #139923306829568]: replica 3 calling init per replica done, doing barrier
[HCTR][06:40:16.525][ERROR][RK0][tid #139923373938432]: replica 4 calling init per replica done, doing barrier
[HCTR][06:40:16.525][ERROR][RK0][tid #139923373938432]: replica 2 calling init per replica done, doing barrier done
[HCTR][06:40:16.525][ERROR][RK0][main]: replica 7 calling init per replica done, doing barrier done
[HCTR][06:40:16.525][ERROR][RK0][tid #139923306829568]: replica 3 calling init per replica done, doing barrier done
[HCTR][06:40:16.525][ERROR][RK0][main]: replica 6 calling init per replica done, doing barrier done
[HCTR][06:40:16.525][ERROR][RK0][main]: replica 0 calling init per replica done, doing barrier done
[HCTR][06:40:16.525][ERROR][RK0][tid #139923373938432]: replica 1 calling init per replica done, doing barrier done
[HCTR][06:40:16.525][ERROR][RK0][tid #139923701090048]: replica 5 calling init per replica done, doing barrier done
[HCTR][06:40:16.525][ERROR][RK0][tid #139923373938432]: replica 4 calling init per replica done, doing barrier done
[HCTR][06:40:16.525][ERROR][RK0][tid #139923373938432]: init per replica done
[HCTR][06:40:16.525][ERROR][RK0][main]: init per replica done
[HCTR][06:40:16.525][ERROR][RK0][tid #139923306829568]: init per replica done
[HCTR][06:40:16.525][ERROR][RK0][main]: init per replica done
[HCTR][06:40:16.525][ERROR][RK0][tid #139923373938432]: init per replica done
[HCTR][06:40:16.525][ERROR][RK0][tid #139923701090048]: init per replica done
[HCTR][06:40:16.525][ERROR][RK0][tid #139923373938432]: init per replica done
[HCTR][06:40:16.528][ERROR][RK0][main]: init per replica done
[HCTR][06:40:16.531][ERROR][RK0][main]: 6 allocated 3276800 at 0x7f4452520000
[HCTR][06:40:16.531][ERROR][RK0][main]: 6 allocated 6553600 at 0x7f4452a00000
[HCTR][06:40:16.531][ERROR][RK0][main]: 6 allocated 3276800 at 0x7f4453040000
[HCTR][06:40:16.531][ERROR][RK0][main]: 6 allocated 6553600 at 0x7f4453360000
[HCTR][06:40:16.532][ERROR][RK0][tid #139923701090048]: 5 allocated 3276800 at 0x7f4450520000
[HCTR][06:40:16.532][ERROR][RK0][tid #139923701090048]: 5 allocated 6553600 at 0x7f4450a00000
[HCTR][06:40:16.532][ERROR][RK0][tid #139923701090048]: 5 allocated 3276800 at 0x7f4451040000
[HCTR][06:40:16.532][ERROR][RK0][tid #139923701090048]: 5 allocated 6553600 at 0x7f4451360000
[HCTR][06:40:16.532][ERROR][RK0][tid #139923843700480]: 7 allocated 3276800 at 0x7f4450520000
[HCTR][06:40:16.532][ERROR][RK0][tid #139923843700480]: 7 allocated 6553600 at 0x7f4450a00000
[HCTR][06:40:16.532][ERROR][RK0][main]: 2 allocated 3276800 at 0x7f4450520000
[HCTR][06:40:16.532][ERROR][RK0][tid #139923843700480]: 7 allocated 3276800 at 0x7f4451040000
[HCTR][06:40:16.532][ERROR][RK0][main]: 3 allocated 3276800 at 0x7f4450520000
[HCTR][06:40:16.532][ERROR][RK0][tid #139923843700480]: 7 allocated 6553600 at 0x7f4451360000
[HCTR][06:40:16.532][ERROR][RK0][main]: 2 allocated 6553600 at 0x7f4450a00000
[HCTR][06:40:16.532][ERROR][RK0][main]: 3 allocated 6553600 at 0x7f4450a00000
[HCTR][06:40:16.532][ERROR][RK0][main]: 2 allocated 3276800 at 0x7f4451040000
[HCTR][06:40:16.532][ERROR][RK0][main]: 2 allocated 6553600 at 0x7f4451360000
[HCTR][06:40:16.532][ERROR][RK0][main]: 3 allocated 3276800 at 0x7f4451040000
[HCTR][06:40:16.532][ERROR][RK0][main]: 3 allocated 6553600 at 0x7f4451360000
[HCTR][06:40:16.532][ERROR][RK0][main]: 4 allocated 3276800 at 0x7f4450520000
[HCTR][06:40:16.532][ERROR][RK0][main]: 4 allocated 6553600 at 0x7f4450a00000
[HCTR][06:40:16.532][ERROR][RK0][main]: 4 allocated 3276800 at 0x7f4451040000
[HCTR][06:40:16.532][ERROR][RK0][main]: 1 allocated 3276800 at 0x7f4450520000
[HCTR][06:40:16.532][ERROR][RK0][main]: 4 allocated 6553600 at 0x7f4451360000
[HCTR][06:40:16.532][ERROR][RK0][main]: 1 allocated 6553600 at 0x7f4450a00000
[HCTR][06:40:16.532][ERROR][RK0][main]: 1 allocated 3276800 at 0x7f4451040000
[HCTR][06:40:16.532][ERROR][RK0][main]: 1 allocated 6553600 at 0x7f4451360000
[HCTR][06:40:16.535][ERROR][RK0][main]: 0 allocated 3276800 at 0x7f2a43120000
[HCTR][06:40:16.535][ERROR][RK0][main]: 0 allocated 6553600 at 0x7f2a43600000
[HCTR][06:40:16.535][ERROR][RK0][main]: 0 allocated 3276800 at 0x7f265a50e800
[HCTR][06:40:16.535][ERROR][RK0][main]: 0 allocated 6553600 at 0x7f265a82e800








