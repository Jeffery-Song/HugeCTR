2022-12-12 00:46:11.595526: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.604835: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.613441: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.617378: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.622092: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.628130: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.639174: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.653454: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.705245: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.712046: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.714923: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.716453: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.717044: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.717585: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.718704: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.719120: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.720425: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.720578: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.721994: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.722027: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.723579: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.723637: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.725079: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.725170: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.726512: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.726735: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.727869: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.728345: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.729180: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.730247: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.731139: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.732154: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.733948: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.735083: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.736021: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.736945: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.737866: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.738881: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.739915: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.741188: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.743691: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.744943: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.746039: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.747432: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 00:46:11.747564: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.748918: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.749151: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.750365: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.750894: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.752021: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.752453: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.753744: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.754039: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.755704: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.756277: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.756960: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.757653: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.758501: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.758944: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.760089: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.769963: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.771928: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.773479: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.773537: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.774265: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.777495: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.777544: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.777634: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.778182: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.780577: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.780784: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.780823: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.781023: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.781437: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.799201: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.805062: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.816795: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.817492: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.817626: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.817854: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.818443: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.819575: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.819645: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.821138: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.822622: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.822925: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.823209: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.823691: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.824112: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.824335: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.826624: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.828245: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.829345: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.829620: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.830041: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.830123: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.830351: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.831687: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.833870: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.834185: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.834252: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.834523: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.834720: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.836036: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.838949: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.839120: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.839278: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.839320: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.839418: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.840751: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.843775: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.843866: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.844156: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.844165: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.845159: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.847426: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.847903: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.848041: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.848484: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.850332: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.850730: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.851567: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.852791: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.853522: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.853930: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.855678: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.856413: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.856752: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.858145: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.858995: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.859267: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.860701: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.861513: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.861616: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.863237: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.863888: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.864768: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.865303: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.871600: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.873016: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.873284: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.874204: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.875706: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.876793: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.877710: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.877802: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.878882: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.880242: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.880294: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.881290: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.883064: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.883082: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.883535: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.885427: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.885612: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.886135: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.888056: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.888355: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.888680: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.890839: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.891036: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.891728: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.892901: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.893476: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.894118: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.894938: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 00:46:11.895269: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.895567: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.896033: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.898299: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.899021: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.900284: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.900553: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 00:46:11.901048: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.902220: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.902748: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.903122: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 00:46:11.903774: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.904110: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.904740: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.906188: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.906399: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.907411: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.908049: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.908742: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.908929: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.909252: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.910444: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.911313: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.912077: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.912458: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.912657: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.914244: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.915242: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.916267: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.916401: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.916515: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.916794: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.918500: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.947233: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.948404: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.948514: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.948703: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.950709: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.951763: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.952855: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.953656: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.955363: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.956439: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.957725: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.958427: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.959977: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.960854: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.963322: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.963944: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.965430: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.967094: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.968204: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.968783: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.971536: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.972465: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.973529: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.974116: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.977464: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.977480: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.978605: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.979170: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.981601: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.981703: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.982705: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.984057: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.985676: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.985894: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.986996: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.989434: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.991773: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.992048: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.992673: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.994155: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.995407: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.995664: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.996009: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:11.998813: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:12.028903: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:12.030320: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:12.030694: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:12.041028: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:12.042515: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:12.042854: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:12.042929: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:12.046193: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:12.078111: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:12.078111: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:12.078144: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:12.080405: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:12.110352: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:12.110605: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:12.113220: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:12.114206: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 00:46:12.117058: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:12.118061: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:12.122076: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:12.122276: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:12.122865: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 00:46:12.124277: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:12.127193: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:12.127366: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:12.129021: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:12.131234: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:12.132577: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:12.133039: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:12.134757: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:12.137157: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:12.138488: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:12.140045: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:12.142470: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:12.144181: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:12.145619: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:12.151307: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:12.153920: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 00:46:12.155764: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:12.162820: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:12.164489: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 00:46:12.167264: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:12.171590: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:12.173310: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:12.177832: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:12.182396: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:13.033328: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:13.033934: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:13.034468: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:13.034928: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 00:46:13.034981: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-12 00:46:13.052432: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:13.053731: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:13.054693: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:13.055767: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:13.056890: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:13.058099: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-12 00:46:13.102767: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 00:46:13.102950: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 00:46:13.136426: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12340
2022-12-12 00:46:13.266387: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:13.267994: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:13.269243: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:13.270634: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 00:46:13.270692: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-12 00:46:13.289876: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:13.290912: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:13.292116: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:13.293994: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:13.295003: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:13.296353: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:1/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-12 00:46:13.320911: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:13.321125: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:13.323615: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:13.323721: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:13.325397: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:13.325431: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:13.326437: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 00:46:13.326476: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 00:46:13.326498: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-12 00:46:13.326530: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-12 00:46:13.344220: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:13.344812: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:13.345557: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:13.345670: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:13.346803: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:13.346947: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:13.347724: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:13.348045: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:13.348687: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:2/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-12 00:46:13.348996: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:13.349525: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:13.349994: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:3/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-12 00:46:13.372642: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 00:46:13.372816: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 00:46:13.374649: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12341
2022-12-12 00:46:13.391174: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:13.392367: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:13.393800: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:13.394896: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 00:46:13.394955: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-12 00:46:13.395990: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 00:46:13.396173: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 00:46:13.398891: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12343
2022-12-12 00:46:13.406783: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:13.408106: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:13.409441: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:13.410451: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 00:46:13.410509: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-12 00:46:13.412984: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:13.414188: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:13.415362: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:13.416649: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:13.418060: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:13.419183: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:7/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-12 00:46:13.422858: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:13.424204: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:13.425274: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:13.426279: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 00:46:13.426341: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-12 00:46:13.426934: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 00:46:13.427098: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 00:46:13.428699: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:13.428967: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12342
2022-12-12 00:46:13.434950: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:13.436455: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:13.437717: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:13.438905: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:13.441309: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:6/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-12 00:46:13.443601: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:13.444202: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:13.444739: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:13.445320: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:13.445837: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:13.446516: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:5/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-12 00:46:13.447696: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:13.448315: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:13.448831: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:13.465468: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 00:46:13.465649: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 00:46:13.465790: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 00:46:13.465872: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-12 00:46:13.467350: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12347
2022-12-12 00:46:13.484937: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:13.485617: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:13.486149: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:13.486330: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 00:46:13.486474: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 00:46:13.486726: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:13.487266: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:46:13.487739: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:4/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-12 00:46:13.488272: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12346
2022-12-12 00:46:13.490658: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 00:46:13.490830: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 00:46:13.492623: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12345
2022-12-12 00:46:13.531572: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 00:46:13.531772: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 00:46:13.533525: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12344
[HCTR][00:46:14.817][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][00:46:14.817][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][00:46:14.817][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][00:46:14.817][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][00:46:14.817][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][00:46:14.817][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][00:46:14.817][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][00:46:14.818][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 1it [00:01,  1.60s/it]warmup run: 1it [00:01,  1.53s/it]warmup run: 95it [00:01, 77.44it/s]warmup run: 100it [00:01, 84.96it/s]warmup run: 1it [00:01,  1.53s/it]warmup run: 1it [00:01,  1.55s/it]warmup run: 189it [00:01, 167.62it/s]warmup run: 200it [00:01, 184.21it/s]warmup run: 1it [00:01,  1.48s/it]warmup run: 1it [00:01,  1.57s/it]warmup run: 99it [00:01, 84.10it/s]warmup run: 89it [00:01, 74.89it/s]warmup run: 1it [00:01,  1.52s/it]warmup run: 285it [00:01, 270.20it/s]warmup run: 299it [00:01, 292.33it/s]warmup run: 100it [00:01, 87.43it/s]warmup run: 92it [00:01, 76.51it/s]warmup run: 199it [00:01, 183.31it/s]warmup run: 179it [00:01, 163.65it/s]warmup run: 1it [00:01,  1.47s/it]warmup run: 99it [00:01, 84.84it/s]warmup run: 383it [00:02, 380.80it/s]warmup run: 399it [00:01, 405.89it/s]warmup run: 200it [00:01, 188.88it/s]warmup run: 176it [00:01, 157.76it/s]warmup run: 298it [00:01, 291.46it/s]warmup run: 272it [00:01, 265.36it/s]warmup run: 94it [00:01, 83.00it/s]warmup run: 198it [00:01, 183.76it/s]warmup run: 480it [00:02, 487.97it/s]warmup run: 499it [00:02, 516.43it/s]warmup run: 301it [00:01, 301.25it/s]warmup run: 277it [00:01, 270.02it/s]warmup run: 398it [00:01, 404.72it/s]warmup run: 364it [00:01, 369.49it/s]warmup run: 188it [00:01, 179.21it/s]warmup run: 295it [00:01, 289.83it/s]warmup run: 579it [00:02, 590.87it/s]warmup run: 602it [00:02, 623.58it/s]warmup run: 402it [00:01, 417.24it/s]warmup run: 378it [00:01, 386.93it/s]warmup run: 500it [00:02, 518.87it/s]warmup run: 457it [00:02, 472.55it/s]warmup run: 283it [00:01, 285.57it/s]warmup run: 394it [00:01, 402.68it/s]warmup run: 677it [00:02, 678.82it/s]warmup run: 705it [00:02, 716.39it/s]warmup run: 503it [00:01, 529.17it/s]warmup run: 479it [00:02, 501.21it/s]warmup run: 599it [00:02, 617.05it/s]warmup run: 552it [00:02, 572.22it/s]warmup run: 377it [00:01, 393.35it/s]warmup run: 495it [00:02, 515.73it/s]warmup run: 773it [00:02, 746.00it/s]warmup run: 807it [00:02, 791.55it/s]warmup run: 606it [00:02, 634.88it/s]warmup run: 579it [00:02, 603.97it/s]warmup run: 699it [00:02, 703.84it/s]warmup run: 648it [00:02, 660.04it/s]warmup run: 472it [00:01, 498.66it/s]warmup run: 598it [00:02, 623.91it/s]warmup run: 869it [00:02, 800.50it/s]warmup run: 909it [00:02, 849.91it/s]warmup run: 708it [00:02, 723.88it/s]warmup run: 679it [00:02, 693.83it/s]warmup run: 796it [00:02, 769.64it/s]warmup run: 742it [00:02, 728.20it/s]warmup run: 568it [00:02, 596.12it/s]warmup run: 699it [00:02, 712.55it/s]warmup run: 965it [00:02, 842.47it/s]warmup run: 1011it [00:02, 895.51it/s]warmup run: 810it [00:02, 797.55it/s]warmup run: 781it [00:02, 773.66it/s]warmup run: 893it [00:02, 820.36it/s]warmup run: 841it [00:02, 795.56it/s]warmup run: 663it [00:02, 678.35it/s]warmup run: 797it [00:02, 778.64it/s]warmup run: 1062it [00:02, 876.38it/s]warmup run: 1114it [00:02, 933.06it/s]warmup run: 912it [00:02, 854.19it/s]warmup run: 883it [00:02, 836.60it/s]warmup run: 990it [00:02, 858.27it/s]warmup run: 940it [00:02, 847.14it/s]warmup run: 758it [00:02, 744.65it/s]warmup run: 895it [00:02, 830.55it/s]warmup run: 1160it [00:02, 905.35it/s]warmup run: 1217it [00:02, 959.90it/s]warmup run: 1014it [00:02, 898.10it/s]warmup run: 985it [00:02, 884.19it/s]warmup run: 1087it [00:02, 884.87it/s]warmup run: 1039it [00:02, 884.66it/s]warmup run: 853it [00:02, 798.07it/s]warmup run: 994it [00:02, 872.62it/s]warmup run: 1259it [00:02, 927.02it/s]warmup run: 1319it [00:02, 973.07it/s]warmup run: 1117it [00:02, 933.16it/s]warmup run: 1089it [00:02, 925.28it/s]warmup run: 1188it [00:02, 917.91it/s]warmup run: 1136it [00:02, 908.60it/s]warmup run: 949it [00:02, 839.88it/s]warmup run: 1094it [00:02, 907.83it/s]warmup run: 1358it [00:03, 943.90it/s]warmup run: 1421it [00:02, 986.05it/s]warmup run: 1219it [00:02, 957.18it/s]warmup run: 1192it [00:02, 954.55it/s]warmup run: 1288it [00:02, 939.68it/s]warmup run: 1235it [00:02, 930.20it/s]warmup run: 1045it [00:02, 872.34it/s]warmup run: 1195it [00:02, 935.12it/s]warmup run: 1457it [00:03, 956.38it/s]warmup run: 1523it [00:03, 993.69it/s]warmup run: 1321it [00:02, 972.82it/s]warmup run: 1294it [00:02, 971.91it/s]warmup run: 1389it [00:02, 958.16it/s]warmup run: 1335it [00:02, 948.15it/s]warmup run: 1144it [00:02, 903.64it/s]warmup run: 1295it [00:02, 951.47it/s]warmup run: 1556it [00:03, 963.98it/s]warmup run: 1626it [00:03, 1002.03it/s]warmup run: 1424it [00:02, 988.94it/s]warmup run: 1396it [00:02, 975.16it/s]warmup run: 1490it [00:03, 972.44it/s]warmup run: 1435it [00:03, 962.06it/s]warmup run: 1245it [00:02, 932.28it/s]warmup run: 1396it [00:02, 967.58it/s]warmup run: 1654it [00:03, 968.57it/s]warmup run: 1729it [00:03, 1008.49it/s]warmup run: 1526it [00:02, 995.50it/s]warmup run: 1592it [00:03, 984.66it/s]warmup run: 1497it [00:03, 967.01it/s]warmup run: 1535it [00:03, 973.09it/s]warmup run: 1343it [00:02, 943.36it/s]warmup run: 1496it [00:03, 975.36it/s]warmup run: 1755it [00:03, 978.22it/s]warmup run: 1832it [00:03, 1012.48it/s]warmup run: 1628it [00:03, 998.06it/s]warmup run: 1694it [00:03, 994.92it/s]warmup run: 1597it [00:03, 976.22it/s]warmup run: 1634it [00:03, 967.89it/s]warmup run: 1440it [00:02, 950.86it/s]warmup run: 1596it [00:03, 981.97it/s]warmup run: 1854it [00:03, 979.47it/s]warmup run: 1934it [00:03, 1013.90it/s]warmup run: 1731it [00:03, 1007.23it/s]warmup run: 1796it [00:03, 1001.55it/s]warmup run: 1697it [00:03, 975.80it/s]warmup run: 1732it [00:03, 958.09it/s]warmup run: 1541it [00:03, 965.80it/s]warmup run: 1696it [00:03, 986.59it/s]warmup run: 1953it [00:03, 982.01it/s]warmup run: 2040it [00:03, 1027.22it/s]warmup run: 1835it [00:03, 1015.92it/s]warmup run: 1898it [00:03, 1006.64it/s]warmup run: 1801it [00:03, 992.80it/s]warmup run: 1829it [00:03, 951.51it/s]warmup run: 1642it [00:03, 978.55it/s]warmup run: 1797it [00:03, 992.35it/s]warmup run: 2061it [00:03, 1009.00it/s]warmup run: 2159it [00:03, 1075.41it/s]warmup run: 1939it [00:03, 1020.32it/s]warmup run: 2000it [00:03, 1009.75it/s]warmup run: 1904it [00:03, 1003.24it/s]warmup run: 1925it [00:03, 952.05it/s]warmup run: 1742it [00:03, 983.15it/s]warmup run: 1897it [00:03, 993.67it/s]warmup run: 2177it [00:03, 1052.83it/s]warmup run: 2279it [00:03, 1111.83it/s]warmup run: 2049it [00:03, 1042.17it/s]warmup run: 2119it [00:03, 1061.40it/s]warmup run: 2007it [00:03, 1009.34it/s]warmup run: 2025it [00:03, 965.23it/s]warmup run: 1843it [00:03, 990.68it/s]warmup run: 1998it [00:03, 996.54it/s]warmup run: 2295it [00:03, 1088.76it/s]warmup run: 2398it [00:03, 1134.52it/s]warmup run: 2170it [00:03, 1089.51it/s]warmup run: 2238it [00:03, 1098.12it/s]warmup run: 2127it [00:03, 1063.33it/s]warmup run: 2144it [00:03, 1030.83it/s]warmup run: 1944it [00:03, 994.97it/s]warmup run: 2117it [00:03, 1052.61it/s]warmup run: 2415it [00:04, 1121.28it/s]warmup run: 2517it [00:03, 1150.72it/s]warmup run: 2291it [00:03, 1123.53it/s]warmup run: 2357it [00:03, 1123.75it/s]warmup run: 2247it [00:03, 1101.97it/s]warmup run: 2263it [00:03, 1076.91it/s]warmup run: 2052it [00:03, 1018.08it/s]warmup run: 2237it [00:03, 1096.00it/s]warmup run: 2535it [00:04, 1143.94it/s]warmup run: 2637it [00:04, 1163.60it/s]warmup run: 2412it [00:03, 1147.61it/s]warmup run: 2477it [00:03, 1144.62it/s]warmup run: 2367it [00:03, 1128.88it/s]warmup run: 2383it [00:03, 1110.92it/s]warmup run: 2169it [00:03, 1062.95it/s]warmup run: 2357it [00:03, 1127.06it/s]warmup run: 2654it [00:04, 1156.74it/s]warmup run: 2755it [00:04, 1168.23it/s]warmup run: 2533it [00:03, 1165.73it/s]warmup run: 2597it [00:04, 1158.33it/s]warmup run: 2486it [00:03, 1147.00it/s]warmup run: 2502it [00:04, 1134.27it/s]warmup run: 2286it [00:03, 1094.00it/s]warmup run: 2477it [00:03, 1148.64it/s]warmup run: 2773it [00:04, 1163.78it/s]warmup run: 2875it [00:04, 1174.85it/s]warmup run: 2655it [00:03, 1180.07it/s]warmup run: 2716it [00:04, 1167.47it/s]warmup run: 2605it [00:04, 1159.63it/s]warmup run: 2621it [00:04, 1149.55it/s]warmup run: 2403it [00:03, 1115.35it/s]warmup run: 2597it [00:04, 1162.05it/s]warmup run: 2893it [00:04, 1172.45it/s]warmup run: 2995it [00:04, 1179.62it/s]warmup run: 3000it [00:04, 688.74it/s] warmup run: 2776it [00:04, 1187.60it/s]warmup run: 2833it [00:04, 1161.84it/s]warmup run: 2724it [00:04, 1166.35it/s]warmup run: 2740it [00:04, 1159.36it/s]warmup run: 2520it [00:03, 1130.42it/s]warmup run: 2717it [00:04, 1170.11it/s]warmup run: 3000it [00:04, 664.19it/s] warmup run: 2898it [00:04, 1196.67it/s]warmup run: 2950it [00:04, 1158.78it/s]warmup run: 2841it [00:04, 1166.86it/s]warmup run: 2860it [00:04, 1168.71it/s]warmup run: 2640it [00:04, 1149.68it/s]warmup run: 2835it [00:04, 1169.31it/s]warmup run: 3000it [00:04, 681.62it/s] warmup run: 3000it [00:04, 699.28it/s] warmup run: 2960it [00:04, 1171.00it/s]warmup run: 2980it [00:04, 1175.93it/s]warmup run: 2758it [00:04, 1157.95it/s]warmup run: 2953it [00:04, 1171.23it/s]warmup run: 3000it [00:04, 666.71it/s] warmup run: 3000it [00:04, 676.42it/s] warmup run: 3000it [00:04, 685.33it/s] warmup run: 2879it [00:04, 1171.05it/s]warmup run: 3000it [00:04, 683.28it/s] 

warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]

warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]



warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   6%|         | 165/3000 [00:00<00:01, 1648.29it/s]warmup should be done:   5%|         | 153/3000 [00:00<00:01, 1525.76it/s]warmup should be done:   5%|         | 161/3000 [00:00<00:01, 1603.35it/s]warmup should be done:   5%|         | 164/3000 [00:00<00:01, 1634.67it/s]warmup should be done:   5%|         | 152/3000 [00:00<00:01, 1513.24it/s]warmup should be done:   5%|         | 162/3000 [00:00<00:01, 1611.41it/s]warmup should be done:   5%|         | 155/3000 [00:00<00:01, 1542.78it/s]warmup should be done:   6%|         | 165/3000 [00:00<00:01, 1639.90it/s]warmup should be done:  10%|         | 307/3000 [00:00<00:01, 1532.99it/s]warmup should be done:  11%|         | 331/3000 [00:00<00:01, 1649.95it/s]warmup should be done:  11%|         | 330/3000 [00:00<00:01, 1645.74it/s]warmup should be done:  11%|         | 323/3000 [00:00<00:01, 1610.85it/s]warmup should be done:  11%|         | 329/3000 [00:00<00:01, 1639.43it/s]warmup should be done:  10%|         | 305/3000 [00:00<00:01, 1518.77it/s]warmup should be done:  11%|         | 325/3000 [00:00<00:01, 1617.95it/s]warmup should be done:  10%|         | 310/3000 [00:00<00:01, 1519.22it/s]warmup should be done:  15%|        | 461/3000 [00:00<00:01, 1532.74it/s]warmup should be done:  17%|        | 496/3000 [00:00<00:01, 1646.34it/s]warmup should be done:  15%|        | 457/3000 [00:00<00:01, 1518.02it/s]warmup should be done:  16%|        | 495/3000 [00:00<00:01, 1643.47it/s]warmup should be done:  16%|        | 493/3000 [00:00<00:01, 1637.13it/s]warmup should be done:  16%|        | 487/3000 [00:00<00:01, 1610.67it/s]warmup should be done:  16%|        | 485/3000 [00:00<00:01, 1602.32it/s]warmup should be done:  15%|        | 462/3000 [00:00<00:01, 1509.54it/s]warmup should be done:  21%|        | 625/3000 [00:00<00:01, 1572.77it/s]warmup should be done:  22%|       | 660/3000 [00:00<00:01, 1645.63it/s]warmup should be done:  22%|       | 657/3000 [00:00<00:01, 1636.91it/s]warmup should be done:  22%|       | 661/3000 [00:00<00:01, 1644.17it/s]warmup should be done:  20%|        | 615/3000 [00:00<00:01, 1539.02it/s]warmup should be done:  22%|       | 646/3000 [00:00<00:01, 1599.64it/s]warmup should be done:  22%|       | 649/3000 [00:00<00:01, 1606.37it/s]warmup should be done:  20%|        | 613/3000 [00:00<00:01, 1500.13it/s]warmup should be done:  26%|       | 790/3000 [00:00<00:01, 1600.37it/s]warmup should be done:  28%|       | 825/3000 [00:00<00:01, 1646.76it/s]warmup should be done:  27%|       | 821/3000 [00:00<00:01, 1636.43it/s]warmup should be done:  26%|       | 774/3000 [00:00<00:01, 1556.43it/s]warmup should be done:  28%|       | 826/3000 [00:00<00:01, 1637.16it/s]warmup should be done:  27%|       | 806/3000 [00:00<00:01, 1598.20it/s]warmup should be done:  27%|       | 810/3000 [00:00<00:01, 1603.97it/s]warmup should be done:  25%|       | 764/3000 [00:00<00:01, 1489.82it/s]warmup should be done:  33%|      | 990/3000 [00:00<00:01, 1647.65it/s]warmup should be done:  32%|      | 954/3000 [00:00<00:01, 1611.12it/s]warmup should be done:  31%|       | 932/3000 [00:00<00:01, 1563.83it/s]warmup should be done:  33%|      | 985/3000 [00:00<00:01, 1635.77it/s]warmup should be done:  32%|      | 966/3000 [00:00<00:01, 1595.90it/s]warmup should be done:  33%|      | 990/3000 [00:00<00:01, 1631.39it/s]warmup should be done:  32%|      | 972/3000 [00:00<00:01, 1607.44it/s]warmup should be done:  30%|       | 914/3000 [00:00<00:01, 1491.45it/s]warmup should be done:  38%|      | 1155/3000 [00:00<00:01, 1645.15it/s]warmup should be done:  36%|      | 1089/3000 [00:00<00:01, 1565.54it/s]warmup should be done:  37%|      | 1116/3000 [00:00<00:01, 1608.16it/s]warmup should be done:  38%|      | 1149/3000 [00:00<00:01, 1632.16it/s]warmup should be done:  38%|      | 1133/3000 [00:00<00:01, 1605.94it/s]warmup should be done:  38%|      | 1126/3000 [00:00<00:01, 1587.51it/s]warmup should be done:  38%|      | 1154/3000 [00:00<00:01, 1617.92it/s]warmup should be done:  35%|      | 1064/3000 [00:00<00:01, 1492.60it/s]warmup should be done:  44%|     | 1320/3000 [00:00<00:01, 1645.70it/s]warmup should be done:  42%|     | 1247/3000 [00:00<00:01, 1568.82it/s]warmup should be done:  43%|     | 1278/3000 [00:00<00:01, 1610.53it/s]warmup should be done:  44%|     | 1313/3000 [00:00<00:01, 1631.67it/s]warmup should be done:  43%|     | 1294/3000 [00:00<00:01, 1606.92it/s]warmup should be done:  43%|     | 1285/3000 [00:00<00:01, 1588.14it/s]warmup should be done:  44%|     | 1317/3000 [00:00<00:01, 1619.30it/s]warmup should be done:  40%|      | 1214/3000 [00:00<00:01, 1490.69it/s]warmup should be done:  50%|     | 1485/3000 [00:00<00:00, 1644.79it/s]warmup should be done:  47%|     | 1405/3000 [00:00<00:01, 1570.43it/s]warmup should be done:  48%|     | 1440/3000 [00:00<00:00, 1610.13it/s]warmup should be done:  49%|     | 1477/3000 [00:00<00:00, 1630.96it/s]warmup should be done:  48%|     | 1455/3000 [00:00<00:00, 1607.03it/s]warmup should be done:  48%|     | 1444/3000 [00:00<00:00, 1588.01it/s]warmup should be done:  49%|     | 1480/3000 [00:00<00:00, 1620.46it/s]warmup should be done:  46%|     | 1366/3000 [00:00<00:01, 1497.68it/s]warmup should be done:  55%|    | 1650/3000 [00:01<00:00, 1643.98it/s]warmup should be done:  52%|    | 1563/3000 [00:01<00:00, 1570.94it/s]warmup should be done:  53%|    | 1602/3000 [00:01<00:00, 1610.36it/s]warmup should be done:  55%|    | 1641/3000 [00:01<00:00, 1628.45it/s]warmup should be done:  54%|    | 1616/3000 [00:01<00:00, 1606.02it/s]warmup should be done:  53%|    | 1603/3000 [00:01<00:00, 1587.31it/s]warmup should be done:  55%|    | 1643/3000 [00:01<00:00, 1620.79it/s]warmup should be done:  51%|     | 1516/3000 [00:01<00:00, 1498.13it/s]warmup should be done:  60%|    | 1815/3000 [00:01<00:00, 1644.76it/s]warmup should be done:  57%|    | 1721/3000 [00:01<00:00, 1572.73it/s]warmup should be done:  59%|    | 1764/3000 [00:01<00:00, 1610.30it/s]warmup should be done:  60%|    | 1804/3000 [00:01<00:00, 1623.56it/s]warmup should be done:  60%|    | 1806/3000 [00:01<00:00, 1620.08it/s]warmup should be done:  59%|    | 1777/3000 [00:01<00:00, 1584.87it/s]warmup should be done:  59%|    | 1762/3000 [00:01<00:00, 1569.76it/s]warmup should be done:  56%|    | 1666/3000 [00:01<00:00, 1491.90it/s]warmup should be done:  66%|   | 1980/3000 [00:01<00:00, 1645.06it/s]warmup should be done:  63%|   | 1879/3000 [00:01<00:00, 1574.03it/s]warmup should be done:  64%|   | 1926/3000 [00:01<00:00, 1610.46it/s]warmup should be done:  66%|   | 1967/3000 [00:01<00:00, 1619.97it/s]warmup should be done:  66%|   | 1969/3000 [00:01<00:00, 1619.81it/s]warmup should be done:  64%|   | 1921/3000 [00:01<00:00, 1575.34it/s]warmup should be done:  65%|   | 1939/3000 [00:01<00:00, 1592.46it/s]warmup should be done:  61%|    | 1816/3000 [00:01<00:00, 1488.28it/s]warmup should be done:  72%|  | 2145/3000 [00:01<00:00, 1645.28it/s]warmup should be done:  68%|   | 2037/3000 [00:01<00:00, 1573.32it/s]warmup should be done:  70%|   | 2088/3000 [00:01<00:00, 1610.85it/s]warmup should be done:  71%|   | 2129/3000 [00:01<00:00, 1616.80it/s]warmup should be done:  71%|   | 2131/3000 [00:01<00:00, 1619.16it/s]warmup should be done:  69%|   | 2081/3000 [00:01<00:00, 1580.37it/s]warmup should be done:  70%|   | 2101/3000 [00:01<00:00, 1598.21it/s]warmup should be done:  66%|   | 1965/3000 [00:01<00:00, 1485.36it/s]warmup should be done:  77%|  | 2310/3000 [00:01<00:00, 1645.17it/s]warmup should be done:  73%|  | 2195/3000 [00:01<00:00, 1570.25it/s]warmup should be done:  75%|  | 2250/3000 [00:01<00:00, 1612.19it/s]warmup should be done:  76%|  | 2291/3000 [00:01<00:00, 1614.78it/s]warmup should be done:  76%|  | 2294/3000 [00:01<00:00, 1620.07it/s]warmup should be done:  75%|  | 2240/3000 [00:01<00:00, 1582.60it/s]warmup should be done:  75%|  | 2262/3000 [00:01<00:00, 1601.47it/s]warmup should be done:  70%|   | 2114/3000 [00:01<00:00, 1475.14it/s]warmup should be done:  82%| | 2475/3000 [00:01<00:00, 1641.47it/s]warmup should be done:  78%|  | 2354/3000 [00:01<00:00, 1574.67it/s]warmup should be done:  80%|  | 2413/3000 [00:01<00:00, 1614.83it/s]warmup should be done:  82%| | 2453/3000 [00:01<00:00, 1609.71it/s]warmup should be done:  82%| | 2457/3000 [00:01<00:00, 1618.67it/s]warmup should be done:  80%|  | 2399/3000 [00:01<00:00, 1581.13it/s]warmup should be done:  81%|  | 2423/3000 [00:01<00:00, 1601.49it/s]warmup should be done:  75%|  | 2264/3000 [00:01<00:00, 1481.51it/s]warmup should be done:  88%| | 2640/3000 [00:01<00:00, 1641.69it/s]warmup should be done:  84%| | 2513/3000 [00:01<00:00, 1577.67it/s]warmup should be done:  86%| | 2578/3000 [00:01<00:00, 1623.33it/s]warmup should be done:  87%| | 2614/3000 [00:01<00:00, 1607.96it/s]warmup should be done:  87%| | 2620/3000 [00:01<00:00, 1620.69it/s]warmup should be done:  86%| | 2584/3000 [00:01<00:00, 1602.71it/s]warmup should be done:  85%| | 2558/3000 [00:01<00:00, 1582.16it/s]warmup should be done:  80%|  | 2413/3000 [00:01<00:00, 1483.95it/s]warmup should be done:  94%|| 2805/3000 [00:01<00:00, 1641.56it/s]warmup should be done:  89%| | 2673/3000 [00:01<00:00, 1582.10it/s]warmup should be done:  91%|| 2743/3000 [00:01<00:00, 1630.71it/s]warmup should be done:  92%|| 2775/3000 [00:01<00:00, 1607.39it/s]warmup should be done:  92%|| 2745/3000 [00:01<00:00, 1603.29it/s]warmup should be done:  93%|| 2783/3000 [00:01<00:00, 1620.71it/s]warmup should be done:  91%| | 2717/3000 [00:01<00:00, 1578.50it/s]warmup should be done:  86%| | 2567/3000 [00:01<00:00, 1499.52it/s]warmup should be done:  99%|| 2972/3000 [00:01<00:00, 1647.58it/s]warmup should be done:  94%|| 2832/3000 [00:01<00:00, 1578.41it/s]warmup should be done:  97%|| 2908/3000 [00:01<00:00, 1635.19it/s]warmup should be done:  98%|| 2938/3000 [00:01<00:00, 1611.92it/s]warmup should be done:  98%|| 2947/3000 [00:01<00:00, 1626.36it/s]warmup should be done:  97%|| 2908/3000 [00:01<00:00, 1608.97it/s]warmup should be done:  96%|| 2877/3000 [00:01<00:00, 1582.49it/s]warmup should be done:  91%| | 2724/3000 [00:01<00:00, 1518.98it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1644.65it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1625.80it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1622.03it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1609.01it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1604.45it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1586.42it/s]warmup should be done: 100%|| 2990/3000 [00:01<00:00, 1573.58it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1566.16it/s]warmup should be done:  96%|| 2882/3000 [00:01<00:00, 1534.32it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1504.27it/s]


warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]


warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]

warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   5%|         | 162/3000 [00:00<00:01, 1619.11it/s]warmup should be done:   6%|         | 168/3000 [00:00<00:01, 1677.35it/s]warmup should be done:   6%|         | 167/3000 [00:00<00:01, 1666.55it/s]warmup should be done:   5%|         | 162/3000 [00:00<00:01, 1611.63it/s]warmup should be done:   5%|         | 164/3000 [00:00<00:01, 1633.26it/s]warmup should be done:   6%|         | 167/3000 [00:00<00:01, 1660.85it/s]warmup should be done:   5%|         | 160/3000 [00:00<00:01, 1591.76it/s]warmup should be done:   6%|         | 165/3000 [00:00<00:01, 1641.42it/s]warmup should be done:  11%|         | 326/3000 [00:00<00:01, 1630.45it/s]warmup should be done:  11%|         | 337/3000 [00:00<00:01, 1681.10it/s]warmup should be done:  11%|         | 334/3000 [00:00<00:01, 1665.26it/s]warmup should be done:  11%|         | 331/3000 [00:00<00:01, 1651.52it/s]warmup should be done:  11%|         | 330/3000 [00:00<00:01, 1644.63it/s]warmup should be done:  11%|         | 335/3000 [00:00<00:01, 1667.29it/s]warmup should be done:  11%|         | 324/3000 [00:00<00:01, 1607.59it/s]warmup should be done:  11%|         | 320/3000 [00:00<00:01, 1586.64it/s]warmup should be done:  16%|        | 491/3000 [00:00<00:01, 1635.79it/s]warmup should be done:  17%|        | 498/3000 [00:00<00:01, 1658.55it/s]warmup should be done:  17%|        | 506/3000 [00:00<00:01, 1681.69it/s]warmup should be done:  17%|        | 501/3000 [00:00<00:01, 1664.22it/s]warmup should be done:  17%|        | 496/3000 [00:00<00:01, 1649.96it/s]warmup should be done:  17%|        | 503/3000 [00:00<00:01, 1671.33it/s]warmup should be done:  16%|        | 486/3000 [00:00<00:01, 1611.83it/s]warmup should be done:  16%|        | 480/3000 [00:00<00:01, 1590.25it/s]warmup should be done:  22%|       | 664/3000 [00:00<00:01, 1658.73it/s]warmup should be done:  22%|       | 656/3000 [00:00<00:01, 1638.56it/s]warmup should be done:  23%|       | 676/3000 [00:00<00:01, 1687.28it/s]warmup should be done:  22%|       | 661/3000 [00:00<00:01, 1648.14it/s]warmup should be done:  22%|       | 669/3000 [00:00<00:01, 1666.81it/s]warmup should be done:  22%|       | 671/3000 [00:00<00:01, 1672.49it/s]warmup should be done:  22%|       | 648/3000 [00:00<00:01, 1614.54it/s]warmup should be done:  21%|       | 640/3000 [00:00<00:01, 1594.01it/s]warmup should be done:  28%|       | 830/3000 [00:00<00:01, 1658.44it/s]warmup should be done:  27%|       | 822/3000 [00:00<00:01, 1645.31it/s]warmup should be done:  28%|       | 846/3000 [00:00<00:01, 1689.79it/s]warmup should be done:  28%|       | 837/3000 [00:00<00:01, 1669.98it/s]warmup should be done:  27%|       | 810/3000 [00:00<00:01, 1615.16it/s]warmup should be done:  28%|       | 826/3000 [00:00<00:01, 1644.93it/s]warmup should be done:  28%|       | 839/3000 [00:00<00:01, 1671.00it/s]warmup should be done:  27%|       | 800/3000 [00:00<00:01, 1593.61it/s]warmup should be done:  33%|      | 997/3000 [00:00<00:01, 1661.06it/s]warmup should be done:  33%|      | 989/3000 [00:00<00:01, 1651.82it/s]warmup should be done:  34%|      | 1015/3000 [00:00<00:01, 1688.78it/s]warmup should be done:  34%|      | 1005/3000 [00:00<00:01, 1670.96it/s]warmup should be done:  33%|      | 991/3000 [00:00<00:01, 1643.97it/s]warmup should be done:  32%|      | 972/3000 [00:00<00:01, 1613.17it/s]warmup should be done:  34%|      | 1007/3000 [00:00<00:01, 1670.83it/s]warmup should be done:  32%|      | 960/3000 [00:00<00:01, 1592.32it/s]warmup should be done:  39%|      | 1164/3000 [00:00<00:01, 1661.75it/s]warmup should be done:  38%|      | 1134/3000 [00:00<00:01, 1613.90it/s]warmup should be done:  39%|      | 1184/3000 [00:00<00:01, 1683.07it/s]warmup should be done:  39%|      | 1173/3000 [00:00<00:01, 1669.94it/s]warmup should be done:  39%|      | 1175/3000 [00:00<00:01, 1671.75it/s]warmup should be done:  37%|      | 1121/3000 [00:00<00:01, 1596.58it/s]warmup should be done:  39%|      | 1156/3000 [00:00<00:01, 1641.05it/s]warmup should be done:  38%|      | 1155/3000 [00:00<00:01, 1585.83it/s]warmup should be done:  44%|     | 1331/3000 [00:00<00:01, 1658.02it/s]warmup should be done:  45%|     | 1340/3000 [00:00<00:00, 1669.76it/s]warmup should be done:  45%|     | 1343/3000 [00:00<00:00, 1673.20it/s]warmup should be done:  43%|     | 1296/3000 [00:00<00:01, 1608.61it/s]warmup should be done:  45%|     | 1353/3000 [00:00<00:00, 1677.11it/s]warmup should be done:  43%|     | 1281/3000 [00:00<00:01, 1591.92it/s]warmup should be done:  44%|     | 1321/3000 [00:00<00:01, 1638.06it/s]warmup should be done:  44%|     | 1315/3000 [00:00<00:01, 1544.11it/s]warmup should be done:  50%|     | 1497/3000 [00:00<00:00, 1656.18it/s]warmup should be done:  50%|     | 1507/3000 [00:00<00:00, 1667.44it/s]warmup should be done:  50%|     | 1511/3000 [00:00<00:00, 1672.27it/s]warmup should be done:  49%|     | 1458/3000 [00:00<00:00, 1611.29it/s]warmup should be done:  48%|     | 1442/3000 [00:00<00:00, 1596.15it/s]warmup should be done:  50%|     | 1486/3000 [00:00<00:00, 1640.48it/s]warmup should be done:  51%|     | 1521/3000 [00:00<00:00, 1670.45it/s]warmup should be done:  49%|     | 1481/3000 [00:00<00:00, 1577.18it/s]warmup should be done:  55%|    | 1664/3000 [00:01<00:00, 1659.30it/s]warmup should be done:  56%|    | 1674/3000 [00:01<00:00, 1667.17it/s]warmup should be done:  56%|    | 1679/3000 [00:01<00:00, 1673.95it/s]warmup should be done:  55%|    | 1651/3000 [00:01<00:00, 1643.13it/s]warmup should be done:  54%|    | 1621/3000 [00:01<00:00, 1614.07it/s]warmup should be done:  53%|    | 1604/3000 [00:01<00:00, 1600.88it/s]warmup should be done:  56%|    | 1690/3000 [00:01<00:00, 1674.25it/s]warmup should be done:  55%|    | 1648/3000 [00:01<00:00, 1602.45it/s]warmup should be done:  61%|   | 1841/3000 [00:01<00:00, 1667.80it/s]warmup should be done:  61%|    | 1831/3000 [00:01<00:00, 1661.00it/s]warmup should be done:  62%|   | 1847/3000 [00:01<00:00, 1674.84it/s]warmup should be done:  61%|    | 1816/3000 [00:01<00:00, 1644.29it/s]warmup should be done:  59%|    | 1783/3000 [00:01<00:00, 1613.44it/s]warmup should be done:  59%|    | 1765/3000 [00:01<00:00, 1597.25it/s]warmup should be done:  62%|   | 1860/3000 [00:01<00:00, 1679.17it/s]warmup should be done:  60%|    | 1814/3000 [00:01<00:00, 1618.76it/s]warmup should be done:  67%|   | 2008/3000 [00:01<00:00, 1665.82it/s]warmup should be done:  67%|   | 1998/3000 [00:01<00:00, 1659.88it/s]warmup should be done:  67%|   | 2015/3000 [00:01<00:00, 1673.19it/s]warmup should be done:  66%|   | 1981/3000 [00:01<00:00, 1643.33it/s]warmup should be done:  65%|   | 1945/3000 [00:01<00:00, 1613.07it/s]warmup should be done:  64%|   | 1925/3000 [00:01<00:00, 1595.75it/s]warmup should be done:  68%|   | 2030/3000 [00:01<00:00, 1682.60it/s]warmup should be done:  66%|   | 1979/3000 [00:01<00:00, 1627.60it/s]warmup should be done:  72%|  | 2175/3000 [00:01<00:00, 1664.43it/s]warmup should be done:  72%|  | 2164/3000 [00:01<00:00, 1658.21it/s]warmup should be done:  73%|  | 2183/3000 [00:01<00:00, 1671.71it/s]warmup should be done:  72%|  | 2146/3000 [00:01<00:00, 1642.78it/s]warmup should be done:  70%|   | 2108/3000 [00:01<00:00, 1615.42it/s]warmup should be done:  70%|   | 2085/3000 [00:01<00:00, 1596.87it/s]warmup should be done:  73%|  | 2199/3000 [00:01<00:00, 1683.19it/s]warmup should be done:  72%|  | 2145/3000 [00:01<00:00, 1634.85it/s]warmup should be done:  78%|  | 2342/3000 [00:01<00:00, 1665.36it/s]warmup should be done:  78%|  | 2331/3000 [00:01<00:00, 1659.53it/s]warmup should be done:  78%|  | 2351/3000 [00:01<00:00, 1672.38it/s]warmup should be done:  77%|  | 2311/3000 [00:01<00:00, 1644.69it/s]warmup should be done:  76%|  | 2271/3000 [00:01<00:00, 1617.07it/s]warmup should be done:  75%|  | 2245/3000 [00:01<00:00, 1596.22it/s]warmup should be done:  79%|  | 2369/3000 [00:01<00:00, 1685.20it/s]warmup should be done:  77%|  | 2311/3000 [00:01<00:00, 1640.70it/s]warmup should be done:  84%| | 2510/3000 [00:01<00:00, 1667.05it/s]warmup should be done:  83%| | 2498/3000 [00:01<00:00, 1661.32it/s]warmup should be done:  84%| | 2519/3000 [00:01<00:00, 1673.33it/s]warmup should be done:  83%| | 2476/3000 [00:01<00:00, 1643.91it/s]warmup should be done:  81%|  | 2433/3000 [00:01<00:00, 1616.41it/s]warmup should be done:  80%|  | 2405/3000 [00:01<00:00, 1593.39it/s]warmup should be done:  85%| | 2539/3000 [00:01<00:00, 1687.19it/s]warmup should be done:  83%| | 2477/3000 [00:01<00:00, 1645.70it/s]warmup should be done:  89%| | 2679/3000 [00:01<00:00, 1672.31it/s]warmup should be done:  89%| | 2665/3000 [00:01<00:00, 1660.80it/s]warmup should be done:  90%| | 2687/3000 [00:01<00:00, 1673.38it/s]warmup should be done:  88%| | 2641/3000 [00:01<00:00, 1641.78it/s]warmup should be done:  86%| | 2595/3000 [00:01<00:00, 1614.04it/s]warmup should be done:  86%| | 2565/3000 [00:01<00:00, 1595.16it/s]warmup should be done:  88%| | 2643/3000 [00:01<00:00, 1647.55it/s]warmup should be done:  90%| | 2708/3000 [00:01<00:00, 1498.52it/s]warmup should be done:  95%|| 2848/3000 [00:01<00:00, 1675.63it/s]warmup should be done:  94%|| 2832/3000 [00:01<00:00, 1660.11it/s]warmup should be done:  95%|| 2855/3000 [00:01<00:00, 1672.13it/s]warmup should be done:  94%|| 2806/3000 [00:01<00:00, 1643.11it/s]warmup should be done:  91%| | 2726/3000 [00:01<00:00, 1596.91it/s]warmup should be done:  92%|| 2757/3000 [00:01<00:00, 1611.40it/s]warmup should be done:  94%|| 2809/3000 [00:01<00:00, 1649.69it/s]warmup should be done:  96%|| 2873/3000 [00:01<00:00, 1537.75it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1672.08it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1670.79it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1664.60it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1659.74it/s]warmup should be done:  99%|| 2973/3000 [00:01<00:00, 1648.97it/s]warmup should be done:  97%|| 2919/3000 [00:01<00:00, 1608.02it/s]warmup should be done:  96%|| 2886/3000 [00:01<00:00, 1576.06it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1644.23it/s]warmup should be done:  99%|| 2976/3000 [00:01<00:00, 1653.35it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1640.68it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1628.04it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1612.17it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1591.98it/s]
WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7fceefbc1100>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7fceefbd0040>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7fceefbd0190>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7fceefbc22b0>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7fceefc8e730>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7fceefc90d30>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7fceefbc1130>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7fceefc8de80>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
2022-12-12 00:47:44.942161: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fca1f02e7c0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 00:47:44.942230: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 00:47:44.951314: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fca268305e0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 00:47:44.951360: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 00:47:44.951634: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 00:47:44.958804: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 00:47:45.282524: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fca1f02a660 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 00:47:45.282585: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 00:47:45.292232: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 00:47:45.893033: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fca1e831530 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 00:47:45.893095: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 00:47:45.902565: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 00:47:45.931682: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fca1f029760 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 00:47:45.931750: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 00:47:45.941759: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 00:47:45.943304: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fca22834b20 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 00:47:45.943364: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 00:47:45.950908: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 00:47:45.968686: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fca268347f0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 00:47:45.968748: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 00:47:45.969915: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fca1ef922b0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 00:47:45.969956: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 00:47:45.977587: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 00:47:45.977805: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 00:47:52.219459: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 00:47:52.268561: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 00:47:52.328072: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 00:47:52.618277: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 00:47:52.936475: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 00:47:52.936676: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 00:47:52.943202: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 00:47:52.943618: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
[HCTR][00:48:50.970][ERROR][RK0][tid #140506365425408]: replica 7 reaches 1000, calling init pre replica
[HCTR][00:48:50.970][ERROR][RK0][tid #140506365425408]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][00:48:50.978][ERROR][RK0][tid #140506365425408]: coll ps creation done
[HCTR][00:48:50.978][ERROR][RK0][tid #140506365425408]: replica 7 waits for coll ps creation barrier
[HCTR][00:48:51.050][ERROR][RK0][tid #140506365425408]: replica 2 reaches 1000, calling init pre replica
[HCTR][00:48:51.050][ERROR][RK0][tid #140506365425408]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][00:48:51.056][ERROR][RK0][tid #140506365425408]: coll ps creation done
[HCTR][00:48:51.056][ERROR][RK0][tid #140506365425408]: replica 2 waits for coll ps creation barrier
[HCTR][00:48:51.074][ERROR][RK0][tid #140507162339072]: replica 3 reaches 1000, calling init pre replica
[HCTR][00:48:51.075][ERROR][RK0][tid #140507162339072]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][00:48:51.080][ERROR][RK0][tid #140507162339072]: coll ps creation done
[HCTR][00:48:51.080][ERROR][RK0][tid #140507162339072]: replica 3 waits for coll ps creation barrier
[HCTR][00:48:51.146][ERROR][RK0][main]: replica 6 reaches 1000, calling init pre replica
[HCTR][00:48:51.146][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][00:48:51.151][ERROR][RK0][main]: coll ps creation done
[HCTR][00:48:51.151][ERROR][RK0][main]: replica 6 waits for coll ps creation barrier
[HCTR][00:48:51.185][ERROR][RK0][main]: replica 1 reaches 1000, calling init pre replica
[HCTR][00:48:51.185][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][00:48:51.193][ERROR][RK0][main]: coll ps creation done
[HCTR][00:48:51.193][ERROR][RK0][main]: replica 1 waits for coll ps creation barrier
[HCTR][00:48:51.262][ERROR][RK0][tid #140506298316544]: replica 0 reaches 1000, calling init pre replica
[HCTR][00:48:51.262][ERROR][RK0][tid #140506298316544]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][00:48:51.270][ERROR][RK0][tid #140506298316544]: coll ps creation done
[HCTR][00:48:51.270][ERROR][RK0][tid #140506298316544]: replica 0 waits for coll ps creation barrier
[HCTR][00:48:51.281][ERROR][RK0][tid #140506298316544]: replica 4 reaches 1000, calling init pre replica
[HCTR][00:48:51.281][ERROR][RK0][tid #140506298316544]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][00:48:51.286][ERROR][RK0][tid #140506298316544]: coll ps creation done
[HCTR][00:48:51.286][ERROR][RK0][tid #140506298316544]: replica 4 waits for coll ps creation barrier
[HCTR][00:48:51.514][ERROR][RK0][tid #140506365425408]: replica 5 reaches 1000, calling init pre replica
[HCTR][00:48:51.514][ERROR][RK0][tid #140506365425408]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][00:48:51.523][ERROR][RK0][tid #140506365425408]: coll ps creation done
[HCTR][00:48:51.523][ERROR][RK0][tid #140506365425408]: replica 5 waits for coll ps creation barrier
[HCTR][00:48:51.523][ERROR][RK0][tid #140506298316544]: replica 0 preparing frequency
[HCTR][00:48:52.364][ERROR][RK0][tid #140506298316544]: replica 0 preparing frequency done
[HCTR][00:48:52.397][ERROR][RK0][tid #140506298316544]: replica 0 calling init per replica
[HCTR][00:48:52.397][ERROR][RK0][tid #140506365425408]: replica 5 calling init per replica
[HCTR][00:48:52.397][ERROR][RK0][main]: replica 1 calling init per replica
[HCTR][00:48:52.397][ERROR][RK0][tid #140506298316544]: replica 4 calling init per replica
[HCTR][00:48:52.397][ERROR][RK0][tid #140506365425408]: replica 7 calling init per replica
[HCTR][00:48:52.397][ERROR][RK0][main]: replica 6 calling init per replica
[HCTR][00:48:52.397][ERROR][RK0][tid #140507162339072]: replica 3 calling init per replica
[HCTR][00:48:52.397][ERROR][RK0][tid #140506365425408]: replica 2 calling init per replica
[HCTR][00:48:52.397][ERROR][RK0][tid #140506298316544]: Calling build_v2
[HCTR][00:48:52.397][ERROR][RK0][tid #140506365425408]: Calling build_v2
[HCTR][00:48:52.398][ERROR][RK0][main]: Calling build_v2
[HCTR][00:48:52.398][ERROR][RK0][tid #140506298316544]: Calling build_v2
[HCTR][00:48:52.398][ERROR][RK0][tid #140506365425408]: Calling build_v2
[HCTR][00:48:52.398][ERROR][RK0][main]: Calling build_v2
[HCTR][00:48:52.398][ERROR][RK0][tid #140507162339072]: Calling build_v2
[HCTR][00:48:52.398][ERROR][RK0][tid #140506298316544]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][00:48:52.398][ERROR][RK0][tid #140506365425408]: Calling build_v2
[HCTR][00:48:52.398][ERROR][RK0][tid #140506365425408]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][00:48:52.398][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][00:48:52.398][ERROR][RK0][tid #140506298316544]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][00:48:52.398][ERROR][RK0][tid #140506365425408]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][00:48:52.398][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][00:48:52.398][ERROR][RK0][tid #140507162339072]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][00:48:52.398][ERROR][RK0][tid #140506365425408]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[[[[2022-12-12 00:48:522022-12-12 00:48:52[[[2022-12-12 00:48:52[.2022-12-12 00:48:52..2022-12-12 00:48:522022-12-12 00:48:523981512022-12-12 00:48:52.398150398151..: .2022-12-12 00:48:52398150: : 398171398171E398167.: EE: :  : 398181E  EE/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.ccE:  /hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc  : E/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc::/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc136/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc :136136::] :/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc136] ] 136136using concurrent impl MPS136:] using concurrent impl MPSusing concurrent impl MPS] ] 
] 136using concurrent impl MPS

using concurrent impl MPSusing concurrent impl MPSusing concurrent impl MPS] 



using concurrent impl MPS
[2022-12-12 00:48:52.402470: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:178] v100x8, slow pcie
[[2022-12-12 00:48:522022-12-12 00:48:52..402515402521: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc::178196] ] v100x8, slow pcieassigning 8 to cpu[

2022-12-12 00:48:52.402561[: 2022-12-12 00:48:52E. 402578/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: [:E2022-12-12 00:48:52178 .] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc402595v100x8, slow pcie[:: 
2022-12-12 00:48:52196E.] [ 402608assigning 8 to cpu2022-12-12 00:48:52/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: 
.:E402634212 : [] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE2022-12-12 00:48:52build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8: .
178/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc402654[] :: 2022-12-12 00:48:52v100x8, slow pcie196E.[
] [ 4026912022-12-12 00:48:52assigning 8 to cpu[2022-12-12 00:48:52/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: .
2022-12-12 00:48:52.:E402701.402698178 : [402719: ] [/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE2022-12-12 00:48:52[: Ev100x8, slow pcie2022-12-12 00:48:52: .2022-12-12 00:48:52E 
.212/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc402750. /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc402787] [:: 402784/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:: build asymm link desc with 8X Tesla V100-SXM2-32GB out of 82022-12-12 00:48:52213E: :178E
.]  E196[]  402864remote time is 8.68421/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc ] 2022-12-12 00:48:52v100x8, slow pcie/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: 
:/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccassigning 8 to cpu.
:E178:[
402948178 [] 2122022-12-12 00:48:52: ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 00:48:52v100x8, slow pcie] .E[v100x8, slow pcie:.
build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8402998 2022-12-12 00:48:52
196403013
: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[.] [: E:2022-12-12 00:48:52[403053assigning 8 to cpu2022-12-12 00:48:52E 213.2022-12-12 00:48:52: 
. /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] 403096.E403109/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:remote time is 8.68421: 403125 : :[214
E: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE1962022-12-12 00:48:52]  E[: ] .cpu time is 97.0588/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc 2022-12-12 00:48:52212/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccassigning 8 to cpu403223
:/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.] :
: 196:403264build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8196E] 213: 
]  assigning 8 to cpu] Eassigning 8 to cpu/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[
remote time is 8.68421[ 
:2022-12-12 00:48:52
2022-12-12 00:48:52/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc212..:[] 4033984034022142022-12-12 00:48:52[build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8[: : ] .2022-12-12 00:48:52
2022-12-12 00:48:52EEcpu time is 97.0588403454.. [ 
: 403468403473/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 00:48:52/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE: : :.: EE213403530212/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc  ] : ] :/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccremote time is 8.68421Ebuild asymm link desc with 8X Tesla V100-SXM2-32GB out of 8214::
 
[] 212212/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 00:48:52cpu time is 97.0588] ] :.[
build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8build asymm link desc with 8X Tesla V100-SXM2-32GB out of 82134036962022-12-12 00:48:52

] : .remote time is 8.68421E403713[[
 : 2022-12-12 00:48:522022-12-12 00:48:52/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE.[.: 4037712022-12-12 00:48:52403772214/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: .: ] :E403795Ecpu time is 97.0588213 :  
] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccremote time is 8.68421: :
213/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc213] :] [remote time is 8.68421214remote time is 8.684212022-12-12 00:48:52
] 
.cpu time is 97.0588[403930[
2022-12-12 00:48:52: 2022-12-12 00:48:52.E.403966 403972: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: E:E 214 /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:cpu time is 97.0588:214
214] ] cpu time is 97.0588cpu time is 97.0588

[2022-12-12 00:50:11.715093: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:118] solver created. now build & solve
[2022-12-12 00:50:11.755176: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 381.47 MB
[2022-12-12 00:50:11.755265: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 381.47 MB
[2022-12-12 00:50:11.756280: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:74] mapping nid to rank...
[2022-12-12 00:50:11.829341: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:91] counting slots...
[2022-12-12 00:50:12.208856: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:105] Final num slot is 49
[2022-12-12 00:50:12.208950: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:109] counting blocks...
[2022-12-12 00:50:19.836506: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:118] Final num block is 1024
[2022-12-12 00:50:19.836604: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:123] counting freq and density...
[2022-12-12 00:50:21.542767: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:154] averaging freq and density...
[2022-12-12 00:50:21.542872: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:155] 1024
[2022-12-12 00:50:21.545770: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:123] solver built. now solve
[2022-12-12 00:50:21.545833: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:354] constructing optimal solver, device=8, stream=1
1024 blocks, 8 devices
[2022-12-12 00:50:21.827741: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:527] Add Var...
[2022-12-12 00:50:21.858494: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:545] Capacity...
[2022-12-12 00:50:21.859952: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:548] Connect CPU...
[2022-12-12 00:50:21.881756: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:550] Connect Access To Storage...
[2022-12-12 00:50:22.463868: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:554] Time...
[2022-12-12 00:50:53.180773: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:569] Coll Cache init block placement array
[2022-12-12 00:50:53.188617: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:645] Coll Cache init block placement array done
[2022-12-12 00:50:53.190877: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:647] Coll Cache model reset done
[2022-12-12 00:50:53.237955: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:127] solver solved
[2022-12-12 00:50:53.238053: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:195] 0 solved master
[2022-12-12 00:50:53.238087: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 0 solved
[2022-12-12 00:50:53.238118: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 0 initing device 0
[2022-12-12 00:50:53.238717: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-12 00:50:53.238767: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 00:50:53.239695: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 00:50:53.240349: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 00:50:53.253397: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 3 solved
[2022-12-12 00:50:53.253484: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 3 initing device 3
[[2022-12-12 00:50:532022-12-12 00:50:53..253734253741: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc::202202] ] 2 solved1 solved

[[2022-12-12 00:50:532022-12-12 00:50:53..253829253831: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc::205205] ] worker 0 thread 2 initing device 2worker 0 thread 1 initing device 1

[2022-12-12 00:50:53.253940: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-12 00:50:53.253994: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 00:50:53.[2542862022-12-12 00:50:53: .E254292 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE: 1815/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu] :Building Coll Cache with ... num gpu device is 81815
] Building Coll Cache with ... num gpu device is 8
[2022-12-12 00:50:53[.2022-12-12 00:50:53254356.: 254361E:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu :/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu1980:] 1980eager alloc mem 381.47 MB] 
eager alloc mem 381.47 MB
[2022-12-12 00:50:53.254401: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 4 solved
[[2022-12-12 00:50:532022-12-12 00:50:53..254457254442: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc::205202] ] worker 0 thread 4 initing device 47 solved

[2022-12-12 00:50:53.254523: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 7 initing device 7
[2022-12-12 00:50:53.254911: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-12 00:50:53.[2549462022-12-12 00:50:53: .E254955 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE: 1815/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu] :Building Coll Cache with ... num gpu device is 81980
] eager alloc mem 381.47 MB
[2022-12-12 00:50:53.255024: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 00:50:53.256754: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] [6 solved2022-12-12 00:50:53
.256780: E[ 2022-12-12 00:50:53/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc.:256831202: ] E5 solved 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 6 initing device 6[
2022-12-12 00:50:53.256887: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 5 initing device 5
[2022-12-12 00:50:53.257145: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[[2022-12-12 00:50:532022-12-12 00:50:53..257275257287: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::19801815[] ] 2022-12-12 00:50:53eager alloc mem 381.47 MBBuilding Coll Cache with ... num gpu device is 8.

257323: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8[
2022-12-12 00:50:53.257374: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980[] [2022-12-12 00:50:53eager alloc mem 381.47 MB2022-12-12 00:50:53.
.257392257403: : E[E 2022-12-12 00:50:53 /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu./hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:257441:1980: 1980] E] eager alloc mem 381.47 MB eager alloc mem 381.47 MB
/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu
:1980] eager alloc mem 381.47 MB
[2022-12-12 00:50:53.257945: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 00:50:53.261718: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 00:50:53.261826: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 00:50:53.261905: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 00:50:53.261937: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 00:50:53.261994: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 00:50:53.262075: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 00:50:53.262113: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 00:50:53.266114: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 00:50:53.266271: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 00:50:53.317387: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1024.00 Bytes
[2022-12-12 00:50:53.322919: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-12 00:50:53.323055: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 00:50:53.323903: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 15.64 MB
[2022-12-12 00:50:53.324501: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:50:53.325504: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:50:53.325549: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 15.22 MB
[2022-12-12 00:50:53.328795: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 00:50:53.329566: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 00:50:53.329618: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1.91 GB
[2022-12-12 00:50:53.342387: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1024.00 Bytes
[2022-12-12 00:50:53.342517: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1024.00 Bytes
[[[[2022-12-12 00:50:532022-12-12 00:50:532022-12-12 00:50:532022-12-12 00:50:53....342572342572342576342580: : : : EEEE    /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::::1980198019801980] ] ] ] eager alloc mem 1024.00 Byteseager alloc mem 1024.00 Byteseager alloc mem 1024.00 Byteseager alloc mem 1024.00 Bytes



[2022-12-12 00:50:53.343389: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1024.00 Bytes
[2022-12-12 00:50:53.347249: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-12 00:50:53.347334: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 00:50:53.348424: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 15.64 MB
[2022-12-12 00:50:53.348898: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:50:53.348919: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[[2022-12-12 00:50:532022-12-12 00:50:53..348994349015: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::638638] ] eager release cuda mem 1024eager release cuda mem 400000000

[2022-12-12 00:50:53.349097: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 00:50:53.349180: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-12 00:50:53.[3492652022-12-12 00:50:53: .E349256 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccE: 638/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc] :eager release cuda mem 400000000638
] eager release cuda mem 1024
[2022-12-12 00:50:53[.2022-12-12 00:50:53349334.: 349357E:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc :/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc638:] 638eager release cuda mem 1024] 
eager release cuda mem 400000000
[2022-12-12 00:50:53.349442: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 00:50:53.349494: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-12 00:50:53.349571: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 00:50:53.349858: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:50:53.349902: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] [WORKER[0] alloc host memory 15.21 MB2022-12-12 00:50:53
.349916: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 15.64 MB
[2022-12-12 00:50:53.350470: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 15.64 MB
[2022-12-12 00:50:53.351671: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 15.64 MB
[2022-12-12 00:50:53.352238: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 15.64 MB
[2022-12-12 00:50:53.352750: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 15.64 MB
[2022-12-12 00:50:53.353273: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 15.64 MB
[2022-12-12 00:50:53.353805: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:50:53.353917: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:50:53.354391: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:50:53.354431: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:50:53.354484: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:50:53.354517: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:50:53.354773: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:50:53.354816: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 15.22 MB
[2022-12-12 00:50:53.354880: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:50:53.354924: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 15.20 MB
[2022-12-12 00:50:53.355368: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:50:53.355396: E [/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc2022-12-12 00:50:53:.638355412] : eager release cuda mem 625663W
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 15.24 MB
[2022-12-12 00:50:53.355451: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] [WORKER[0] alloc host memory 15.22 MB2022-12-12 00:50:53
.355468: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:50:53.355503: E[ 2022-12-12 00:50:53/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc.:355517638: ] Weager release cuda mem 625663 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 15.24 MB
[2022-12-12 00:50:53.355562: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 15.22 MB
[2022-12-12 00:50:53.360174: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 00:50:53.360780: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 00:50:53.360825: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1.91 GB
[2022-12-12 00:50:53.365865: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 00:50:53.366036: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 00:50:53.366366: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 00:50:53.366465: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 00:50:53.366505: [E2022-12-12 00:50:53 ./hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu366501:: 1980E]  eager alloc mem 1.91 GB/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu
:1980] eager alloc mem 25.25 KB
[2022-12-12 00:50:53.366637: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 00:50:53.366690: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1.90 GB
[2022-12-12 00:50:53.366715: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 00:50:53.366967: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 00:50:53.367010: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1.91 GB
[2022-12-12 00:50:53.367122: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 00:50:53.367177: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1.91 GB
[2022-12-12 00:50:53.367325: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 00:50:53.367368: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1.91 GB
[2022-12-12 00:50:53.367409: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 00:50:53.368013: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 00:50:53.368054: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1.91 GB
[[[[[[[[2022-12-12 00:50:532022-12-12 00:50:532022-12-12 00:50:532022-12-12 00:50:532022-12-12 00:50:532022-12-12 00:50:532022-12-12 00:50:532022-12-12 00:50:53........972962972962972962972963972962972963972963972963: : : : : : : : EEEEEEEE        /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::::::::19261926192619261926192619261926] ] ] ] ] ] ] ] Device 7 init p2p of link 4Device 2 init p2p of link 1Device 1 init p2p of link 7Device 3 init p2p of link 2Device 6 init p2p of link 0Device 5 init p2p of link 6Device 4 init p2p of link 5Device 0 init p2p of link 3







[[[[[[2022-12-12 00:50:532022-12-12 00:50:532022-12-12 00:50:53[2022-12-12 00:50:532022-12-12 00:50:532022-12-12 00:50:53...[2022-12-12 00:50:53...9734679734689734672022-12-12 00:50:53.973467973470973472: : : .973480: : E: EEE973492: E E   : E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::: /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980:198019801980/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] 1980] ] ] :1980] eager alloc mem 611.00 KB] eager alloc mem 611.00 KBeager alloc mem 611.00 KBeager alloc mem 611.00 KB1980] eager alloc mem 611.00 KB
eager alloc mem 611.00 KB


] eager alloc mem 611.00 KB

eager alloc mem 611.00 KB

[2022-12-12 00:50:53.[974491[2022-12-12 00:50:53: 2022-12-12 00:50:53[.E.[2022-12-12 00:50:53974499[ 974501[2022-12-12 00:50:53.: 2022-12-12 00:50:53/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc[: 2022-12-12 00:50:53.974507E.:2022-12-12 00:50:53E.974513:  974518638. 974523: E/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc: ] 974532/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc: E :Eeager release cuda mem 625663: :E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc638 
E638 /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:] /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638eager release cuda mem 625663:/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cceager release cuda mem 625663:638] 
638:
638] eager release cuda mem 625663] 638] eager release cuda mem 625663
eager release cuda mem 625663] eager release cuda mem 625663

eager release cuda mem 625663

[2022-12-12 00:50:53.987790: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 2
[2022-12-12 00:50:53.987932: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:50:53.988466: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 2 init p2p of link 3
[2022-12-12 00:50:53.988618: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:50:53.988644: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 7 init p2p of link 1
[2022-12-12 00:50:53.988744: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:50:53.988787: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:50:53.988816: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 3 init p2p of link 0
[2022-12-12 00:50:53.988919: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 6 init p2p of link 5
[2022-12-12 00:50:53.988968: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:50:53.989069: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:50:53.989304: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 0 init p2p of link 6
[2022-12-12 00:50:53.989381: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 4 init p2p of link 7
[2022-12-12 00:50:53.989420: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:50:53.989446: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:50:53.989523: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:50:53.989584: E [/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc2022-12-12 00:50:53:.638989587] : eager release cuda mem 625663E
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 5 init p2p of link 4
[2022-12-12 00:50:53.989759: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] [eager alloc mem 611.00 KB2022-12-12 00:50:53
.989776: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:50:53.989879: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:50:53.990239: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:50:53.990333: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:50:53.990573: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:50:54.   139: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 3
[2022-12-12 00:50:54.   253: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:50:54.  1052: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:50:54.  1825: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 2 init p2p of link 0
[2022-12-12 00:50:54.  1940: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:50:54.  2043: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 7 init p2p of link 6
[2022-12-12 00:50:54.  2155: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:50:54.  2499: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 3 init p2p of link 5
[[2022-12-12 00:50:542022-12-12 00:50:54..  2606  2620: : EE[  2022-12-12 00:50:54/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu.::  263119261980: ] ] EDevice 6 init p2p of link 4eager alloc mem 611.00 KB 

/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 0 init p2p of link 1
[2022-12-12 00:50:54.  2739: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] [eager release cuda mem 6256632022-12-12 00:50:54
.  2760: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980[] 2022-12-12 00:50:54eager alloc mem 611.00 KB.
  2781: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:50:54.  2971: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:50:54.  3025: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 4 init p2p of link 2
[2022-12-12 00:50:54.  3158: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:50:54.  3330: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 5 init p2p of link 7
[[2022-12-12 00:50:542022-12-12 00:50:54..  3447  3448: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::6381980] ] eager release cuda mem 625663eager alloc mem 611.00 KB

[2022-12-12 00:50:54.  3577: E [/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc2022-12-12 00:50:54:.638  3587] : eager release cuda mem 625663E
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:50:54.  3946: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:50:54.  4315: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:50:54. 16034: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 0
[2022-12-12 00:50:54. 16149: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:50:54. 16943: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:50:54. 18126: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 3 init p2p of link 1
[2022-12-12 00:50:54. 18239: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:50:54. 18297: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 2 init p2p of link 4
[2022-12-12 00:50:54. 18411: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:50:54. 18436: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 7 init p2p of link 5
[2022-12-12 00:50:54. 18547: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:50:54. 18979: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 4 init p2p of link 6
[2022-12-12 00:50:54. 19037: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:50:54. 19102: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:50:54. 19204: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:50:54. 19225: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 6 init p2p of link 7
[2022-12-12 00:50:54. 19339: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB[
2022-12-12 00:50:54. 19360: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:50:54. 19581: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 0 init p2p of link 2
[2022-12-12 00:50:54. 19697: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:50:54. 19915: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:50:54.[ 201392022-12-12 00:50:54: .E 20136 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccE: 638/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu] :eager release cuda mem 6256631926
] Device 5 init p2p of link 3
[2022-12-12 00:50:54. 20300: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:50:54. 20487: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:50:54. 21080: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:50:54. 30480: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 16399996
[2022-12-12 00:50:54. 33288: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 16399996
[2022-12-12 00:50:54. 33365: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 3990905 / 100000000 nodes ( 3.99 %~4.00 %) | remote 11860842 / 100000000 nodes ( 11.86 %) | cpu 84148253 / 100000000 nodes ( 84.15 %) | 1.91 GB | 0.779015 secs 
[2022-12-12 00:50:54. 34194: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 16399996
[2022-12-12 00:50:54. 34291: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 3984569 / 100000000 nodes ( 3.98 %~4.00 %) | remote 11867178 / 100000000 nodes ( 11.87 %) | cpu 84148253 / 100000000 nodes ( 84.15 %) | 1.90 GB | 0.779937 secs 
[2022-12-12 00:50:54. 34532: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 3994000 / 100000000 nodes ( 3.99 %~4.00 %) | remote 11857747 / 100000000 nodes ( 11.86 %) | cpu 84148253 / 100000000 nodes ( 84.15 %) | 1.91 GB | 0.780554 secs 
[2022-12-12 00:50:54. 34894: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 16399996
[2022-12-12 00:50:54. 35144: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 16399996
[2022-12-12 00:50:54. 35219: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 16399996
[2022-12-12 00:50:54. 35322: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 16399996
[2022-12-12 00:50:54. 35624: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 16399996
[2022-12-12 00:50:54. 35677: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 3990055 / 100000000 nodes ( 3.99 %~4.00 %) | remote 11861692 / 100000000 nodes ( 11.86 %) | cpu 84148253 / 100000000 nodes ( 84.15 %) | 1.91 GB | 0.780657 secs 
[2022-12-12 00:50:54. 36825: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 3990730 / 100000000 nodes ( 3.99 %~4.00 %) | remote 11861017 / 100000000 nodes ( 11.86 %) | cpu 84148253 / 100000000 nodes ( 84.15 %) | 1.91 GB | 0.781877 secs 
[2022-12-12 00:50:54. 37923: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 3987425 / 100000000 nodes ( 3.99 %~4.00 %) | remote 11864322 / 100000000 nodes ( 11.86 %) | cpu 84148253 / 100000000 nodes ( 84.15 %) | 1.91 GB | 0.780529 secs 
[2022-12-12 00:50:54. 38213: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 3995767 / 100000000 nodes ( 4.00 %~4.00 %) | remote 11855980 / 100000000 nodes ( 11.86 %) | cpu 84148253 / 100000000 nodes ( 84.15 %) | 1.91 GB | 0.780845 secs 
[2022-12-12 00:50:54. 39237: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 3989047 / 100000000 nodes ( 3.99 %~4.00 %) | remote 11862700 / 100000000 nodes ( 11.86 %) | cpu 84148253 / 100000000 nodes ( 84.15 %) | 1.91 GB | 0.800481 secs 
[2022-12-12 00:50:54. 41550: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 10.23 GB
[2022-12-12 00:50:55.346355: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 10.49 GB
[2022-12-12 00:50:55.346683: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 10.49 GB
[2022-12-12 00:50:55.347058: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 10.49 GB
[2022-12-12 00:50:56.814239: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 10.75 GB
[2022-12-12 00:50:56.814672: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 10.75 GB
[2022-12-12 00:50:56.816850: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 10.75 GB
[2022-12-12 00:50:57.918575: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 10.97 GB
[2022-12-12 00:50:57.918744: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 10.97 GB
[2022-12-12 00:50:57.919178: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 10.97 GB
[2022-12-12 00:50:59.234395: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 11.18 GB
[2022-12-12 00:50:59.234655: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 11.18 GB
[2022-12-12 00:50:59.235090: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 11.18 GB
[2022-12-12 00:51:00.527919: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 11.64 GB
[2022-12-12 00:51:00.528720: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 11.64 GB
[2022-12-12 00:51:00.529754: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 11.64 GB
[2022-12-12 00:51:01.856412: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 11.84 GB
[2022-12-12 00:51:01.857534: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 11.84 GB
[HCTR][00:51:02.866][ERROR][RK0][main]: replica 6 calling init per replica done, doing barrier
[HCTR][00:51:02.866][ERROR][RK0][tid #140507162339072]: replica 3 calling init per replica done, doing barrier
[HCTR][00:51:02.866][ERROR][RK0][tid #140506365425408]: replica 7 calling init per replica done, doing barrier
[HCTR][00:51:02.866][ERROR][RK0][tid #140506365425408]: replica 5 calling init per replica done, doing barrier
[HCTR][00:51:02.866][ERROR][RK0][tid #140506298316544]: replica 0 calling init per replica done, doing barrier
[HCTR][00:51:02.866][ERROR][RK0][tid #140506365425408]: replica 2 calling init per replica done, doing barrier
[HCTR][00:51:02.866][ERROR][RK0][tid #140506298316544]: replica 4 calling init per replica done, doing barrier
[HCTR][00:51:02.866][ERROR][RK0][main]: replica 1 calling init per replica done, doing barrier
[HCTR][00:51:02.866][ERROR][RK0][main]: replica 1 calling init per replica done, doing barrier done
[HCTR][00:51:02.866][ERROR][RK0][tid #140506298316544]: replica 4 calling init per replica done, doing barrier done
[HCTR][00:51:02.866][ERROR][RK0][main]: replica 6 calling init per replica done, doing barrier done
[HCTR][00:51:02.866][ERROR][RK0][tid #140506298316544]: replica 0 calling init per replica done, doing barrier done
[HCTR][00:51:02.866][ERROR][RK0][tid #140507162339072]: replica 3 calling init per replica done, doing barrier done
[HCTR][00:51:02.866][ERROR][RK0][tid #140506365425408]: replica 5 calling init per replica done, doing barrier done
[HCTR][00:51:02.866][ERROR][RK0][tid #140506298316544]: init per replica done
[HCTR][00:51:02.866][ERROR][RK0][tid #140506365425408]: replica 7 calling init per replica done, doing barrier done
[HCTR][00:51:02.866][ERROR][RK0][tid #140506365425408]: replica 2 calling init per replica done, doing barrier done
[HCTR][00:51:02.866][ERROR][RK0][main]: init per replica done
[HCTR][00:51:02.866][ERROR][RK0][main]: init per replica done
[HCTR][00:51:02.866][ERROR][RK0][tid #140507162339072]: init per replica done
[HCTR][00:51:02.866][ERROR][RK0][tid #140506365425408]: init per replica done
[HCTR][00:51:02.866][ERROR][RK0][tid #140506365425408]: init per replica done
[HCTR][00:51:02.866][ERROR][RK0][tid #140506365425408]: init per replica done
[HCTR][00:51:02.869][ERROR][RK0][tid #140506298316544]: init per replica done
[HCTR][00:51:02.905][ERROR][RK0][tid #140506365425408]: 7 allocated 3276800 at 0x7fae3c238400
[HCTR][00:51:02.905][ERROR][RK0][tid #140506365425408]: 7 allocated 6553600 at 0x7fae3c558400
[HCTR][00:51:02.905][ERROR][RK0][tid #140506365425408]: 7 allocated 3276800 at 0x7fae3cb98400
[HCTR][00:51:02.905][ERROR][RK0][tid #140506365425408]: 7 allocated 6553600 at 0x7fae3ceb8400
[HCTR][00:51:02.905][ERROR][RK0][tid #140506365425408]: 2 allocated 3276800 at 0x7fad60238400
[HCTR][00:51:02.905][ERROR][RK0][tid #140506365425408]: 2 allocated 6553600 at 0x7fad60558400
[HCTR][00:51:02.905][ERROR][RK0][tid #140506365425408]: 2 allocated 3276800 at 0x7fad60b98400
[HCTR][00:51:02.905][ERROR][RK0][tid #140506365425408]: 2 allocated 6553600 at 0x7fad60eb8400
[HCTR][00:51:02.905][ERROR][RK0][tid #140506365425408]: 5 allocated 3276800 at 0x7fadac238400
[HCTR][00:51:02.905][ERROR][RK0][tid #140506365425408]: 5 allocated 6553600 at 0x7fadac558400
[HCTR][00:51:02.905][ERROR][RK0][tid #140506365425408]: 5 allocated 3276800 at 0x7fadacb98400
[HCTR][00:51:02.905][ERROR][RK0][tid #140506365425408]: 5 allocated 6553600 at 0x7fadaceb8400
[HCTR][00:51:02.905][ERROR][RK0][main]: 1 allocated 3276800 at 0x7fae7c238400
[HCTR][00:51:02.905][ERROR][RK0][main]: 1 allocated 6553600 at 0x7fae7c558400
[HCTR][00:51:02.905][ERROR][RK0][main]: 1 allocated 3276800 at 0x7fae7cb98400
[HCTR][00:51:02.905][ERROR][RK0][main]: 1 allocated 6553600 at 0x7fae7ceb8400
[HCTR][00:51:02.905][ERROR][RK0][main]: 6 allocated 3276800 at 0x7fae2c238400
[HCTR][00:51:02.905][ERROR][RK0][main]: 6 allocated 6553600 at 0x7fae2c558400
[HCTR][00:51:02.905][ERROR][RK0][main]: 6 allocated 3276800 at 0x7fae2cb98400
[HCTR][00:51:02.905][ERROR][RK0][main]: 6 allocated 6553600 at 0x7fae2ceb8400
[HCTR][00:51:02.905][ERROR][RK0][tid #140506298316544]: 4 allocated 3276800 at 0x7fae7c238400
[HCTR][00:51:02.905][ERROR][RK0][tid #140506298316544]: 4 allocated 6553600 at 0x7fae7c558400
[HCTR][00:51:02.905][ERROR][RK0][main]: 3 allocated 3276800 at 0x7fadec238400
[HCTR][00:51:02.905][ERROR][RK0][tid #140506298316544]: 4 allocated 3276800 at 0x7fae7cb98400
[HCTR][00:51:02.905][ERROR][RK0][main]: 3 allocated 6553600 at 0x7fadec558400
[HCTR][00:51:02.905][ERROR][RK0][tid #140506298316544]: 4 allocated 6553600 at 0x7fae7ceb8400
[HCTR][00:51:02.905][ERROR][RK0][main]: 3 allocated 3276800 at 0x7fadecb98400
[HCTR][00:51:02.905][ERROR][RK0][main]: 3 allocated 6553600 at 0x7fadeceb8400
[HCTR][00:51:02.908][ERROR][RK0][tid #140506298316544]: 0 allocated 3276800 at 0x7fae4e320000
[HCTR][00:51:02.908][ERROR][RK0][tid #140506298316544]: 0 allocated 6553600 at 0x7fae4e640000
[HCTR][00:51:02.908][ERROR][RK0][tid #140506298316544]: 0 allocated 3276800 at 0x7fae4ec80000
[HCTR][00:51:02.908][ERROR][RK0][tid #140506298316544]: 0 allocated 6553600 at 0x7fae4efa0000
