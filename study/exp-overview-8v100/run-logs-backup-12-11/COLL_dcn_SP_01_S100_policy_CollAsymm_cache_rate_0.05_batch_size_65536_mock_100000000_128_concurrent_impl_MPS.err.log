2022-12-12 01:58:27.451616: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.452876: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.454032: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.455155: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.456238: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.457312: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.458320: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.459333: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.463959: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.465183: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.466142: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.467440: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.469399: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.469996: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.470945: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.472224: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.473117: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.474127: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.474149: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.475016: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.476351: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.476352: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.478226: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.478261: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.479888: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.479939: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.481538: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.481593: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.483302: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.483358: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.485279: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.487189: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.487951: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.490002: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.490328: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.491628: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.492669: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.493937: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.494565: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.494586: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.496094: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.496727: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.496861: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.498083: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.498710: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.499049: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.500299: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.500897: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.501368: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.501937: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.502287: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.502918: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.503403: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.504225: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.504990: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.505166: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.505310: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.506168: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.507285: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.507436: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.508089: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.508783: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.509095: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.509736: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.510296: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.511223: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.511450: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.512326: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.512448: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.513465: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.513507: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.514596: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.515120: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.515656: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.516179: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.516686: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.517194: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.517696: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.518198: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.518697: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.520035: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.520640: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.521197: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.521710: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.521726: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.522851: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.522893: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.523988: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.523998: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.525102: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.525119: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.526221: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.526227: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.527357: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.527872: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.528377: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.528866: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.529380: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.529617: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.530091: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.530630: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.530920: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.531618: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.531631: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 01:58:27.531809: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.532517: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.532712: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.533423: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.533456: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.533626: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.535010: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.535112: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.535220: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.536670: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.536807: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.537574: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.537598: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.537783: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.538997: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.539176: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.539235: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.540328: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.540658: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.540701: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.540846: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.541892: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.542575: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.542586: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.542757: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.543915: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.544469: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.544480: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.544759: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.545796: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.546363: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.546448: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.547692: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.548293: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.548361: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.548842: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.549792: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.550647: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.550804: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.551514: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.551513: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.552682: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.554031: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.554514: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.554671: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.554737: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.556347: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.556755: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.557401: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.557589: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.557696: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.558091: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 01:58:27.558427: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.559711: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.559946: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.561033: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.561104: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.561384: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.562477: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.564024: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.564135: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.565192: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.565442: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.565617: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.566671: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.567116: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.568006: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.568183: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.569445: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.569688: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.569928: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.571279: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.571804: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.572612: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.573444: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.573748: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.574047: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.574265: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.576325: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.576907: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.577875: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.579049: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.579519: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.583719: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.584063: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.585612: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.588564: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.589734: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.590399: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.590641: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.590996: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.592614: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.595577: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.595662: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.596250: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.596489: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.596710: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.598212: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.600387: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.600940: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.601292: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.601387: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.603793: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 01:58:27.612600: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.636346: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.638604: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.639215: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.639891: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.640391: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.640540: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.641437: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.645024: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.645668: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.645971: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.646076: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.646225: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.647718: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.650950: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.651556: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.651856: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.652074: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.653075: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.656700: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.656986: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.657236: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.657988: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.659879: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 01:58:27.661393: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.661538: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.662068: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.662606: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.666022: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.666726: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.667053: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.667073: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.668726: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.671326: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.671711: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.673005: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.673310: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.675311: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.677252: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.678256: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.678312: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.678491: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.679720: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.682264: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.683172: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.683349: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.683580: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.716196: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.717426: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.717971: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.718023: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.722666: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.723428: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.723951: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.724036: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.729072: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.730769: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.731152: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.731202: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.736794: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.737875: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.737915: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.740976: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 01:58:27.741937: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.743182: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.746601: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 01:58:27.750143: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.750573: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 01:58:27.755624: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.758667: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.776617: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.781751: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.781806: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.781825: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.786310: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 01:58:27.786589: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.786706: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.786740: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.795016: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.808013: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:27.815631: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:28.976931: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:28.977557: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:28.978296: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:28.978761: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 01:58:28.978817: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-12 01:58:28.998474: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:28.999560: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:29.002219: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:29.004088: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:29.005251: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:29.006559: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-12 01:58:29.052561: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 01:58:29.052752: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 01:58:29.056237: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:29.056827: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:29.057363: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:29.058203: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 01:58:29.058263: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-12 01:58:29.076722: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:29.077477: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:29.078376: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:29.078949: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:29.079494: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:29.079987: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:1/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-12 01:58:29.103279: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:29.104118: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:29.104807: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:29.106456: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 01:58:29.106518: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-12 01:58:29.107823: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12340
2022-12-12 01:58:29.124483: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:29.125693: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:29.128417: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:29.128486: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:29.130146: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:29.130214: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:29.132088: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:29.132158: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:29.133775: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:3/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-12 01:58:29.133813: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 01:58:29.133862: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-12 01:58:29.152217: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:29.153476: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:29.154931: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:29.156291: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:29.157313: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:29.158610: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:2/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-12 01:58:29.166734: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 01:58:29.166917: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 01:58:29.171645: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12341
2022-12-12 01:58:29.176948: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 01:58:29.177106: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 01:58:29.179122: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12343
2022-12-12 01:58:29.209920: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:29.210539: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:29.211104: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:29.211303: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:29.212205: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 01:58:29.212273: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-12 01:58:29.212454: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:29.213092: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:29.213556: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 01:58:29.213601: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-12 01:58:29.215282: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:29.216461: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:29.218290: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:29.219505: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 01:58:29.219558: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-12 01:58:29.222604: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:29.223901: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:29.225011: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:29.226086: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 01:58:29.226141: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-12 01:58:29.229265: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:29.230421: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:29.230900: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:29.231905: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:29.233316: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:29.234390: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:29.234727: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:29.235998: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:29.236439: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:29.237501: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:4/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-12 01:58:29.237990: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:29.238445: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:29.239333: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:6/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-12 01:58:29.239997: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:29.241075: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:29.242276: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:29.243692: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:29.243935: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:29.245365: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:29.245484: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:5/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-12 01:58:29.246134: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 01:58:29.246315: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 01:58:29.246508: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:29.247724: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:29.248021: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12342
2022-12-12 01:58:29.249441: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:58:29.251062: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:7/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-12 01:58:29.281670: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 01:58:29.281862: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 01:58:29.282881: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 01:58:29.283024: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 01:58:29.284058: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12344
2022-12-12 01:58:29.285175: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12346
2022-12-12 01:58:29.291026: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 01:58:29.291189: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 01:58:29.293036: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12345
2022-12-12 01:58:29.295864: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 01:58:29.296011: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 01:58:29.297844: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12347
[HCTR][01:58:30.557][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][01:58:30.558][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][01:58:30.558][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][01:58:30.558][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][01:58:30.560][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][01:58:30.562][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][01:58:30.562][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][01:58:30.562][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 1it [00:01,  1.54s/it]warmup run: 1it [00:01,  1.54s/it]warmup run: 102it [00:01, 86.03it/s]warmup run: 95it [00:01, 80.51it/s]warmup run: 204it [00:01, 186.70it/s]warmup run: 189it [00:01, 173.53it/s]warmup run: 1it [00:01,  1.56s/it]warmup run: 1it [00:01,  1.55s/it]warmup run: 306it [00:01, 297.89it/s]warmup run: 1it [00:01,  1.59s/it]warmup run: 1it [00:01,  1.60s/it]warmup run: 284it [00:01, 277.41it/s]warmup run: 1it [00:01,  1.61s/it]warmup run: 97it [00:01, 81.17it/s]warmup run: 1it [00:01,  1.56s/it]warmup run: 103it [00:01, 86.54it/s]warmup run: 409it [00:01, 414.46it/s]warmup run: 98it [00:01, 80.36it/s]warmup run: 94it [00:01, 76.73it/s]warmup run: 380it [00:01, 386.41it/s]warmup run: 94it [00:01, 76.52it/s]warmup run: 195it [00:01, 177.40it/s]warmup run: 98it [00:01, 82.06it/s]warmup run: 205it [00:01, 186.72it/s]warmup run: 510it [00:02, 525.01it/s]warmup run: 198it [00:01, 177.06it/s]warmup run: 188it [00:01, 167.10it/s]warmup run: 477it [00:02, 494.61it/s]warmup run: 189it [00:01, 167.59it/s]warmup run: 294it [00:01, 284.99it/s]warmup run: 198it [00:01, 180.36it/s]warmup run: 305it [00:01, 294.69it/s]warmup run: 613it [00:02, 630.56it/s]warmup run: 300it [00:01, 287.05it/s]warmup run: 283it [00:01, 268.71it/s]warmup run: 574it [00:02, 594.16it/s]warmup run: 286it [00:01, 271.59it/s]warmup run: 299it [00:01, 290.23it/s]warmup run: 394it [00:01, 398.24it/s]warmup run: 404it [00:01, 405.23it/s]warmup run: 717it [00:02, 723.69it/s]warmup run: 403it [00:01, 403.36it/s]warmup run: 380it [00:02, 378.06it/s]warmup run: 673it [00:02, 684.70it/s]warmup run: 386it [00:02, 384.85it/s]warmup run: 400it [00:01, 404.85it/s]warmup run: 485it [00:02, 492.32it/s]warmup run: 507it [00:02, 520.60it/s]warmup run: 821it [00:02, 800.25it/s]warmup run: 505it [00:02, 516.06it/s]warmup run: 478it [00:02, 487.53it/s]warmup run: 771it [00:02, 757.52it/s]warmup run: 500it [00:02, 514.95it/s]warmup run: 584it [00:02, 595.65it/s]warmup run: 475it [00:02, 462.39it/s]warmup run: 611it [00:02, 629.07it/s]warmup run: 924it [00:02, 860.06it/s]warmup run: 610it [00:02, 625.90it/s]warmup run: 575it [00:02, 587.22it/s]warmup run: 870it [00:02, 818.08it/s]warmup run: 602it [00:02, 620.57it/s]warmup run: 684it [00:02, 687.02it/s]warmup run: 564it [00:02, 547.71it/s]warmup run: 715it [00:02, 722.65it/s]warmup run: 1026it [00:02, 903.19it/s]warmup run: 714it [00:02, 719.39it/s]warmup run: 672it [00:02, 674.32it/s]warmup run: 969it [00:02, 863.55it/s]warmup run: 704it [00:02, 711.83it/s]warmup run: 783it [00:02, 760.24it/s]warmup run: 655it [00:02, 628.32it/s]warmup run: 817it [00:02, 795.69it/s]warmup run: 1128it [00:02, 935.73it/s]warmup run: 817it [00:02, 794.84it/s]warmup run: 769it [00:02, 745.89it/s]warmup run: 1068it [00:02, 898.72it/s]warmup run: 806it [00:02, 786.88it/s]warmup run: 881it [00:02, 816.40it/s]warmup run: 750it [00:02, 705.25it/s]warmup run: 919it [00:02, 853.69it/s]warmup run: 1233it [00:02, 966.26it/s]warmup run: 920it [00:02, 853.95it/s]warmup run: 866it [00:02, 802.29it/s]warmup run: 1168it [00:02, 927.03it/s]warmup run: 908it [00:02, 846.06it/s]warmup run: 978it [00:02, 854.03it/s]warmup run: 846it [00:02, 768.55it/s]warmup run: 1022it [00:02, 899.95it/s]warmup run: 1336it [00:02, 983.99it/s]warmup run: 1022it [00:02, 896.36it/s]warmup run: 962it [00:02, 844.06it/s]warmup run: 1270it [00:02, 952.56it/s]warmup run: 1010it [00:02, 891.45it/s]warmup run: 1076it [00:02, 888.50it/s]warmup run: 944it [00:02, 825.04it/s]warmup run: 1126it [00:02, 937.38it/s]warmup run: 1440it [00:02, 997.80it/s]warmup run: 1124it [00:02, 929.80it/s]warmup run: 1059it [00:02, 876.95it/s]warmup run: 1374it [00:02, 975.66it/s]warmup run: 1113it [00:02, 928.83it/s]warmup run: 1174it [00:02, 914.25it/s]warmup run: 1042it [00:02, 867.36it/s]warmup run: 1229it [00:02, 963.01it/s]warmup run: 1545it [00:03, 1010.76it/s]warmup run: 1226it [00:02, 955.23it/s]warmup run: 1157it [00:02, 904.00it/s]warmup run: 1479it [00:03, 994.98it/s]warmup run: 1217it [00:02, 958.37it/s]warmup run: 1271it [00:02, 929.20it/s]warmup run: 1143it [00:02, 905.28it/s]warmup run: 1332it [00:02, 979.41it/s]warmup run: 1649it [00:03, 1018.05it/s]warmup run: 1328it [00:02, 971.45it/s]warmup run: 1257it [00:02, 929.78it/s]warmup run: 1583it [00:03, 1005.63it/s]warmup run: 1369it [00:02, 941.99it/s]warmup run: 1319it [00:02, 966.19it/s]warmup run: 1242it [00:02, 927.29it/s]warmup run: 1435it [00:02, 992.16it/s]warmup run: 1754it [00:03, 1025.21it/s]warmup run: 1432it [00:03, 988.79it/s]warmup run: 1356it [00:03, 947.08it/s]warmup run: 1688it [00:03, 1016.56it/s]warmup run: 1469it [00:03, 957.39it/s]warmup run: 1420it [00:02, 972.38it/s]warmup run: 1339it [00:03, 934.26it/s]warmup run: 1538it [00:03, 1001.17it/s]warmup run: 1859it [00:03, 1030.01it/s]warmup run: 1536it [00:03, 1001.79it/s]warmup run: 1455it [00:03, 959.21it/s]warmup run: 1792it [00:03, 1021.23it/s]warmup run: 1568it [00:03, 964.16it/s]warmup run: 1521it [00:03, 974.38it/s]warmup run: 1437it [00:03, 946.61it/s]warmup run: 1641it [00:03, 1002.61it/s]warmup run: 1963it [00:03, 1030.57it/s]warmup run: 1553it [00:03, 963.98it/s]warmup run: 1639it [00:03, 1007.69it/s]warmup run: 1896it [00:03, 1026.67it/s]warmup run: 1666it [00:03, 964.50it/s]warmup run: 1621it [00:03, 975.37it/s]warmup run: 1538it [00:03, 962.59it/s]warmup run: 1743it [00:03, 990.20it/s] warmup run: 2077it [00:03, 1061.95it/s]warmup run: 1651it [00:03, 967.83it/s]warmup run: 1742it [00:03, 1013.64it/s]warmup run: 2001it [00:03, 1031.39it/s]warmup run: 1764it [00:03, 966.71it/s]warmup run: 1720it [00:03, 972.80it/s]warmup run: 1639it [00:03, 975.11it/s]warmup run: 1844it [00:03, 989.06it/s]warmup run: 2198it [00:03, 1104.40it/s]warmup run: 1847it [00:03, 1024.20it/s]warmup run: 1750it [00:03, 971.99it/s]warmup run: 2124it [00:03, 1088.67it/s]warmup run: 1863it [00:03, 971.57it/s]warmup run: 1819it [00:03, 969.01it/s]warmup run: 1742it [00:03, 988.61it/s]warmup run: 1944it [00:03, 991.14it/s]warmup run: 2319it [00:03, 1135.02it/s]warmup run: 1953it [00:03, 1032.74it/s]warmup run: 1848it [00:03, 972.04it/s]warmup run: 2247it [00:03, 1129.43it/s]warmup run: 1961it [00:03, 966.08it/s]warmup run: 1917it [00:03, 967.76it/s]warmup run: 1844it [00:03, 997.30it/s]warmup run: 2052it [00:03, 1015.64it/s]warmup run: 2440it [00:03, 1156.86it/s]warmup run: 2066it [00:03, 1061.53it/s]warmup run: 1946it [00:03, 972.73it/s]warmup run: 2370it [00:03, 1159.30it/s]warmup run: 2073it [00:03, 1011.33it/s]warmup run: 2018it [00:03, 979.72it/s]warmup run: 1946it [00:03, 1002.36it/s]warmup run: 2173it [00:03, 1071.12it/s]warmup run: 2561it [00:03, 1172.68it/s]warmup run: 2188it [00:03, 1107.06it/s]warmup run: 2051it [00:03, 995.38it/s]warmup run: 2494it [00:03, 1180.63it/s]warmup run: 2196it [00:03, 1074.56it/s]warmup run: 2136it [00:03, 1036.71it/s]warmup run: 2057it [00:03, 1033.52it/s]warmup run: 2294it [00:03, 1110.68it/s]warmup run: 2683it [00:04, 1184.56it/s]warmup run: 2308it [00:03, 1133.66it/s]warmup run: 2168it [00:03, 1046.11it/s]warmup run: 2617it [00:04, 1195.10it/s]warmup run: 2319it [00:03, 1119.80it/s]warmup run: 2254it [00:03, 1077.91it/s]warmup run: 2177it [00:03, 1080.79it/s]warmup run: 2413it [00:03, 1132.43it/s]warmup run: 2803it [00:04, 1187.46it/s]warmup run: 2428it [00:03, 1151.87it/s]warmup run: 2284it [00:03, 1077.40it/s]warmup run: 2741it [00:04, 1205.86it/s]warmup run: 2442it [00:03, 1150.09it/s]warmup run: 2373it [00:03, 1110.88it/s]warmup run: 2297it [00:03, 1114.26it/s]warmup run: 2533it [00:03, 1151.45it/s]warmup run: 2924it [00:04, 1193.32it/s]warmup run: 2548it [00:04, 1165.17it/s]warmup run: 2400it [00:04, 1101.30it/s]warmup run: 2863it [00:04, 1209.14it/s]warmup run: 2563it [00:04, 1166.72it/s]warmup run: 3000it [00:04, 693.40it/s] warmup run: 2490it [00:03, 1127.33it/s]warmup run: 2415it [00:04, 1132.71it/s]warmup run: 2653it [00:04, 1164.62it/s]warmup run: 2668it [00:04, 1173.29it/s]warmup run: 2516it [00:04, 1117.93it/s]warmup run: 2986it [00:04, 1214.50it/s]warmup run: 3000it [00:04, 687.00it/s] warmup run: 2684it [00:04, 1177.07it/s]warmup run: 2608it [00:04, 1142.95it/s]warmup run: 2536it [00:04, 1154.45it/s]warmup run: 2772it [00:04, 1172.00it/s]warmup run: 2633it [00:04, 1132.76it/s]warmup run: 2786it [00:04, 1170.84it/s]warmup run: 2805it [00:04, 1186.15it/s]warmup run: 2727it [00:04, 1155.54it/s]warmup run: 2658it [00:04, 1172.08it/s]warmup run: 2893it [00:04, 1181.25it/s]warmup run: 2905it [00:04, 1176.29it/s]warmup run: 2751it [00:04, 1144.00it/s]warmup run: 2928it [00:04, 1198.53it/s]warmup run: 2845it [00:04, 1161.41it/s]warmup run: 2780it [00:04, 1183.73it/s]warmup run: 3000it [00:04, 686.51it/s] warmup run: 3000it [00:04, 682.77it/s] warmup run: 3000it [00:04, 675.14it/s] warmup run: 2866it [00:04, 1142.89it/s]warmup run: 2965it [00:04, 1170.17it/s]warmup run: 2901it [00:04, 1189.72it/s]warmup run: 3000it [00:04, 678.14it/s] warmup run: 2987it [00:04, 1160.66it/s]warmup run: 3000it [00:04, 661.76it/s] warmup run: 3000it [00:04, 662.39it/s] 


warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]


warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   5%|         | 162/3000 [00:00<00:01, 1617.94it/s]warmup should be done:   5%|         | 162/3000 [00:00<00:01, 1619.08it/s]warmup should be done:   5%|         | 164/3000 [00:00<00:01, 1632.99it/s]warmup should be done:   5%|         | 163/3000 [00:00<00:01, 1623.28it/s]warmup should be done:   5%|         | 162/3000 [00:00<00:01, 1611.09it/s]warmup should be done:   5%|         | 164/3000 [00:00<00:01, 1630.39it/s]warmup should be done:   5%|         | 161/3000 [00:00<00:01, 1601.08it/s]warmup should be done:   6%|         | 165/3000 [00:00<00:01, 1641.30it/s]warmup should be done:  11%|         | 328/3000 [00:00<00:01, 1639.32it/s]warmup should be done:  11%|         | 326/3000 [00:00<00:01, 1629.52it/s]warmup should be done:  11%|         | 328/3000 [00:00<00:01, 1638.94it/s]warmup should be done:  11%|         | 329/3000 [00:00<00:01, 1639.16it/s]warmup should be done:  11%|         | 327/3000 [00:00<00:01, 1631.01it/s]warmup should be done:  11%|         | 331/3000 [00:00<00:01, 1650.55it/s]warmup should be done:  11%|         | 328/3000 [00:00<00:01, 1629.23it/s]warmup should be done:  11%|         | 326/3000 [00:00<00:01, 1609.40it/s]warmup should be done:  16%|        | 492/3000 [00:00<00:01, 1636.69it/s]warmup should be done:  16%|        | 492/3000 [00:00<00:01, 1632.86it/s]warmup should be done:  16%|        | 489/3000 [00:00<00:01, 1622.91it/s]warmup should be done:  16%|        | 492/3000 [00:00<00:01, 1631.95it/s]warmup should be done:  16%|        | 493/3000 [00:00<00:01, 1630.95it/s]warmup should be done:  16%|        | 491/3000 [00:00<00:01, 1626.72it/s]warmup should be done:  17%|        | 497/3000 [00:00<00:01, 1646.05it/s]warmup should be done:  16%|        | 490/3000 [00:00<00:01, 1620.26it/s]warmup should be done:  22%|       | 656/3000 [00:00<00:01, 1635.94it/s]warmup should be done:  22%|       | 656/3000 [00:00<00:01, 1632.55it/s]warmup should be done:  22%|       | 662/3000 [00:00<00:01, 1647.11it/s]warmup should be done:  22%|       | 655/3000 [00:00<00:01, 1628.29it/s]warmup should be done:  22%|       | 654/3000 [00:00<00:01, 1627.88it/s]warmup should be done:  22%|       | 652/3000 [00:00<00:01, 1611.64it/s]warmup should be done:  22%|       | 656/3000 [00:00<00:01, 1617.71it/s]warmup should be done:  22%|       | 657/3000 [00:00<00:01, 1588.28it/s]warmup should be done:  27%|       | 820/3000 [00:00<00:01, 1634.57it/s]warmup should be done:  27%|       | 820/3000 [00:00<00:01, 1633.55it/s]warmup should be done:  28%|       | 827/3000 [00:00<00:01, 1646.35it/s]warmup should be done:  27%|       | 819/3000 [00:00<00:01, 1631.34it/s]warmup should be done:  27%|       | 818/3000 [00:00<00:01, 1631.67it/s]warmup should be done:  27%|       | 814/3000 [00:00<00:01, 1600.47it/s]warmup should be done:  27%|       | 818/3000 [00:00<00:01, 1604.37it/s]warmup should be done:  27%|       | 820/3000 [00:00<00:01, 1599.43it/s]warmup should be done:  33%|      | 984/3000 [00:00<00:01, 1630.63it/s]warmup should be done:  33%|      | 992/3000 [00:00<00:01, 1644.44it/s]warmup should be done:  33%|      | 982/3000 [00:00<00:01, 1631.86it/s]warmup should be done:  33%|      | 983/3000 [00:00<00:01, 1629.77it/s]warmup should be done:  33%|      | 984/3000 [00:00<00:01, 1614.93it/s]warmup should be done:  33%|      | 978/3000 [00:00<00:01, 1610.62it/s]warmup should be done:  33%|      | 983/3000 [00:00<00:01, 1616.67it/s]warmup should be done:  33%|      | 981/3000 [00:00<00:01, 1588.87it/s]warmup should be done:  38%|      | 1146/3000 [00:00<00:01, 1625.16it/s]warmup should be done:  38%|      | 1146/3000 [00:00<00:01, 1629.36it/s]warmup should be done:  39%|      | 1157/3000 [00:00<00:01, 1637.37it/s]warmup should be done:  38%|      | 1148/3000 [00:00<00:01, 1624.15it/s]warmup should be done:  38%|      | 1147/3000 [00:00<00:01, 1623.06it/s]warmup should be done:  38%|      | 1146/3000 [00:00<00:01, 1607.81it/s]warmup should be done:  38%|      | 1140/3000 [00:00<00:01, 1603.09it/s]warmup should be done:  38%|      | 1141/3000 [00:00<00:01, 1592.02it/s]warmup should be done:  44%|     | 1309/3000 [00:00<00:01, 1625.58it/s]warmup should be done:  44%|     | 1321/3000 [00:00<00:01, 1638.01it/s]warmup should be done:  44%|     | 1309/3000 [00:00<00:01, 1627.35it/s]warmup should be done:  44%|     | 1311/3000 [00:00<00:01, 1623.61it/s]warmup should be done:  44%|     | 1307/3000 [00:00<00:01, 1606.69it/s]warmup should be done:  44%|     | 1310/3000 [00:00<00:01, 1614.48it/s]warmup should be done:  43%|     | 1301/3000 [00:00<00:01, 1600.53it/s]warmup should be done:  43%|     | 1303/3000 [00:00<00:01, 1600.16it/s]warmup should be done:  49%|     | 1472/3000 [00:00<00:00, 1625.46it/s]warmup should be done:  49%|     | 1474/3000 [00:00<00:00, 1622.77it/s]warmup should be done:  49%|     | 1472/3000 [00:00<00:00, 1617.82it/s]warmup should be done:  49%|     | 1470/3000 [00:00<00:00, 1611.03it/s]warmup should be done:  50%|     | 1485/3000 [00:00<00:00, 1617.15it/s]warmup should be done:  49%|     | 1462/3000 [00:00<00:00, 1593.44it/s]warmup should be done:  49%|     | 1472/3000 [00:00<00:00, 1602.77it/s]warmup should be done:  49%|     | 1467/3000 [00:00<00:00, 1610.68it/s]warmup should be done:  55%|    | 1635/3000 [00:01<00:00, 1625.81it/s]warmup should be done:  55%|    | 1637/3000 [00:01<00:00, 1622.90it/s]warmup should be done:  54%|    | 1634/3000 [00:01<00:00, 1611.17it/s]warmup should be done:  54%|    | 1634/3000 [00:01<00:00, 1617.65it/s]warmup should be done:  55%|    | 1647/3000 [00:01<00:00, 1603.25it/s]warmup should be done:  54%|    | 1624/3000 [00:01<00:00, 1599.24it/s]warmup should be done:  54%|    | 1634/3000 [00:01<00:00, 1605.58it/s]warmup should be done:  54%|    | 1629/3000 [00:01<00:00, 1589.64it/s]warmup should be done:  60%|    | 1798/3000 [00:01<00:00, 1625.86it/s]warmup should be done:  60%|    | 1800/3000 [00:01<00:00, 1621.74it/s]warmup should be done:  60%|    | 1798/3000 [00:01<00:00, 1618.22it/s]warmup should be done:  60%|    | 1797/3000 [00:01<00:00, 1621.23it/s]warmup should be done:  60%|    | 1809/3000 [00:01<00:00, 1605.77it/s]warmup should be done:  60%|    | 1791/3000 [00:01<00:00, 1619.46it/s]warmup should be done:  60%|    | 1799/3000 [00:01<00:00, 1616.96it/s]warmup should be done:  60%|    | 1795/3000 [00:01<00:00, 1608.15it/s]warmup should be done:  65%|   | 1961/3000 [00:01<00:00, 1626.04it/s]warmup should be done:  66%|   | 1965/3000 [00:01<00:00, 1627.35it/s]warmup should be done:  65%|   | 1961/3000 [00:01<00:00, 1621.74it/s]warmup should be done:  65%|   | 1960/3000 [00:01<00:00, 1623.00it/s]warmup should be done:  65%|   | 1958/3000 [00:01<00:00, 1632.82it/s]warmup should be done:  66%|   | 1972/3000 [00:01<00:00, 1610.33it/s]warmup should be done:  65%|   | 1964/3000 [00:01<00:00, 1624.92it/s]warmup should be done:  65%|   | 1961/3000 [00:01<00:00, 1623.15it/s]warmup should be done:  71%|   | 2124/3000 [00:01<00:00, 1625.81it/s]warmup should be done:  71%|   | 2131/3000 [00:01<00:00, 1635.63it/s]warmup should be done:  71%|   | 2124/3000 [00:01<00:00, 1624.00it/s]warmup should be done:  71%|   | 2123/3000 [00:01<00:00, 1624.01it/s]warmup should be done:  71%|   | 2125/3000 [00:01<00:00, 1641.63it/s]warmup should be done:  71%|   | 2135/3000 [00:01<00:00, 1613.40it/s]warmup should be done:  71%|   | 2129/3000 [00:01<00:00, 1630.53it/s]warmup should be done:  71%|   | 2128/3000 [00:01<00:00, 1634.67it/s]warmup should be done:  76%|  | 2287/3000 [00:01<00:00, 1624.40it/s]warmup should be done:  77%|  | 2296/3000 [00:01<00:00, 1639.76it/s]warmup should be done:  76%|  | 2287/3000 [00:01<00:00, 1625.14it/s]warmup should be done:  76%|  | 2286/3000 [00:01<00:00, 1622.69it/s]warmup should be done:  76%|  | 2291/3000 [00:01<00:00, 1645.31it/s]warmup should be done:  77%|  | 2299/3000 [00:01<00:00, 1618.94it/s]warmup should be done:  76%|  | 2294/3000 [00:01<00:00, 1635.37it/s]warmup should be done:  76%|  | 2295/3000 [00:01<00:00, 1643.52it/s]warmup should be done:  82%| | 2450/3000 [00:01<00:00, 1621.61it/s]warmup should be done:  82%| | 2461/3000 [00:01<00:00, 1642.57it/s]warmup should be done:  82%| | 2450/3000 [00:01<00:00, 1622.99it/s]warmup should be done:  82%| | 2449/3000 [00:01<00:00, 1619.70it/s]warmup should be done:  82%| | 2464/3000 [00:01<00:00, 1626.61it/s]warmup should be done:  82%| | 2458/3000 [00:01<00:00, 1635.55it/s]warmup should be done:  82%| | 2456/3000 [00:01<00:00, 1641.73it/s]warmup should be done:  82%| | 2461/3000 [00:01<00:00, 1646.97it/s]warmup should be done:  87%| | 2613/3000 [00:01<00:00, 1622.36it/s]warmup should be done:  88%| | 2628/3000 [00:01<00:00, 1647.90it/s]warmup should be done:  87%| | 2614/3000 [00:01<00:00, 1625.33it/s]warmup should be done:  87%| | 2612/3000 [00:01<00:00, 1621.96it/s]warmup should be done:  88%| | 2630/3000 [00:01<00:00, 1636.13it/s]warmup should be done:  87%| | 2623/3000 [00:01<00:00, 1639.46it/s]warmup should be done:  87%| | 2621/3000 [00:01<00:00, 1641.29it/s]warmup should be done:  88%| | 2628/3000 [00:01<00:00, 1651.04it/s]warmup should be done:  93%|| 2776/3000 [00:01<00:00, 1622.79it/s]warmup should be done:  93%|| 2794/3000 [00:01<00:00, 1649.60it/s]warmup should be done:  93%|| 2777/3000 [00:01<00:00, 1626.21it/s]warmup should be done:  92%|| 2775/3000 [00:01<00:00, 1622.71it/s]warmup should be done:  93%|| 2796/3000 [00:01<00:00, 1642.33it/s]warmup should be done:  93%|| 2788/3000 [00:01<00:00, 1640.51it/s]warmup should be done:  93%|| 2786/3000 [00:01<00:00, 1640.80it/s]warmup should be done:  93%|| 2795/3000 [00:01<00:00, 1654.17it/s]warmup should be done:  98%|| 2942/3000 [00:01<00:00, 1631.21it/s]warmup should be done:  99%|| 2961/3000 [00:01<00:00, 1655.19it/s]warmup should be done:  98%|| 2942/3000 [00:01<00:00, 1632.37it/s]warmup should be done:  98%|| 2940/3000 [00:01<00:00, 1628.55it/s]warmup should be done:  99%|| 2963/3000 [00:01<00:00, 1648.97it/s]warmup should be done:  98%|| 2954/3000 [00:01<00:00, 1644.79it/s]warmup should be done:  98%|| 2952/3000 [00:01<00:00, 1645.55it/s]warmup should be done:  99%|| 2964/3000 [00:01<00:00, 1662.27it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1637.04it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1632.83it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1627.10it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1627.18it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1626.46it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1625.15it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1624.96it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1622.05it/s]






warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   6%|         | 170/3000 [00:00<00:01, 1697.06it/s]warmup should be done:   6%|         | 168/3000 [00:00<00:01, 1674.01it/s]warmup should be done:   6%|         | 170/3000 [00:00<00:01, 1693.48it/s]warmup should be done:   6%|         | 168/3000 [00:00<00:01, 1674.78it/s]warmup should be done:   6%|         | 170/3000 [00:00<00:01, 1692.18it/s]warmup should be done:   6%|         | 171/3000 [00:00<00:01, 1701.56it/s]warmup should be done:   6%|         | 168/3000 [00:00<00:01, 1671.61it/s]warmup should be done:   6%|         | 169/3000 [00:00<00:01, 1680.56it/s]warmup should be done:  11%|        | 340/3000 [00:00<00:01, 1695.19it/s]warmup should be done:  11%|        | 340/3000 [00:00<00:01, 1695.53it/s]warmup should be done:  11%|        | 342/3000 [00:00<00:01, 1704.69it/s]warmup should be done:  11%|        | 340/3000 [00:00<00:01, 1691.89it/s]warmup should be done:  11%|         | 336/3000 [00:00<00:01, 1672.02it/s]warmup should be done:  11%|         | 336/3000 [00:00<00:01, 1670.63it/s]warmup should be done:  11%|         | 336/3000 [00:00<00:01, 1670.47it/s]warmup should be done:  11%|        | 338/3000 [00:00<00:01, 1677.60it/s]warmup should be done:  17%|        | 510/3000 [00:00<00:01, 1696.09it/s]warmup should be done:  17%|        | 513/3000 [00:00<00:01, 1704.92it/s]warmup should be done:  17%|        | 511/3000 [00:00<00:01, 1698.85it/s]warmup should be done:  17%|        | 504/3000 [00:00<00:01, 1673.56it/s]warmup should be done:  17%|        | 504/3000 [00:00<00:01, 1672.10it/s]warmup should be done:  17%|        | 504/3000 [00:00<00:01, 1670.87it/s]warmup should be done:  17%|        | 510/3000 [00:00<00:01, 1690.29it/s]warmup should be done:  17%|        | 507/3000 [00:00<00:01, 1679.15it/s]warmup should be done:  23%|       | 681/3000 [00:00<00:01, 1699.81it/s]warmup should be done:  22%|       | 672/3000 [00:00<00:01, 1676.02it/s]warmup should be done:  23%|       | 685/3000 [00:00<00:01, 1708.34it/s]warmup should be done:  23%|       | 682/3000 [00:00<00:01, 1699.37it/s]warmup should be done:  22%|       | 673/3000 [00:00<00:01, 1675.29it/s]warmup should be done:  22%|       | 673/3000 [00:00<00:01, 1675.19it/s]warmup should be done:  23%|       | 680/3000 [00:00<00:01, 1690.09it/s]warmup should be done:  23%|       | 676/3000 [00:00<00:01, 1681.91it/s]warmup should be done:  28%|       | 852/3000 [00:00<00:01, 1700.52it/s]warmup should be done:  28%|       | 840/3000 [00:00<00:01, 1675.66it/s]warmup should be done:  28%|       | 852/3000 [00:00<00:01, 1698.48it/s]warmup should be done:  29%|       | 857/3000 [00:00<00:01, 1709.20it/s]warmup should be done:  28%|       | 841/3000 [00:00<00:01, 1675.73it/s]warmup should be done:  28%|       | 850/3000 [00:00<00:01, 1692.95it/s]warmup should be done:  28%|       | 842/3000 [00:00<00:01, 1678.11it/s]warmup should be done:  28%|       | 845/3000 [00:00<00:01, 1683.76it/s]warmup should be done:  34%|      | 1029/3000 [00:00<00:01, 1712.42it/s]warmup should be done:  34%|      | 1023/3000 [00:00<00:01, 1699.11it/s]warmup should be done:  34%|      | 1008/3000 [00:00<00:01, 1671.80it/s]warmup should be done:  34%|      | 1010/3000 [00:00<00:01, 1677.17it/s]warmup should be done:  34%|      | 1020/3000 [00:00<00:01, 1692.91it/s]warmup should be done:  34%|      | 1014/3000 [00:00<00:01, 1683.10it/s]warmup should be done:  34%|      | 1009/3000 [00:00<00:01, 1672.59it/s]warmup should be done:  34%|      | 1023/3000 [00:00<00:01, 1692.79it/s]warmup should be done:  40%|      | 1201/3000 [00:00<00:01, 1711.68it/s]warmup should be done:  40%|      | 1193/3000 [00:00<00:01, 1695.99it/s]warmup should be done:  39%|      | 1178/3000 [00:00<00:01, 1674.09it/s]warmup should be done:  39%|      | 1176/3000 [00:00<00:01, 1668.63it/s]warmup should be done:  39%|      | 1177/3000 [00:00<00:01, 1672.05it/s]warmup should be done:  39%|      | 1183/3000 [00:00<00:01, 1680.39it/s]warmup should be done:  40%|      | 1190/3000 [00:00<00:01, 1687.87it/s]warmup should be done:  40%|      | 1193/3000 [00:00<00:01, 1687.30it/s]warmup should be done:  46%|     | 1375/3000 [00:00<00:00, 1717.64it/s]warmup should be done:  45%|     | 1364/3000 [00:00<00:00, 1698.95it/s]warmup should be done:  45%|     | 1346/3000 [00:00<00:00, 1675.50it/s]warmup should be done:  45%|     | 1345/3000 [00:00<00:00, 1674.31it/s]warmup should be done:  45%|     | 1344/3000 [00:00<00:00, 1670.44it/s]warmup should be done:  45%|     | 1352/3000 [00:00<00:00, 1681.90it/s]warmup should be done:  45%|     | 1360/3000 [00:00<00:00, 1690.42it/s]warmup should be done:  45%|     | 1362/3000 [00:00<00:00, 1687.81it/s]warmup should be done:  51%|     | 1534/3000 [00:00<00:00, 1699.09it/s]warmup should be done:  52%|    | 1548/3000 [00:00<00:00, 1719.65it/s]warmup should be done:  50%|     | 1514/3000 [00:00<00:00, 1675.33it/s]warmup should be done:  50%|     | 1513/3000 [00:00<00:00, 1675.06it/s]warmup should be done:  51%|     | 1521/3000 [00:00<00:00, 1680.74it/s]warmup should be done:  51%|     | 1531/3000 [00:00<00:00, 1685.50it/s]warmup should be done:  50%|     | 1512/3000 [00:00<00:00, 1664.62it/s]warmup should be done:  51%|     | 1530/3000 [00:00<00:00, 1685.37it/s]warmup should be done:  57%|    | 1721/3000 [00:01<00:00, 1721.69it/s]warmup should be done:  57%|    | 1705/3000 [00:01<00:00, 1700.61it/s]warmup should be done:  56%|    | 1682/3000 [00:01<00:00, 1676.93it/s]warmup should be done:  56%|    | 1682/3000 [00:01<00:00, 1673.42it/s]warmup should be done:  56%|    | 1690/3000 [00:01<00:00, 1681.16it/s]warmup should be done:  57%|    | 1700/3000 [00:01<00:00, 1685.22it/s]warmup should be done:  56%|    | 1679/3000 [00:01<00:00, 1663.58it/s]warmup should be done:  57%|    | 1700/3000 [00:01<00:00, 1688.24it/s]warmup should be done:  63%|   | 1876/3000 [00:01<00:00, 1702.50it/s]warmup should be done:  63%|   | 1895/3000 [00:01<00:00, 1725.04it/s]warmup should be done:  62%|   | 1851/3000 [00:01<00:00, 1678.26it/s]warmup should be done:  62%|   | 1859/3000 [00:01<00:00, 1682.10it/s]warmup should be done:  62%|   | 1869/3000 [00:01<00:00, 1686.49it/s]warmup should be done:  62%|   | 1850/3000 [00:01<00:00, 1668.13it/s]warmup should be done:  62%|   | 1870/3000 [00:01<00:00, 1691.00it/s]warmup should be done:  62%|   | 1846/3000 [00:01<00:00, 1664.48it/s]warmup should be done:  68%|   | 2047/3000 [00:01<00:00, 1703.44it/s]warmup should be done:  69%|   | 2069/3000 [00:01<00:00, 1726.81it/s]warmup should be done:  67%|   | 2019/3000 [00:01<00:00, 1677.97it/s]warmup should be done:  68%|   | 2038/3000 [00:01<00:00, 1686.01it/s]warmup should be done:  68%|   | 2028/3000 [00:01<00:00, 1681.45it/s]warmup should be done:  67%|   | 2013/3000 [00:01<00:00, 1663.77it/s]warmup should be done:  68%|   | 2041/3000 [00:01<00:00, 1693.89it/s]warmup should be done:  67%|   | 2017/3000 [00:01<00:00, 1661.76it/s]warmup should be done:  74%|  | 2218/3000 [00:01<00:00, 1701.77it/s]warmup should be done:  75%|  | 2242/3000 [00:01<00:00, 1726.74it/s]warmup should be done:  73%|  | 2187/3000 [00:01<00:00, 1676.42it/s]warmup should be done:  74%|  | 2207/3000 [00:01<00:00, 1683.10it/s]warmup should be done:  73%|  | 2197/3000 [00:01<00:00, 1679.35it/s]warmup should be done:  74%|  | 2211/3000 [00:01<00:00, 1693.31it/s]warmup should be done:  73%|  | 2180/3000 [00:01<00:00, 1661.39it/s]warmup should be done:  73%|  | 2184/3000 [00:01<00:00, 1655.98it/s]warmup should be done:  80%|  | 2415/3000 [00:01<00:00, 1724.60it/s]warmup should be done:  78%|  | 2355/3000 [00:01<00:00, 1676.49it/s]warmup should be done:  80%|  | 2389/3000 [00:01<00:00, 1699.30it/s]warmup should be done:  79%|  | 2376/3000 [00:01<00:00, 1682.00it/s]warmup should be done:  79%|  | 2366/3000 [00:01<00:00, 1679.76it/s]warmup should be done:  78%|  | 2347/3000 [00:01<00:00, 1662.31it/s]warmup should be done:  79%|  | 2381/3000 [00:01<00:00, 1691.09it/s]warmup should be done:  78%|  | 2350/3000 [00:01<00:00, 1654.98it/s]warmup should be done:  86%| | 2588/3000 [00:01<00:00, 1723.77it/s]warmup should be done:  85%| | 2560/3000 [00:01<00:00, 1700.86it/s]warmup should be done:  84%| | 2524/3000 [00:01<00:00, 1678.59it/s]warmup should be done:  84%| | 2535/3000 [00:01<00:00, 1680.58it/s]warmup should be done:  85%| | 2551/3000 [00:01<00:00, 1693.68it/s]warmup should be done:  85%| | 2545/3000 [00:01<00:00, 1679.84it/s]warmup should be done:  84%| | 2514/3000 [00:01<00:00, 1662.57it/s]warmup should be done:  84%| | 2516/3000 [00:01<00:00, 1655.02it/s]warmup should be done:  92%|| 2761/3000 [00:01<00:00, 1724.42it/s]warmup should be done:  91%| | 2731/3000 [00:01<00:00, 1702.49it/s]warmup should be done:  90%| | 2693/3000 [00:01<00:00, 1679.38it/s]warmup should be done:  90%| | 2713/3000 [00:01<00:00, 1679.25it/s]warmup should be done:  91%| | 2721/3000 [00:01<00:00, 1693.72it/s]warmup should be done:  90%| | 2704/3000 [00:01<00:00, 1679.91it/s]warmup should be done:  89%| | 2681/3000 [00:01<00:00, 1661.46it/s]warmup should be done:  89%| | 2682/3000 [00:01<00:00, 1653.42it/s]warmup should be done:  98%|| 2934/3000 [00:01<00:00, 1723.89it/s]warmup should be done:  95%|| 2861/3000 [00:01<00:00, 1678.22it/s]warmup should be done:  97%|| 2902/3000 [00:01<00:00, 1701.12it/s]warmup should be done:  96%|| 2872/3000 [00:01<00:00, 1679.33it/s]warmup should be done:  96%|| 2891/3000 [00:01<00:00, 1692.74it/s]warmup should be done:  96%|| 2881/3000 [00:01<00:00, 1675.49it/s]warmup should be done:  95%|| 2848/3000 [00:01<00:00, 1659.86it/s]warmup should be done:  95%|| 2848/3000 [00:01<00:00, 1650.90it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1718.51it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1699.78it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1691.26it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1684.64it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1680.55it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1676.28it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1665.59it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1663.74it/s]
WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f7e96f7dd30>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f7e96c370d0>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f7e96c380a0>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f7e96f7c9d0>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f7e96c38190>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f7e96f7ae80>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f7e96c461c0>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f7e96c3a280>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
2022-12-12 01:59:59.963585: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f79b682fd70 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 01:59:59.963646: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 01:59:59.973139: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 01:59:59.995693: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f79af02c5f0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 01:59:59.995760: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 02:00:00.003056: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 02:00:00.024003: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f79ab028740 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 02:00:00.024051: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 02:00:00.032913: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 02:00:00.113736: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f79b30306e0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 02:00:00.113791: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 02:00:00.121675: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 02:00:00.770748: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f79b282ef60 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 02:00:00.770812: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 02:00:00.770821: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f79af02d080 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 02:00:00.770868: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 02:00:00.778885: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 02:00:00.779848: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 02:00:00.795908: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f79ae8373a0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 02:00:00.795963: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 02:00:00.804247: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 02:00:00.805944: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f79aef92150 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 02:00:00.806002: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 02:00:00.813898: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 02:00:07.071192: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 02:00:07.156017: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 02:00:07.217239: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 02:00:07.309525: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 02:00:07.481017: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 02:00:07.701723: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 02:00:07.879460: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 02:00:07.883107: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
[HCTR][02:01:04.665][ERROR][RK0][main]: replica 7 reaches 1000, calling init pre replica
[HCTR][02:01:04.665][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][02:01:04.673][ERROR][RK0][main]: coll ps creation done
[HCTR][02:01:04.673][ERROR][RK0][main]: replica 7 waits for coll ps creation barrier
[HCTR][02:01:04.843][ERROR][RK0][tid #140160930924288]: replica 3 reaches 1000, calling init pre replica
[HCTR][02:01:04.843][ERROR][RK0][tid #140160930924288]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][02:01:04.852][ERROR][RK0][tid #140160930924288]: coll ps creation done
[HCTR][02:01:04.852][ERROR][RK0][tid #140160930924288]: replica 3 waits for coll ps creation barrier
[HCTR][02:01:04.908][ERROR][RK0][tid #140160880600832]: replica 6 reaches 1000, calling init pre replica
[HCTR][02:01:04.909][ERROR][RK0][tid #140160880600832]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][02:01:04.914][ERROR][RK0][tid #140160880600832]: replica 4 reaches 1000, calling init pre replica
[HCTR][02:01:04.915][ERROR][RK0][tid #140160880600832]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][02:01:04.917][ERROR][RK0][tid #140160880600832]: coll ps creation done
[HCTR][02:01:04.917][ERROR][RK0][tid #140160880600832]: replica 6 waits for coll ps creation barrier
[HCTR][02:01:04.919][ERROR][RK0][tid #140160880600832]: coll ps creation done
[HCTR][02:01:04.919][ERROR][RK0][tid #140160880600832]: replica 4 waits for coll ps creation barrier
[HCTR][02:01:05.007][ERROR][RK0][main]: replica 1 reaches 1000, calling init pre replica
[HCTR][02:01:05.007][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][02:01:05.012][ERROR][RK0][main]: coll ps creation done
[HCTR][02:01:05.012][ERROR][RK0][main]: replica 1 waits for coll ps creation barrier
[HCTR][02:01:05.055][ERROR][RK0][tid #140160930924288]: replica 5 reaches 1000, calling init pre replica
[HCTR][02:01:05.055][ERROR][RK0][tid #140160930924288]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][02:01:05.062][ERROR][RK0][tid #140160930924288]: coll ps creation done
[HCTR][02:01:05.062][ERROR][RK0][tid #140160930924288]: replica 5 waits for coll ps creation barrier
[HCTR][02:01:05.064][ERROR][RK0][main]: replica 0 reaches 1000, calling init pre replica
[HCTR][02:01:05.064][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][02:01:05.068][ERROR][RK0][main]: coll ps creation done
[HCTR][02:01:05.068][ERROR][RK0][main]: replica 0 waits for coll ps creation barrier
[HCTR][02:01:05.083][ERROR][RK0][tid #140160939316992]: replica 2 reaches 1000, calling init pre replica
[HCTR][02:01:05.084][ERROR][RK0][tid #140160939316992]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][02:01:05.089][ERROR][RK0][tid #140160939316992]: coll ps creation done
[HCTR][02:01:05.089][ERROR][RK0][tid #140160939316992]: replica 2 waits for coll ps creation barrier
[HCTR][02:01:05.089][ERROR][RK0][main]: replica 0 preparing frequency
[HCTR][02:01:05.937][ERROR][RK0][main]: replica 0 preparing frequency done
[HCTR][02:01:05.972][ERROR][RK0][main]: replica 0 calling init per replica
[HCTR][02:01:05.972][ERROR][RK0][tid #140160939316992]: replica 2 calling init per replica
[HCTR][02:01:05.972][ERROR][RK0][tid #140160880600832]: replica 4 calling init per replica
[HCTR][02:01:05.972][ERROR][RK0][tid #140160930924288]: replica 5 calling init per replica
[HCTR][02:01:05.972][ERROR][RK0][tid #140160880600832]: replica 6 calling init per replica
[HCTR][02:01:05.972][ERROR][RK0][main]: replica 7 calling init per replica
[HCTR][02:01:05.972][ERROR][RK0][main]: replica 1 calling init per replica
[HCTR][02:01:05.972][ERROR][RK0][tid #140160930924288]: replica 3 calling init per replica
[HCTR][02:01:05.972][ERROR][RK0][main]: Calling build_v2
[HCTR][02:01:05.972][ERROR][RK0][tid #140160939316992]: Calling build_v2
[HCTR][02:01:05.972][ERROR][RK0][tid #140160880600832]: Calling build_v2
[HCTR][02:01:05.972][ERROR][RK0][tid #140160930924288]: Calling build_v2
[HCTR][02:01:05.972][ERROR][RK0][tid #140160880600832]: Calling build_v2
[HCTR][02:01:05.972][ERROR][RK0][main]: Calling build_v2
[HCTR][02:01:05.972][ERROR][RK0][main]: Calling build_v2
[HCTR][02:01:05.972][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][02:01:05.972][ERROR][RK0][tid #140160930924288]: Calling build_v2
[HCTR][02:01:05.972][ERROR][RK0][tid #140160939316992]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][02:01:05.972][ERROR][RK0][tid #140160880600832]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][02:01:05.972][ERROR][RK0][tid #140160930924288]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][02:01:05.972][ERROR][RK0][tid #140160880600832]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][02:01:05.972][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][02:01:05.972][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][02:01:05.972][ERROR][RK0][tid #140160930924288]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[[[2022-12-12 02:01:052022-12-12 02:01:05[..2022-12-12 02:01:05972564[[[972564[2022-12-12 02:01:05.: : 2022-12-12 02:01:052022-12-12 02:01:052022-12-12 02:01:05.9725722022-12-12 02:01:05EE...972589: .  972596972593972589: E972600/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc: : : E : ::EEE /hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.ccE136136   /hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc: ] ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc:136/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.ccusing concurrent impl MPSusing concurrent impl MPS:::136] :

136136136] using concurrent impl MPS136] ] ] using concurrent impl MPS
] using concurrent impl MPSusing concurrent impl MPSusing concurrent impl MPSusing concurrent impl MPS




[2022-12-12 02:01:05.976874: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:178] v100x8, slow pcie
[2022-12-12 02:01:05.976918: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:196] assigning 8 to cpu
[2022-12-12 02:01:05.976980: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:212] build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8
[2022-12-12 02:01:05.977020: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:213] remote time is 8.68421
[2022-12-12 02:01:05.977049: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:214] cpu time is 97.0588
[2022-12-12 02:01:05.977124: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:178] v100x8, slow pcie
[2022-12-12 02:01:05.977160: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:196] assigning 8 to cpu[
2022-12-12 02:01:05.977181: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:[1782022-12-12 02:01:05] .v100x8, slow pcie977225
[: 2022-12-12 02:01:05E. [977250/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 02:01:05: :.E178977257 ] [: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccv100x8, slow pcie2022-12-12 02:01:05E:
. 212[977277[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] 2022-12-12 02:01:05: 2022-12-12 02:01:05:build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8.E.196
977322 977330] : /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: [[assigning 8 to cpuE:E2022-12-12 02:01:052022-12-12 02:01:05
 178 [../hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 02:01:05977388977399:v100x8, slow pcie:[.: : 178
1962022-12-12 02:01:05977429EE] ] [.:   v100x8, slow pcieassigning 8 to cpu2022-12-12 02:01:05977477E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc

.[:  ::1789775272022-12-12 02:01:05E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc213] : . :] [v100x8, slow pcieE977617/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc178remote time is 8.684212022-12-12 02:01:05
 : :] 
./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE212[v100x8, slow pcie977660:[ ] 2022-12-12 02:01:05
: 196[2022-12-12 02:01:05/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccbuild asymm link desc with 8X Tesla V100-SXM2-32GB out of 8.E] 2022-12-12 02:01:05.:
977722 assigning 8 to cpu.977741196: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
977795[: E ] E:: 2022-12-12 02:01:05/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccassigning 8 to cpu 212E[.:
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc]  2022-12-12 02:01:05977855214:build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.: ] 196
:977908[E[cpu time is 97.0588] 196: 2022-12-12 02:01:05 2022-12-12 02:01:05
assigning 8 to cpu] E./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.
assigning 8 to cpu 977982:978005
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: 213: :E] E212 [remote time is 8.68421 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 02:01:05
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccbuild asymm link desc with 8X Tesla V100-SXM2-32GB out of 8[:.:
2022-12-12 02:01:05212[978091213.] 2022-12-12 02:01:05: [] 978121build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8.E2022-12-12 02:01:05remote time is 8.68421: 
978141 .
E: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[978168 [E:2022-12-12 02:01:05: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 02:01:05 212.] E:./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc978239build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8 212978252:: 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] : 214E:build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8E] [ 213
 cpu time is 97.05882022-12-12 02:01:05/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
.[:remote time is 8.68421:9783702022-12-12 02:01:05213
214: .] [] E978413remote time is 8.684212022-12-12 02:01:05cpu time is 97.0588 : 
.
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE978464[: : 2022-12-12 02:01:05213/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE.] : 978521213remote time is 8.68421/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: ] 
:Eremote time is 8.68421214 
] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[cpu time is 97.0588:[2022-12-12 02:01:05
2142022-12-12 02:01:05.] .978613cpu time is 97.0588978623: 
: EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc::214214] ] cpu time is 97.0588cpu time is 97.0588

[2022-12-12 02:02:23.554990: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:118] solver created. now build & solve
[2022-12-12 02:02:23.595108: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 381.47 MB
[2022-12-12 02:02:23.595201: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 381.47 MB
[2022-12-12 02:02:23.596790: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:74] mapping nid to rank...
[2022-12-12 02:02:23.667840: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:91] counting slots...
[2022-12-12 02:02:24. 76108: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:105] Final num slot is 49
[2022-12-12 02:02:24. 76206: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:109] counting blocks...
[2022-12-12 02:02:30.659985: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:118] Final num block is 1024
[2022-12-12 02:02:30.660087: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:123] counting freq and density...
[2022-12-12 02:02:32.383085: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:154] averaging freq and density...
[2022-12-12 02:02:32.383193: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:155] 1024
[2022-12-12 02:02:32.388612: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:123] solver built. now solve
[2022-12-12 02:02:32.388674: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:354] constructing optimal solver, device=8, stream=1
1024 blocks, 8 devices
[2022-12-12 02:02:32.629991: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:527] Add Var...
[2022-12-12 02:02:32.658503: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:545] Capacity...
[2022-12-12 02:02:32.659955: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:548] Connect CPU...
[2022-12-12 02:02:32.680952: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:550] Connect Access To Storage...
[2022-12-12 02:02:33.201906: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:554] Time...
[2022-12-12 02:03:27.627809: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:569] Coll Cache init block placement array
[2022-12-12 02:03:27.636380: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:645] Coll Cache init block placement array done
[2022-12-12 02:03:27.638336: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:647] Coll Cache model reset done
[2022-12-12 02:03:27.682843: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:127] solver solved
[2022-12-12 02:03:27.682942: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:195] 0 solved master
[2022-12-12 02:03:27.682990: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 0 solved
[2022-12-12 02:03:27.683019: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 0 initing device 0
[2022-12-12 02:03:27.683686: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-12 02:03:27.683743: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 02:03:27.684833: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 02:03:27.685511: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 02:03:27.698303: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 6 solved
[2022-12-12 02:03:27.698376: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 6 initing device 6
[2022-12-12 02:03:27.698821: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-12 02:03:27.698872: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[[2022-12-12 02:03:27.2022-12-12 02:03:27699056.: 699068E: E  /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc::202202] ] 4 solved
2 solved
[2022-12-12 02:03:27.[6991652022-12-12 02:03:27: .E699170 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.ccE: 205/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc] :worker 0 thread 4 initing device 4205
] worker 0 thread 2 initing device 2
[[2022-12-12 02:03:272022-12-12 02:03:27..699238699244: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc::202202] ] 1 solved7 solved

[2022-12-12 02:03:27[.2022-12-12 02:03:27699363.: 699366E:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc :/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc205:] 205worker 0 thread 1 initing device 1] 
worker 0 thread 7 initing device 7
[[2022-12-12 02:03:272022-12-12 02:03:27..699646699646: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::18151815] ] Building Coll Cache with ... num gpu device is 8Building Coll Cache with ... num gpu device is 8

[[2022-12-12 02:03:272022-12-12 02:03:27..699727699727: : EE [ /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu2022-12-12 02:03:27/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:.:19806997441980] : ] eager alloc mem 381.47 MBEeager alloc mem 381.47 MB
 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 02:03:27.699843: E[ 2022-12-12 02:03:27/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu.:6998561815: ] EBuilding Coll Cache with ... num gpu device is 8 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-12 02:03:27.699929: E[ 2022-12-12 02:03:27/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu.:6999411980: ] Eeager alloc mem 381.47 MB 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 02:03:27.700061: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 3 solved
[2022-12-12 02:03:27.700114: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 3 initing device 3
[2022-12-12 02:03:27.700166: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 5 solved
[2022-12-12 02:03:27.700223: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 5 initing device 5
[2022-12-12 02:03:27.700520: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-12 02:03:27.700569: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 02:03:27.700667: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-12 02:03:27.700717: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 02:03:27.704082: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 02:03:27.704318: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 02:03:27.704371: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 02:03:27.704424: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 02:03:27.704482: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 02:03:27.704977: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 02:03:27.705043: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 02:03:27.708568: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 02:03:27.708616: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 02:03:27.708674: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 02:03:27.708738: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 02:03:27.708779: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 02:03:27.708835: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 02:03:27.761924: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1024.00 Bytes
[2022-12-12 02:03:27.768020: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-12 02:03:27.768162: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 02:03:27.769129: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 19.45 MB
[2022-12-12 02:03:27.769955: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 02:03:27.770988: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:03:27.771038: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 19.07 MB
[2022-12-12 02:03:27.774669: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 02:03:27.775450: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 02:03:27.775498: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 2.39 GB
[2022-12-12 02:03:27.784813: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1024.00 Bytes[
2022-12-12 02:03:27.784866: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1024.00 Bytes
[2022-12-12 02:03:27.784993: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1024.00 Bytes
[2022-12-12 02:03:27.786499: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1024.00 Bytes
[[2022-12-12 02:03:272022-12-12 02:03:27..786566786566: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::19801980] ] eager alloc mem 1024.00 Byteseager alloc mem 1024.00 Bytes

[2022-12-12 02:03:27.786647: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1024.00 Bytes
[2022-12-12 02:03:27.790370: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-12 02:03:27.790464: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 02:03:27.792128: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-12 02:03:27.792209: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc[:2022-12-12 02:03:27638.] 792211eager release cuda mem 400000000: 
E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-12 02:03:27.792297: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 02:03:27.800519: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 19.45 MB
[2022-12-12 02:03:27.800932: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-12 02:03:27.801018: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 02:03:27.801336: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-12 02:03:27.801418: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] [eager release cuda mem 4000000002022-12-12 02:03:27
.801423: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-12 02:03:27.801508: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc[:2022-12-12 02:03:27638.] 801509eager release cuda mem 400000000: 
E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-12 02:03:27.801601: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 02:03:27.806937: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 19.45 MB
[2022-12-12 02:03:27.807442: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 19.45 MB
[2022-12-12 02:03:27.814150: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 19.45 MB
[2022-12-12 02:03:27.814656: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 19.45 MB
[2022-12-12 02:03:27.815259: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 19.45 MB
[2022-12-12 02:03:27.815787: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 19.45 MB
[2022-12-12 02:03:27.816103: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 02:03:27.816467: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 02:03:27.816592: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 02:03:27.817046: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:03:27.817090: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 19.06 MB
[2022-12-12 02:03:27.817168: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 02:03:27.817210: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 02:03:27.817328: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 02:03:27.817371: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 02:03:27.817446: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:03:27.817493: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 19.07 MB
[2022-12-12 02:03:27.817585: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:03:27.817633: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 18.96 MB
[2022-12-12 02:03:27.818143: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:03:27.818190: W[ 2022-12-12 02:03:27/hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc.:81819643: ] EWORKER[0] alloc host memory 18.95 MB 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:03:27.818257: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 19.02 MB
[2022-12-12 02:03:27.818293: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:03:27.818339: W [/hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc2022-12-12 02:03:27:.43818347] : WORKER[0] alloc host memory 18.83 MBE
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:03:27.818406: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 19.01 MB
[2022-12-12 02:03:27.830012: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 02:03:27.830627: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 02:03:27.830670: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 2.39 GB
[2022-12-12 02:03:27.830752: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 02:03:27.830940: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 02:03:27.831067: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 02:03:27.831350: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 02:03:27.831394: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 2.39 GB
[2022-12-12 02:03:27.831540: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 02:03:27.831588: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 2.38 GB
[2022-12-12 02:03:27.831697: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 02:03:27.831744: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 2.38 GB
[2022-12-12 02:03:27.831918: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 02:03:27.832024: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980[] 2022-12-12 02:03:27eager alloc mem 25.25 KB.
832048: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 02:03:27.832517: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 02:03:27.832558: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 2.37 GB
[2022-12-12 02:03:27.832631: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855[
2022-12-12 02:03:27.832652: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] [eager release cuda mem 258552022-12-12 02:03:27
.832678: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 2.36 GB[
2022-12-12 02:03:27.832703: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 2.38 GB
[[[[[[[[2022-12-12 02:03:282022-12-12 02:03:282022-12-12 02:03:282022-12-12 02:03:282022-12-12 02:03:282022-12-12 02:03:282022-12-12 02:03:282022-12-12 02:03:28........635818635818635820635819635819635818635818635818: : : : : : : : EEEEEEEE        /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::::::::19261926192619261926192619261926] ] Device 2 init p2p of link 1] ] ] ] ] ] Device 5 init p2p of link 6
Device 0 init p2p of link 3Device 4 init p2p of link 5Device 1 init p2p of link 7Device 7 init p2p of link 4Device 6 init p2p of link 0Device 3 init p2p of link 2






[[2022-12-12 02:03:282022-12-12 02:03:28..636310[636310: [2022-12-12 02:03:28: E2022-12-12 02:03:28.E[ [[.636325 [2022-12-12 02:03:28/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu2022-12-12 02:03:282022-12-12 02:03:28636327: /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu2022-12-12 02:03:28.:..: E:.6363381980636341636341E 1980636351: ] : :  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu] : Eeager alloc mem 611.00 KBEE/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:eager alloc mem 611.00 KBE 
  :1980
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu1980] :/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::] eager alloc mem 611.00 KB1980:19801980eager alloc mem 611.00 KB
] 1980] ] 
eager alloc mem 611.00 KB] eager alloc mem 611.00 KBeager alloc mem 611.00 KB
eager alloc mem 611.00 KB


[2022-12-12 02:03:28.637321[: 2022-12-12 02:03:28E. 637330/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc: :E638 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cceager release cuda mem 625663:
638] eager release cuda mem 625663
[2022-12-12 02:03:28.637372: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:03:28.637403: E [/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc[2022-12-12 02:03:28:2022-12-12 02:03:28[[.638.2022-12-12 02:03:282022-12-12 02:03:28637416] 637422..: eager release cuda mem 625663: 637430637440E
E: :   EE/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc  ::/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc638638::] ] 638638eager release cuda mem 625663eager release cuda mem 625663] ] 

eager release cuda mem 625663eager release cuda mem 625663

[2022-12-12 02:03:28.651166: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 2
[2022-12-12 02:03:28.651313: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 02:03:28.651604: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 5 init p2p of link 4
[2022-12-12 02:03:28.651760: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 02:03:28.651799: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 2 init p2p of link 3
[2022-12-12 02:03:28.651947: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[[2022-12-12 02:03:282022-12-12 02:03:28..652126652136: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::1926638] ] Device 4 init p2p of link 7eager release cuda mem 625663

[2022-12-12 02:03:28.652212: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 7 init p2p of link 1
[2022-12-12 02:03:28.652279: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[[2022-12-12 02:03:282022-12-12 02:03:28..652359652350: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::19801926] ] eager alloc mem 611.00 KBDevice 6 init p2p of link 5

[2022-12-12 02:03:28.652526: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 02:03:28.652575: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:03:28.652653: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 0 init p2p of link 6
[2022-12-12 02:03:28.652784: E[ 2022-12-12 02:03:28/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc.[:6527852022-12-12 02:03:28638: .] E652810eager release cuda mem 625663 : 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE: 1926/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu] :Device 3 init p2p of link 01980
] eager alloc mem 611.00 KB
[2022-12-12 02:03:28.653013: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 02:03:28.653089: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:03:28.653183: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:03:28.653338: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:03:28.653690: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:03:28.653824: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:03:28.665615: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 3
[2022-12-12 02:03:28.665729: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 02:03:28.666102: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 5 init p2p of link 7
[2022-12-12 02:03:28.666212: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 02:03:28.666318[: 2022-12-12 02:03:28E. 666330/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu: :E1926 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuDevice 7 init p2p of link 6:
1926] Device 2 init p2p of link 0
[2022-12-12 02:03:28[.2022-12-12 02:03:28666486.: 666490E:  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE: 1980/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu] [:eager alloc mem 611.00 KB2022-12-12 02:03:281980
.] 666546eager alloc mem 611.00 KB: 
E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:03:28.666686: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 4 init p2p of link 2
[2022-12-12 02:03:28.666812: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 02:03:28.666876: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 6 init p2p of link 4
[2022-12-12 02:03:28.666990: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 02:03:28.667028: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:03:28.667216: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 0 init p2p of link 1
[2022-12-12 02:03:28.667334: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980[] 2022-12-12 02:03:28eager alloc mem 611.00 KB.[
6673522022-12-12 02:03:28: .E667365 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccE: 638/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc[] :2022-12-12 02:03:28eager release cuda mem 625663638.
] 667414eager release cuda mem 625663: 
E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 3 init p2p of link 5
[2022-12-12 02:03:28.667570: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980[] 2022-12-12 02:03:28eager alloc mem 611.00 KB.
667586: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:03:28.667794: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:03:28.668151: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:03:28.668374: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:03:28.682750: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 0
[2022-12-12 02:03:28.682862: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 02:03:28.682992: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 4 init p2p of link 6
[2022-12-12 02:03:28.683108: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 02:03:28.683583: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 5 init p2p of link 3
[2022-12-12 02:03:28.683681: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc[:2022-12-12 02:03:28638.] 683697eager release cuda mem 625663: 
E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 02:03:28.683944: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:03:28.684360: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 7 init p2p of link 5
[2022-12-12 02:03:28.684478: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 02:03:28.684506: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:03:28.684583: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 2 init p2p of link 4
[2022-12-12 02:03:28.684696: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 02:03:28.684814: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 3 init p2p of link 1
[2022-12-12 02:03:28.684927: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 02:03:28.685072: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 6 init p2p of link 7
[2022-12-12 02:03:28.685196: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 02:03:28.685276: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:03:28.685337: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 0 init p2p of link 2
[2022-12-12 02:03:28.685468[: 2022-12-12 02:03:28E. 685476/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu: :E1980 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cceager alloc mem 611.00 KB:
638] eager release cuda mem 625663
[2022-12-12 02:03:28.685727: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:03:28.685997: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:03:28.686297: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:03:28.699610: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 20400000
[2022-12-12 02:03:28.699995: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 20400000
[2022-12-12 02:03:28.700147: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 4983898 / 100000000 nodes ( 4.98 %~5.00 %) | remote 14872269 / 100000000 nodes ( 14.87 %) | cpu 80143833 / 100000000 nodes ( 80.14 %) | 2.38 GB | 1.00023 secs 
[2022-12-12 02:03:28.700238: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 20400000
[2022-12-12 02:03:28.700400: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 4986171 / 100000000 nodes ( 4.99 %~5.00 %) | remote 14869996 / 100000000 nodes ( 14.87 %) | cpu 80143833 / 100000000 nodes ( 80.14 %) | 2.38 GB | 0.999695 secs 
[2022-12-12 02:03:28.700651: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 4968814 / 100000000 nodes ( 4.97 %~5.00 %) | remote 14887353 / 100000000 nodes ( 14.89 %) | cpu 80143833 / 100000000 nodes ( 80.14 %) | 2.37 GB | 1.00094 secs 
[2022-12-12 02:03:28.700847: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 20400000
[2022-12-12 02:03:28.701460: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 20400000
[2022-12-12 02:03:28.701731: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 4936516 / 100000000 nodes ( 4.94 %~5.00 %) | remote 14919651 / 100000000 nodes ( 14.92 %) | cpu 80143833 / 100000000 nodes ( 80.14 %) | 2.36 GB | 1.00202 secs 
[2022-12-12 02:03:28.702182: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 20400000
[2022-12-12 02:03:28.702298: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 20400000
[2022-12-12 02:03:28.702495: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 20400000
[2022-12-12 02:03:28.704465: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 4998093 / 100000000 nodes ( 5.00 %~5.00 %) | remote 14858074 / 100000000 nodes ( 14.86 %) | cpu 80143833 / 100000000 nodes ( 80.14 %) | 2.39 GB | 1.00453 secs 
[2022-12-12 02:03:28.704698: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 4995168 / 100000000 nodes ( 5.00 %~5.00 %) | remote 14860999 / 100000000 nodes ( 14.86 %) | cpu 80143833 / 100000000 nodes ( 80.14 %) | 2.39 GB | 1.00584 secs 
[2022-12-12 02:03:28.704846: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 4971541 / 100000000 nodes ( 4.97 %~5.00 %) | remote 14884626 / 100000000 nodes ( 14.88 %) | cpu 80143833 / 100000000 nodes ( 80.14 %) | 2.38 GB | 1.00429 secs 
[2022-12-12 02:03:28.705274: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 4998154 / 100000000 nodes ( 5.00 %~5.00 %) | remote 14858013 / 100000000 nodes ( 14.86 %) | cpu 80143833 / 100000000 nodes ( 80.14 %) | 2.39 GB | 1.02155 secs 
[2022-12-12 02:03:28.708081: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 10.71 GB
[2022-12-12 02:03:30.  8518: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 10.97 GB
[2022-12-12 02:03:30.  8660: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 10.97 GB
[2022-12-12 02:03:30.  8986: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 10.97 GB
[2022-12-12 02:03:31.251623: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 11.23 GB
[2022-12-12 02:03:31.251752: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 11.23 GB
[2022-12-12 02:03:31.252043: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 11.23 GB
[2022-12-12 02:03:32.519620: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 11.45 GB
[2022-12-12 02:03:32.519778: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 11.45 GB
[2022-12-12 02:03:32.520132: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 11.45 GB
[2022-12-12 02:03:33.560612: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 11.66 GB
[2022-12-12 02:03:33.561244: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 11.66 GB
[2022-12-12 02:03:33.562008: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 11.66 GB
[2022-12-12 02:03:34.731086: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 12.12 GB
[2022-12-12 02:03:34.731985: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 12.12 GB
[2022-12-12 02:03:34.755234: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 12.12 GB
[2022-12-12 02:03:36.189012: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 12.32 GB
[2022-12-12 02:03:36.189219: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 12.32 GB
[HCTR][02:03:37.228][ERROR][RK0][tid #140160930924288]: replica 5 calling init per replica done, doing barrier
[HCTR][02:03:37.228][ERROR][RK0][tid #140160880600832]: replica 6 calling init per replica done, doing barrier
[HCTR][02:03:37.228][ERROR][RK0][tid #140160939316992]: replica 2 calling init per replica done, doing barrier
[HCTR][02:03:37.228][ERROR][RK0][main]: replica 1 calling init per replica done, doing barrier
[HCTR][02:03:37.228][ERROR][RK0][tid #140160880600832]: replica 4 calling init per replica done, doing barrier
[HCTR][02:03:37.228][ERROR][RK0][main]: replica 7 calling init per replica done, doing barrier
[HCTR][02:03:37.228][ERROR][RK0][tid #140160930924288]: replica 3 calling init per replica done, doing barrier
[HCTR][02:03:37.228][ERROR][RK0][main]: replica 0 calling init per replica done, doing barrier
[HCTR][02:03:37.228][ERROR][RK0][main]: replica 0 calling init per replica done, doing barrier done
[HCTR][02:03:37.228][ERROR][RK0][tid #140160880600832]: replica 4 calling init per replica done, doing barrier done
[HCTR][02:03:37.228][ERROR][RK0][tid #140160930924288]: replica 5 calling init per replica done, doing barrier done
[HCTR][02:03:37.228][ERROR][RK0][tid #140160880600832]: replica 6 calling init per replica done, doing barrier done
[HCTR][02:03:37.228][ERROR][RK0][main]: replica 1 calling init per replica done, doing barrier done
[HCTR][02:03:37.228][ERROR][RK0][main]: replica 7 calling init per replica done, doing barrier done
[HCTR][02:03:37.228][ERROR][RK0][tid #140160930924288]: replica 3 calling init per replica done, doing barrier done
[HCTR][02:03:37.228][ERROR][RK0][tid #140160880600832]: init per replica done
[HCTR][02:03:37.228][ERROR][RK0][tid #140160939316992]: replica 2 calling init per replica done, doing barrier done
[HCTR][02:03:37.228][ERROR][RK0][tid #140160880600832]: init per replica done
[HCTR][02:03:37.228][ERROR][RK0][tid #140160930924288]: init per replica done
[HCTR][02:03:37.228][ERROR][RK0][main]: init per replica done
[HCTR][02:03:37.228][ERROR][RK0][main]: init per replica done
[HCTR][02:03:37.228][ERROR][RK0][tid #140160930924288]: init per replica done
[HCTR][02:03:37.228][ERROR][RK0][tid #140160939316992]: init per replica done
[HCTR][02:03:37.231][ERROR][RK0][main]: init per replica done
[HCTR][02:03:37.267][ERROR][RK0][main]: 1 allocated 3276800 at 0x7f5de0238400
[HCTR][02:03:37.267][ERROR][RK0][main]: 1 allocated 6553600 at 0x7f5de0558400
[HCTR][02:03:37.267][ERROR][RK0][main]: 1 allocated 3276800 at 0x7f5de0b98400
[HCTR][02:03:37.267][ERROR][RK0][main]: 1 allocated 6553600 at 0x7f5de0eb8400
[HCTR][02:03:37.267][ERROR][RK0][tid #140160930924288]: 7 allocated 3276800 at 0x7f5dea238400
[HCTR][02:03:37.267][ERROR][RK0][tid #140160930924288]: 7 allocated 6553600 at 0x7f5dea558400
[HCTR][02:03:37.267][ERROR][RK0][tid #140160930924288]: 7 allocated 3276800 at 0x7f5deab98400
[HCTR][02:03:37.267][ERROR][RK0][tid #140160930924288]: 7 allocated 6553600 at 0x7f5deaeb8400
[HCTR][02:03:37.267][ERROR][RK0][tid #140160930924288]: 3 allocated 3276800 at 0x7f5dc8238400
[HCTR][02:03:37.267][ERROR][RK0][tid #140160930924288]: 3 allocated 6553600 at 0x7f5dc8558400
[HCTR][02:03:37.267][ERROR][RK0][main]: 6 allocated 3276800 at 0x7f5d04238400
[HCTR][02:03:37.267][ERROR][RK0][tid #140160930924288]: 3 allocated 3276800 at 0x7f5dc8b98400
[HCTR][02:03:37.267][ERROR][RK0][main]: 6 allocated 6553600 at 0x7f5d04558400
[HCTR][02:03:37.267][ERROR][RK0][tid #140160930924288]: 3 allocated 6553600 at 0x7f5dc8eb8400
[HCTR][02:03:37.267][ERROR][RK0][main]: 6 allocated 3276800 at 0x7f5d04b98400
[HCTR][02:03:37.267][ERROR][RK0][main]: 6 allocated 6553600 at 0x7f5d04eb8400
[HCTR][02:03:37.267][ERROR][RK0][tid #140160939316992]: 2 allocated 3276800 at 0x7f5de8238400
[HCTR][02:03:37.267][ERROR][RK0][main]: 4 allocated 3276800 at 0x7f5cf0238400
[HCTR][02:03:37.267][ERROR][RK0][tid #140160939316992]: 2 allocated 6553600 at 0x7f5de8558400
[HCTR][02:03:37.267][ERROR][RK0][tid #140160939316992]: 2 allocated 3276800 at 0x7f5de8b98400
[HCTR][02:03:37.267][ERROR][RK0][main]: 4 allocated 6553600 at 0x7f5cf0558400
[HCTR][02:03:37.267][ERROR][RK0][tid #140160939316992]: 2 allocated 6553600 at 0x7f5de8eb8400
[HCTR][02:03:37.267][ERROR][RK0][main]: 4 allocated 3276800 at 0x7f5cf0b98400
[HCTR][02:03:37.267][ERROR][RK0][main]: 4 allocated 6553600 at 0x7f5cf0eb8400
[HCTR][02:03:37.267][ERROR][RK0][tid #140160930924288]: 5 allocated 3276800 at 0x7f5dea238400
[HCTR][02:03:37.267][ERROR][RK0][tid #140160930924288]: 5 allocated 6553600 at 0x7f5dea558400
[HCTR][02:03:37.268][ERROR][RK0][tid #140160930924288]: 5 allocated 3276800 at 0x7f5deab98400
[HCTR][02:03:37.268][ERROR][RK0][tid #140160930924288]: 5 allocated 6553600 at 0x7f5deaeb8400
[HCTR][02:03:37.270][ERROR][RK0][tid #140161534904064]: 0 allocated 3276800 at 0x7f5dba320000
[HCTR][02:03:37.270][ERROR][RK0][tid #140161534904064]: 0 allocated 6553600 at 0x7f5dba640000
[HCTR][02:03:37.270][ERROR][RK0][tid #140161534904064]: 0 allocated 3276800 at 0x7f5dbac80000
[HCTR][02:03:37.270][ERROR][RK0][tid #140161534904064]: 0 allocated 6553600 at 0x7f5dbafa0000
