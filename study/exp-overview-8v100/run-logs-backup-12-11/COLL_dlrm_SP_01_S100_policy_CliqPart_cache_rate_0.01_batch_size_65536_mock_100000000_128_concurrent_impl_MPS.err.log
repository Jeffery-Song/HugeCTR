2022-12-11 20:16:30.950608: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:30.955424: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:30.961969: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:30.973417: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:30.981269: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:30.991644: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:30.998720: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.004423: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.051155: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.052626: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.053579: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.054599: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.055672: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.056789: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.057758: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.058745: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.059445: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.061236: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.062239: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.063217: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.064160: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.065312: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.066397: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.067458: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.068560: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.069639: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.070677: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.071719: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.072731: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.073878: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.074946: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.075993: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.077902: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.079037: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.080044: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.081153: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.082190: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.083302: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.084436: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.085476: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.089189: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.090784: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.090829: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 20:16:31.092212: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.092281: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.094507: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.094544: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.096674: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.097265: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.097371: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.099529: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.099710: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.099752: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.099955: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.099982: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.102613: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.102850: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.102943: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.103067: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.103209: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.105910: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.106247: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.106351: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.106409: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.106760: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.106860: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.109930: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.110574: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.110866: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.111025: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.112316: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.113611: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.114285: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.114435: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.114872: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.116149: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.117465: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.117845: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.118427: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.120119: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.121222: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.121343: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.121628: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.121868: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.123532: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.124540: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.125189: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.125289: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.126587: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.127158: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.128084: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.129272: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.129564: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.130463: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.132216: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.132385: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.133316: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.134479: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.134593: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.135546: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.136326: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.136863: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.137357: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.138770: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.140024: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.141351: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.141982: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.155169: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.163211: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.168663: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.172100: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.177111: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.179099: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.179140: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.179181: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.179266: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.180245: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.180394: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.182810: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.182899: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.182980: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.183170: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.184147: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.184237: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.184928: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.187117: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.187270: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.187461: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.187506: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.189296: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.189488: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.190182: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.191731: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.191900: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.192030: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.192075: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.193540: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.193548: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.194227: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.196252: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.196426: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.196558: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.196597: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.197774: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.197870: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.198454: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.200777: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.200816: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.200969: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.201113: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.202092: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.202262: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.203105: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.204621: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.204983: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.205094: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.205285: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.206373: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.206471: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.207193: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.208855: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.208961: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.209099: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.209310: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.210411: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.210493: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.211196: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.212917: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.213003: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.213181: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.213401: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.214491: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.215332: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.216862: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.216903: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.217223: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.217272: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.218334: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.218834: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 20:16:31.219172: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.220393: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.220522: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.220860: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.220906: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.221871: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.222894: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.224744: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.225078: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.225116: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.225117: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.225931: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.227125: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.227624: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.228480: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.229208: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.229280: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.229388: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.230429: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.231692: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.232192: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.233272: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.233692: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.233728: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.233869: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.234971: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.236238: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.236842: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.237781: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.238131: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.238174: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.238304: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.239343: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.241267: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.243484: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.243902: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.244013: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.244151: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.244459: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.245374: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.247816: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.248464: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.248592: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.248971: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.249903: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.249926: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.252327: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.252973: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.253061: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.253609: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.254454: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.255610: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.256668: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.257735: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.257877: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.258159: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.259065: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.261226: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.262219: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.262922: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.263254: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.263470: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.264299: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.266144: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.267036: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.267769: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.268047: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.268330: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.269056: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.270495: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.271805: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.272956: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.273201: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.273212: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.273620: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.275303: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.276242: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.277519: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.277627: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.277744: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.278175: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.279904: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.282537: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.283102: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.284553: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 20:16:31.285423: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 20:16:31.285435: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 20:16:31.285764: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 20:16:31.285824: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.288241: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 20:16:31.292661: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 20:16:31.294030: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.295157: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.295224: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.295265: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.297722: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.301528: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.325237: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.325410: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.325426: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.325441: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.325498: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.325536: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.330994: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.331062: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.331088: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.331144: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.331196: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:31.331249: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:32.387837: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:32.388460: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:32.389173: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:32.389894: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 20:16:32.389951: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-11 20:16:32.407331: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:32.407961: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:32.408473: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:32.409042: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:32.409642: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:32.410917: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-11 20:16:32.457618: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 20:16:32.457840: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 20:16:32.507855: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12340
2022-12-11 20:16:32.580227: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:32.580853: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:32.581381: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:32.581846: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 20:16:32.581899: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-11 20:16:32.599805: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:32.600455: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:32.600958: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:32.601537: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:32.602250: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:32.602724: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:1/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-11 20:16:32.669088: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:32.669698: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:32.670881: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:32.671371: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 20:16:32.671428: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-11 20:16:32.673157: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 20:16:32.673329: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 20:16:32.674998: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12341
2022-12-11 20:16:32.684825: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:32.685653: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:32.686561: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:32.687034: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 20:16:32.687085: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-11 20:16:32.688266: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:32.689268: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:32.689840: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:32.690451: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:32.690969: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:32.691468: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:2/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-11 20:16:32.695510: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:32.696505: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:32.697028: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:32.697501: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 20:16:32.697551: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-11 20:16:32.701550: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:32.702132: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:32.702240: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:32.702346: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:32.703671: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:32.703791: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:32.703899: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:32.704942: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 20:16:32.704994: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-11 20:16:32.705170: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:32.705316: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:32.705337: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:32.706890: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 20:16:32.706938: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-11 20:16:32.707044: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 20:16:32.707091: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-11 20:16:32.707253: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:32.708188: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:32.708955: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:32.709686: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:32.710323: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:7/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-11 20:16:32.715255: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:32.715906: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:32.716430: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:32.716998: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:32.717518: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:32.717982: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:3/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-11 20:16:32.722764: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:32.723433: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:32.723940: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:32.724275: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:32.724604: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:32.725238: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:32.725244: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:32.725538: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:32.726728: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:32.726801: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:32.726899: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:4/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-11 20:16:32.727850: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:32.727884: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:32.728870: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:32.728899: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:32.729798: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:6/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-11 20:16:32.729888: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:16:32.730359: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:5/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-11 20:16:32.754493: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 20:16:32.754708: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 20:16:32.756472: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12347
2022-12-11 20:16:32.763706: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 20:16:32.763902: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 20:16:32.768426: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12343
2022-12-11 20:16:32.773186: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 20:16:32.773367: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 20:16:32.775006: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12344
2022-12-11 20:16:32.775679: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 20:16:32.775832: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 20:16:32.776328: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 20:16:32.776504: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 20:16:32.776860: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12346
2022-12-11 20:16:32.778344: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12345
2022-12-11 20:16:32.780998: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 20:16:32.781156: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 20:16:32.783624: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12342
[HCTR][20:16:34.009][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][20:16:34.009][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][20:16:34.038][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][20:16:34.041][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][20:16:34.041][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][20:16:34.041][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][20:16:34.045][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][20:16:34.046][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 1it [00:01,  1.54s/it]warmup run: 1it [00:01,  1.57s/it]warmup run: 1it [00:01,  1.53s/it]warmup run: 98it [00:01, 82.87it/s]warmup run: 1it [00:01,  1.51s/it]warmup run: 95it [00:01, 78.93it/s]warmup run: 1it [00:01,  1.53s/it]warmup run: 92it [00:01, 78.11it/s]warmup run: 1it [00:01,  1.49s/it]warmup run: 1it [00:01,  1.47s/it]warmup run: 197it [00:01, 180.76it/s]warmup run: 94it [00:01, 80.99it/s]warmup run: 190it [00:01, 171.50it/s]warmup run: 90it [00:01, 76.70it/s]warmup run: 1it [00:01,  1.57s/it]warmup run: 185it [00:01, 170.42it/s]warmup run: 100it [00:01, 86.99it/s]warmup run: 100it [00:01, 87.97it/s]warmup run: 295it [00:01, 287.30it/s]warmup run: 190it [00:01, 177.40it/s]warmup run: 288it [00:01, 277.76it/s]warmup run: 185it [00:01, 171.72it/s]warmup run: 95it [00:01, 79.09it/s]warmup run: 278it [00:01, 272.17it/s]warmup run: 200it [00:01, 187.96it/s]warmup run: 200it [00:01, 190.00it/s]warmup run: 394it [00:01, 399.63it/s]warmup run: 286it [00:01, 283.33it/s]warmup run: 383it [00:01, 384.18it/s]warmup run: 281it [00:01, 277.72it/s]warmup run: 191it [00:01, 172.92it/s]warmup run: 372it [00:01, 379.01it/s]warmup run: 300it [00:01, 298.67it/s]warmup run: 300it [00:01, 301.35it/s]warmup run: 493it [00:02, 509.20it/s]warmup run: 382it [00:01, 393.04it/s]warmup run: 480it [00:02, 491.44it/s]warmup run: 378it [00:01, 388.51it/s]warmup run: 284it [00:01, 272.91it/s]warmup run: 466it [00:02, 483.17it/s]warmup run: 400it [00:01, 412.83it/s]warmup run: 401it [00:01, 417.56it/s]warmup run: 595it [00:02, 615.73it/s]warmup run: 479it [00:02, 500.82it/s]warmup run: 578it [00:02, 592.40it/s]warmup run: 477it [00:02, 499.78it/s]warmup run: 377it [00:01, 377.02it/s]warmup run: 562it [00:02, 582.89it/s]warmup run: 501it [00:01, 524.67it/s]warmup run: 502it [00:01, 529.87it/s]warmup run: 697it [00:02, 708.89it/s]warmup run: 576it [00:02, 599.75it/s]warmup run: 676it [00:02, 680.55it/s]warmup run: 574it [00:02, 597.61it/s]warmup run: 471it [00:02, 480.65it/s]warmup run: 658it [00:02, 669.37it/s]warmup run: 603it [00:02, 628.76it/s]warmup run: 605it [00:02, 635.33it/s]warmup run: 798it [00:02, 783.07it/s]warmup run: 673it [00:02, 685.14it/s]warmup run: 774it [00:02, 752.36it/s]warmup run: 670it [00:02, 680.90it/s]warmup run: 566it [00:02, 577.35it/s]warmup run: 755it [00:02, 742.35it/s]warmup run: 706it [00:02, 720.53it/s]warmup run: 706it [00:02, 721.18it/s]warmup run: 899it [00:02, 840.97it/s]warmup run: 770it [00:02, 755.76it/s]warmup run: 872it [00:02, 810.54it/s]warmup run: 769it [00:02, 756.26it/s]warmup run: 661it [00:02, 662.21it/s]warmup run: 855it [00:02, 808.15it/s]warmup run: 808it [00:02, 794.32it/s]warmup run: 807it [00:02, 792.01it/s]warmup run: 1000it [00:02, 884.35it/s]warmup run: 867it [00:02, 810.87it/s]warmup run: 968it [00:02, 849.57it/s]warmup run: 870it [00:02, 820.70it/s]warmup run: 758it [00:02, 736.58it/s]warmup run: 955it [00:02, 858.72it/s]warmup run: 910it [00:02, 851.97it/s]warmup run: 907it [00:02, 845.19it/s]warmup run: 1101it [00:02, 917.97it/s]warmup run: 965it [00:02, 855.82it/s]warmup run: 1064it [00:02, 872.57it/s]warmup run: 971it [00:02, 871.42it/s]warmup run: 855it [00:02, 796.89it/s]warmup run: 1055it [00:02, 897.29it/s]warmup run: 1011it [00:02, 894.30it/s]warmup run: 1007it [00:02, 886.19it/s]warmup run: 1202it [00:02, 942.40it/s]warmup run: 1062it [00:02, 887.55it/s]warmup run: 1159it [00:02, 894.27it/s]warmup run: 1073it [00:02, 910.87it/s]warmup run: 955it [00:02, 849.81it/s]warmup run: 1153it [00:02, 917.54it/s]warmup run: 1113it [00:02, 927.33it/s]warmup run: 1108it [00:02, 920.43it/s]warmup run: 1304it [00:02, 964.53it/s]warmup run: 1161it [00:02, 915.41it/s]warmup run: 1255it [00:02, 912.97it/s]warmup run: 1175it [00:02, 941.13it/s]warmup run: 1052it [00:02, 882.94it/s]warmup run: 1251it [00:02, 926.31it/s]warmup run: 1214it [00:02, 948.31it/s]warmup run: 1209it [00:02, 944.01it/s]warmup run: 1407it [00:02, 980.75it/s]warmup run: 1262it [00:02, 940.23it/s]warmup run: 1351it [00:02, 925.46it/s]warmup run: 1276it [00:02, 958.53it/s]warmup run: 1153it [00:02, 917.72it/s]warmup run: 1348it [00:02, 934.83it/s]warmup run: 1315it [00:02, 963.11it/s]warmup run: 1311it [00:02, 963.79it/s]warmup run: 1509it [00:03, 990.69it/s]warmup run: 1363it [00:02, 958.76it/s]warmup run: 1447it [00:03, 929.89it/s]warmup run: 1376it [00:02, 967.38it/s]warmup run: 1256it [00:02, 947.52it/s]warmup run: 1445it [00:03, 928.23it/s]warmup run: 1416it [00:02, 975.90it/s]warmup run: 1413it [00:02, 978.94it/s]warmup run: 1611it [00:03, 996.89it/s]warmup run: 1464it [00:03, 971.79it/s]warmup run: 1543it [00:03, 936.57it/s]warmup run: 1477it [00:03, 977.41it/s]warmup run: 1355it [00:02, 959.83it/s]warmup run: 1540it [00:03, 925.91it/s]warmup run: 1514it [00:02, 987.53it/s]warmup run: 1517it [00:03, 981.96it/s]warmup run: 1713it [00:03, 1003.71it/s]warmup run: 1564it [00:03, 980.04it/s]warmup run: 1639it [00:03, 942.54it/s]warmup run: 1579it [00:03, 987.67it/s]warmup run: 1456it [00:03, 971.88it/s]warmup run: 1634it [00:03, 925.64it/s]warmup run: 1616it [00:03, 996.39it/s]warmup run: 1618it [00:03, 988.78it/s]warmup run: 1816it [00:03, 1011.30it/s]warmup run: 1664it [00:03, 985.72it/s]warmup run: 1738it [00:03, 956.19it/s]warmup run: 1681it [00:03, 995.59it/s]warmup run: 1557it [00:03, 981.85it/s]warmup run: 1728it [00:03, 924.77it/s]warmup run: 1720it [00:03, 995.09it/s]warmup run: 1718it [00:03, 997.24it/s]warmup run: 1918it [00:03, 1013.59it/s]warmup run: 1764it [00:03, 984.60it/s]warmup run: 1836it [00:03, 962.84it/s]warmup run: 1782it [00:03, 991.44it/s]warmup run: 1659it [00:03, 990.23it/s]warmup run: 1822it [00:03, 921.37it/s]warmup run: 1821it [00:03, 997.81it/s]warmup run: 1820it [00:03, 1002.88it/s]warmup run: 2024it [00:03, 1025.56it/s]warmup run: 1864it [00:03, 987.13it/s]warmup run: 1934it [00:03, 965.64it/s]warmup run: 1882it [00:03, 982.94it/s]warmup run: 1760it [00:03, 992.78it/s]warmup run: 1915it [00:03, 919.99it/s]warmup run: 1922it [00:03, 1000.68it/s]warmup run: 1922it [00:03, 1007.46it/s]warmup run: 2147it [00:03, 1084.13it/s]warmup run: 1964it [00:03, 988.44it/s]warmup run: 2038it [00:03, 986.93it/s]warmup run: 1981it [00:03, 977.80it/s]warmup run: 1861it [00:03, 997.24it/s]warmup run: 2028it [00:03, 1021.34it/s]warmup run: 2027it [00:03, 1013.36it/s]warmup run: 2008it [00:03, 919.82it/s]warmup run: 2270it [00:03, 1124.93it/s]warmup run: 2076it [00:03, 1026.95it/s]warmup run: 2156it [00:03, 1043.17it/s]warmup run: 2097it [00:03, 1030.27it/s]warmup run: 1962it [00:03, 997.70it/s]warmup run: 2148it [00:03, 1074.39it/s]warmup run: 2148it [00:03, 1069.67it/s]warmup run: 2125it [00:03, 992.70it/s]warmup run: 2393it [00:03, 1154.03it/s]warmup run: 2197it [00:03, 1080.37it/s]warmup run: 2274it [00:03, 1082.25it/s]warmup run: 2218it [00:03, 1082.87it/s]warmup run: 2063it [00:03, 978.69it/s]warmup run: 2268it [00:03, 1110.88it/s]warmup run: 2269it [00:03, 1110.41it/s]warmup run: 2243it [00:03, 1045.68it/s]warmup run: 2509it [00:03, 1061.77it/s]warmup run: 2319it [00:03, 1119.82it/s]warmup run: 2392it [00:03, 1109.48it/s]warmup run: 2339it [00:03, 1120.07it/s]warmup run: 2162it [00:03, 979.39it/s]warmup run: 2388it [00:03, 1136.41it/s]warmup run: 2390it [00:03, 1137.69it/s]warmup run: 2361it [00:03, 1083.12it/s]warmup run: 2627it [00:04, 1092.64it/s]warmup run: 2441it [00:03, 1148.22it/s]warmup run: 2510it [00:04, 1128.33it/s]warmup run: 2460it [00:03, 1144.91it/s]warmup run: 2284it [00:03, 1048.24it/s]warmup run: 2508it [00:03, 1155.39it/s]warmup run: 2511it [00:03, 1157.95it/s]warmup run: 2479it [00:04, 1109.55it/s]warmup run: 2746it [00:04, 1120.39it/s]warmup run: 2564it [00:04, 1170.14it/s]warmup run: 2628it [00:04, 1143.23it/s]warmup run: 2581it [00:04, 1163.11it/s]warmup run: 2406it [00:03, 1097.42it/s]warmup run: 2627it [00:03, 1165.05it/s]warmup run: 2631it [00:04, 1170.42it/s]warmup run: 2597it [00:04, 1128.48it/s]warmup run: 2865it [00:04, 1140.44it/s]warmup run: 2686it [00:04, 1182.26it/s]warmup run: 2745it [00:04, 1150.04it/s]warmup run: 2701it [00:04, 1172.09it/s]warmup run: 2528it [00:04, 1131.61it/s]warmup run: 2748it [00:04, 1175.97it/s]warmup run: 2753it [00:04, 1182.47it/s]warmup run: 2715it [00:04, 1141.64it/s]warmup run: 2984it [00:04, 1154.56it/s]warmup run: 2809it [00:04, 1194.54it/s]warmup run: 3000it [00:04, 681.17it/s] warmup run: 2863it [00:04, 1158.63it/s]warmup run: 2821it [00:04, 1179.76it/s]warmup run: 2648it [00:04, 1151.38it/s]warmup run: 2868it [00:04, 1181.87it/s]warmup run: 2875it [00:04, 1191.86it/s]warmup run: 2831it [00:04, 1146.53it/s]warmup run: 2932it [00:04, 1204.28it/s]warmup run: 2981it [00:04, 1164.38it/s]warmup run: 2941it [00:04, 1183.74it/s]warmup run: 2771it [00:04, 1173.21it/s]warmup run: 3000it [00:04, 664.92it/s] warmup run: 2989it [00:04, 1187.52it/s]warmup run: 2997it [00:04, 1200.06it/s]warmup run: 2949it [00:04, 1155.06it/s]warmup run: 3000it [00:04, 694.39it/s] warmup run: 3000it [00:04, 683.78it/s] warmup run: 3000it [00:04, 696.64it/s] warmup run: 3000it [00:04, 680.73it/s] warmup run: 3000it [00:04, 663.61it/s] warmup run: 2891it [00:04, 1180.14it/s]warmup run: 3000it [00:04, 668.53it/s] 
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]





warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   6%|▌         | 166/3000 [00:00<00:01, 1658.87it/s]warmup should be done:   5%|▌         | 161/3000 [00:00<00:01, 1607.96it/s]warmup should be done:   5%|▌         | 159/3000 [00:00<00:01, 1587.46it/s]warmup should be done:   5%|▌         | 164/3000 [00:00<00:01, 1639.15it/s]warmup should be done:   6%|▌         | 165/3000 [00:00<00:01, 1643.30it/s]warmup should be done:   5%|▌         | 162/3000 [00:00<00:01, 1617.66it/s]warmup should be done:   5%|▌         | 162/3000 [00:00<00:01, 1613.98it/s]warmup should be done:   6%|▌         | 166/3000 [00:00<00:01, 1652.65it/s]warmup should be done:  11%|█         | 324/3000 [00:00<00:01, 1618.56it/s]warmup should be done:  11%|█         | 321/3000 [00:00<00:01, 1604.62it/s]warmup should be done:  11%|█         | 334/3000 [00:00<00:01, 1667.43it/s]warmup should be done:  11%|█         | 328/3000 [00:00<00:01, 1636.85it/s]warmup should be done:  11%|█         | 328/3000 [00:00<00:01, 1637.97it/s]warmup should be done:  11%|█         | 331/3000 [00:00<00:01, 1647.25it/s]warmup should be done:  11%|█         | 326/3000 [00:00<00:01, 1623.40it/s]warmup should be done:  11%|█         | 332/3000 [00:00<00:01, 1637.78it/s]warmup should be done:  16%|█▌        | 482/3000 [00:00<00:01, 1604.97it/s]warmup should be done:  16%|█▌        | 486/3000 [00:00<00:01, 1616.34it/s]warmup should be done:  17%|█▋        | 501/3000 [00:00<00:01, 1665.51it/s]warmup should be done:  16%|█▋        | 494/3000 [00:00<00:01, 1644.61it/s]warmup should be done:  16%|█▋        | 492/3000 [00:00<00:01, 1633.25it/s]warmup should be done:  17%|█▋        | 496/3000 [00:00<00:01, 1643.79it/s]warmup should be done:  16%|█▋        | 489/3000 [00:00<00:01, 1623.06it/s]warmup should be done:  17%|█▋        | 498/3000 [00:00<00:01, 1643.49it/s]warmup should be done:  21%|██▏       | 643/3000 [00:00<00:01, 1605.14it/s]warmup should be done:  22%|██▏       | 648/3000 [00:00<00:01, 1613.71it/s]warmup should be done:  22%|██▏       | 668/3000 [00:00<00:01, 1663.71it/s]warmup should be done:  22%|██▏       | 660/3000 [00:00<00:01, 1646.81it/s]warmup should be done:  22%|██▏       | 652/3000 [00:00<00:01, 1623.69it/s]warmup should be done:  22%|██▏       | 656/3000 [00:00<00:01, 1629.53it/s]warmup should be done:  22%|██▏       | 661/3000 [00:00<00:01, 1641.36it/s]warmup should be done:  22%|██▏       | 663/3000 [00:00<00:01, 1640.93it/s]warmup should be done:  27%|██▋       | 804/3000 [00:00<00:01, 1605.75it/s]warmup should be done:  27%|██▋       | 815/3000 [00:00<00:01, 1624.60it/s]warmup should be done:  28%|██▊       | 825/3000 [00:00<00:01, 1646.06it/s]warmup should be done:  27%|██▋       | 810/3000 [00:00<00:01, 1611.82it/s]warmup should be done:  27%|██▋       | 819/3000 [00:00<00:01, 1626.58it/s]warmup should be done:  28%|██▊       | 826/3000 [00:00<00:01, 1637.80it/s]warmup should be done:  28%|██▊       | 835/3000 [00:00<00:01, 1652.82it/s]warmup should be done:  28%|██▊       | 829/3000 [00:00<00:01, 1644.77it/s]warmup should be done:  32%|███▏      | 965/3000 [00:00<00:01, 1601.08it/s]warmup should be done:  33%|███▎      | 983/3000 [00:00<00:01, 1630.90it/s]warmup should be done:  33%|███▎      | 990/3000 [00:00<00:01, 1642.46it/s]warmup should be done:  33%|███▎      | 978/3000 [00:00<00:01, 1620.82it/s]warmup should be done:  32%|███▏      | 972/3000 [00:00<00:01, 1608.44it/s]warmup should be done:  33%|███▎      | 990/3000 [00:00<00:01, 1629.54it/s]warmup should be done:  33%|███▎      | 994/3000 [00:00<00:01, 1642.73it/s]warmup should be done:  33%|███▎      | 1001/3000 [00:00<00:01, 1640.36it/s]warmup should be done:  38%|███▊      | 1126/3000 [00:00<00:01, 1601.25it/s]warmup should be done:  38%|███▊      | 1149/3000 [00:00<00:01, 1639.02it/s]warmup should be done:  38%|███▊      | 1155/3000 [00:00<00:01, 1642.50it/s]warmup should be done:  38%|███▊      | 1141/3000 [00:00<00:01, 1621.17it/s]warmup should be done:  38%|███▊      | 1133/3000 [00:00<00:01, 1604.56it/s]warmup should be done:  38%|███▊      | 1153/3000 [00:00<00:01, 1628.76it/s]warmup should be done:  39%|███▊      | 1159/3000 [00:00<00:01, 1643.66it/s]warmup should be done:  39%|███▉      | 1166/3000 [00:00<00:01, 1636.18it/s]warmup should be done:  44%|████▍     | 1315/3000 [00:00<00:01, 1643.46it/s]warmup should be done:  44%|████▍     | 1320/3000 [00:00<00:01, 1643.65it/s]warmup should be done:  43%|████▎     | 1294/3000 [00:00<00:01, 1604.99it/s]warmup should be done:  43%|████▎     | 1304/3000 [00:00<00:01, 1618.73it/s]warmup should be done:  44%|████▍     | 1317/3000 [00:00<00:01, 1629.24it/s]warmup should be done:  44%|████▍     | 1324/3000 [00:00<00:01, 1642.90it/s]warmup should be done:  43%|████▎     | 1287/3000 [00:00<00:01, 1587.13it/s]warmup should be done:  44%|████▍     | 1330/3000 [00:00<00:01, 1635.06it/s]warmup should be done:  50%|████▉     | 1485/3000 [00:00<00:00, 1644.94it/s]warmup should be done:  48%|████▊     | 1455/3000 [00:00<00:00, 1604.70it/s]warmup should be done:  49%|████▉     | 1466/3000 [00:00<00:00, 1614.84it/s]warmup should be done:  49%|████▉     | 1480/3000 [00:00<00:00, 1633.32it/s]warmup should be done:  49%|████▉     | 1480/3000 [00:00<00:00, 1628.99it/s]warmup should be done:  50%|████▉     | 1489/3000 [00:00<00:00, 1643.10it/s]warmup should be done:  48%|████▊     | 1447/3000 [00:00<00:00, 1588.28it/s]warmup should be done:  50%|████▉     | 1494/3000 [00:00<00:00, 1634.24it/s]warmup should be done:  55%|█████▌    | 1650/3000 [00:01<00:00, 1645.60it/s]warmup should be done:  54%|█████▍    | 1616/3000 [00:01<00:00, 1604.00it/s]warmup should be done:  55%|█████▍    | 1643/3000 [00:01<00:00, 1627.41it/s]warmup should be done:  55%|█████▍    | 1646/3000 [00:01<00:00, 1639.01it/s]warmup should be done:  55%|█████▌    | 1654/3000 [00:01<00:00, 1643.82it/s]warmup should be done:  54%|█████▍    | 1628/3000 [00:01<00:00, 1608.45it/s]warmup should be done:  54%|█████▎    | 1608/3000 [00:01<00:00, 1592.30it/s]warmup should be done:  55%|█████▌    | 1658/3000 [00:01<00:00, 1632.50it/s]warmup should be done:  60%|██████    | 1815/3000 [00:01<00:00, 1644.63it/s]warmup should be done:  60%|██████    | 1811/3000 [00:01<00:00, 1642.02it/s]warmup should be done:  59%|█████▉    | 1777/3000 [00:01<00:00, 1599.77it/s]warmup should be done:  60%|██████    | 1806/3000 [00:01<00:00, 1627.14it/s]warmup should be done:  61%|██████    | 1819/3000 [00:01<00:00, 1644.60it/s]warmup should be done:  59%|█████▉    | 1768/3000 [00:01<00:00, 1593.57it/s]warmup should be done:  60%|█████▉    | 1789/3000 [00:01<00:00, 1604.94it/s]warmup should be done:  61%|██████    | 1822/3000 [00:01<00:00, 1630.32it/s]warmup should be done:  66%|██████▌   | 1980/3000 [00:01<00:00, 1644.02it/s]warmup should be done:  66%|██████▌   | 1976/3000 [00:01<00:00, 1642.25it/s]warmup should be done:  66%|██████▌   | 1969/3000 [00:01<00:00, 1625.50it/s]warmup should be done:  66%|██████▌   | 1985/3000 [00:01<00:00, 1646.36it/s]warmup should be done:  65%|██████▍   | 1937/3000 [00:01<00:00, 1594.49it/s]warmup should be done:  64%|██████▍   | 1928/3000 [00:01<00:00, 1590.88it/s]warmup should be done:  65%|██████▌   | 1950/3000 [00:01<00:00, 1603.17it/s]warmup should be done:  66%|██████▌   | 1986/3000 [00:01<00:00, 1624.41it/s]warmup should be done:  72%|███████▏  | 2145/3000 [00:01<00:00, 1642.65it/s]warmup should be done:  71%|███████▏  | 2141/3000 [00:01<00:00, 1642.33it/s]warmup should be done:  72%|███████▏  | 2150/3000 [00:01<00:00, 1647.22it/s]warmup should be done:  70%|██████▉   | 2097/3000 [00:01<00:00, 1590.42it/s]warmup should be done:  70%|███████   | 2111/3000 [00:01<00:00, 1601.74it/s]warmup should be done:  70%|██████▉   | 2088/3000 [00:01<00:00, 1579.65it/s]warmup should be done:  72%|███████▏  | 2149/3000 [00:01<00:00, 1612.74it/s]warmup should be done:  71%|███████   | 2132/3000 [00:01<00:00, 1585.03it/s]warmup should be done:  77%|███████▋  | 2310/3000 [00:01<00:00, 1640.18it/s]warmup should be done:  77%|███████▋  | 2315/3000 [00:01<00:00, 1646.56it/s]warmup should be done:  77%|███████▋  | 2306/3000 [00:01<00:00, 1641.28it/s]warmup should be done:  75%|███████▌  | 2257/3000 [00:01<00:00, 1588.43it/s]warmup should be done:  76%|███████▌  | 2272/3000 [00:01<00:00, 1599.02it/s]warmup should be done:  75%|███████▍  | 2246/3000 [00:01<00:00, 1578.94it/s]warmup should be done:  76%|███████▋  | 2293/3000 [00:01<00:00, 1589.86it/s]warmup should be done:  77%|███████▋  | 2311/3000 [00:01<00:00, 1604.13it/s]warmup should be done:  82%|████████▎ | 2475/3000 [00:01<00:00, 1639.16it/s]warmup should be done:  83%|████████▎ | 2481/3000 [00:01<00:00, 1648.05it/s]warmup should be done:  82%|████████▏ | 2471/3000 [00:01<00:00, 1641.10it/s]warmup should be done:  81%|████████  | 2416/3000 [00:01<00:00, 1585.06it/s]warmup should be done:  81%|████████  | 2433/3000 [00:01<00:00, 1600.47it/s]warmup should be done:  80%|████████  | 2406/3000 [00:01<00:00, 1582.52it/s]warmup should be done:  82%|████████▏ | 2455/3000 [00:01<00:00, 1596.95it/s]warmup should be done:  82%|████████▏ | 2472/3000 [00:01<00:00, 1602.14it/s]warmup should be done:  88%|████████▊ | 2640/3000 [00:01<00:00, 1641.52it/s]warmup should be done:  88%|████████▊ | 2646/3000 [00:01<00:00, 1648.49it/s]warmup should be done:  88%|████████▊ | 2637/3000 [00:01<00:00, 1643.82it/s]warmup should be done:  86%|████████▌ | 2575/3000 [00:01<00:00, 1585.26it/s]warmup should be done:  86%|████████▋ | 2594/3000 [00:01<00:00, 1600.15it/s]warmup should be done:  87%|████████▋ | 2615/3000 [00:01<00:00, 1591.30it/s]warmup should be done:  88%|████████▊ | 2633/3000 [00:01<00:00, 1600.80it/s]warmup should be done:  86%|████████▌ | 2565/3000 [00:01<00:00, 1492.42it/s]warmup should be done:  94%|█████████▎| 2805/3000 [00:01<00:00, 1642.53it/s]warmup should be done:  94%|█████████▎| 2811/3000 [00:01<00:00, 1647.20it/s]warmup should be done:  93%|█████████▎| 2802/3000 [00:01<00:00, 1644.56it/s]warmup should be done:  91%|█████████ | 2734/3000 [00:01<00:00, 1584.83it/s]warmup should be done:  92%|█████████▏| 2755/3000 [00:01<00:00, 1601.83it/s]warmup should be done:  93%|█████████▎| 2778/3000 [00:01<00:00, 1599.97it/s]warmup should be done:  93%|█████████▎| 2794/3000 [00:01<00:00, 1600.38it/s]warmup should be done:  91%|█████████ | 2725/3000 [00:01<00:00, 1523.26it/s]warmup should be done:  99%|█████████▉| 2971/3000 [00:01<00:00, 1646.98it/s]warmup should be done:  99%|█████████▉| 2978/3000 [00:01<00:00, 1651.31it/s]warmup should be done:  99%|█████████▉| 2968/3000 [00:01<00:00, 1648.37it/s]warmup should be done:  96%|█████████▋| 2895/3000 [00:01<00:00, 1590.47it/s]warmup should be done:  97%|█████████▋| 2919/3000 [00:01<00:00, 1610.38it/s]warmup should be done:  98%|█████████▊| 2942/3000 [00:01<00:00, 1608.94it/s]warmup should be done:  99%|█████████▊| 2957/3000 [00:01<00:00, 1607.61it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1645.99it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1643.11it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1640.31it/s]warmup should be done:  96%|█████████▌| 2887/3000 [00:01<00:00, 1549.50it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1624.70it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1617.10it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1610.81it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1597.84it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1576.52it/s]



warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]



warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   6%|▌         | 169/3000 [00:00<00:01, 1688.07it/s]warmup should be done:   6%|▌         | 166/3000 [00:00<00:01, 1656.88it/s]warmup should be done:   6%|▌         | 167/3000 [00:00<00:01, 1666.42it/s]warmup should be done:   6%|▌         | 168/3000 [00:00<00:01, 1674.78it/s]warmup should be done:   6%|▌         | 168/3000 [00:00<00:01, 1672.81it/s]warmup should be done:   6%|▌         | 166/3000 [00:00<00:01, 1653.14it/s]warmup should be done:   6%|▌         | 168/3000 [00:00<00:01, 1673.33it/s]warmup should be done:   5%|▌         | 164/3000 [00:00<00:01, 1631.28it/s]warmup should be done:  11%|█         | 334/3000 [00:00<00:01, 1667.19it/s]warmup should be done:  11%|█▏        | 339/3000 [00:00<00:01, 1691.04it/s]warmup should be done:  11%|█         | 333/3000 [00:00<00:01, 1662.22it/s]warmup should be done:  11%|█         | 336/3000 [00:00<00:01, 1675.82it/s]warmup should be done:  11%|█▏        | 339/3000 [00:00<00:01, 1690.81it/s]warmup should be done:  11%|█         | 332/3000 [00:00<00:01, 1652.32it/s]warmup should be done:  11%|█         | 337/3000 [00:00<00:01, 1679.84it/s]warmup should be done:  11%|█         | 329/3000 [00:00<00:01, 1638.03it/s]warmup should be done:  17%|█▋        | 509/3000 [00:00<00:01, 1694.13it/s]warmup should be done:  17%|█▋        | 501/3000 [00:00<00:01, 1668.19it/s]warmup should be done:  17%|█▋        | 505/3000 [00:00<00:01, 1679.08it/s]warmup should be done:  17%|█▋        | 498/3000 [00:00<00:01, 1654.44it/s]warmup should be done:  17%|█▋        | 510/3000 [00:00<00:01, 1697.53it/s]warmup should be done:  17%|█▋        | 507/3000 [00:00<00:01, 1686.22it/s]warmup should be done:  16%|█▋        | 494/3000 [00:00<00:01, 1640.31it/s]warmup should be done:  17%|█▋        | 501/3000 [00:00<00:01, 1658.35it/s]warmup should be done:  23%|██▎       | 679/3000 [00:00<00:01, 1696.38it/s]warmup should be done:  22%|██▏       | 674/3000 [00:00<00:01, 1682.77it/s]warmup should be done:  22%|██▏       | 669/3000 [00:00<00:01, 1669.36it/s]warmup should be done:  23%|██▎       | 677/3000 [00:00<00:01, 1689.13it/s]warmup should be done:  22%|██▏       | 664/3000 [00:00<00:01, 1652.94it/s]warmup should be done:  23%|██▎       | 681/3000 [00:00<00:01, 1698.54it/s]warmup should be done:  22%|██▏       | 659/3000 [00:00<00:01, 1637.43it/s]warmup should be done:  22%|██▏       | 667/3000 [00:00<00:01, 1651.65it/s]warmup should be done:  28%|██▊       | 843/3000 [00:00<00:01, 1685.16it/s]warmup should be done:  28%|██▊       | 850/3000 [00:00<00:01, 1698.41it/s]warmup should be done:  28%|██▊       | 851/3000 [00:00<00:01, 1698.96it/s]warmup should be done:  28%|██▊       | 836/3000 [00:00<00:01, 1667.56it/s]warmup should be done:  28%|██▊       | 847/3000 [00:00<00:01, 1689.90it/s]warmup should be done:  28%|██▊       | 830/3000 [00:00<00:01, 1651.22it/s]warmup should be done:  27%|██▋       | 823/3000 [00:00<00:01, 1637.83it/s]warmup should be done:  28%|██▊       | 833/3000 [00:00<00:01, 1650.13it/s]warmup should be done:  34%|███▍      | 1020/3000 [00:00<00:01, 1698.22it/s]warmup should be done:  34%|███▎      | 1012/3000 [00:00<00:01, 1684.54it/s]warmup should be done:  33%|███▎      | 1003/3000 [00:00<00:01, 1668.19it/s]warmup should be done:  34%|███▍      | 1022/3000 [00:00<00:01, 1701.05it/s]warmup should be done:  33%|███▎      | 996/3000 [00:00<00:01, 1652.71it/s]warmup should be done:  34%|███▍      | 1017/3000 [00:00<00:01, 1690.30it/s]warmup should be done:  33%|███▎      | 989/3000 [00:00<00:01, 1644.00it/s]warmup should be done:  33%|███▎      | 999/3000 [00:00<00:01, 1650.58it/s]warmup should be done:  39%|███▉      | 1170/3000 [00:00<00:01, 1668.02it/s]warmup should be done:  40%|███▉      | 1190/3000 [00:00<00:01, 1694.79it/s]warmup should be done:  39%|███▉      | 1181/3000 [00:00<00:01, 1681.57it/s]warmup should be done:  40%|███▉      | 1193/3000 [00:00<00:01, 1700.69it/s]warmup should be done:  39%|███▊      | 1162/3000 [00:00<00:01, 1653.64it/s]warmup should be done:  38%|███▊      | 1155/3000 [00:00<00:01, 1648.71it/s]warmup should be done:  40%|███▉      | 1187/3000 [00:00<00:01, 1690.66it/s]warmup should be done:  39%|███▉      | 1165/3000 [00:00<00:01, 1647.52it/s]warmup should be done:  45%|████▍     | 1338/3000 [00:00<00:00, 1668.94it/s]warmup should be done:  45%|████▌     | 1361/3000 [00:00<00:00, 1697.24it/s]warmup should be done:  45%|████▌     | 1350/3000 [00:00<00:00, 1683.46it/s]warmup should be done:  45%|████▌     | 1364/3000 [00:00<00:00, 1702.38it/s]warmup should be done:  45%|████▌     | 1357/3000 [00:00<00:00, 1692.76it/s]warmup should be done:  44%|████▍     | 1321/3000 [00:00<00:01, 1649.44it/s]warmup should be done:  44%|████▍     | 1328/3000 [00:00<00:01, 1650.66it/s]warmup should be done:  44%|████▍     | 1330/3000 [00:00<00:01, 1635.64it/s]warmup should be done:  51%|█████     | 1531/3000 [00:00<00:00, 1697.72it/s]warmup should be done:  50%|█████     | 1505/3000 [00:00<00:00, 1668.49it/s]warmup should be done:  51%|█████     | 1519/3000 [00:00<00:00, 1683.61it/s]warmup should be done:  51%|█████     | 1535/3000 [00:00<00:00, 1702.62it/s]warmup should be done:  50%|████▉     | 1488/3000 [00:00<00:00, 1655.36it/s]warmup should be done:  51%|█████     | 1527/3000 [00:00<00:00, 1692.67it/s]warmup should be done:  50%|████▉     | 1494/3000 [00:00<00:00, 1651.61it/s]warmup should be done:  50%|████▉     | 1497/3000 [00:00<00:00, 1644.44it/s]warmup should be done:  57%|█████▋    | 1701/3000 [00:01<00:00, 1697.67it/s]warmup should be done:  56%|█████▌    | 1673/3000 [00:01<00:00, 1669.68it/s]warmup should be done:  56%|█████▋    | 1688/3000 [00:01<00:00, 1684.64it/s]warmup should be done:  57%|█████▋    | 1706/3000 [00:01<00:00, 1702.83it/s]warmup should be done:  57%|█████▋    | 1697/3000 [00:01<00:00, 1693.12it/s]warmup should be done:  55%|█████▌    | 1656/3000 [00:01<00:00, 1660.13it/s]warmup should be done:  55%|█████▌    | 1660/3000 [00:01<00:00, 1653.20it/s]warmup should be done:  55%|█████▌    | 1664/3000 [00:01<00:00, 1651.72it/s]warmup should be done:  62%|██████▏   | 1872/3000 [00:01<00:00, 1700.11it/s]warmup should be done:  61%|██████▏   | 1841/3000 [00:01<00:00, 1672.28it/s]warmup should be done:  62%|██████▏   | 1858/3000 [00:01<00:00, 1687.03it/s]warmup should be done:  63%|██████▎   | 1877/3000 [00:01<00:00, 1704.90it/s]warmup should be done:  62%|██████▏   | 1867/3000 [00:01<00:00, 1693.67it/s]warmup should be done:  61%|██████    | 1824/3000 [00:01<00:00, 1664.70it/s]warmup should be done:  61%|██████    | 1826/3000 [00:01<00:00, 1654.00it/s]warmup should be done:  61%|██████    | 1831/3000 [00:01<00:00, 1657.09it/s]warmup should be done:  68%|██████▊   | 2043/3000 [00:01<00:00, 1701.78it/s]warmup should be done:  68%|██████▊   | 2027/3000 [00:01<00:00, 1687.51it/s]warmup should be done:  67%|██████▋   | 2009/3000 [00:01<00:00, 1672.11it/s]warmup should be done:  68%|██████▊   | 2049/3000 [00:01<00:00, 1706.53it/s]warmup should be done:  68%|██████▊   | 2037/3000 [00:01<00:00, 1694.31it/s]warmup should be done:  66%|██████▋   | 1991/3000 [00:01<00:00, 1664.36it/s]warmup should be done:  66%|██████▋   | 1992/3000 [00:01<00:00, 1654.28it/s]warmup should be done:  67%|██████▋   | 2000/3000 [00:01<00:00, 1665.47it/s]warmup should be done:  74%|███████▍  | 2214/3000 [00:01<00:00, 1701.22it/s]warmup should be done:  73%|███████▎  | 2196/3000 [00:01<00:00, 1686.07it/s]warmup should be done:  73%|███████▎  | 2177/3000 [00:01<00:00, 1670.59it/s]warmup should be done:  74%|███████▍  | 2220/3000 [00:01<00:00, 1704.86it/s]warmup should be done:  72%|███████▏  | 2159/3000 [00:01<00:00, 1657.55it/s]warmup should be done:  72%|███████▏  | 2159/3000 [00:01<00:00, 1666.75it/s]warmup should be done:  74%|███████▎  | 2207/3000 [00:01<00:00, 1692.53it/s]warmup should be done:  72%|███████▏  | 2171/3000 [00:01<00:00, 1676.38it/s]warmup should be done:  79%|███████▉  | 2365/3000 [00:01<00:00, 1686.94it/s]warmup should be done:  80%|███████▉  | 2385/3000 [00:01<00:00, 1701.61it/s]warmup should be done:  78%|███████▊  | 2345/3000 [00:01<00:00, 1672.14it/s]warmup should be done:  80%|███████▉  | 2391/3000 [00:01<00:00, 1704.61it/s]warmup should be done:  78%|███████▊  | 2326/3000 [00:01<00:00, 1661.01it/s]warmup should be done:  78%|███████▊  | 2327/3000 [00:01<00:00, 1670.42it/s]warmup should be done:  79%|███████▉  | 2377/3000 [00:01<00:00, 1691.81it/s]warmup should be done:  78%|███████▊  | 2342/3000 [00:01<00:00, 1685.24it/s]warmup should be done:  85%|████████▌ | 2556/3000 [00:01<00:00, 1703.51it/s]warmup should be done:  84%|████████▍ | 2535/3000 [00:01<00:00, 1687.88it/s]warmup should be done:  84%|████████▍ | 2513/3000 [00:01<00:00, 1673.26it/s]warmup should be done:  85%|████████▌ | 2562/3000 [00:01<00:00, 1705.40it/s]warmup should be done:  83%|████████▎ | 2497/3000 [00:01<00:00, 1677.87it/s]warmup should be done:  83%|████████▎ | 2493/3000 [00:01<00:00, 1659.32it/s]warmup should be done:  85%|████████▍ | 2547/3000 [00:01<00:00, 1692.58it/s]warmup should be done:  84%|████████▍ | 2513/3000 [00:01<00:00, 1691.84it/s]warmup should be done:  91%|█████████ | 2727/3000 [00:01<00:00, 1703.43it/s]warmup should be done:  90%|█████████ | 2704/3000 [00:01<00:00, 1687.58it/s]warmup should be done:  89%|████████▉ | 2681/3000 [00:01<00:00, 1670.71it/s]warmup should be done:  89%|████████▉ | 2667/3000 [00:01<00:00, 1683.01it/s]warmup should be done:  91%|█████████ | 2717/3000 [00:01<00:00, 1694.47it/s]warmup should be done:  89%|████████▊ | 2659/3000 [00:01<00:00, 1656.35it/s]warmup should be done:  91%|█████████ | 2733/3000 [00:01<00:00, 1696.13it/s]warmup should be done:  89%|████████▉ | 2684/3000 [00:01<00:00, 1695.21it/s]warmup should be done:  96%|█████████▌| 2873/3000 [00:01<00:00, 1686.27it/s]warmup should be done:  97%|█████████▋| 2898/3000 [00:01<00:00, 1702.34it/s]warmup should be done:  95%|█████████▍| 2837/3000 [00:01<00:00, 1685.46it/s]warmup should be done:  95%|█████████▍| 2849/3000 [00:01<00:00, 1669.29it/s]warmup should be done:  96%|█████████▌| 2887/3000 [00:01<00:00, 1692.74it/s]warmup should be done:  94%|█████████▍| 2825/3000 [00:01<00:00, 1651.34it/s]warmup should be done:  97%|█████████▋| 2903/3000 [00:01<00:00, 1684.92it/s]warmup should be done:  95%|█████████▌| 2854/3000 [00:01<00:00, 1696.12it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1699.42it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1696.73it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1690.97it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1684.91it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1670.60it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1669.77it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1664.27it/s]warmup should be done: 100%|█████████▉| 2991/3000 [00:01<00:00, 1647.61it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1652.57it/s]2022-12-11 20:18:06.644577: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7ef69bf93050 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 20:18:06.644637: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 20:18:08.767781: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7ef697830e30 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 20:18:08.767808: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7ef69bffee80 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 20:18:08.767845: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 20:18:08.767853: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 20:18:08.867643: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 20:18:09.532035: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7ef68f830700 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 20:18:09.532099: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 20:18:09.532294: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7ef69782c860 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 20:18:09.532345: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 20:18:09.599203: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7ef69f833830 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 20:18:09.599267: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 20:18:09.611595: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7edb00030ff0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 20:18:09.611658: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 20:18:09.616807: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7ef69f830d80 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 20:18:09.616862: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 20:18:10.982289: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 20:18:11.011923: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 20:18:11.769921: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 20:18:11.820422: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 20:18:11.859979: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 20:18:11.930197: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 20:18:11.936831: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 20:18:11.973928: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 20:18:13.792050: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 20:18:13.855265: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 20:18:14.723091: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 20:18:14.827097: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 20:18:14.884219: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 20:18:14.887595: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 20:18:14.948017: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
[HCTR][20:18:58.045][ERROR][RK0][main]: replica 0 reaches 1000, calling init pre replica
[HCTR][20:18:58.045][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][20:18:58.051][ERROR][RK0][main]: coll ps creation done
[HCTR][20:18:58.051][ERROR][RK0][main]: replica 0 waits for coll ps creation barrier
[HCTR][20:18:58.064][ERROR][RK0][main]: replica 1 reaches 1000, calling init pre replica
[HCTR][20:18:58.064][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][20:18:58.072][ERROR][RK0][main]: coll ps creation done
[HCTR][20:18:58.072][ERROR][RK0][main]: replica 1 waits for coll ps creation barrier
[HCTR][20:18:58.159][ERROR][RK0][main]: replica 4 reaches 1000, calling init pre replica
[HCTR][20:18:58.159][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][20:18:58.167][ERROR][RK0][main]: coll ps creation done
[HCTR][20:18:58.167][ERROR][RK0][main]: replica 4 waits for coll ps creation barrier
[HCTR][20:18:58.181][ERROR][RK0][tid #139598088881920]: replica 5 reaches 1000, calling init pre replica
[HCTR][20:18:58.181][ERROR][RK0][tid #139598088881920]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][20:18:58.186][ERROR][RK0][tid #139598088881920]: coll ps creation done
[HCTR][20:18:58.186][ERROR][RK0][tid #139598088881920]: replica 5 waits for coll ps creation barrier
[HCTR][20:18:58.360][ERROR][RK0][main]: replica 6 reaches 1000, calling init pre replica
[HCTR][20:18:58.360][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][20:18:58.365][ERROR][RK0][main]: coll ps creation done
[HCTR][20:18:58.365][ERROR][RK0][main]: replica 6 waits for coll ps creation barrier
[HCTR][20:18:58.376][ERROR][RK0][tid #139598147598080]: replica 2 reaches 1000, calling init pre replica
[HCTR][20:18:58.376][ERROR][RK0][tid #139598147598080]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][20:18:58.377][ERROR][RK0][main]: replica 7 reaches 1000, calling init pre replica
[HCTR][20:18:58.377][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][20:18:58.384][ERROR][RK0][tid #139598147598080]: coll ps creation done
[HCTR][20:18:58.384][ERROR][RK0][tid #139598147598080]: replica 2 waits for coll ps creation barrier
[HCTR][20:18:58.386][ERROR][RK0][main]: coll ps creation done
[HCTR][20:18:58.386][ERROR][RK0][main]: replica 7 waits for coll ps creation barrier
[HCTR][20:18:58.393][ERROR][RK0][tid #139598013380352]: replica 3 reaches 1000, calling init pre replica
[HCTR][20:18:58.393][ERROR][RK0][tid #139598013380352]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][20:18:58.397][ERROR][RK0][tid #139598013380352]: coll ps creation done
[HCTR][20:18:58.397][ERROR][RK0][tid #139598013380352]: replica 3 waits for coll ps creation barrier
[HCTR][20:18:58.397][ERROR][RK0][main]: replica 0 preparing frequency
[HCTR][20:18:59.355][ERROR][RK0][main]: replica 0 preparing frequency done
[HCTR][20:18:59.385][ERROR][RK0][main]: replica 0 calling init per replica
[HCTR][20:18:59.385][ERROR][RK0][main]: replica 1 calling init per replica
[HCTR][20:18:59.385][ERROR][RK0][tid #139598147598080]: replica 2 calling init per replica
[HCTR][20:18:59.385][ERROR][RK0][main]: replica 4 calling init per replica
[HCTR][20:18:59.385][ERROR][RK0][main]: replica 7 calling init per replica
[HCTR][20:18:59.385][ERROR][RK0][tid #139598013380352]: replica 3 calling init per replica
[HCTR][20:18:59.385][ERROR][RK0][tid #139598088881920]: replica 5 calling init per replica
[HCTR][20:18:59.385][ERROR][RK0][main]: replica 6 calling init per replica
[HCTR][20:18:59.385][ERROR][RK0][main]: Calling build_v2
[HCTR][20:18:59.385][ERROR][RK0][main]: Calling build_v2
[HCTR][20:18:59.386][ERROR][RK0][tid #139598147598080]: Calling build_v2
[HCTR][20:18:59.386][ERROR][RK0][main]: Calling build_v2
[HCTR][20:18:59.386][ERROR][RK0][main]: Calling build_v2
[HCTR][20:18:59.386][ERROR][RK0][tid #139598013380352]: Calling build_v2
[HCTR][20:18:59.386][ERROR][RK0][tid #139598088881920]: Calling build_v2
[HCTR][20:18:59.386][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][20:18:59.386][ERROR][RK0][main]: Calling build_v2
[HCTR][20:18:59.386][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][20:18:59.386][ERROR][RK0][tid #139598147598080]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][20:18:59.386][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][20:18:59.386][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][20:18:59.386][ERROR][RK0][tid #139598013380352]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][20:18:59.386][ERROR][RK0][tid #139598088881920]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][20:18:59.386][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[[[[2022-12-11 20:18:59[[2022-12-11 20:18:59[[2022-12-11 20:18:59.2022-12-11 20:18:59.2022-12-11 20:18:592022-12-11 20:18:59.3861422022-12-11 20:18:59.386142.2022-12-11 20:18:59.386142: .386153: 386159.386160: E386166: E: 386166: E : E E: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.ccE /hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc E /hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc: /hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc:/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc /hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc:136/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc:136:/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc:136] :136] 136:136] using concurrent impl MPS136] using concurrent impl MPS] 136] using concurrent impl MPS
] using concurrent impl MPS
using concurrent impl MPS] using concurrent impl MPS
using concurrent impl MPS

using concurrent impl MPS


[2022-12-11 20:18:59.390420: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:178] v100x8, slow pcie
[2022-12-11 20:18:59.390457: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[:2022-12-11 20:18:59196.] 390465assigning 8 to cpu: 
E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:178] v100x8, slow pcie
[2022-12-11 20:18:59.390511[: [2022-12-11 20:18:59E2022-12-11 20:18:59. .390512/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc390519: :: E196E ]  /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccassigning 8 to cpu[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:
2022-12-11 20:18:59:178.212] 390554] v100x8, slow pcie: build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8
E
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[:2022-12-11 20:18:59178.] 390601[[v100x8, slow pcie[: 2022-12-11 20:18:592022-12-11 20:18:59
2022-12-11 20:18:59E... [390609[390604390615/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-11 20:18:59: 2022-12-11 20:18:59: : :.E.EE196390647[ 390647  ] : 2022-12-11 20:18:59/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccassigning 8 to cpuE[.:E::
 2022-12-11 20:18:59390691212 178213/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.: ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] ] :390734Ebuild asymm link desc with 8X Tesla V100-SXM2-32GB out of 8:[v100x8, slow pcieremote time is 8.68421196:  
1782022-12-11 20:18:59

] E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] .[assigning 8 to cpu[ [:v100x8, slow pcie3908312022-12-11 20:18:59
2022-12-11 20:18:59/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-11 20:18:59178
: ..:.] E390893[390898178390902v100x8, slow pcie : 2022-12-11 20:18:59: [] : 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE.E2022-12-11 20:18:59v100x8, slow pcieE: [390962 .
 212/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-11 20:18:59: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc390989/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] [:.E:: :build asymm link desc with 8X Tesla V100-SXM2-32GB out of 82142022-12-11 20:18:59213391036 196E
] .] : [/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc]  cpu time is 97.0588391082remote time is 8.68421E2022-12-11 20:18:59:assigning 8 to cpu/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
: 
 .196
:E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[391172] 212 :2022-12-11 20:18:59: assigning 8 to cpu[] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc196.E
2022-12-11 20:18:59build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8:] 391235 .
196assigning 8 to cpu: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc391285] 
E[[:: assigning 8 to cpu 2022-12-11 20:18:592022-12-11 20:18:59213E
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc..[]  :3913503913662022-12-11 20:18:59remote time is 8.68421/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc214: : .[
:] EE3914022022-12-11 20:18:59212[cpu time is 97.0588  : .] 2022-12-11 20:18:59
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE391449build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8.:: : 
391480213212/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE: ] ] : [Eremote time is 8.68421build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8212/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-11 20:18:59 

] :./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccbuild asymm link desc with 8X Tesla V100-SXM2-32GB out of 8212[391564:[
] 2022-12-11 20:18:59: [2142022-12-11 20:18:59build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8.E2022-12-11 20:18:59] .
391613 .cpu time is 97.0588391621: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc391655[
: E:: 2022-12-11 20:18:59E 213E. /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc]  391702/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:remote time is 8.68421/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: :214
:E213] 213 ] [cpu time is 97.0588] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccremote time is 8.684212022-12-11 20:18:59
remote time is 8.68421:
.
213391807] : [remote time is 8.68421[E2022-12-11 20:18:59
2022-12-11 20:18:59 ../hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc391855[391859:: 2022-12-11 20:18:59: 214E.E]  391878 cpu time is 97.0588/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
:E:214 214] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] cpu time is 97.0588:cpu time is 97.0588
214
] cpu time is 97.0588
[2022-12-11 20:20:16.403277: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:118] solver created. now build & solve
[2022-12-11 20:20:16.443224: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:123] solver built. now solve
block 0 storage is 00010001
	access is	0	0	0	0	4	4	4	4	
block 1 storage is 00100010
	access is	1	1	1	1	5	5	5	5	
block 2 storage is 01000100
	access is	2	2	2	2	6	6	6	6	
block 3 storage is 10001000
	access is	3	3	3	3	7	7	7	7	
block 4 storage is 00000000
	access is	8	8	8	8	8	8	8	8	
[2022-12-11 20:20:16.568215: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:127] solver solved
[2022-12-11 20:20:16.568279: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:195] 0 solved master
[2022-12-11 20:20:16.626841: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 0 solved
[2022-12-11 20:20:16.626881: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 0 initing device 0
[2022-12-11 20:20:16.627449: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-11 20:20:16.627499: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 20:20:16.628498: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 20:20:16.629386: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 20:20:16.642111: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 1 solved
[2022-12-11 20:20:16.642173: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 1 initing device 1
[2022-12-11 20:20:16.642590: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-11 20:20:16.642631: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 20:20:16.643607: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 20:20:16.643876: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 3 solved
[2022-12-11 20:20:16.643931: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 3 initing device 3
[2022-12-11 20:20:16.643949: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 5 solved
[2022-12-11 20:20:16.644022: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 5 initing device 5
[2022-12-11 20:20:16.644264: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] [4 solved2022-12-11 20:20:16
.644292: E[ 2022-12-11 20:20:16/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc.:[6443372022022-12-11 20:20:16: ] .E7 solved644353 
: /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.ccE:[ 2052022-12-11 20:20:16/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu] .:worker 0 thread 4 initing device 46444101815
: [] E2022-12-11 20:20:16Building Coll Cache with ... num gpu device is 8 .
/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc644451:: 205E]  worker 0 thread 7 initing device 7/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu[
[:2022-12-11 20:20:16[2022-12-11 20:20:162022-12-11 20:20:161815...] 644506644504644508Building Coll Cache with ... num gpu device is 8: : : 
EE[E  [2022-12-11 20:20:16 /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu2022-12-11 20:20:16./hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc::.644573:19801980644613: 202] ] : E] eager alloc mem 381.47 MBeager alloc mem 381.47 MBE 6 solved

 /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc
/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::2021980[] ] 2022-12-11 20:20:162 solvedeager alloc mem 381.47 MB.

644726: E[ 2022-12-11 20:20:16/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc.:644766205: ] Eworker 0 thread 6 initing device 6 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 2 initing device 2
[2022-12-11 20:20:16.644869: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-11 20:20:16.644912: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 20:20:16.644940: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-11 20:20:16.644988: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 20:20:16.645225: E[ 2022-12-11 20:20:16/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu.:6452371815: ] EBuilding Coll Cache with ... num gpu device is 8 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-11 20:20:16.645306: E[ 2022-12-11 20:20:16/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu.:6453171980: ] Eeager alloc mem 381.47 MB 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 20:20:16.649304: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 20:20:16.649366: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 20:20:16.649418: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 20:20:16.649480: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 20:20:16.649580: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 20:20:16.649637: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 20:20:16.653207: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 20:20:16.653257: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 20:20:16.653362: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 20:20:16.653442: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 20:20:16.653485: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 20:20:16.653617: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 20:20:16.709737: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.00 Bytes
[2022-12-11 20:20:16.715227: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5
[2022-12-11 20:20:16.715350: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-11 20:20:16.716193: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 4.20 MB
[2022-12-11 20:20:16.716763: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 20:20:16.717751: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 20:20:16.718818: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 20:20:16.719548: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 20:20:16.719593: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 493.16 MB
[2022-12-11 20:20:16.730799: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.00 Bytes
[2022-12-11 20:20:16.736733: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5
[2022-12-11 20:20:16.736870: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[[[[2022-12-11 20:20:162022-12-11 20:20:162022-12-11 20:20:162022-12-11 20:20:16....736994736994737003737003: : : : EEEE    /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::::1980198019801980] ] ] ] eager alloc mem 5.00 Byteseager alloc mem 5.00 Byteseager alloc mem 5.00 Byteseager alloc mem 5.00 Bytes



[2022-12-11 20:20:16.737730: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 4.20 MB
[2022-12-11 20:20:16.738258: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 20:20:16.739231: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[[2022-12-11 20:20:162022-12-11 20:20:16..739873739874: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::19801980] ] eager alloc mem 5.00 Byteseager alloc mem 5.00 Bytes

[2022-12-11 20:20:16.740216: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 20:20:16.740954: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 20:20:16.740997: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 493.16 MB
[2022-12-11 20:20:16.742858: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5
[2022-12-11 20:20:16.742933: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000[
2022-12-11 20:20:16.742944: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5
[2022-12-11 20:20:16.743019: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:[6382022-12-11 20:20:16] .eager release cuda mem 400000000743022
: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5
[2022-12-11 20:20:16.743106: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-11 20:20:16.743684: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5
[2022-12-11 20:20:16.743759: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-11 20:20:16.744271: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 4.20 MB
[2022-12-11 20:20:16.744771: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 4.20 MB
[2022-12-11 20:20:16.744934: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5
[2022-12-11 20:20:16.745011: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-11 20:20:16.745080: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5
[2022-12-11 20:20:16.745153: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-11 20:20:16.745285: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 4.20 MB
[2022-12-11 20:20:16.746609: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 4.20 MB
[2022-12-11 20:20:16.747362: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 4.20 MB
[2022-12-11 20:20:16.747874: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 4.20 MB
[2022-12-11 20:20:16.748226: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 20:20:16.748272: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 20:20:16.748427: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 20:20:16.748957: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 20:20:16.749064: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 20:20:16.749096: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 20:20:16.749188: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 20:20:16.749218: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 20:20:16.749383: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 20:20:16.749901: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 20:20:16.750015: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] [eager release cuda mem 6256632022-12-11 20:20:16
.750036: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 20:20:16.750352: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 20:20:16.750855: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 20:20:16.750999: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 20:20:16.751056: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 20:20:16.751103: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 493.16 MB
[2022-12-11 20:20:16.751562: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 20:20:16.751605: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 493.16 MB
[2022-12-11 20:20:16.751703: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 20:20:16.751749: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 493.16 MB
[2022-12-11 20:20:16.752886: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 20:20:16.752962: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 20:20:16.753591: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 20:20:16.753632: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 493.16 MB
[2022-12-11 20:20:16.753660: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 20:20:16.753701: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 493.16 MB
[2022-12-11 20:20:16.754393: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 20:20:16.755103: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 20:20:16.755158: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 493.16 MB
[[[[[[[[2022-12-11 20:20:162022-12-11 20:20:162022-12-11 20:20:162022-12-11 20:20:162022-12-11 20:20:162022-12-11 20:20:162022-12-11 20:20:162022-12-11 20:20:16........931554931554931554931555931554931554931553931554: : : : : : : : EEEEEEEE        /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::1926:::::19261926] 19261926192619261926] ] Device 2 init p2p of link 1] ] ] ] ] Device 7 init p2p of link 4Device 5 init p2p of link 6
Device 0 init p2p of link 3Device 6 init p2p of link 0Device 4 init p2p of link 5Device 1 init p2p of link 7Device 3 init p2p of link 2






[2022-12-11 20:20:16.932004: E [/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu2022-12-11 20:20:16:[.[1980[[2022-12-11 20:20:169320152022-12-11 20:20:16] [2022-12-11 20:20:162022-12-11 20:20:16.[: .eager alloc mem 611.00 KB2022-12-11 20:20:16..9320252022-12-11 20:20:16E932029
.932035932036: . : 932045: : E932053/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE: EE : : E  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE1980/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu: ] :/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::1980/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cueager alloc mem 611.00 KB1980:19801980] :
] 1980] ] eager alloc mem 611.00 KB1980eager alloc mem 611.00 KB] eager alloc mem 611.00 KBeager alloc mem 611.00 KB
] 
eager alloc mem 611.00 KB

eager alloc mem 611.00 KB

[2022-12-11 20:20:16.932873: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 20:20:16.933070: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 20:20:16[.2022-12-11 20:20:16933095[[.: [2022-12-11 20:20:162022-12-11 20:20:16933100E[2022-12-11 20:20:16..:  2022-12-11 20:20:16.933109933110E/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc.933118: :  :933130: EE/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc638: E  :] E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc638eager release cuda mem 625663 /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::] 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638638eager release cuda mem 625663:638] ] 
638] eager release cuda mem 625663eager release cuda mem 625663] eager release cuda mem 625663

eager release cuda mem 625663

[2022-12-11 20:20:16.946117: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] [Device 2 init p2p of link 32022-12-11 20:20:16
.946141: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 6 init p2p of link 5
[2022-12-11 20:20:16.946263: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 20:20:16.946291: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 20:20:16.946358: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 7 init p2p of link 1
[2022-12-11 20:20:16.946497: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 20:20:16.946723: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 5 init p2p of link 4
[2022-12-11 20:20:16.946806: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 2
[2022-12-11 20:20:16.946868: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 20:20:16.946964: E [/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu2022-12-11 20:20:16:.1980946967] : eager alloc mem 611.00 KBE
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 3 init p2p of link 0
[2022-12-11 20:20:16.947047: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 20:20:16.947113: E[ 2022-12-11 20:20:16/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc.:947125638: ] Eeager release cuda mem 625663 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 20:20:16.947294: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 20:20:16.947390: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 4 init p2p of link 7
[2022-12-11 20:20:16.947536: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 20:20:16.947639: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 0 init p2p of link 6
[2022-12-11 20:20:16.947683: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 20:20:16.947785: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:[19802022-12-11 20:20:16] .eager alloc mem 611.00 KB947803
: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 20:20:16.947968: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 20:20:16.948340: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 20:20:16.948595: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 20:20:16.964592: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 2 init p2p of link 0
[2022-12-11 20:20:16.964704: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 20:20:16.964832: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 7 init p2p of link 6
[2022-12-11 20:20:16.964958: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 20:20:16.964992: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 6 init p2p of link 4
[2022-12-11 20:20:16.965111: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 20:20:16.965458: E[ 2022-12-11 20:20:16/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu.:9654801926: ] EDevice 5 init p2p of link 7 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 20:20:16.965609: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 20:20:16.965643: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 3
[2022-12-11 20:20:16.965769[: 2022-12-11 20:20:16E. 965783/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc: :E638 [] /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu2022-12-11 20:20:16eager release cuda mem 625663:.
1980965818] : eager alloc mem 611.00 KBE
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 3 init p2p of link 5
[2022-12-11 20:20:16.965912: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 20:20:16.965948: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu[:2022-12-11 20:20:161926.] 965976Device 0 init p2p of link 1: 
E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 20:20:16.966075: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 20:20:16.966111: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 4 init p2p of link 2
[2022-12-11 20:20:16.966225: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 20:20:16.966388: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 20:20:16.966653: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 20:20:16.966792: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 20:20:16.966890: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 20:20:16.967022: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 20:20:16.994681: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 2 init p2p of link 4
[2022-12-11 20:20:16.994909: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 20:20:16.995230: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 7 init p2p of link 5
[2022-12-11 20:20:16.995355: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 20:20:16.995578: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 6 init p2p of link 7
[2022-12-11 20:20:16.995666: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926[] 2022-12-11 20:20:16[Device 3 init p2p of link 1.2022-12-11 20:20:16
995698.: 995702E:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu :/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:6381980] ] eager release cuda mem 625663eager alloc mem 611.00 KB

[2022-12-11 20:20:16.995809: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 20:20:16.995861: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 5 init p2p of link 3
[2022-12-11 20:20:16.995983: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[[2022-12-11 20:20:162022-12-11 20:20:16..996154996165: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::1926638] ] Device 1 init p2p of link 0eager release cuda mem 625663

[2022-12-11 20:20:16.996340: E[ 2022-12-11 20:20:16/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu.:9963451980: ] Eeager alloc mem 611.00 KB 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 4 init p2p of link 6
[2022-12-11 20:20:16.996473: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 20:20:16.996555: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 20:20:16.996625: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 20:20:16[.2022-12-11 20:20:16996729.: 996745: EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::1926638] ] Device 0 init p2p of link 2eager release cuda mem 625663

[2022-12-11 20:20:16.996898: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 20:20:16.997167: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 20:20:16.997283: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 20:20:16.997692: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 20:20:17. 14794: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 4399996
[2022-12-11 20:20:17. 15981: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 4399996
[2022-12-11 20:20:17. 16275: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 4399996
[2022-12-11 20:20:17. 16331: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 4399996
[2022-12-11 20:20:17. 16662: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 4399996
[2022-12-11 20:20:17. 17374: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 4399996
[2022-12-11 20:20:17. 17719: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 4399996
[2022-12-11 20:20:17. 18055: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 4399996
[2022-12-11 20:20:17. 18098: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 999999 / 100000000 nodes ( 1.00 %~1.00 %) | remote 2999997 / 100000000 nodes ( 3.00 %) | cpu 96000004 / 100000000 nodes ( 96.00 %) | 493.16 MB | 0.372785 secs 
[2022-12-11 20:20:17. 18629: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 999999 / 100000000 nodes ( 1.00 %~1.00 %) | remote 2999997 / 100000000 nodes ( 3.00 %) | cpu 96000004 / 100000000 nodes ( 96.00 %) | 493.16 MB | 0.373651 secs 
[2022-12-11 20:20:17. 18765: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 999999 / 100000000 nodes ( 1.00 %~1.00 %) | remote 2999997 / 100000000 nodes ( 3.00 %) | cpu 96000004 / 100000000 nodes ( 96.00 %) | 493.16 MB | 0.374163 secs 
[2022-12-11 20:20:17. 19268: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 999999 / 100000000 nodes ( 1.00 %~1.00 %) | remote 2999997 / 100000000 nodes ( 3.00 %) | cpu 96000004 / 100000000 nodes ( 96.00 %) | 493.16 MB | 0.373968 secs 
[2022-12-11 20:20:17. 19398: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 999999 / 100000000 nodes ( 1.00 %~1.00 %) | remote 2999997 / 100000000 nodes ( 3.00 %) | cpu 96000004 / 100000000 nodes ( 96.00 %) | 493.16 MB | 0.374898 secs 
[2022-12-11 20:20:17. 19983: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 999999 / 100000000 nodes ( 1.00 %~1.00 %) | remote 2999997 / 100000000 nodes ( 3.00 %) | cpu 96000004 / 100000000 nodes ( 96.00 %) | 493.16 MB | 0.377359 secs 
[2022-12-11 20:20:17. 20602: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 999999 / 100000000 nodes ( 1.00 %~1.00 %) | remote 2999997 / 100000000 nodes ( 3.00 %) | cpu 96000004 / 100000000 nodes ( 96.00 %) | 493.16 MB | 0.375696 secs 
[2022-12-11 20:20:17. 21623: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 999999 / 100000000 nodes ( 1.00 %~1.00 %) | remote 2999997 / 100000000 nodes ( 3.00 %) | cpu 96000004 / 100000000 nodes ( 96.00 %) | 493.16 MB | 0.394133 secs 
[2022-12-11 20:20:17. 32991: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 7.12 GB
[2022-12-11 20:20:18.458912: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 7.38 GB
[2022-12-11 20:20:18.459822: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 7.38 GB
[2022-12-11 20:20:18.464666: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 7.38 GB
[2022-12-11 20:20:20.194796: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 7.65 GB
[2022-12-11 20:20:20.195725: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 7.65 GB
[2022-12-11 20:20:20.196914: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 7.65 GB
[2022-12-11 20:20:21.414300: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 7.86 GB
[2022-12-11 20:20:21.414469: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 7.86 GB
[2022-12-11 20:20:21.415607: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 7.86 GB
[2022-12-11 20:20:22.454469: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 8.08 GB
[2022-12-11 20:20:22.455256: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 8.08 GB
[2022-12-11 20:20:22.456180: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 8.08 GB
[2022-12-11 20:20:23.298376: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 8.53 GB
[2022-12-11 20:20:23.298661: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 8.53 GB
[2022-12-11 20:20:23.299062: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 8.53 GB
[2022-12-11 20:20:24.711978: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 8.73 GB
[2022-12-11 20:20:24.712602: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 8.73 GB
[HCTR][20:20:26.246][ERROR][RK0][tid #139598013380352]: replica 3 calling init per replica done, doing barrier
[HCTR][20:20:26.246][ERROR][RK0][main]: replica 7 calling init per replica done, doing barrier
[HCTR][20:20:26.246][ERROR][RK0][main]: replica 0 calling init per replica done, doing barrier
[HCTR][20:20:26.246][ERROR][RK0][main]: replica 1 calling init per replica done, doing barrier
[HCTR][20:20:26.246][ERROR][RK0][main]: replica 4 calling init per replica done, doing barrier
[HCTR][20:20:26.246][ERROR][RK0][tid #139598088881920]: replica 5 calling init per replica done, doing barrier
[HCTR][20:20:26.246][ERROR][RK0][main]: replica 6 calling init per replica done, doing barrier
[HCTR][20:20:26.246][ERROR][RK0][tid #139598147598080]: replica 2 calling init per replica done, doing barrier
[HCTR][20:20:26.246][ERROR][RK0][main]: replica 0 calling init per replica done, doing barrier done
[HCTR][20:20:26.246][ERROR][RK0][main]: replica 1 calling init per replica done, doing barrier done
[HCTR][20:20:26.246][ERROR][RK0][tid #139598147598080]: replica 2 calling init per replica done, doing barrier done
[HCTR][20:20:26.246][ERROR][RK0][tid #139598088881920]: replica 5 calling init per replica done, doing barrier done
[HCTR][20:20:26.246][ERROR][RK0][main]: replica 6 calling init per replica done, doing barrier done
[HCTR][20:20:26.246][ERROR][RK0][main]: replica 7 calling init per replica done, doing barrier done
[HCTR][20:20:26.246][ERROR][RK0][tid #139598013380352]: replica 3 calling init per replica done, doing barrier done
[HCTR][20:20:26.246][ERROR][RK0][main]: replica 4 calling init per replica done, doing barrier done
[HCTR][20:20:26.246][ERROR][RK0][main]: init per replica done
[HCTR][20:20:26.246][ERROR][RK0][tid #139598147598080]: init per replica done
[HCTR][20:20:26.246][ERROR][RK0][tid #139598088881920]: init per replica done
[HCTR][20:20:26.246][ERROR][RK0][main]: init per replica done
[HCTR][20:20:26.246][ERROR][RK0][main]: init per replica done
[HCTR][20:20:26.246][ERROR][RK0][main]: init per replica done
[HCTR][20:20:26.246][ERROR][RK0][tid #139598013380352]: init per replica done
[HCTR][20:20:26.248][ERROR][RK0][main]: init per replica done
[HCTR][20:20:26.252][ERROR][RK0][tid #139598147598080]: 2 allocated 3276800 at 0x7ef884f20000
[HCTR][20:20:26.252][ERROR][RK0][tid #139598147598080]: 2 allocated 6553600 at 0x7ef885400000
[HCTR][20:20:26.252][ERROR][RK0][tid #139598147598080]: 2 allocated 3276800 at 0x7ef885a40000
[HCTR][20:20:26.252][ERROR][RK0][tid #139598147598080]: 2 allocated 6553600 at 0x7ef885d60000
[HCTR][20:20:26.252][ERROR][RK0][tid #139598072096512]: 1 allocated 3276800 at 0x7ef882f20000
[HCTR][20:20:26.252][ERROR][RK0][main]: 4 allocated 3276800 at 0x7ef880f20000
[HCTR][20:20:26.252][ERROR][RK0][tid #139598072096512]: 1 allocated 6553600 at 0x7ef883400000
[HCTR][20:20:26.252][ERROR][RK0][main]: 4 allocated 6553600 at 0x7ef881400000
[HCTR][20:20:26.252][ERROR][RK0][tid #139598072096512]: 1 allocated 3276800 at 0x7ef883a40000
[HCTR][20:20:26.252][ERROR][RK0][main]: 4 allocated 3276800 at 0x7ef881a40000
[HCTR][20:20:26.252][ERROR][RK0][tid #139598072096512]: 1 allocated 6553600 at 0x7ef883d60000
[HCTR][20:20:26.252][ERROR][RK0][main]: 4 allocated 6553600 at 0x7ef881d60000
[HCTR][20:20:26.252][ERROR][RK0][main]: 3 allocated 3276800 at 0x7ef884f20000
[HCTR][20:20:26.252][ERROR][RK0][main]: 5 allocated 3276800 at 0x7ef882f20000
[HCTR][20:20:26.252][ERROR][RK0][main]: 3 allocated 6553600 at 0x7ef885400000
[HCTR][20:20:26.252][ERROR][RK0][main]: 5 allocated 6553600 at 0x7ef883400000
[HCTR][20:20:26.252][ERROR][RK0][main]: 3 allocated 3276800 at 0x7ef885a40000
[HCTR][20:20:26.252][ERROR][RK0][main]: 5 allocated 3276800 at 0x7ef883a40000
[HCTR][20:20:26.252][ERROR][RK0][main]: 3 allocated 6553600 at 0x7ef885d60000
[HCTR][20:20:26.252][ERROR][RK0][main]: 5 allocated 6553600 at 0x7ef883d60000
[HCTR][20:20:26.252][ERROR][RK0][tid #139598281815808]: 6 allocated 3276800 at 0x7ef884f20000
[HCTR][20:20:26.252][ERROR][RK0][tid #139598281815808]: 6 allocated 6553600 at 0x7ef885400000
[HCTR][20:20:26.252][ERROR][RK0][tid #139598281815808]: 6 allocated 3276800 at 0x7ef885a40000
[HCTR][20:20:26.252][ERROR][RK0][tid #139598281815808]: 6 allocated 6553600 at 0x7ef885d60000
[HCTR][20:20:26.252][ERROR][RK0][tid #139598013380352]: 7 allocated 3276800 at 0x7ef884f20000
[HCTR][20:20:26.252][ERROR][RK0][tid #139598013380352]: 7 allocated 6553600 at 0x7ef885400000
[HCTR][20:20:26.252][ERROR][RK0][tid #139598013380352]: 7 allocated 3276800 at 0x7ef885a40000
[HCTR][20:20:26.252][ERROR][RK0][tid #139598013380352]: 7 allocated 6553600 at 0x7ef885d60000
[HCTR][20:20:26.255][ERROR][RK0][main]: 0 allocated 3276800 at 0x7ef887520000
[HCTR][20:20:26.255][ERROR][RK0][main]: 0 allocated 6553600 at 0x7ef887a00000
[HCTR][20:20:26.255][ERROR][RK0][main]: 0 allocated 3276800 at 0x7ef88870e800
[HCTR][20:20:26.255][ERROR][RK0][main]: 0 allocated 6553600 at 0x7ef888a2e800








