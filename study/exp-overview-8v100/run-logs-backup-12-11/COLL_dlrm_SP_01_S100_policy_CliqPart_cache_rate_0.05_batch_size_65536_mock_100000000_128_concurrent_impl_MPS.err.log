2022-12-12 01:05:33.243018: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.248499: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.257696: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.261851: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.268868: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.281255: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.288574: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.299975: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.350454: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.356145: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.359404: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.361096: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.361626: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.362394: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.363323: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.363888: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.364855: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.365246: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.366419: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.366614: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.367886: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.367969: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.369364: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.369408: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.371041: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.371176: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.372548: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.372858: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.373819: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.374899: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.375936: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.376893: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.378617: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.379722: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.380756: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.381836: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.382779: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.383859: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.384811: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.385762: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.391076: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 01:05:33.392256: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.393366: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.394934: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.396176: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.396274: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.397818: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.397981: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.399338: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.399990: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.400182: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.401836: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.402791: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.403056: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.404129: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.404173: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.405274: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.405514: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.406930: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.408229: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.409685: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.410803: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.412599: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.413461: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.413485: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.415581: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.416579: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.418128: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.418173: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.419392: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.421149: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.421568: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.422875: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.424223: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.424356: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.424997: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.426146: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.427551: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.428093: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.428621: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.429346: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.430088: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.430632: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.431013: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.431814: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.432247: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.433047: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.436665: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.438217: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.438538: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.439644: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.440081: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.440879: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.441945: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.442425: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.443017: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.451635: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.459992: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.472151: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.477730: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.478532: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.479178: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.479920: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.479970: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.480641: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.482204: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.483271: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.483715: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.484162: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.484470: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.484561: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.485100: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.488611: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.488651: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.490015: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.490173: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.490257: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.490808: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.493608: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.493968: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.494131: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.494250: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.494737: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.497549: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.497960: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.498088: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.498269: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.498778: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.501472: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.501852: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.501896: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.502060: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.502360: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.504929: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.505421: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.505467: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.505558: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.505960: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.508805: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.508848: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.508988: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.509197: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.509293: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.511934: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.512090: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.512236: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.512832: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.512869: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.516484: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.516574: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.516802: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.517571: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.517757: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.520025: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.520142: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.520340: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.520890: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.521286: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.524005: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.524088: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.524231: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.524705: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.524899: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.527526: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.527722: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.527952: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.528426: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.528474: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.530906: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.531084: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.531143: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.531402: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.531785: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.532093: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.534891: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.535048: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.535064: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.535275: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.535664: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.536376: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.536637: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.540036: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.540131: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.541599: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.542094: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.542304: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.543014: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.544150: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.544195: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.545148: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.545428: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 01:05:33.546037: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.546069: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.547408: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.548292: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.548730: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.549425: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.550135: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.550144: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.551553: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.552530: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.552865: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.553219: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.553719: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.553934: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.554051: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.555706: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.556364: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.557399: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.557764: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.558472: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.558772: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.559502: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.561272: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.561793: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.562145: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.562531: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.563267: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.563464: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.564001: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.565542: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.565899: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.566286: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.566959: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.567610: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.568218: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.569644: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.569915: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.570552: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.571123: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.571826: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.572429: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.573862: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.574105: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.574680: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.575965: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.578465: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.579060: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 01:05:33.579214: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.579630: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 01:05:33.580253: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.581656: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.582239: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 01:05:33.583155: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.584331: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.585360: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 01:05:33.585593: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.586927: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.587622: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.587848: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.588183: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.590073: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.590615: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.592359: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.592470: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.593439: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.593532: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.596753: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.596943: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.598008: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.598238: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.598897: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.598986: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.602026: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.602158: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.603374: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.604214: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.606468: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.615255: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.645845: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.648585: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.650929: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.681901: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.684811: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.688067: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.692273: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.695620: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.700138: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.702067: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.706942: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.708172: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 01:05:33.714794: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 01:05:33.716290: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.723076: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.731123: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.733971: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.737631: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:33.740860: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:34.728812: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:34.729453: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:34.730187: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:34.730658: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 01:05:34.730713: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-12 01:05:34.748286: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:34.749224: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:34.749949: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:34.750926: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:34.751611: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:34.752092: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-12 01:05:34.799101: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 01:05:34.799354: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 01:05:34.835780: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12340
2022-12-12 01:05:34.965324: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:34.965996: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:34.966540: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:34.967002: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 01:05:34.967057: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-12 01:05:34.984485: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:34.985111: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:34.985832: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:34.986629: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:34.987445: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:34.987926: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:1/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-12 01:05:35.022598: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:35.023217: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:35.023745: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:35.024201: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 01:05:35.024262: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-12 01:05:35.040556: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:35.041164: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:35.041674: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:35.042248: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:35.042981: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:35.043466: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:2/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-12 01:05:35.048943: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:35.049587: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:35.049612: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:35.050722: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:35.050747: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:35.051722: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 01:05:35.051743: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:35.051784: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-12 01:05:35.052758: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 01:05:35.052804: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-12 01:05:35.060346: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:35.060957: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:35.061489: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:35.061952: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 01:05:35.062004: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-12 01:05:35.069949: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:35.069949: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:35.071736: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:35.071777: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:35.072807: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:35.072895: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:35.073203: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 01:05:35.073357: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 01:05:35.073896: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:35.074028: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:35.074336: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12341
2022-12-12 01:05:35.075058: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:35.075355: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:35.076084: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:5/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-12 01:05:35.076345: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:4/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-12 01:05:35.078636: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:35.079260: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:35.079781: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:35.080565: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:35.081137: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:35.081610: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:7/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-12 01:05:35.094087: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:35.094686: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:35.095216: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:35.095690: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 01:05:35.095739: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-12 01:05:35.111255: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:35.111934: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:35.112556: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:35.113044: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 01:05:35.113096: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-12 01:05:35.113205: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:35.114056: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:35.114578: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:35.115162: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:35.115696: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:35.116162: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:6/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-12 01:05:35.121513: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 01:05:35.121673: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 01:05:35.122277: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 01:05:35.122475: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 01:05:35.123467: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12345
2022-12-12 01:05:35.123487: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12344
2022-12-12 01:05:35.127562: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 01:05:35.127708: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 01:05:35.128291: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 01:05:35.128437: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 01:05:35.128661: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12342
2022-12-12 01:05:35.130106: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:35.130185: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12347
2022-12-12 01:05:35.130762: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:35.131296: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:35.131875: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:35.132393: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:05:35.132858: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:3/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-12 01:05:35.162411: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 01:05:35.162606: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 01:05:35.164472: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12346
2022-12-12 01:05:35.176831: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 01:05:35.177009: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 01:05:35.178628: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12343
[HCTR][01:05:36.436][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][01:05:36.437][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][01:05:36.437][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][01:05:36.438][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][01:05:36.438][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][01:05:36.438][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][01:05:36.477][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][01:05:36.477][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 1it [00:01,  1.58s/it]warmup run: 1it [00:01,  1.55s/it]warmup run: 1it [00:01,  1.54s/it]warmup run: 1it [00:01,  1.51s/it]warmup run: 1it [00:01,  1.58s/it]warmup run: 96it [00:01, 79.57it/s]warmup run: 102it [00:01, 85.99it/s]warmup run: 92it [00:01, 77.87it/s]warmup run: 1it [00:01,  1.51s/it]warmup run: 91it [00:01, 78.16it/s]warmup run: 95it [00:01, 78.44it/s]warmup run: 193it [00:01, 173.97it/s]warmup run: 204it [00:01, 186.48it/s]warmup run: 1it [00:01,  1.49s/it]warmup run: 193it [00:01, 178.55it/s]warmup run: 101it [00:01, 86.83it/s]warmup run: 182it [00:01, 169.29it/s]warmup run: 192it [00:01, 172.65it/s]warmup run: 289it [00:01, 277.41it/s]warmup run: 304it [00:01, 294.89it/s]warmup run: 98it [00:01, 85.72it/s]warmup run: 293it [00:01, 288.39it/s]warmup run: 1it [00:01,  1.48s/it]warmup run: 204it [00:01, 190.08it/s]warmup run: 272it [00:01, 267.92it/s]warmup run: 289it [00:01, 277.05it/s]warmup run: 385it [00:01, 385.10it/s]warmup run: 405it [00:01, 408.71it/s]warmup run: 196it [00:01, 185.11it/s]warmup run: 393it [00:01, 402.19it/s]warmup run: 98it [00:01, 86.13it/s]warmup run: 306it [00:01, 302.03it/s]warmup run: 363it [00:01, 371.36it/s]warmup run: 387it [00:01, 387.71it/s]warmup run: 482it [00:02, 492.30it/s]warmup run: 503it [00:02, 514.77it/s]warmup run: 293it [00:01, 292.62it/s]warmup run: 493it [00:02, 513.30it/s]warmup run: 196it [00:01, 185.97it/s]warmup run: 407it [00:01, 416.33it/s]warmup run: 456it [00:02, 475.73it/s]warmup run: 486it [00:02, 497.64it/s]warmup run: 582it [00:02, 596.90it/s]warmup run: 603it [00:02, 615.67it/s]warmup run: 391it [00:01, 404.79it/s]warmup run: 594it [00:02, 616.52it/s]warmup run: 294it [00:01, 295.11it/s]warmup run: 508it [00:02, 527.23it/s]warmup run: 550it [00:02, 573.08it/s]warmup run: 585it [00:02, 599.61it/s]warmup run: 682it [00:02, 687.91it/s]warmup run: 704it [00:02, 705.90it/s]warmup run: 489it [00:01, 513.14it/s]warmup run: 696it [00:02, 708.55it/s]warmup run: 392it [00:01, 407.21it/s]warmup run: 611it [00:02, 632.57it/s]warmup run: 645it [00:02, 659.27it/s]warmup run: 685it [00:02, 690.20it/s]warmup run: 782it [00:02, 764.63it/s]warmup run: 807it [00:02, 784.58it/s]warmup run: 590it [00:02, 618.09it/s]warmup run: 797it [00:02, 781.59it/s]warmup run: 490it [00:01, 515.75it/s]warmup run: 715it [00:02, 725.36it/s]warmup run: 739it [00:02, 728.79it/s]warmup run: 785it [00:02, 764.96it/s]warmup run: 882it [00:02, 824.51it/s]warmup run: 690it [00:02, 705.51it/s]warmup run: 898it [00:02, 840.61it/s]warmup run: 590it [00:02, 618.16it/s]warmup run: 906it [00:02, 806.58it/s]warmup run: 818it [00:02, 800.66it/s]warmup run: 833it [00:02, 783.37it/s]warmup run: 884it [00:02, 822.90it/s]warmup run: 981it [00:02, 867.61it/s]warmup run: 790it [00:02, 778.52it/s]warmup run: 1000it [00:02, 888.93it/s]warmup run: 691it [00:02, 708.82it/s]warmup run: 1001it [00:02, 829.67it/s]warmup run: 922it [00:02, 862.41it/s]warmup run: 984it [00:02, 870.22it/s]warmup run: 926it [00:02, 818.48it/s]warmup run: 1079it [00:02, 893.23it/s]warmup run: 889it [00:02, 832.80it/s]warmup run: 1105it [00:02, 931.40it/s]warmup run: 792it [00:02, 782.32it/s]warmup run: 1099it [00:02, 868.86it/s]warmup run: 1027it [00:02, 912.97it/s]warmup run: 1084it [00:02, 905.83it/s]warmup run: 1018it [00:02, 841.29it/s]warmup run: 1177it [00:02, 913.64it/s]warmup run: 989it [00:02, 875.84it/s]warmup run: 1209it [00:02, 961.89it/s]warmup run: 893it [00:02, 840.80it/s]warmup run: 1196it [00:02, 895.38it/s]warmup run: 1130it [00:02, 932.62it/s]warmup run: 1186it [00:02, 935.90it/s]warmup run: 1110it [00:02, 862.40it/s]warmup run: 1274it [00:02, 929.28it/s]warmup run: 1088it [00:02, 907.68it/s]warmup run: 1314it [00:02, 987.02it/s]warmup run: 994it [00:02, 885.09it/s]warmup run: 1295it [00:02, 920.75it/s]warmup run: 1232it [00:02, 941.21it/s]warmup run: 1286it [00:02, 952.68it/s]warmup run: 1202it [00:02, 872.54it/s]warmup run: 1375it [00:02, 951.47it/s]warmup run: 1187it [00:02, 930.60it/s]warmup run: 1418it [00:02, 1002.39it/s]warmup run: 1096it [00:02, 920.20it/s]warmup run: 1394it [00:02, 939.34it/s]warmup run: 1332it [00:02, 947.80it/s]warmup run: 1387it [00:02, 968.98it/s]warmup run: 1294it [00:02, 885.65it/s]warmup run: 1477it [00:03, 968.97it/s]warmup run: 1288it [00:02, 952.09it/s]warmup run: 1522it [00:03, 1011.12it/s]warmup run: 1197it [00:02, 945.04it/s]warmup run: 1494it [00:03, 954.79it/s]warmup run: 1431it [00:02, 949.60it/s]warmup run: 1488it [00:03, 978.72it/s]warmup run: 1386it [00:03, 894.77it/s]warmup run: 1578it [00:03, 980.64it/s]warmup run: 1391it [00:02, 972.16it/s]warmup run: 1626it [00:03, 994.30it/s] warmup run: 1298it [00:02, 962.31it/s]warmup run: 1594it [00:03, 967.09it/s]warmup run: 1529it [00:03, 941.37it/s]warmup run: 1588it [00:03, 979.55it/s]warmup run: 1481it [00:03, 909.69it/s]warmup run: 1680it [00:03, 990.44it/s]warmup run: 1495it [00:02, 992.04it/s]warmup run: 1730it [00:03, 1005.73it/s]warmup run: 1400it [00:02, 976.37it/s]warmup run: 1694it [00:03, 975.97it/s]warmup run: 1628it [00:03, 953.01it/s]warmup run: 1688it [00:03, 981.34it/s]warmup run: 1576it [00:03, 920.73it/s]warmup run: 1782it [00:03, 997.00it/s]warmup run: 1600it [00:03, 1006.34it/s]warmup run: 1833it [00:03, 1012.58it/s]warmup run: 1501it [00:02, 985.59it/s]warmup run: 1795it [00:03, 983.93it/s]warmup run: 1727it [00:03, 960.91it/s]warmup run: 1788it [00:03, 986.28it/s]warmup run: 1672it [00:03, 929.87it/s]warmup run: 1884it [00:03, 1003.56it/s]warmup run: 1705it [00:03, 1017.50it/s]warmup run: 1936it [00:03, 1015.00it/s]warmup run: 1602it [00:03, 992.07it/s]warmup run: 1896it [00:03, 989.96it/s]warmup run: 1826it [00:03, 966.99it/s]warmup run: 1769it [00:03, 941.13it/s]warmup run: 1888it [00:03, 967.43it/s]warmup run: 1987it [00:03, 1011.26it/s]warmup run: 1809it [00:03, 1023.39it/s]warmup run: 2046it [00:03, 1039.93it/s]warmup run: 1703it [00:03, 994.82it/s]warmup run: 1997it [00:03, 995.00it/s]warmup run: 1925it [00:03, 971.57it/s]warmup run: 1865it [00:03, 946.67it/s]warmup run: 1992it [00:03, 988.54it/s]warmup run: 2107it [00:03, 1066.58it/s]warmup run: 1913it [00:03, 1025.68it/s]warmup run: 2169it [00:03, 1094.35it/s]warmup run: 1804it [00:03, 998.01it/s]warmup run: 2117it [00:03, 1055.66it/s]warmup run: 2028it [00:03, 987.14it/s]warmup run: 1960it [00:03, 943.09it/s]warmup run: 2111it [00:03, 1046.31it/s]warmup run: 2231it [00:03, 1116.00it/s]warmup run: 2018it [00:03, 1032.28it/s]warmup run: 2291it [00:03, 1131.49it/s]warmup run: 1905it [00:03, 991.02it/s]warmup run: 2239it [00:03, 1103.67it/s]warmup run: 2146it [00:03, 1042.03it/s]warmup run: 2069it [00:03, 986.41it/s]warmup run: 2232it [00:03, 1092.13it/s]warmup run: 2354it [00:03, 1149.95it/s]warmup run: 2138it [00:03, 1081.04it/s]warmup run: 2414it [00:03, 1158.32it/s]warmup run: 2005it [00:03, 987.87it/s]warmup run: 2361it [00:03, 1136.89it/s]warmup run: 2265it [00:03, 1083.50it/s]warmup run: 2189it [00:03, 1048.60it/s]warmup run: 2353it [00:03, 1125.21it/s]warmup run: 2478it [00:03, 1174.37it/s]warmup run: 2258it [00:03, 1115.54it/s]warmup run: 2537it [00:03, 1176.78it/s]warmup run: 2125it [00:03, 1048.84it/s]warmup run: 2483it [00:03, 1158.97it/s]warmup run: 2384it [00:03, 1113.32it/s]warmup run: 2309it [00:03, 1092.14it/s]warmup run: 2474it [00:04, 1148.27it/s]warmup run: 2602it [00:04, 1191.88it/s]warmup run: 2379it [00:03, 1140.98it/s]warmup run: 2658it [00:04, 1186.54it/s]warmup run: 2245it [00:03, 1092.06it/s]warmup run: 2605it [00:04, 1174.93it/s]warmup run: 2502it [00:03, 1133.09it/s]warmup run: 2429it [00:04, 1122.61it/s]warmup run: 2595it [00:04, 1164.94it/s]warmup run: 2725it [00:04, 1200.83it/s]warmup run: 2499it [00:03, 1158.51it/s]warmup run: 2781it [00:04, 1197.88it/s]warmup run: 2364it [00:03, 1120.14it/s]warmup run: 2726it [00:04, 1182.81it/s]warmup run: 2620it [00:04, 1147.04it/s]warmup run: 2549it [00:04, 1143.25it/s]warmup run: 2716it [00:04, 1176.98it/s]warmup run: 2849it [00:04, 1210.53it/s]warmup run: 2619it [00:03, 1170.61it/s]warmup run: 2904it [00:04, 1204.80it/s]warmup run: 2484it [00:03, 1141.19it/s]warmup run: 2848it [00:04, 1192.02it/s]warmup run: 2737it [00:04, 1153.05it/s]warmup run: 2667it [00:04, 1153.71it/s]warmup run: 2835it [00:04, 1180.76it/s]warmup run: 2973it [00:04, 1217.30it/s]warmup run: 3000it [00:04, 678.61it/s] warmup run: 3000it [00:04, 690.57it/s] warmup run: 2737it [00:04, 1171.42it/s]warmup run: 2602it [00:03, 1152.17it/s]warmup run: 2970it [00:04, 1197.99it/s]warmup run: 2855it [00:04, 1159.82it/s]warmup run: 3000it [00:04, 679.58it/s] warmup run: 2786it [00:04, 1162.55it/s]warmup run: 2954it [00:04, 1171.55it/s]warmup run: 2857it [00:04, 1179.59it/s]warmup run: 3000it [00:04, 673.60it/s] warmup run: 2719it [00:04, 1156.40it/s]warmup run: 2975it [00:04, 1170.52it/s]warmup run: 2905it [00:04, 1169.33it/s]warmup run: 3000it [00:04, 685.15it/s] warmup run: 2976it [00:04, 1181.63it/s]warmup run: 2838it [00:04, 1165.61it/s]warmup run: 3000it [00:04, 693.50it/s] warmup run: 3000it [00:04, 663.47it/s] warmup run: 2955it [00:04, 1164.95it/s]warmup run: 3000it [00:04, 690.90it/s] 
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]





warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   5%|▌         | 163/3000 [00:00<00:01, 1622.54it/s]warmup should be done:   6%|▌         | 166/3000 [00:00<00:01, 1656.44it/s]warmup should be done:   5%|▌         | 162/3000 [00:00<00:01, 1613.01it/s]warmup should be done:   5%|▌         | 161/3000 [00:00<00:01, 1602.09it/s]warmup should be done:   5%|▌         | 163/3000 [00:00<00:01, 1623.35it/s]warmup should be done:   5%|▌         | 161/3000 [00:00<00:01, 1601.73it/s]warmup should be done:   6%|▌         | 165/3000 [00:00<00:01, 1642.05it/s]warmup should be done:   6%|▌         | 166/3000 [00:00<00:01, 1650.02it/s]warmup should be done:  11%|█         | 326/3000 [00:00<00:01, 1625.04it/s]warmup should be done:  11%|█         | 323/3000 [00:00<00:01, 1611.99it/s]warmup should be done:  11%|█         | 335/3000 [00:00<00:01, 1672.21it/s]warmup should be done:  11%|█         | 330/3000 [00:00<00:01, 1645.71it/s]warmup should be done:  11%|█         | 332/3000 [00:00<00:01, 1654.70it/s]warmup should be done:  11%|█         | 326/3000 [00:00<00:01, 1624.71it/s]warmup should be done:  11%|█         | 328/3000 [00:00<00:01, 1634.42it/s]warmup should be done:  11%|█         | 324/3000 [00:00<00:01, 1613.38it/s]warmup should be done:  16%|█▋        | 489/3000 [00:00<00:01, 1622.64it/s]warmup should be done:  17%|█▋        | 503/3000 [00:00<00:01, 1671.99it/s]warmup should be done:  16%|█▌        | 485/3000 [00:00<00:01, 1608.86it/s]warmup should be done:  16%|█▋        | 495/3000 [00:00<00:01, 1641.95it/s]warmup should be done:  17%|█▋        | 498/3000 [00:00<00:01, 1651.34it/s]warmup should be done:  16%|█▋        | 492/3000 [00:00<00:01, 1631.76it/s]warmup should be done:  16%|█▌        | 486/3000 [00:00<00:01, 1608.95it/s]warmup should be done:  16%|█▋        | 489/3000 [00:00<00:01, 1592.15it/s]warmup should be done:  22%|██▏       | 671/3000 [00:00<00:01, 1672.57it/s]warmup should be done:  22%|██▏       | 646/3000 [00:00<00:01, 1608.21it/s]warmup should be done:  22%|██▏       | 660/3000 [00:00<00:01, 1643.60it/s]warmup should be done:  22%|██▏       | 652/3000 [00:00<00:01, 1617.95it/s]warmup should be done:  22%|██▏       | 664/3000 [00:00<00:01, 1648.06it/s]warmup should be done:  22%|██▏       | 656/3000 [00:00<00:01, 1628.63it/s]warmup should be done:  22%|██▏       | 647/3000 [00:00<00:01, 1605.19it/s]warmup should be done:  22%|██▏       | 652/3000 [00:00<00:01, 1605.11it/s]warmup should be done:  27%|██▋       | 807/3000 [00:00<00:01, 1607.73it/s]warmup should be done:  28%|██▊       | 827/3000 [00:00<00:01, 1650.92it/s]warmup should be done:  28%|██▊       | 839/3000 [00:00<00:01, 1671.51it/s]warmup should be done:  27%|██▋       | 814/3000 [00:00<00:01, 1617.43it/s]warmup should be done:  28%|██▊       | 829/3000 [00:00<00:01, 1647.97it/s]warmup should be done:  27%|██▋       | 819/3000 [00:00<00:01, 1625.17it/s]warmup should be done:  27%|██▋       | 808/3000 [00:00<00:01, 1603.14it/s]warmup should be done:  27%|██▋       | 815/3000 [00:00<00:01, 1610.71it/s]warmup should be done:  33%|███▎      | 993/3000 [00:00<00:01, 1653.18it/s]warmup should be done:  32%|███▏      | 968/3000 [00:00<00:01, 1604.92it/s]warmup should be done:  33%|███▎      | 976/3000 [00:00<00:01, 1616.39it/s]warmup should be done:  33%|███▎      | 994/3000 [00:00<00:01, 1644.65it/s]warmup should be done:  34%|███▎      | 1007/3000 [00:00<00:01, 1666.72it/s]warmup should be done:  33%|███▎      | 982/3000 [00:00<00:01, 1618.55it/s]warmup should be done:  32%|███▏      | 969/3000 [00:00<00:01, 1595.65it/s]warmup should be done:  33%|███▎      | 980/3000 [00:00<00:01, 1622.41it/s]warmup should be done:  39%|███▊      | 1160/3000 [00:00<00:01, 1656.01it/s]warmup should be done:  38%|███▊      | 1138/3000 [00:00<00:01, 1617.47it/s]warmup should be done:  38%|███▊      | 1129/3000 [00:00<00:01, 1602.00it/s]warmup should be done:  39%|███▉      | 1174/3000 [00:00<00:01, 1666.13it/s]warmup should be done:  39%|███▊      | 1160/3000 [00:00<00:01, 1647.16it/s]warmup should be done:  38%|███▊      | 1144/3000 [00:00<00:01, 1616.76it/s]warmup should be done:  38%|███▊      | 1129/3000 [00:00<00:01, 1595.44it/s]warmup should be done:  38%|███▊      | 1146/3000 [00:00<00:01, 1634.51it/s]warmup should be done:  43%|████▎     | 1300/3000 [00:00<00:01, 1616.03it/s]warmup should be done:  44%|████▍     | 1325/3000 [00:00<00:01, 1647.69it/s]warmup should be done:  45%|████▍     | 1341/3000 [00:00<00:00, 1665.21it/s]warmup should be done:  43%|████▎     | 1290/3000 [00:00<00:01, 1599.04it/s]warmup should be done:  44%|████▍     | 1326/3000 [00:00<00:01, 1644.02it/s]warmup should be done:  44%|████▎     | 1306/3000 [00:00<00:01, 1615.92it/s]warmup should be done:  43%|████▎     | 1289/3000 [00:00<00:01, 1595.17it/s]warmup should be done:  44%|████▍     | 1313/3000 [00:00<00:01, 1643.01it/s]warmup should be done:  49%|████▊     | 1462/3000 [00:00<00:00, 1613.92it/s]warmup should be done:  50%|████▉     | 1491/3000 [00:00<00:00, 1648.66it/s]warmup should be done:  48%|████▊     | 1450/3000 [00:00<00:00, 1597.95it/s]warmup should be done:  50%|█████     | 1508/3000 [00:00<00:00, 1663.19it/s]warmup should be done:  49%|████▉     | 1468/3000 [00:00<00:00, 1616.15it/s]warmup should be done:  48%|████▊     | 1449/3000 [00:00<00:00, 1595.33it/s]warmup should be done:  50%|████▉     | 1491/3000 [00:00<00:00, 1633.62it/s]warmup should be done:  49%|████▉     | 1480/3000 [00:00<00:00, 1649.23it/s]warmup should be done:  54%|█████▍    | 1624/3000 [00:01<00:00, 1613.54it/s]warmup should be done:  55%|█████▌    | 1657/3000 [00:01<00:00, 1650.49it/s]warmup should be done:  54%|█████▎    | 1610/3000 [00:01<00:00, 1596.58it/s]warmup should be done:  56%|█████▌    | 1675/3000 [00:01<00:00, 1663.14it/s]warmup should be done:  54%|█████▍    | 1630/3000 [00:01<00:00, 1616.17it/s]warmup should be done:  54%|█████▎    | 1609/3000 [00:01<00:00, 1595.69it/s]warmup should be done:  55%|█████▍    | 1647/3000 [00:01<00:00, 1653.53it/s]warmup should be done:  55%|█████▌    | 1657/3000 [00:01<00:00, 1639.07it/s]warmup should be done:  60%|█████▉    | 1786/3000 [00:01<00:00, 1613.01it/s]warmup should be done:  61%|██████    | 1823/3000 [00:01<00:00, 1650.38it/s]warmup should be done:  59%|█████▉    | 1770/3000 [00:01<00:00, 1596.10it/s]warmup should be done:  61%|██████▏   | 1842/3000 [00:01<00:00, 1662.52it/s]warmup should be done:  60%|█████▉    | 1792/3000 [00:01<00:00, 1616.54it/s]warmup should be done:  59%|█████▉    | 1769/3000 [00:01<00:00, 1594.77it/s]warmup should be done:  61%|██████    | 1824/3000 [00:01<00:00, 1646.10it/s]warmup should be done:  60%|██████    | 1814/3000 [00:01<00:00, 1656.05it/s]warmup should be done:  65%|██████▍   | 1948/3000 [00:01<00:00, 1610.41it/s]warmup should be done:  66%|██████▋   | 1989/3000 [00:01<00:00, 1650.65it/s]warmup should be done:  64%|██████▍   | 1932/3000 [00:01<00:00, 1600.72it/s]warmup should be done:  65%|██████▌   | 1954/3000 [00:01<00:00, 1615.58it/s]warmup should be done:  64%|██████▍   | 1929/3000 [00:01<00:00, 1593.88it/s]warmup should be done:  67%|██████▋   | 2009/3000 [00:01<00:00, 1655.04it/s]warmup should be done:  66%|██████▌   | 1980/3000 [00:01<00:00, 1656.95it/s]warmup should be done:  66%|██████▋   | 1989/3000 [00:01<00:00, 1640.71it/s]warmup should be done:  70%|██████▉   | 2094/3000 [00:01<00:00, 1604.73it/s]warmup should be done:  72%|███████▏  | 2155/3000 [00:01<00:00, 1649.24it/s]warmup should be done:  70%|███████   | 2110/3000 [00:01<00:00, 1608.24it/s]warmup should be done:  71%|███████   | 2116/3000 [00:01<00:00, 1615.79it/s]warmup should be done:  70%|██████▉   | 2089/3000 [00:01<00:00, 1592.62it/s]warmup should be done:  72%|███████▏  | 2146/3000 [00:01<00:00, 1656.93it/s]warmup should be done:  72%|███████▎  | 2175/3000 [00:01<00:00, 1649.22it/s]warmup should be done:  72%|███████▏  | 2154/3000 [00:01<00:00, 1634.35it/s]warmup should be done:  75%|███████▌  | 2256/3000 [00:01<00:00, 1607.28it/s]warmup should be done:  77%|███████▋  | 2320/3000 [00:01<00:00, 1647.12it/s]warmup should be done:  76%|███████▌  | 2278/3000 [00:01<00:00, 1611.76it/s]warmup should be done:  76%|███████▌  | 2271/3000 [00:01<00:00, 1600.90it/s]warmup should be done:  75%|███████▍  | 2249/3000 [00:01<00:00, 1588.74it/s]warmup should be done:  78%|███████▊  | 2340/3000 [00:01<00:00, 1641.76it/s]warmup should be done:  77%|███████▋  | 2312/3000 [00:01<00:00, 1647.93it/s]warmup should be done:  77%|███████▋  | 2318/3000 [00:01<00:00, 1624.88it/s]warmup should be done:  81%|████████  | 2417/3000 [00:01<00:00, 1607.91it/s]warmup should be done:  83%|████████▎ | 2486/3000 [00:01<00:00, 1648.16it/s]warmup should be done:  81%|████████  | 2432/3000 [00:01<00:00, 1599.95it/s]warmup should be done:  80%|████████  | 2409/3000 [00:01<00:00, 1590.03it/s]warmup should be done:  81%|████████▏ | 2440/3000 [00:01<00:00, 1604.25it/s]warmup should be done:  84%|████████▎ | 2505/3000 [00:01<00:00, 1640.39it/s]warmup should be done:  83%|████████▎ | 2477/3000 [00:01<00:00, 1642.74it/s]warmup should be done:  83%|████████▎ | 2481/3000 [00:01<00:00, 1618.19it/s]warmup should be done:  86%|████████▌ | 2580/3000 [00:01<00:00, 1612.61it/s]warmup should be done:  88%|████████▊ | 2652/3000 [00:01<00:00, 1648.98it/s]warmup should be done:  86%|████████▋ | 2594/3000 [00:01<00:00, 1603.43it/s]warmup should be done:  86%|████████▌ | 2569/3000 [00:01<00:00, 1588.08it/s]warmup should be done:  87%|████████▋ | 2601/3000 [00:01<00:00, 1601.79it/s]warmup should be done:  89%|████████▉ | 2670/3000 [00:01<00:00, 1636.56it/s]warmup should be done:  88%|████████▊ | 2642/3000 [00:01<00:00, 1639.70it/s]warmup should be done:  88%|████████▊ | 2643/3000 [00:01<00:00, 1609.73it/s]warmup should be done:  91%|█████████▏| 2744/3000 [00:01<00:00, 1618.32it/s]warmup should be done:  94%|█████████▍| 2817/3000 [00:01<00:00, 1648.19it/s]warmup should be done:  92%|█████████▏| 2756/3000 [00:01<00:00, 1608.11it/s]warmup should be done:  91%|█████████ | 2729/3000 [00:01<00:00, 1588.87it/s]warmup should be done:  92%|█████████▏| 2762/3000 [00:01<00:00, 1596.63it/s]warmup should be done:  94%|█████████▍| 2834/3000 [00:01<00:00, 1637.02it/s]warmup should be done:  94%|█████████▎| 2806/3000 [00:01<00:00, 1637.55it/s]warmup should be done:  93%|█████████▎| 2804/3000 [00:01<00:00, 1609.45it/s]warmup should be done:  97%|█████████▋| 2908/3000 [00:01<00:00, 1624.38it/s]warmup should be done:  99%|█████████▉| 2983/3000 [00:01<00:00, 1649.18it/s]warmup should be done:  97%|█████████▋| 2920/3000 [00:01<00:00, 1615.80it/s]warmup should be done:  96%|█████████▋| 2890/3000 [00:01<00:00, 1594.33it/s]warmup should be done:  97%|█████████▋| 2922/3000 [00:01<00:00, 1596.37it/s]warmup should be done: 100%|█████████▉| 2999/3000 [00:01<00:00, 1639.49it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1653.60it/s]warmup should be done:  99%|█████████▉| 2971/3000 [00:01<00:00, 1638.38it/s]warmup should be done:  99%|█████████▉| 2969/3000 [00:01<00:00, 1621.12it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1648.70it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1637.32it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1633.26it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1612.92it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1611.66it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1609.26it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1595.81it/s]



warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]


warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   5%|▌         | 162/3000 [00:00<00:01, 1617.06it/s]warmup should be done:   5%|▌         | 164/3000 [00:00<00:01, 1634.57it/s]warmup should be done:   5%|▌         | 156/3000 [00:00<00:01, 1554.47it/s]warmup should be done:   6%|▌         | 167/3000 [00:00<00:01, 1664.05it/s]warmup should be done:   6%|▌         | 167/3000 [00:00<00:01, 1663.81it/s]warmup should be done:   6%|▌         | 168/3000 [00:00<00:01, 1671.06it/s]warmup should be done:   6%|▌         | 168/3000 [00:00<00:01, 1671.30it/s]warmup should be done:   5%|▌         | 163/3000 [00:00<00:01, 1621.17it/s]warmup should be done:  11%|█         | 335/3000 [00:00<00:01, 1672.58it/s]warmup should be done:  11%|█         | 324/3000 [00:00<00:01, 1616.57it/s]warmup should be done:  11%|█         | 336/3000 [00:00<00:01, 1674.79it/s]warmup should be done:  11%|█         | 336/3000 [00:00<00:01, 1675.48it/s]warmup should be done:  11%|█         | 333/3000 [00:00<00:01, 1665.69it/s]warmup should be done:  11%|█         | 335/3000 [00:00<00:01, 1668.44it/s]warmup should be done:  11%|█         | 321/3000 [00:00<00:01, 1605.17it/s]warmup should be done:  11%|█         | 328/3000 [00:00<00:01, 1629.28it/s]warmup should be done:  17%|█▋        | 504/3000 [00:00<00:01, 1677.11it/s]warmup should be done:  17%|█▋        | 504/3000 [00:00<00:01, 1677.38it/s]warmup should be done:  16%|█▋        | 488/3000 [00:00<00:01, 1623.78it/s]warmup should be done:  16%|█▋        | 492/3000 [00:00<00:01, 1634.01it/s]warmup should be done:  17%|█▋        | 501/3000 [00:00<00:01, 1668.73it/s]warmup should be done:  16%|█▌        | 487/3000 [00:00<00:01, 1627.30it/s]warmup should be done:  17%|█▋        | 503/3000 [00:00<00:01, 1669.82it/s]warmup should be done:  17%|█▋        | 504/3000 [00:00<00:01, 1670.88it/s]warmup should be done:  22%|██▏       | 673/3000 [00:00<00:01, 1678.62it/s]warmup should be done:  22%|██▏       | 670/3000 [00:00<00:01, 1676.86it/s]warmup should be done:  22%|██▏       | 656/3000 [00:00<00:01, 1635.58it/s]warmup should be done:  22%|██▏       | 670/3000 [00:00<00:01, 1667.59it/s]warmup should be done:  22%|██▏       | 653/3000 [00:00<00:01, 1637.02it/s]warmup should be done:  22%|██▏       | 651/3000 [00:00<00:01, 1620.52it/s]warmup should be done:  22%|██▏       | 672/3000 [00:00<00:01, 1670.61it/s]warmup should be done:  22%|██▏       | 672/3000 [00:00<00:01, 1661.08it/s]warmup should be done:  28%|██▊       | 841/3000 [00:00<00:01, 1677.94it/s]warmup should be done:  28%|██▊       | 840/3000 [00:00<00:01, 1683.02it/s]warmup should be done:  28%|██▊       | 837/3000 [00:00<00:01, 1668.00it/s]warmup should be done:  27%|██▋       | 821/3000 [00:00<00:01, 1637.47it/s]warmup should be done:  27%|██▋       | 818/3000 [00:00<00:01, 1640.68it/s]warmup should be done:  28%|██▊       | 840/3000 [00:00<00:01, 1670.66it/s]warmup should be done:  27%|██▋       | 814/3000 [00:00<00:01, 1619.34it/s]warmup should be done:  28%|██▊       | 839/3000 [00:00<00:01, 1656.45it/s]warmup should be done:  34%|███▎      | 1011/3000 [00:00<00:01, 1690.67it/s]warmup should be done:  34%|███▎      | 1010/3000 [00:00<00:01, 1678.48it/s]warmup should be done:  34%|███▎      | 1005/3000 [00:00<00:01, 1670.95it/s]warmup should be done:  33%|███▎      | 986/3000 [00:00<00:01, 1639.61it/s]warmup should be done:  33%|███▎      | 983/3000 [00:00<00:01, 1639.04it/s]warmup should be done:  33%|███▎      | 978/3000 [00:00<00:01, 1624.40it/s]warmup should be done:  34%|███▎      | 1009/3000 [00:00<00:01, 1674.80it/s]warmup should be done:  34%|███▎      | 1006/3000 [00:00<00:01, 1657.55it/s]warmup should be done:  38%|███▊      | 1151/3000 [00:00<00:01, 1642.81it/s]warmup should be done:  39%|███▉      | 1179/3000 [00:00<00:01, 1679.46it/s]warmup should be done:  39%|███▉      | 1173/3000 [00:00<00:01, 1670.57it/s]warmup should be done:  39%|███▉      | 1177/3000 [00:00<00:01, 1675.96it/s]warmup should be done:  38%|███▊      | 1149/3000 [00:00<00:01, 1644.05it/s]warmup should be done:  38%|███▊      | 1142/3000 [00:00<00:01, 1628.08it/s]warmup should be done:  39%|███▉      | 1181/3000 [00:00<00:01, 1673.84it/s]warmup should be done:  39%|███▉      | 1172/3000 [00:00<00:01, 1655.69it/s]warmup should be done:  45%|████▍     | 1348/3000 [00:00<00:00, 1680.55it/s]warmup should be done:  45%|████▍     | 1341/3000 [00:00<00:00, 1670.75it/s]warmup should be done:  44%|████▍     | 1316/3000 [00:00<00:01, 1639.44it/s]warmup should be done:  45%|████▍     | 1346/3000 [00:00<00:00, 1678.40it/s]warmup should be done:  44%|████▍     | 1314/3000 [00:00<00:01, 1642.27it/s]warmup should be done:  44%|████▎     | 1305/3000 [00:00<00:01, 1624.56it/s]warmup should be done:  45%|████▍     | 1338/3000 [00:00<00:01, 1654.94it/s]warmup should be done:  45%|████▍     | 1349/3000 [00:00<00:00, 1671.04it/s]warmup should be done:  49%|████▉     | 1481/3000 [00:00<00:00, 1641.67it/s]warmup should be done:  51%|█████     | 1517/3000 [00:00<00:00, 1678.50it/s]warmup should be done:  49%|████▉     | 1468/3000 [00:00<00:00, 1624.59it/s]warmup should be done:  50%|█████     | 1514/3000 [00:00<00:00, 1672.99it/s]warmup should be done:  49%|████▉     | 1479/3000 [00:00<00:00, 1640.43it/s]warmup should be done:  50%|█████     | 1509/3000 [00:00<00:00, 1664.78it/s]warmup should be done:  51%|█████     | 1517/3000 [00:00<00:00, 1669.96it/s]warmup should be done:  50%|█████     | 1504/3000 [00:00<00:00, 1652.57it/s]warmup should be done:  56%|█████▌    | 1685/3000 [00:01<00:00, 1678.08it/s]warmup should be done:  55%|█████▍    | 1646/3000 [00:01<00:00, 1639.55it/s]warmup should be done:  54%|█████▍    | 1631/3000 [00:01<00:00, 1623.60it/s]warmup should be done:  55%|█████▍    | 1645/3000 [00:01<00:00, 1644.81it/s]warmup should be done:  56%|█████▌    | 1682/3000 [00:01<00:00, 1671.10it/s]warmup should be done:  56%|█████▌    | 1676/3000 [00:01<00:00, 1660.94it/s]warmup should be done:  56%|█████▌    | 1671/3000 [00:01<00:00, 1657.41it/s]warmup should be done:  56%|█████▋    | 1688/3000 [00:01<00:00, 1679.68it/s]warmup should be done:  62%|██████▏   | 1854/3000 [00:01<00:00, 1679.48it/s]warmup should be done:  60%|██████    | 1811/3000 [00:01<00:00, 1641.76it/s]warmup should be done:  60%|█████▉    | 1795/3000 [00:01<00:00, 1626.77it/s]warmup should be done:  60%|██████    | 1811/3000 [00:01<00:00, 1646.46it/s]warmup should be done:  62%|██████▏   | 1851/3000 [00:01<00:00, 1675.61it/s]warmup should be done:  61%|██████▏   | 1843/3000 [00:01<00:00, 1659.84it/s]warmup should be done:  61%|██████▏   | 1839/3000 [00:01<00:00, 1662.24it/s]warmup should be done:  62%|██████▏   | 1859/3000 [00:01<00:00, 1687.44it/s]warmup should be done:  67%|██████▋   | 2022/3000 [00:01<00:00, 1679.27it/s]warmup should be done:  66%|██████▌   | 1976/3000 [00:01<00:00, 1640.76it/s]warmup should be done:  66%|██████▌   | 1976/3000 [00:01<00:00, 1645.85it/s]warmup should be done:  67%|██████▋   | 2006/3000 [00:01<00:00, 1664.26it/s]warmup should be done:  67%|██████▋   | 2019/3000 [00:01<00:00, 1665.55it/s]warmup should be done:  67%|██████▋   | 2009/3000 [00:01<00:00, 1652.80it/s]warmup should be done:  68%|██████▊   | 2030/3000 [00:01<00:00, 1691.24it/s]warmup should be done:  65%|██████▌   | 1958/3000 [00:01<00:00, 1594.51it/s]warmup should be done:  73%|███████▎  | 2191/3000 [00:01<00:00, 1682.09it/s]warmup should be done:  71%|███████▏  | 2141/3000 [00:01<00:00, 1641.59it/s]warmup should be done:  71%|███████▏  | 2141/3000 [00:01<00:00, 1643.86it/s]warmup should be done:  72%|███████▏  | 2173/3000 [00:01<00:00, 1665.52it/s]warmup should be done:  73%|███████▎  | 2186/3000 [00:01<00:00, 1663.77it/s]warmup should be done:  72%|███████▎  | 2175/3000 [00:01<00:00, 1649.09it/s]warmup should be done:  73%|███████▎  | 2200/3000 [00:01<00:00, 1690.09it/s]warmup should be done:  71%|███████   | 2122/3000 [00:01<00:00, 1605.85it/s]warmup should be done:  79%|███████▊  | 2360/3000 [00:01<00:00, 1684.38it/s]warmup should be done:  77%|███████▋  | 2307/3000 [00:01<00:00, 1648.35it/s]warmup should be done:  77%|███████▋  | 2306/3000 [00:01<00:00, 1639.43it/s]warmup should be done:  78%|███████▊  | 2340/3000 [00:01<00:00, 1661.28it/s]warmup should be done:  78%|███████▊  | 2355/3000 [00:01<00:00, 1669.28it/s]warmup should be done:  78%|███████▊  | 2341/3000 [00:01<00:00, 1650.45it/s]warmup should be done:  79%|███████▉  | 2370/3000 [00:01<00:00, 1688.73it/s]warmup should be done:  76%|███████▌  | 2286/3000 [00:01<00:00, 1614.57it/s]warmup should be done:  84%|████████▍ | 2530/3000 [00:01<00:00, 1687.00it/s]warmup should be done:  82%|████████▏ | 2471/3000 [00:01<00:00, 1640.95it/s]warmup should be done:  82%|████████▏ | 2473/3000 [00:01<00:00, 1649.26it/s]warmup should be done:  84%|████████▍ | 2524/3000 [00:01<00:00, 1673.08it/s]warmup should be done:  84%|████████▎ | 2507/3000 [00:01<00:00, 1659.98it/s]warmup should be done:  84%|████████▎ | 2508/3000 [00:01<00:00, 1653.67it/s]warmup should be done:  85%|████████▍ | 2541/3000 [00:01<00:00, 1692.48it/s]warmup should be done:  82%|████████▏ | 2449/3000 [00:01<00:00, 1617.38it/s]warmup should be done:  90%|█████████ | 2700/3000 [00:01<00:00, 1688.14it/s]warmup should be done:  88%|████████▊ | 2638/3000 [00:01<00:00, 1648.58it/s]warmup should be done:  88%|████████▊ | 2636/3000 [00:01<00:00, 1639.11it/s]warmup should be done:  90%|████████▉ | 2693/3000 [00:01<00:00, 1675.56it/s]warmup should be done:  89%|████████▉ | 2674/3000 [00:01<00:00, 1652.20it/s]warmup should be done:  89%|████████▉ | 2674/3000 [00:01<00:00, 1653.90it/s]warmup should be done:  90%|█████████ | 2711/3000 [00:01<00:00, 1685.08it/s]warmup should be done:  87%|████████▋ | 2612/3000 [00:01<00:00, 1618.28it/s]warmup should be done:  96%|█████████▌| 2869/3000 [00:01<00:00, 1686.13it/s]warmup should be done:  93%|█████████▎| 2804/3000 [00:01<00:00, 1649.52it/s]warmup should be done:  93%|█████████▎| 2801/3000 [00:01<00:00, 1641.15it/s]warmup should be done:  95%|█████████▌| 2862/3000 [00:01<00:00, 1677.72it/s]warmup should be done:  95%|█████████▍| 2840/3000 [00:01<00:00, 1650.68it/s]warmup should be done:  95%|█████████▍| 2840/3000 [00:01<00:00, 1650.73it/s]warmup should be done:  93%|█████████▎| 2776/3000 [00:01<00:00, 1624.56it/s]warmup should be done:  96%|█████████▌| 2880/3000 [00:01<00:00, 1671.72it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1681.69it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1679.08it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1674.63it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1659.33it/s]warmup should be done:  99%|█████████▉| 2970/3000 [00:01<00:00, 1651.08it/s]warmup should be done:  99%|█████████▉| 2967/3000 [00:01<00:00, 1644.42it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1657.91it/s]warmup should be done:  98%|█████████▊| 2940/3000 [00:01<00:00, 1628.72it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1642.18it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1639.99it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1620.43it/s]2022-12-12 01:07:12.547978: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f3b07954a40 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 01:07:12.548041: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 01:07:12.574579: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f3b078301d0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 01:07:12.574642: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 01:07:12.616548: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f3afbf92f60 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 01:07:12.616616: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 01:07:12.926553: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f1e5c02d1c0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 01:07:12.926619: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 01:07:13.057364: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f1e9c02fc80 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 01:07:13.057434: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 01:07:13.120282: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f1f2002cd00 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 01:07:13.120376: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 01:07:13.120428: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f1e14028920 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 01:07:13.120478: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 01:07:13.154966: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f3b0315ea10 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 01:07:13.155042: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 01:07:14.825004: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 01:07:14.842458: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 01:07:14.918619: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 01:07:15.211935: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 01:07:15.337018: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 01:07:15.362819: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 01:07:15.391211: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 01:07:15.411529: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 01:07:17.704370: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 01:07:17.723839: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 01:07:17.847837: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 01:07:18.131990: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 01:07:18.226209: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 01:07:18.241274: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 01:07:18.296983: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 01:07:18.343881: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
[HCTR][01:07:51.459][ERROR][RK0][tid #139891883095808]: replica 3 reaches 1000, calling init pre replica
[HCTR][01:07:51.460][ERROR][RK0][tid #139891883095808]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][01:07:51.468][ERROR][RK0][tid #139891883095808]: coll ps creation done
[HCTR][01:07:51.468][ERROR][RK0][tid #139891883095808]: replica 3 waits for coll ps creation barrier
[HCTR][01:07:51.539][ERROR][RK0][tid #139892076029696]: replica 2 reaches 1000, calling init pre replica
[HCTR][01:07:51.539][ERROR][RK0][tid #139892076029696]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][01:07:51.549][ERROR][RK0][tid #139892076029696]: coll ps creation done
[HCTR][01:07:51.549][ERROR][RK0][tid #139892076029696]: replica 2 waits for coll ps creation barrier
[HCTR][01:07:51.685][ERROR][RK0][tid #139892814227200]: replica 5 reaches 1000, calling init pre replica
[HCTR][01:07:51.685][ERROR][RK0][tid #139892814227200]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][01:07:51.694][ERROR][RK0][tid #139892814227200]: coll ps creation done
[HCTR][01:07:51.694][ERROR][RK0][tid #139892814227200]: replica 5 waits for coll ps creation barrier
[HCTR][01:07:51.721][ERROR][RK0][main]: replica 4 reaches 1000, calling init pre replica
[HCTR][01:07:51.721][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][01:07:51.729][ERROR][RK0][main]: coll ps creation done
[HCTR][01:07:51.729][ERROR][RK0][main]: replica 4 waits for coll ps creation barrier
[HCTR][01:07:51.768][ERROR][RK0][tid #139891883095808]: replica 7 reaches 1000, calling init pre replica
[HCTR][01:07:51.768][ERROR][RK0][tid #139891883095808]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][01:07:51.776][ERROR][RK0][tid #139891883095808]: coll ps creation done
[HCTR][01:07:51.776][ERROR][RK0][tid #139891883095808]: replica 7 waits for coll ps creation barrier
[HCTR][01:07:51.805][ERROR][RK0][main]: replica 6 reaches 1000, calling init pre replica
[HCTR][01:07:51.805][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][01:07:51.809][ERROR][RK0][main]: coll ps creation done
[HCTR][01:07:51.809][ERROR][RK0][main]: replica 6 waits for coll ps creation barrier
[HCTR][01:07:51.942][ERROR][RK0][tid #139892076029696]: replica 0 reaches 1000, calling init pre replica
[HCTR][01:07:51.943][ERROR][RK0][tid #139892076029696]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][01:07:51.950][ERROR][RK0][tid #139892076029696]: coll ps creation done
[HCTR][01:07:51.950][ERROR][RK0][tid #139892017313536]: replica 1 reaches 1000, calling init pre replica
[HCTR][01:07:51.950][ERROR][RK0][tid #139892076029696]: replica 0 waits for coll ps creation barrier
[HCTR][01:07:51.950][ERROR][RK0][tid #139892017313536]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][01:07:51.955][ERROR][RK0][tid #139892017313536]: coll ps creation done
[HCTR][01:07:51.955][ERROR][RK0][tid #139892017313536]: replica 1 waits for coll ps creation barrier
[HCTR][01:07:51.955][ERROR][RK0][tid #139892076029696]: replica 0 preparing frequency
[HCTR][01:07:52.835][ERROR][RK0][tid #139892076029696]: replica 0 preparing frequency done
[HCTR][01:07:52.869][ERROR][RK0][tid #139892076029696]: replica 0 calling init per replica
[HCTR][01:07:52.869][ERROR][RK0][tid #139891883095808]: replica 7 calling init per replica
[HCTR][01:07:52.869][ERROR][RK0][tid #139892017313536]: replica 1 calling init per replica
[HCTR][01:07:52.869][ERROR][RK0][main]: replica 4 calling init per replica
[HCTR][01:07:52.869][ERROR][RK0][tid #139891883095808]: replica 3 calling init per replica
[HCTR][01:07:52.869][ERROR][RK0][tid #139892814227200]: replica 5 calling init per replica
[HCTR][01:07:52.869][ERROR][RK0][tid #139892076029696]: replica 2 calling init per replica
[HCTR][01:07:52.869][ERROR][RK0][main]: replica 6 calling init per replica
[HCTR][01:07:52.869][ERROR][RK0][tid #139892076029696]: Calling build_v2
[HCTR][01:07:52.869][ERROR][RK0][tid #139891883095808]: Calling build_v2
[HCTR][01:07:52.869][ERROR][RK0][tid #139892017313536]: Calling build_v2
[HCTR][01:07:52.870][ERROR][RK0][main]: Calling build_v2
[HCTR][01:07:52.870][ERROR][RK0][tid #139891883095808]: Calling build_v2
[HCTR][01:07:52.870][ERROR][RK0][tid #139892814227200]: Calling build_v2
[HCTR][01:07:52.870][ERROR][RK0][tid #139892076029696]: Calling build_v2
[HCTR][01:07:52.870][ERROR][RK0][tid #139892076029696]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][01:07:52.870][ERROR][RK0][main]: Calling build_v2
[HCTR][01:07:52.870][ERROR][RK0][tid #139891883095808]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][01:07:52.870][ERROR][RK0][tid #139892017313536]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][01:07:52.870][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][01:07:52.870][ERROR][RK0][tid #139891883095808]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][01:07:52.870][ERROR][RK0][tid #139892814227200]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][01:07:52.870][ERROR][RK0][tid #139892076029696]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][01:07:52.870][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[[[[[2022-12-12 01:07:52[[2022-12-12 01:07:522022-12-12 01:07:52[.2022-12-12 01:07:522022-12-12 01:07:52.2022-12-12 01:07:52.2022-12-12 01:07:52870135..870135.8701322022-12-12 01:07:52.: 870150870150: 870150: .870156E: : E: E870170:  EE E : E/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc  /hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc /hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.ccE :/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc:/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc: /hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc136::136:136/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc:] 136136] 136] :136using concurrent impl MPS] ] using concurrent impl MPS] using concurrent impl MPS136] 
using concurrent impl MPSusing concurrent impl MPS
using concurrent impl MPS
] using concurrent impl MPS


using concurrent impl MPS

[2022-12-12 01:07:52.874493: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:178] v100x8, slow pcie
[2022-12-12 01:07:52.874532: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[:2022-12-12 01:07:52196.] 874540assigning 8 to cpu: 
E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:178] v100x8, slow pcie
[[2022-12-12 01:07:522022-12-12 01:07:52..874587874583: : EE[  2022-12-12 01:07:52/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.::874607196178: ] ] Eassigning 8 to cpuv100x8, slow pcie[ 

2022-12-12 01:07:52/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.[:8746352022-12-12 01:07:52212: .[] [E8746652022-12-12 01:07:52build asymm link desc with 8X Tesla V100-SXM2-32GB out of 82022-12-12 01:07:52 : .
./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[E874679874688:2022-12-12 01:07:52 [: : 178./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 01:07:52[EE] 874723:.2022-12-12 01:07:52  v100x8, slow pcie: 196874756.[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
E] : 8747732022-12-12 01:07:52:: assigning 8 to cpu[E: .178212/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
2022-12-12 01:07:52 E874818] ] :./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc : v100x8, slow pciebuild asymm link desc with 8X Tesla V100-SXM2-32GB out of 8178874867[:/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE

] : 2022-12-12 01:07:52213: [v100x8, slow pcieE.] [178/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 01:07:52
 874951remote time is 8.684212022-12-12 01:07:52] :./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: [
.v100x8, slow pcie178874994:E2022-12-12 01:07:52875005
[] : [196 .: 2022-12-12 01:07:52.v100x8, slow pcieE2022-12-12 01:07:52] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc875041E875074
 .assigning 8 to cpu::  : /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc875101[
212E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE:: 2022-12-12 01:07:52]  : 196E[.build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc213/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc]  2022-12-12 01:07:52875180
:] :assigning 8 to cpu/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.: 196remote time is 8.68421214[
:875237E] 
] 2022-12-12 01:07:52196:  assigning 8 to cpucpu time is 97.0588.] [E[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc

875313assigning 8 to cpu2022-12-12 01:07:52 2022-12-12 01:07:52:: 
./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.196E[875363[:875363]  2022-12-12 01:07:52: 2022-12-12 01:07:52212: assigning 8 to cpu/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.E.] E
:875425 875447build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8 213: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] E:E:remote time is 8.68421[ 214[ 212
2022-12-12 01:07:52/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] 2022-12-12 01:07:52/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] .:[cpu time is 97.0588.:build asymm link desc with 8X Tesla V100-SXM2-32GB out of 88755532122022-12-12 01:07:52
875562212
: ] .: ] Ebuild asymm link desc with 8X Tesla V100-SXM2-32GB out of 8875608E[build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8 
:  2022-12-12 01:07:52
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.[: :875682[2022-12-12 01:07:52212/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc213: 2022-12-12 01:07:52.] :] E.875725build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8214remote time is 8.68421 875739: 
] 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: Ecpu time is 97.0588:[[E 
2132022-12-12 01:07:522022-12-12 01:07:52 /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] ../hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:remote time is 8.68421875844875846:213
: : 213] EE[] remote time is 8.68421  2022-12-12 01:07:52remote time is 8.68421
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.[
::8759282022-12-12 01:07:52213214: [.] ] E2022-12-12 01:07:52875971remote time is 8.68421cpu time is 97.0588 .: 

/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc875985E:[:  2142022-12-12 01:07:52E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] . :cpu time is 97.0588876039/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc214
: :] E214cpu time is 97.0588 ] 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:cpu time is 97.0588214
] cpu time is 97.0588
[2022-12-12 01:09:12. 93091: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:118] solver created. now build & solve
[2022-12-12 01:09:12.133414: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:123] solver built. now solve
block 0 storage is 00010001
	access is	0	0	0	0	4	4	4	4	
block 1 storage is 00100010
	access is	1	1	1	1	5	5	5	5	
block 2 storage is 01000100
	access is	2	2	2	2	6	6	6	6	
block 3 storage is 10001000
	access is	3	3	3	3	7	7	7	7	
block 4 storage is 00000000
	access is	8	8	8	8	8	8	8	8	
[2022-12-12 01:09:12.267500: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:127] solver solved
[2022-12-12 01:09:12.267567: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:195] 0 solved master
[2022-12-12 01:09:12.419586: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 0 solved
[2022-12-12 01:09:12.419631: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 0 initing device 0
[2022-12-12 01:09:12.420126: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-12 01:09:12.420181: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 01:09:12.421195: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 01:09:12.422032: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 01:09:12.434818: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 2 solved
[2022-12-12 01:09:12.434885: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 2 initing device 2
[2022-12-12 01:09:12.434977: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 6 solved
[2022-12-12 01:09:12.435034: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 6 initing device 6
[2022-12-12 01:09:12.435321: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-12 01:09:12.435375: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 01:09:12.435476: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-12 01:09:12.435525: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 01:09:12.435546: E [/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc2022-12-12 01:09:12:.202435564[] : 2022-12-12 01:09:123 solved[E2022-12-12 01:09:12.
 [.435596/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc[2022-12-12 01:09:12435617: :2022-12-12 01:09:12.: E202.435636E ] 435664:  /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc4 solved: E/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:
E :202 /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc[202] /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:2022-12-12 01:09:12] 5 solved:202.7 solved
205] [435757
] 1 solved2022-12-12 01:09:12: worker 0 thread 3 initing device 3[
.E
2022-12-12 01:09:12435811 [.: /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc2022-12-12 01:09:12435826E:.:  205435848E/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc] :  :worker 0 thread 4 initing device 4E/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc205
 :] /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc205worker 0 thread 5 initing device 5:] 
205worker 0 thread 7 initing device 7] 
worker 0 thread 1 initing device 1
[2022-12-12 01:09:12.436259: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-12 01:09:12.436308: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980[[] [2022-12-12 01:09:122022-12-12 01:09:12[eager alloc mem 381.47 MB2022-12-12 01:09:12..2022-12-12 01:09:12
.436320436321.436326: : 436334: EE: E  E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:18151815:1815] ] 1815] Building Coll Cache with ... num gpu device is 8Building Coll Cache with ... num gpu device is 8] Building Coll Cache with ... num gpu device is 8

Building Coll Cache with ... num gpu device is 8

[[2022-12-12 01:09:12[2022-12-12 01:09:12[.2022-12-12 01:09:12.2022-12-12 01:09:12436465.436466.: 436469: 436470E: E:  E E/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu :/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu1980:1980:] 1980] 1980eager alloc mem 381.47 MB] eager alloc mem 381.47 MB] 
eager alloc mem 381.47 MB
eager alloc mem 381.47 MB

[2022-12-12 01:09:12.439394: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 01:09:12.439816: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 01:09:12.440688: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 01:09:12.441307: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 01:09:12.441362: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 01:09:12.441417: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 01:09:12.441476: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 01:09:12.444092: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 01:09:12.444472: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 01:09:12.445282: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 01:09:12.445340: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 01:09:12.445875: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 01:09:12.445916: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 01:09:12.446034: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 01:09:12.502712: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.00 Bytes
[2022-12-12 01:09:12.507863: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5
[2022-12-12 01:09:12.507963: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 01:09:12.508758: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 19.45 MB
[2022-12-12 01:09:12.509300: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 01:09:12.510291: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 01:09:12.510336: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 19.07 MB
[2022-12-12 01:09:12.520439: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 01:09:12.521082: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 01:09:12.521126: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 2.39 GB
[[2022-12-12 01:09:122022-12-12 01:09:12..526919526920: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::19801980] ] eager alloc mem 5.00 Byteseager alloc mem 5.00 Bytes

[2022-12-12 01:09:12.532040: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5
[[2022-12-12 01:09:122022-12-12 01:09:12..532110532122: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::638638] ] eager release cuda mem 5eager release cuda mem 400000000

[2022-12-12 01:09:12.532204: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 01:09:12.533618: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 19.45 MB
[2022-12-12 01:09:12.534135: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 19.45 MB
[2022-12-12 01:09:12.534951: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 01:09:12.534978: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 01:09:12.535935: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 01:09:12.535967: [E2022-12-12 01:09:12 ./hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc535980:: 638W]  eager release cuda mem 625663/hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc
:43] WORKER[0] alloc host memory 19.07 MB
[2022-12-12 01:09:12.536029[[: 2022-12-12 01:09:122022-12-12 01:09:12W.. [536003536003/hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc[2022-12-12 01:09:12: : :2022-12-12 01:09:12.EE43.536071  ] 536072: /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuWORKER[0] alloc host memory 19.07 MB: E::
E 19801980 /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu] ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:eager alloc mem 5.00 Byteseager alloc mem 5.00 Bytes:1980

1980] ] eager alloc mem 5.00 Byteseager alloc mem 5.00 Bytes

[2022-12-12 01:09:12.536277: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.00 Bytes
[2022-12-12 01:09:12.542405: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5
[2022-12-12 01:09:12[.2022-12-12 01:09:12542500.: 542491E:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc :/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc638:] 638eager release cuda mem 400000000] 
eager release cuda mem 5
[2022-12-12 01:09:12.542558: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] [eager release cuda mem 52022-12-12 01:09:12
.542593: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 01:09:12.542643: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 01:09:12.542661: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5
[[2022-12-12 01:09:122022-12-12 01:09:12..542722542740: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::638638] ] eager release cuda mem 5eager release cuda mem 400000000

[2022-12-12 01:09:12.542824: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 01:09:12.543635: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 19.45 MB
[2022-12-12 01:09:12.544346: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 19.45 MB
[2022-12-12 01:09:12.545113: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 19.45 MB
[2022-12-12 01:09:12.545778: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 19.45 MB
[2022-12-12 01:09:12.546365: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 19.45 MB
[2022-12-12 01:09:12.547163: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 01:09:12.547372: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 01:09:12.547586: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 01:09:12.547697: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 01:09:12.547726: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 01:09:12.548152: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 01:09:12.548197: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 19.07 MB
[2022-12-12 01:09:12.548349: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 01:09:12.548393: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 19.07 MB
[2022-12-12 01:09:12.548556: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 01:09:12.548602: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 19.07 MB
[2022-12-12 01:09:12.548688: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 01:09:12.[5487232022-12-12 01:09:12: .E548733 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccW: 638/hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc] :eager release cuda mem 62566343
] WORKER[0] alloc host memory 19.07 MB
[2022-12-12 01:09:12.548789: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 19.07 MB
[2022-12-12 01:09:12.549111: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 01:09:12.549310: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 01:09:12.549712: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 01:09:12.549753: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 2.39 GB
[2022-12-12 01:09:12.549901: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 01:09:12.549943: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 2.39 GB
[[2022-12-12 01:09:122022-12-12 01:09:12..561522561523: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::19801980] ] eager alloc mem 25.25 KBeager alloc mem 25.25 KB

[2022-12-12 01:09:12.562103: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[[2022-12-12 01:09:122022-12-12 01:09:12..562147562149: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::638638] ] eager release cuda mem 25855eager release cuda mem 25855

[2022-12-12 01:09:12.562189: E[[ 2022-12-12 01:09:122022-12-12 01:09:12/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu..:5622155622161980: : ] EEeager alloc mem 25.25 KB  
/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::19801980] ] eager alloc mem 2.39 GBeager alloc mem 2.39 GB

[2022-12-12 01:09:12.562693: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 01:09:12.562735: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 2.39 GB
[2022-12-12 01:09:12.562803: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 01:09:12.562844: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 2.39 GB
[2022-12-12 01:09:12.563009: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 01:09:12.563617: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 01:09:12.563661: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 2.39 GB
[[[[[[[[2022-12-12 01:09:132022-12-12 01:09:132022-12-12 01:09:132022-12-12 01:09:132022-12-12 01:09:132022-12-12 01:09:132022-12-12 01:09:132022-12-12 01:09:13........457836457837457836: 457837457836457836457836457838: : E: : : : : EE EEEEE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu     /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::1926:::::19261926] 19261926192619261926] ] Device 6 init p2p of link 0] ] ] ] ] Device 3 init p2p of link 2Device 0 init p2p of link 3
Device 7 init p2p of link 4Device 2 init p2p of link 1Device 4 init p2p of link 5Device 5 init p2p of link 6Device 1 init p2p of link 7






[[[2022-12-12 01:09:13[2022-12-12 01:09:132022-12-12 01:09:13[.[[[2022-12-12 01:09:13..2022-12-12 01:09:134583642022-12-12 01:09:132022-12-12 01:09:132022-12-12 01:09:13.458364458366.: ...458372: : 458372E458372458372458382: EE:  : : : E  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuEEE /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu :   /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu1980/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:19801980:] :::1980] ] 1980eager alloc mem 611.00 KB198019801980] eager alloc mem 611.00 KBeager alloc mem 611.00 KB] 
] ] ] eager alloc mem 611.00 KB

eager alloc mem 611.00 KBeager alloc mem 611.00 KBeager alloc mem 611.00 KBeager alloc mem 611.00 KB




[2022-12-12 01:09:13.459430: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] [eager release cuda mem 625663[2022-12-12 01:09:13
2022-12-12 01:09:13.[.[[459451[2022-12-12 01:09:13[4594542022-12-12 01:09:132022-12-12 01:09:13: 2022-12-12 01:09:13.2022-12-12 01:09:13: ..E.459469.E459476459479 459482: 459484 : : /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc: E: /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccEE:E E:  638 /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc 638/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc] /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc] ::eager release cuda mem 625663:638:eager release cuda mem 625663638638
638] 638
] ] ] eager release cuda mem 625663] eager release cuda mem 625663eager release cuda mem 625663eager release cuda mem 625663
eager release cuda mem 625663



[2022-12-12 01:09:13.472253: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 6 init p2p of link 5
[2022-12-12 01:09:13.472405: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 01:09:13.472496: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 2
[2022-12-12 01:09:13.472660: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 01:09:13.473239: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 01:09:13.473483: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 01:09:13.474838: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 3 init p2p of link 0
[2022-12-12 01:09:13.474991: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 01:09:13.475026: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 5 init p2p of link 4
[2022-12-12 01:09:13.475197: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 01:09:13.475222: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 0 init p2p of link 6
[2022-12-12 01:09:13.475374: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 01:09:13.475462: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 2 init p2p of link 3
[2022-12-12 01:09:13.475604: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 01:09:13.475701: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 7 init p2p of link 1
[2022-12-12 01:09:13.475815: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 01:09:13.475839: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 01:09:13.475891: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 4 init p2p of link 7
[2022-12-12 01:09:13.476029: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] [eager release cuda mem 6256632022-12-12 01:09:13
.476050: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 01:09:13.476143: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 01:09:13.476414: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 01:09:13.476610: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 01:09:13.476860: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 01:09:13.485271: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 6 init p2p of link 4
[2022-12-12 01:09:13.485590: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 01:09:13.485976: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 3
[2022-12-12 01:09:13.486098: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 01:09:13.486487: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 01:09:13.486905: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 01:09:13.486920: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 0 init p2p of link 1
[2022-12-12 01:09:13.487046: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 01:09:13.487284: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 7 init p2p of link 6
[2022-12-12 01:09:13.487396: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 01:09:13.487895: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 01:09:13.488235: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 01:09:13.488454: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 3 init p2p of link 5
[2022-12-12 01:09:13.488568: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 01:09:13.488786: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 5 init p2p of link 7
[2022-12-12 01:09:13.488912: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 01:09:13.489206: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 2 init p2p of link 0
[2022-12-12 01:09:13.489318: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 01:09:13.489356: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 01:09:13.489613: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 4 init p2p of link 2
[2022-12-12 01:09:13.489719: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc[:2022-12-12 01:09:13638.] 489734eager release cuda mem 625663: 
E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 01:09:13.490127: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 01:09:13.490509: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 01:09:13.503438: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 3 init p2p of link 1
[2022-12-12 01:09:13.503552: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 01:09:13.504385: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 01:09:13.504404: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 4 init p2p of link 6
[2022-12-12 01:09:13.504523: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 01:09:13.505350: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 01:09:13.506133: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 6 init p2p of link 7
[2022-12-12 01:09:13.506323: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 01:09:13.506395: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 0
[2022-12-12 01:09:13.506512: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 01:09:13.506816: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 0 init p2p of link 2
[2022-12-12 01:09:13.506935: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[[2022-12-12 01:09:132022-12-12 01:09:13..507203507209: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::1926638] ] Device 7 init p2p of link 5
eager release cuda mem 625663
[2022-12-12 01:09:13.507326: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 01:09:13.507355: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 01:09:13.507757: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 01:09:13.507920: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 5 init p2p of link 3
[2022-12-12 01:09:13.508045: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 01:09:13.508171: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 01:09:13.508254: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 2 init p2p of link 4
[2022-12-12 01:09:13.508390: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 01:09:13.508813: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 01:09:13.509154: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 01:09:13.513632: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 20400000
[2022-12-12 01:09:13.517142: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 5000000 / 100000000 nodes ( 5.00 %~5.00 %) | remote 15000000 / 100000000 nodes ( 15.00 %) | cpu 80000000 / 100000000 nodes ( 80.00 %) | 2.39 GB | 1.08084 secs 
[2022-12-12 01:09:13.519284: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 20400000
[2022-12-12 01:09:13.521942: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 20400000
[2022-12-12 01:09:13.522407: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 20400000
[2022-12-12 01:09:13.522622: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 5000000 / 100000000 nodes ( 5.00 %~5.00 %) | remote 15000000 / 100000000 nodes ( 15.00 %) | cpu 80000000 / 100000000 nodes ( 80.00 %) | 2.39 GB | 1.08617 secs 
[2022-12-12 01:09:13.522909: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 5000000 / 100000000 nodes ( 5.00 %~5.00 %) | remote 15000000 / 100000000 nodes ( 15.00 %) | cpu 80000000 / 100000000 nodes ( 80.00 %) | 2.39 GB | 1.08646 secs 
[2022-12-12 01:09:13.523876: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 5000000 / 100000000 nodes ( 5.00 %~5.00 %) | remote 15000000 / 100000000 nodes ( 15.00 %) | cpu 80000000 / 100000000 nodes ( 80.00 %) | 2.39 GB | 1.08852 secs 
[2022-12-12 01:09:13.524121: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 20400000
[2022-12-12 01:09:13.525055: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 20400000
[2022-12-12 01:09:13.525504: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 20400000
[2022-12-12 01:09:13.525789: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 20400000
[2022-12-12 01:09:13.526224: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 5000000 / 100000000 nodes ( 5.00 %~5.00 %) | remote 15000000 / 100000000 nodes ( 15.00 %) | cpu 80000000 / 100000000 nodes ( 80.00 %) | 2.39 GB | 1.09071 secs 
[2022-12-12 01:09:13.527354: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 5000000 / 100000000 nodes ( 5.00 %~5.00 %) | remote 15000000 / 100000000 nodes ( 15.00 %) | cpu 80000000 / 100000000 nodes ( 80.00 %) | 2.39 GB | 1.09089 secs 
[2022-12-12 01:09:13.527508: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 5000000 / 100000000 nodes ( 5.00 %~5.00 %) | remote 15000000 / 100000000 nodes ( 15.00 %) | cpu 80000000 / 100000000 nodes ( 80.00 %) | 2.39 GB | 1.10734 secs 
[2022-12-12 01:09:13.527609: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 5000000 / 100000000 nodes ( 5.00 %~5.00 %) | remote 15000000 / 100000000 nodes ( 15.00 %) | cpu 80000000 / 100000000 nodes ( 80.00 %) | 2.39 GB | 1.09115 secs 
[2022-12-12 01:09:13.530622: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 9.03 GB
[2022-12-12 01:09:15. 29411: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 9.29 GB
[2022-12-12 01:09:15. 30028: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 9.29 GB
[2022-12-12 01:09:15. 33551: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 9.29 GB
[2022-12-12 01:09:16.549811: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 9.55 GB
[2022-12-12 01:09:16.576653: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 9.55 GB
[2022-12-12 01:09:16.577912: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 9.55 GB
[2022-12-12 01:09:17.753954: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 9.77 GB
[2022-12-12 01:09:17.754084: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 9.77 GB
[2022-12-12 01:09:17.755250: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 9.77 GB
[2022-12-12 01:09:18.996780: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 9.98 GB
[2022-12-12 01:09:18.996935: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 9.98 GB
[2022-12-12 01:09:18.997265: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 9.98 GB
[2022-12-12 01:09:20.  7694: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 10.44 GB
[2022-12-12 01:09:20.  8931: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 10.44 GB
[2022-12-12 01:09:20.  9941: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 10.44 GB
[2022-12-12 01:09:21.388199: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 10.64 GB
[2022-12-12 01:09:21.394547: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 10.64 GB
[HCTR][01:09:22.770][ERROR][RK0][tid #139891883095808]: replica 7 calling init per replica done, doing barrier
[HCTR][01:09:22.770][ERROR][RK0][tid #139892076029696]: replica 2 calling init per replica done, doing barrier
[HCTR][01:09:22.770][ERROR][RK0][tid #139892814227200]: replica 5 calling init per replica done, doing barrier
[HCTR][01:09:22.770][ERROR][RK0][main]: replica 4 calling init per replica done, doing barrier
[HCTR][01:09:22.770][ERROR][RK0][main]: replica 6 calling init per replica done, doing barrier
[HCTR][01:09:22.770][ERROR][RK0][tid #139892017313536]: replica 1 calling init per replica done, doing barrier
[HCTR][01:09:22.770][ERROR][RK0][tid #139891883095808]: replica 3 calling init per replica done, doing barrier
[HCTR][01:09:22.770][ERROR][RK0][tid #139892076029696]: replica 0 calling init per replica done, doing barrier
[HCTR][01:09:22.770][ERROR][RK0][main]: replica 4 calling init per replica done, doing barrier done
[HCTR][01:09:22.770][ERROR][RK0][tid #139891883095808]: replica 7 calling init per replica done, doing barrier done
[HCTR][01:09:22.770][ERROR][RK0][tid #139892076029696]: replica 0 calling init per replica done, doing barrier done
[HCTR][01:09:22.770][ERROR][RK0][main]: replica 6 calling init per replica done, doing barrier done
[HCTR][01:09:22.770][ERROR][RK0][tid #139892017313536]: replica 1 calling init per replica done, doing barrier done
[HCTR][01:09:22.770][ERROR][RK0][tid #139891883095808]: replica 3 calling init per replica done, doing barrier done
[HCTR][01:09:22.770][ERROR][RK0][tid #139892814227200]: replica 5 calling init per replica done, doing barrier done
[HCTR][01:09:22.770][ERROR][RK0][tid #139892076029696]: replica 2 calling init per replica done, doing barrier done
[HCTR][01:09:22.770][ERROR][RK0][main]: init per replica done
[HCTR][01:09:22.770][ERROR][RK0][tid #139891883095808]: init per replica done
[HCTR][01:09:22.770][ERROR][RK0][main]: init per replica done
[HCTR][01:09:22.770][ERROR][RK0][tid #139892017313536]: init per replica done
[HCTR][01:09:22.770][ERROR][RK0][tid #139891883095808]: init per replica done
[HCTR][01:09:22.770][ERROR][RK0][tid #139892814227200]: init per replica done
[HCTR][01:09:22.771][ERROR][RK0][tid #139892076029696]: init per replica done
[HCTR][01:09:22.773][ERROR][RK0][tid #139892076029696]: init per replica done
[HCTR][01:09:22.777][ERROR][RK0][tid #139892017313536]: 1 allocated 3276800 at 0x7f3cf4120000
[HCTR][01:09:22.777][ERROR][RK0][tid #139892076029696]: 2 allocated 3276800 at 0x7f3cf4120000
[HCTR][01:09:22.777][ERROR][RK0][main]: 3 allocated 3276800 at 0x7f3cf4120000
[HCTR][01:09:22.777][ERROR][RK0][tid #139892017313536]: 1 allocated 6553600 at 0x7f3cf4600000
[HCTR][01:09:22.777][ERROR][RK0][tid #139892076029696]: 2 allocated 6553600 at 0x7f3cf4600000
[HCTR][01:09:22.777][ERROR][RK0][main]: 3 allocated 6553600 at 0x7f3cf4600000
[HCTR][01:09:22.777][ERROR][RK0][tid #139892017313536]: 1 allocated 3276800 at 0x7f3cf4c40000
[HCTR][01:09:22.777][ERROR][RK0][tid #139892076029696]: 2 allocated 3276800 at 0x7f3cf4c40000
[HCTR][01:09:22.777][ERROR][RK0][main]: 3 allocated 3276800 at 0x7f3cf4c40000
[HCTR][01:09:22.777][ERROR][RK0][tid #139892017313536]: 1 allocated 6553600 at 0x7f3cf4f60000
[HCTR][01:09:22.777][ERROR][RK0][tid #139892076029696]: 2 allocated 6553600 at 0x7f3cf4f60000
[HCTR][01:09:22.777][ERROR][RK0][main]: 3 allocated 6553600 at 0x7f3cf4f60000
[HCTR][01:09:22.777][ERROR][RK0][main]: 5 allocated 3276800 at 0x7f3cf0120000
[HCTR][01:09:22.777][ERROR][RK0][tid #139891950204672]: 6 allocated 3276800 at 0x7f3cf4120000
[HCTR][01:09:22.777][ERROR][RK0][main]: 5 allocated 6553600 at 0x7f3cf0600000
[HCTR][01:09:22.777][ERROR][RK0][tid #139891950204672]: 6 allocated 6553600 at 0x7f3cf4600000
[HCTR][01:09:22.777][ERROR][RK0][main]: 5 allocated 3276800 at 0x7f3cf0c40000
[HCTR][01:09:22.777][ERROR][RK0][tid #139891950204672]: 6 allocated 3276800 at 0x7f3cf4c40000
[HCTR][01:09:22.777][ERROR][RK0][main]: 5 allocated 6553600 at 0x7f3cf0f60000
[HCTR][01:09:22.777][ERROR][RK0][tid #139891950204672]: 6 allocated 6553600 at 0x7f3cf4f60000
[HCTR][01:09:22.777][ERROR][RK0][tid #139892017313536]: 4 allocated 3276800 at 0x7f3cf4120000
[HCTR][01:09:22.777][ERROR][RK0][tid #139892017313536]: 4 allocated 6553600 at 0x7f3cf4600000
[HCTR][01:09:22.777][ERROR][RK0][tid #139891883095808]: 7 allocated 3276800 at 0x7f3cf4120000
[HCTR][01:09:22.777][ERROR][RK0][tid #139892017313536]: 4 allocated 3276800 at 0x7f3cf4c40000
[HCTR][01:09:22.777][ERROR][RK0][tid #139891883095808]: 7 allocated 6553600 at 0x7f3cf4600000
[HCTR][01:09:22.777][ERROR][RK0][tid #139892017313536]: 4 allocated 6553600 at 0x7f3cf4f60000
[HCTR][01:09:22.777][ERROR][RK0][tid #139891883095808]: 7 allocated 3276800 at 0x7f3cf4c40000
[HCTR][01:09:22.777][ERROR][RK0][tid #139891883095808]: 7 allocated 6553600 at 0x7f3cf4f60000
[HCTR][01:09:22.780][ERROR][RK0][main]: 0 allocated 3276800 at 0x7f3cf6720000
[HCTR][01:09:22.780][ERROR][RK0][main]: 0 allocated 6553600 at 0x7f3cf6c00000
[HCTR][01:09:22.780][ERROR][RK0][main]: 0 allocated 3276800 at 0x7f22b250e800
[HCTR][01:09:22.780][ERROR][RK0][main]: 0 allocated 6553600 at 0x7f22b282e800








