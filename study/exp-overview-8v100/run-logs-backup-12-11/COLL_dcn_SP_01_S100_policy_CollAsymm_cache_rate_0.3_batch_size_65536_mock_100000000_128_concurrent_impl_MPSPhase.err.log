2022-12-12 08:23:36.012409: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.020795: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.025938: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.029710: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.042257: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.057101: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.064583: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.071358: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.125468: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.131218: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.133237: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.134150: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.135230: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.136183: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.137140: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.138123: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.139213: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.140486: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.142258: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.143163: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.143377: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.144848: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.144862: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.146416: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.146607: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.147865: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.148254: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.149605: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.149925: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.151570: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.152071: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.152968: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.153739: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.154661: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.155434: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.156758: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.157711: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.158692: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.159734: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.160687: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.165119: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.166096: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 08:23:36.166259: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.167287: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.168361: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.169433: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.171058: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.172816: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.173376: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.174149: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.175018: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.175936: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.176418: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.177503: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.177834: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.178967: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.179205: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.180685: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.182256: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.183855: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.189802: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.191781: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.191839: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.193751: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.193783: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.195939: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.196024: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.201416: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.206793: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.206933: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.207436: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.207458: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.209719: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.210056: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.210556: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.210792: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.211169: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.220493: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.229964: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.248027: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.248334: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.248415: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.249238: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.249561: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.250626: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.250846: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.252566: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.252609: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.252696: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.254068: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.254501: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.255306: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.255392: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.257673: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.258155: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.259049: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.259427: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.260761: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.260808: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.261983: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.263695: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.264103: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.264144: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.264301: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.265333: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.267355: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.267576: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.267660: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.267750: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.268601: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.271166: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.271256: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.271373: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.271508: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.272386: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.274472: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.274689: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.275070: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.275433: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.277268: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.277402: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.282286: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.283546: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.283628: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.284408: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.286040: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.286174: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.286883: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.288321: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.288364: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.289086: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.290623: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.290667: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.291302: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.292935: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.293023: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.294270: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.294920: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.294966: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.296457: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.297000: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.297085: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.298786: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.300252: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.300354: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.300371: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.302641: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.302685: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.302767: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.303065: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.305078: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.305136: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.305394: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.305949: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.306948: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.307810: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.308124: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.308436: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.308942: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.310281: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.311340: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.311526: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.311837: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.312327: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.313493: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.314469: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.314550: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.315507: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.316457: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.317484: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.318104: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.318314: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.318936: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.319215: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.319714: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 08:23:36.320150: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.320972: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.321252: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.322284: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.322396: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.324329: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.325404: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.325600: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.325790: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.326939: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.327647: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 08:23:36.328142: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 08:23:36.328184: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.328420: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.328686: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.329539: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.329694: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.331081: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.331433: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.331907: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.333079: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.333303: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.334965: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.335327: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.335835: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.337039: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.337188: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.337204: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.337605: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.338646: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.339024: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.339918: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.341521: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.341642: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.341864: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.342853: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.343224: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.344039: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.345698: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.345918: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.346235: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.346961: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.347280: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.348366: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.350113: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.351058: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.351576: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.352291: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.384921: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.385811: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.385991: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.386623: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.389860: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.390227: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.390437: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.391148: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.394326: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.394882: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.395312: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.395508: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.399508: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.399969: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.400525: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.400676: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.404024: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.404423: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.406216: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.406378: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.408988: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.409306: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.410045: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.410270: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.413978: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.414854: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.415203: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.415531: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.426128: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.427232: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.427399: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.427673: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.429752: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.430740: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.431177: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.432114: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.464486: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 08:23:36.474390: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.490142: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.490694: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.492196: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.495126: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.496504: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.497436: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.499784: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 08:23:36.500359: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.509822: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.531669: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.561816: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.564502: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.566007: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.566739: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.571293: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.572083: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.574083: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.580540: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.583794: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 08:23:36.584649: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.592748: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.593113: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 08:23:36.597567: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.602276: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.602536: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.607219: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:36.612659: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:37.474477: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:37.475532: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:37.476076: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:37.476541: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 08:23:37.476591: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-12 08:23:37.495625: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:37.496263: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:37.496776: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:37.497343: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:37.498099: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:37.498567: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-12 08:23:37.545869: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 08:23:37.546097: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 08:23:37.604480: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12340
2022-12-12 08:23:37.752482: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:37.753123: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:37.754043: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:37.754528: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 08:23:37.754588: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-12 08:23:37.760131: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:37.760748: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:37.761271: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:37.761745: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 08:23:37.761807: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-12 08:23:37.773956: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:37.774806: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:37.775347: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:37.776088: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:37.776605: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:37.777212: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:1/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-12 08:23:37.780288: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:37.780899: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:37.781555: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:37.782139: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:37.782649: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:37.783123: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:2/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-12 08:23:37.790361: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:37.790987: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:37.791524: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:37.792706: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 08:23:37.792791: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-12 08:23:37.811611: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:37.812251: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:37.812880: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:37.813450: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:37.813972: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:37.814439: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:3/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-12 08:23:37.846163: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:37.846796: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:37.847340: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:37.847820: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 08:23:37.847870: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-12 08:23:37.854994: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:37.855662: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:37.856202: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:37.856346: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 08:23:37.856533: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 08:23:37.856668: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 08:23:37.856744: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-12 08:23:37.858397: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12341
2022-12-12 08:23:37.861266: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 08:23:37.861468: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 08:23:37.863518: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12343
2022-12-12 08:23:37.866469: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:37.866944: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:37.867255: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:37.867533: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:37.868031: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 08:23:37.868185: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 08:23:37.868259: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:37.868836: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:37.869240: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:37.869742: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:37.870549: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:37.870861: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:37.871260: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 08:23:37.871339: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-12 08:23:37.872154: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:37.872196: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 08:23:37.872243: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-12 08:23:37.872865: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:6/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-12 08:23:37.872917: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12342
2022-12-12 08:23:37.874952: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:37.875562: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:37.876082: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:37.876636: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:37.877151: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:37.877609: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:7/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-12 08:23:37.891014: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:37.891421: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:37.891831: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:37.892409: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:37.892715: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:37.893340: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:37.893753: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:37.894272: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:37.894575: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:37.895225: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:23:37.895460: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:4/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-12 08:23:37.895815: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:5/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-12 08:23:37.918126: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 08:23:37.918309: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 08:23:37.920050: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12346
2022-12-12 08:23:37.923846: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 08:23:37.923994: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 08:23:37.925834: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12347
2022-12-12 08:23:37.941587: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 08:23:37.941779: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 08:23:37.942159: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 08:23:37.942303: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 08:23:37.943483: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12345
2022-12-12 08:23:37.944104: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12344
[HCTR][08:23:39.202][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][08:23:39.202][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][08:23:39.202][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][08:23:39.202][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][08:23:39.203][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][08:23:39.203][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][08:23:39.228][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][08:23:39.229][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 1it [00:01,  1.59s/it]warmup run: 94it [00:01, 77.26it/s]warmup run: 1it [00:01,  1.53s/it]warmup run: 1it [00:01,  1.52s/it]warmup run: 187it [00:01, 166.94it/s]warmup run: 1it [00:01,  1.50s/it]warmup run: 97it [00:01, 82.57it/s]warmup run: 101it [00:01, 86.42it/s]warmup run: 280it [00:01, 266.61it/s]warmup run: 102it [00:01, 88.37it/s]warmup run: 1it [00:01,  1.53s/it]warmup run: 195it [00:01, 180.03it/s]warmup run: 1it [00:01,  1.48s/it]warmup run: 203it [00:01, 188.31it/s]warmup run: 373it [00:01, 370.77it/s]warmup run: 205it [00:01, 192.16it/s]warmup run: 98it [00:01, 83.45it/s]warmup run: 295it [00:01, 289.89it/s]warmup run: 101it [00:01, 88.35it/s]warmup run: 305it [00:01, 300.15it/s]warmup run: 1it [00:01,  1.49s/it]warmup run: 1it [00:01,  1.49s/it]warmup run: 466it [00:02, 472.49it/s]warmup run: 300it [00:01, 295.28it/s]warmup run: 196it [00:01, 180.89it/s]warmup run: 200it [00:01, 188.77it/s]warmup run: 380it [00:01, 373.32it/s]warmup run: 406it [00:01, 414.54it/s]warmup run: 96it [00:01, 83.51it/s]warmup run: 100it [00:01, 86.96it/s]warmup run: 560it [00:02, 569.19it/s]warmup run: 397it [00:01, 405.10it/s]warmup run: 294it [00:01, 288.20it/s]warmup run: 300it [00:01, 299.85it/s]warmup run: 482it [00:02, 491.92it/s]warmup run: 508it [00:02, 527.77it/s]warmup run: 193it [00:01, 181.64it/s]warmup run: 201it [00:01, 189.13it/s]warmup run: 657it [00:02, 660.37it/s]warmup run: 498it [00:02, 518.35it/s]warmup run: 393it [00:01, 400.74it/s]warmup run: 398it [00:01, 411.00it/s]warmup run: 583it [00:02, 599.59it/s]warmup run: 611it [00:02, 632.79it/s]warmup run: 302it [00:01, 301.22it/s]warmup run: 292it [00:01, 291.84it/s]warmup run: 754it [00:02, 734.55it/s]warmup run: 597it [00:02, 617.28it/s]warmup run: 492it [00:02, 509.94it/s]warmup run: 494it [00:01, 515.05it/s]warmup run: 685it [00:02, 694.52it/s]warmup run: 714it [00:02, 723.76it/s]warmup run: 404it [00:01, 418.05it/s]warmup run: 391it [00:01, 405.42it/s]warmup run: 851it [00:02, 794.88it/s]warmup run: 698it [00:02, 707.39it/s]warmup run: 591it [00:02, 610.86it/s]warmup run: 593it [00:02, 615.18it/s]warmup run: 784it [00:02, 766.56it/s]warmup run: 816it [00:02, 796.64it/s]warmup run: 491it [00:01, 517.07it/s]warmup run: 506it [00:01, 530.96it/s]warmup run: 953it [00:02, 854.02it/s]warmup run: 799it [00:02, 781.94it/s]warmup run: 691it [00:02, 699.22it/s]warmup run: 690it [00:02, 697.18it/s]warmup run: 885it [00:02, 828.96it/s]warmup run: 916it [00:02, 846.92it/s]warmup run: 610it [00:02, 637.91it/s]warmup run: 592it [00:02, 620.55it/s]warmup run: 1050it [00:02, 875.52it/s]warmup run: 897it [00:02, 832.38it/s]warmup run: 789it [00:02, 767.75it/s]warmup run: 788it [00:02, 767.34it/s]warmup run: 986it [00:02, 875.34it/s]warmup run: 692it [00:02, 708.36it/s]warmup run: 713it [00:02, 727.88it/s]warmup run: 1016it [00:02, 864.59it/s]warmup run: 1146it [00:02, 899.25it/s]warmup run: 996it [00:02, 875.04it/s]warmup run: 889it [00:02, 827.74it/s]warmup run: 886it [00:02, 821.19it/s]warmup run: 1087it [00:02, 910.32it/s]warmup run: 815it [00:02, 799.62it/s]warmup run: 793it [00:02, 781.65it/s]warmup run: 1113it [00:02, 893.00it/s]warmup run: 1242it [00:02, 912.93it/s]warmup run: 1097it [00:02, 910.39it/s]warmup run: 989it [00:02, 872.92it/s]warmup run: 984it [00:02, 863.16it/s]warmup run: 1188it [00:02, 937.04it/s]warmup run: 916it [00:02, 854.29it/s]warmup run: 893it [00:02, 837.40it/s]warmup run: 1214it [00:02, 923.99it/s]warmup run: 1342it [00:03, 936.85it/s]warmup run: 1198it [00:02, 936.32it/s]warmup run: 1089it [00:02, 906.92it/s]warmup run: 1081it [00:02, 892.20it/s]warmup run: 1290it [00:02, 958.35it/s]warmup run: 1018it [00:02, 899.00it/s]warmup run: 992it [00:02, 876.36it/s]warmup run: 1317it [00:02, 951.91it/s]warmup run: 1444it [00:03, 961.12it/s]warmup run: 1299it [00:02, 956.37it/s]warmup run: 1189it [00:02, 931.90it/s]warmup run: 1178it [00:02, 913.56it/s]warmup run: 1392it [00:02, 975.86it/s]warmup run: 1120it [00:02, 932.18it/s]warmup run: 1091it [00:02, 907.37it/s]warmup run: 1420it [00:02, 973.13it/s]warmup run: 1546it [00:03, 977.99it/s]warmup run: 1400it [00:02, 971.62it/s]warmup run: 1288it [00:02, 946.32it/s]warmup run: 1277it [00:02, 933.44it/s]warmup run: 1495it [00:03, 989.70it/s]warmup run: 1222it [00:02, 955.57it/s]warmup run: 1192it [00:02, 934.64it/s]warmup run: 1523it [00:03, 988.11it/s]warmup run: 1648it [00:03, 989.69it/s]warmup run: 1501it [00:03, 982.13it/s]warmup run: 1388it [00:02, 959.93it/s]warmup run: 1378it [00:02, 954.79it/s]warmup run: 1598it [00:03, 1001.01it/s]warmup run: 1324it [00:02, 971.65it/s]warmup run: 1293it [00:02, 954.08it/s]warmup run: 1626it [00:03, 998.31it/s]warmup run: 1750it [00:03, 997.14it/s]warmup run: 1602it [00:03, 989.44it/s]warmup run: 1488it [00:03, 971.38it/s]warmup run: 1480it [00:02, 972.12it/s]warmup run: 1700it [00:03, 1003.84it/s]warmup run: 1426it [00:02, 984.69it/s]warmup run: 1394it [00:02, 969.11it/s]warmup run: 1729it [00:03, 1005.81it/s]warmup run: 1851it [00:03, 1000.64it/s]warmup run: 1703it [00:03, 989.51it/s]warmup run: 1588it [00:03, 977.24it/s]warmup run: 1580it [00:03, 980.21it/s]warmup run: 1802it [00:03, 1003.45it/s]warmup run: 1494it [00:03, 977.83it/s]warmup run: 1528it [00:03, 973.53it/s]warmup run: 1833it [00:03, 1013.76it/s]warmup run: 1954it [00:03, 1006.69it/s]warmup run: 1804it [00:03, 992.96it/s]warmup run: 1688it [00:03, 982.28it/s]warmup run: 1680it [00:03, 980.18it/s]warmup run: 1904it [00:03, 995.83it/s] warmup run: 1595it [00:03, 985.04it/s]warmup run: 1628it [00:03, 974.62it/s]warmup run: 1936it [00:03, 1014.03it/s]warmup run: 2063it [00:03, 1031.41it/s]warmup run: 1904it [00:03, 953.61it/s]warmup run: 1788it [00:03, 972.33it/s]warmup run: 1779it [00:03, 980.93it/s]warmup run: 2005it [00:03, 999.37it/s]warmup run: 1696it [00:03, 991.67it/s]warmup run: 1727it [00:03, 977.65it/s]warmup run: 2044it [00:03, 1033.22it/s]warmup run: 2182it [00:03, 1078.76it/s]warmup run: 1887it [00:03, 976.02it/s]warmup run: 2001it [00:03, 935.92it/s]warmup run: 1878it [00:03, 982.92it/s]warmup run: 2124it [00:03, 1055.72it/s]warmup run: 1797it [00:03, 994.89it/s]warmup run: 1829it [00:03, 987.54it/s]warmup run: 2165it [00:03, 1084.38it/s]warmup run: 2302it [00:03, 1113.48it/s]warmup run: 1986it [00:03, 975.59it/s]warmup run: 2119it [00:03, 1005.13it/s]warmup run: 1979it [00:03, 988.33it/s]warmup run: 2244it [00:03, 1096.95it/s]warmup run: 1898it [00:03, 997.67it/s]warmup run: 1931it [00:03, 995.99it/s]warmup run: 2285it [00:03, 1117.74it/s]warmup run: 2422it [00:04, 1137.56it/s]warmup run: 2100it [00:03, 1024.07it/s]warmup run: 2238it [00:03, 1057.39it/s]warmup run: 2095it [00:03, 1037.27it/s]warmup run: 2364it [00:03, 1126.63it/s]warmup run: 1999it [00:03, 999.16it/s]warmup run: 2038it [00:03, 1017.33it/s]warmup run: 2405it [00:03, 1140.37it/s]warmup run: 2542it [00:04, 1154.24it/s]warmup run: 2219it [00:03, 1072.64it/s]warmup run: 2357it [00:03, 1094.77it/s]warmup run: 2216it [00:03, 1086.62it/s]warmup run: 2484it [00:03, 1146.87it/s]warmup run: 2116it [00:03, 1047.98it/s]warmup run: 2158it [00:03, 1071.31it/s]warmup run: 2526it [00:03, 1158.99it/s]warmup run: 2662it [00:04, 1165.69it/s]warmup run: 2338it [00:03, 1105.47it/s]warmup run: 2475it [00:03, 1119.72it/s]warmup run: 2336it [00:03, 1117.74it/s]warmup run: 2602it [00:04, 1155.93it/s]warmup run: 2234it [00:03, 1087.02it/s]warmup run: 2275it [00:03, 1099.09it/s]warmup run: 2648it [00:04, 1176.72it/s]warmup run: 2779it [00:04, 1166.56it/s]warmup run: 2456it [00:03, 1127.37it/s]warmup run: 2593it [00:04, 1137.21it/s]warmup run: 2455it [00:03, 1138.48it/s]warmup run: 2722it [00:04, 1167.32it/s]warmup run: 2353it [00:03, 1115.17it/s]warmup run: 2769it [00:04, 1184.80it/s]warmup run: 2386it [00:03, 1042.15it/s]warmup run: 2899it [00:04, 1174.64it/s]warmup run: 2575it [00:04, 1144.10it/s]warmup run: 2712it [00:04, 1151.40it/s]warmup run: 2575it [00:03, 1155.11it/s]warmup run: 2840it [00:04, 1168.73it/s]warmup run: 2472it [00:03, 1135.10it/s]warmup run: 2890it [00:04, 1191.10it/s]warmup run: 2505it [00:03, 1083.49it/s]warmup run: 3000it [00:04, 667.12it/s] warmup run: 2693it [00:04, 1154.61it/s]warmup run: 2830it [00:04, 1157.10it/s]warmup run: 2696it [00:04, 1169.60it/s]warmup run: 2960it [00:04, 1176.76it/s]warmup run: 2590it [00:04, 1146.69it/s]warmup run: 3000it [00:04, 690.17it/s] warmup run: 2624it [00:04, 1113.08it/s]warmup run: 3000it [00:04, 681.90it/s] warmup run: 2810it [00:04, 1158.12it/s]warmup run: 2949it [00:04, 1164.16it/s]warmup run: 2816it [00:04, 1177.76it/s]warmup run: 2709it [00:04, 1156.97it/s]warmup run: 2745it [00:04, 1139.97it/s]warmup run: 3000it [00:04, 683.81it/s] warmup run: 2929it [00:04, 1165.05it/s]warmup run: 2938it [00:04, 1190.12it/s]warmup run: 2827it [00:04, 1162.32it/s]warmup run: 2863it [00:04, 1149.23it/s]warmup run: 3000it [00:04, 688.53it/s] warmup run: 3000it [00:04, 679.39it/s] warmup run: 2945it [00:04, 1167.38it/s]warmup run: 2982it [00:04, 1160.02it/s]warmup run: 3000it [00:04, 688.54it/s] warmup run: 3000it [00:04, 687.64it/s] 
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]





warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   5%|▌         | 162/3000 [00:00<00:01, 1618.33it/s]warmup should be done:   6%|▌         | 165/3000 [00:00<00:01, 1648.68it/s]warmup should be done:   5%|▌         | 162/3000 [00:00<00:01, 1617.21it/s]warmup should be done:   5%|▌         | 162/3000 [00:00<00:01, 1614.14it/s]warmup should be done:   5%|▌         | 162/3000 [00:00<00:01, 1615.57it/s]warmup should be done:   5%|▌         | 163/3000 [00:00<00:01, 1625.13it/s]warmup should be done:   5%|▌         | 163/3000 [00:00<00:01, 1625.56it/s]warmup should be done:   5%|▌         | 160/3000 [00:00<00:01, 1591.54it/s]warmup should be done:  11%|█         | 331/3000 [00:00<00:01, 1653.62it/s]warmup should be done:  11%|█         | 325/3000 [00:00<00:01, 1620.68it/s]warmup should be done:  11%|█         | 324/3000 [00:00<00:01, 1616.22it/s]warmup should be done:  11%|█         | 327/3000 [00:00<00:01, 1632.82it/s]warmup should be done:  11%|█         | 328/3000 [00:00<00:01, 1636.28it/s]warmup should be done:  11%|█         | 325/3000 [00:00<00:01, 1619.31it/s]warmup should be done:  11%|█         | 328/3000 [00:00<00:01, 1636.13it/s]warmup should be done:  11%|█         | 322/3000 [00:00<00:01, 1604.76it/s]warmup should be done:  16%|█▋        | 494/3000 [00:00<00:01, 1645.81it/s]warmup should be done:  16%|█▋        | 491/3000 [00:00<00:01, 1630.79it/s]warmup should be done:  17%|█▋        | 497/3000 [00:00<00:01, 1649.51it/s]warmup should be done:  16%|█▌        | 486/3000 [00:00<00:01, 1611.54it/s]warmup should be done:  16%|█▋        | 492/3000 [00:00<00:01, 1632.32it/s]warmup should be done:  16%|█▌        | 483/3000 [00:00<00:01, 1602.41it/s]warmup should be done:  16%|█▋        | 488/3000 [00:00<00:01, 1615.85it/s]warmup should be done:  16%|█▌        | 487/3000 [00:00<00:01, 1613.45it/s]warmup should be done:  22%|██▏       | 662/3000 [00:00<00:01, 1649.44it/s]warmup should be done:  22%|██▏       | 660/3000 [00:00<00:01, 1649.13it/s]warmup should be done:  22%|██▏       | 650/3000 [00:00<00:01, 1614.35it/s]warmup should be done:  21%|██▏       | 644/3000 [00:00<00:01, 1601.35it/s]warmup should be done:  22%|██▏       | 648/3000 [00:00<00:01, 1605.37it/s]warmup should be done:  22%|██▏       | 649/3000 [00:00<00:01, 1608.10it/s]warmup should be done:  22%|██▏       | 655/3000 [00:00<00:01, 1621.03it/s]warmup should be done:  22%|██▏       | 656/3000 [00:00<00:01, 1622.21it/s]warmup should be done:  28%|██▊       | 827/3000 [00:00<00:01, 1648.59it/s]warmup should be done:  28%|██▊       | 825/3000 [00:00<00:01, 1648.95it/s]warmup should be done:  27%|██▋       | 805/3000 [00:00<00:01, 1600.17it/s]warmup should be done:  27%|██▋       | 812/3000 [00:00<00:01, 1611.03it/s]warmup should be done:  27%|██▋       | 811/3000 [00:00<00:01, 1609.70it/s]warmup should be done:  27%|██▋       | 819/3000 [00:00<00:01, 1616.93it/s]warmup should be done:  27%|██▋       | 818/3000 [00:00<00:01, 1614.17it/s]warmup should be done:  27%|██▋       | 809/3000 [00:00<00:01, 1559.91it/s]warmup should be done:  33%|███▎      | 990/3000 [00:00<00:01, 1647.04it/s]warmup should be done:  33%|███▎      | 992/3000 [00:00<00:01, 1645.45it/s]warmup should be done:  32%|███▏      | 973/3000 [00:00<00:01, 1611.08it/s]warmup should be done:  32%|███▏      | 966/3000 [00:00<00:01, 1596.82it/s]warmup should be done:  32%|███▏      | 974/3000 [00:00<00:01, 1606.81it/s]warmup should be done:  33%|███▎      | 980/3000 [00:00<00:01, 1609.56it/s]warmup should be done:  33%|███▎      | 981/3000 [00:00<00:01, 1611.14it/s]warmup should be done:  32%|███▏      | 966/3000 [00:00<00:01, 1545.25it/s]warmup should be done:  38%|███▊      | 1135/3000 [00:00<00:01, 1610.48it/s]warmup should be done:  38%|███▊      | 1155/3000 [00:00<00:01, 1633.59it/s]warmup should be done:  38%|███▊      | 1126/3000 [00:00<00:01, 1591.48it/s]warmup should be done:  38%|███▊      | 1135/3000 [00:00<00:01, 1601.02it/s]warmup should be done:  38%|███▊      | 1141/3000 [00:00<00:01, 1602.98it/s]warmup should be done:  38%|███▊      | 1143/3000 [00:00<00:01, 1604.97it/s]warmup should be done:  39%|███▊      | 1157/3000 [00:00<00:01, 1601.85it/s]warmup should be done:  37%|███▋      | 1124/3000 [00:00<00:01, 1554.88it/s]warmup should be done:  44%|████▍     | 1320/3000 [00:00<00:01, 1636.86it/s]warmup should be done:  43%|████▎     | 1297/3000 [00:00<00:01, 1610.30it/s]warmup should be done:  43%|████▎     | 1286/3000 [00:00<00:01, 1589.62it/s]warmup should be done:  43%|████▎     | 1296/3000 [00:00<00:01, 1599.90it/s]warmup should be done:  43%|████▎     | 1304/3000 [00:00<00:01, 1605.33it/s]warmup should be done:  43%|████▎     | 1302/3000 [00:00<00:01, 1600.82it/s]warmup should be done:  43%|████▎     | 1283/3000 [00:00<00:01, 1562.78it/s]warmup should be done:  44%|████▍     | 1318/3000 [00:00<00:01, 1435.79it/s]warmup should be done:  50%|████▉     | 1485/3000 [00:00<00:00, 1638.49it/s]warmup should be done:  49%|████▊     | 1459/3000 [00:00<00:00, 1611.05it/s]warmup should be done:  48%|████▊     | 1445/3000 [00:00<00:00, 1587.27it/s]warmup should be done:  49%|████▊     | 1456/3000 [00:00<00:00, 1597.58it/s]warmup should be done:  49%|████▉     | 1465/3000 [00:00<00:00, 1605.60it/s]warmup should be done:  49%|████▉     | 1463/3000 [00:00<00:00, 1600.27it/s]warmup should be done:  48%|████▊     | 1441/3000 [00:00<00:00, 1567.11it/s]warmup should be done:  49%|████▉     | 1484/3000 [00:00<00:01, 1497.30it/s]warmup should be done:  55%|█████▌    | 1651/3000 [00:01<00:00, 1642.14it/s]warmup should be done:  54%|█████▍    | 1622/3000 [00:01<00:00, 1614.15it/s]warmup should be done:  54%|█████▎    | 1605/3000 [00:01<00:00, 1589.69it/s]warmup should be done:  54%|█████▍    | 1616/3000 [00:01<00:00, 1595.24it/s]warmup should be done:  54%|█████▍    | 1626/3000 [00:01<00:00, 1605.37it/s]warmup should be done:  54%|█████▍    | 1624/3000 [00:01<00:00, 1598.66it/s]warmup should be done:  53%|█████▎    | 1600/3000 [00:01<00:00, 1573.90it/s]warmup should be done:  55%|█████▌    | 1650/3000 [00:01<00:00, 1542.18it/s]warmup should be done:  61%|██████    | 1816/3000 [00:01<00:00, 1643.62it/s]warmup should be done:  60%|█████▉    | 1786/3000 [00:01<00:00, 1619.15it/s]warmup should be done:  59%|█████▉    | 1766/3000 [00:01<00:00, 1594.28it/s]warmup should be done:  59%|█████▉    | 1776/3000 [00:01<00:00, 1596.65it/s]warmup should be done:  60%|█████▉    | 1792/3000 [00:01<00:00, 1619.79it/s]warmup should be done:  59%|█████▉    | 1784/3000 [00:01<00:00, 1598.29it/s]warmup should be done:  59%|█████▊    | 1758/3000 [00:01<00:00, 1573.83it/s]warmup should be done:  60%|██████    | 1815/3000 [00:01<00:00, 1572.58it/s]warmup should be done:  65%|██████▌   | 1950/3000 [00:01<00:00, 1623.90it/s]warmup should be done:  66%|██████▌   | 1981/3000 [00:01<00:00, 1641.67it/s]warmup should be done:  64%|██████▍   | 1926/3000 [00:01<00:00, 1593.52it/s]warmup should be done:  65%|██████▍   | 1936/3000 [00:01<00:00, 1595.68it/s]warmup should be done:  65%|██████▌   | 1958/3000 [00:01<00:00, 1630.26it/s]warmup should be done:  65%|██████▍   | 1944/3000 [00:01<00:00, 1597.34it/s]warmup should be done:  64%|██████▍   | 1916/3000 [00:01<00:00, 1573.10it/s]warmup should be done:  66%|██████▌   | 1981/3000 [00:01<00:00, 1595.42it/s]warmup should be done:  70%|███████   | 2114/3000 [00:01<00:00, 1628.31it/s]warmup should be done:  70%|██████▉   | 2096/3000 [00:01<00:00, 1596.28it/s]warmup should be done:  72%|███████▏  | 2146/3000 [00:01<00:00, 1637.00it/s]warmup should be done:  71%|███████   | 2124/3000 [00:01<00:00, 1638.95it/s]warmup should be done:  70%|███████   | 2104/3000 [00:01<00:00, 1597.73it/s]warmup should be done:  70%|██████▉   | 2086/3000 [00:01<00:00, 1553.66it/s]warmup should be done:  69%|██████▉   | 2077/3000 [00:01<00:00, 1581.58it/s]warmup should be done:  72%|███████▏  | 2147/3000 [00:01<00:00, 1612.81it/s]warmup should be done:  76%|███████▌  | 2278/3000 [00:01<00:00, 1630.67it/s]warmup should be done:  77%|███████▋  | 2310/3000 [00:01<00:00, 1637.39it/s]warmup should be done:  75%|███████▌  | 2257/3000 [00:01<00:00, 1598.07it/s]warmup should be done:  76%|███████▋  | 2291/3000 [00:01<00:00, 1646.13it/s]warmup should be done:  75%|███████▌  | 2264/3000 [00:01<00:00, 1597.76it/s]warmup should be done:  75%|███████▍  | 2245/3000 [00:01<00:00, 1562.60it/s]warmup should be done:  75%|███████▍  | 2238/3000 [00:01<00:00, 1587.83it/s]warmup should be done:  77%|███████▋  | 2313/3000 [00:01<00:00, 1625.32it/s]warmup should be done:  81%|████████▏ | 2442/3000 [00:01<00:00, 1631.40it/s]warmup should be done:  82%|████████▏ | 2474/3000 [00:01<00:00, 1637.05it/s]warmup should be done:  81%|████████  | 2417/3000 [00:01<00:00, 1597.43it/s]warmup should be done:  82%|████████▏ | 2457/3000 [00:01<00:00, 1647.68it/s]warmup should be done:  81%|████████  | 2424/3000 [00:01<00:00, 1596.41it/s]warmup should be done:  80%|████████  | 2404/3000 [00:01<00:00, 1570.44it/s]warmup should be done:  80%|███████▉  | 2399/3000 [00:01<00:00, 1593.17it/s]warmup should be done:  83%|████████▎ | 2478/3000 [00:01<00:00, 1629.81it/s]warmup should be done:  87%|████████▋ | 2606/3000 [00:01<00:00, 1632.64it/s]warmup should be done:  86%|████████▌ | 2578/3000 [00:01<00:00, 1600.31it/s]warmup should be done:  88%|████████▊ | 2640/3000 [00:01<00:00, 1641.17it/s]warmup should be done:  87%|████████▋ | 2623/3000 [00:01<00:00, 1649.17it/s]warmup should be done:  86%|████████▌ | 2585/3000 [00:01<00:00, 1598.28it/s]warmup should be done:  85%|████████▌ | 2563/3000 [00:01<00:00, 1575.81it/s]warmup should be done:  85%|████████▌ | 2559/3000 [00:01<00:00, 1587.32it/s]warmup should be done:  88%|████████▊ | 2644/3000 [00:01<00:00, 1635.90it/s]warmup should be done:  92%|█████████▏| 2770/3000 [00:01<00:00, 1633.23it/s]warmup should be done:  91%|█████████▏| 2740/3000 [00:01<00:00, 1605.07it/s]warmup should be done:  94%|█████████▎| 2806/3000 [00:01<00:00, 1645.63it/s]warmup should be done:  93%|█████████▎| 2789/3000 [00:01<00:00, 1652.25it/s]warmup should be done:  92%|█████████▏| 2746/3000 [00:01<00:00, 1601.18it/s]warmup should be done:  91%|█████████ | 2723/3000 [00:01<00:00, 1580.61it/s]warmup should be done:  91%|█████████ | 2720/3000 [00:01<00:00, 1591.20it/s]warmup should be done:  94%|█████████▎| 2810/3000 [00:01<00:00, 1640.48it/s]warmup should be done:  98%|█████████▊| 2936/3000 [00:01<00:00, 1639.96it/s]warmup should be done:  97%|█████████▋| 2904/3000 [00:01<00:00, 1614.37it/s]warmup should be done:  99%|█████████▉| 2973/3000 [00:01<00:00, 1651.98it/s]warmup should be done:  98%|█████████▊| 2955/3000 [00:01<00:00, 1653.71it/s]warmup should be done:  97%|█████████▋| 2909/3000 [00:01<00:00, 1607.85it/s]warmup should be done:  96%|█████████▌| 2884/3000 [00:01<00:00, 1587.40it/s]warmup should be done:  96%|█████████▌| 2882/3000 [00:01<00:00, 1599.32it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1642.55it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1631.17it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1623.84it/s]warmup should be done:  99%|█████████▉| 2978/3000 [00:01<00:00, 1649.53it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1606.96it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1605.58it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1605.04it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1586.76it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1584.01it/s]





warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   6%|▌         | 170/3000 [00:00<00:01, 1699.22it/s]warmup should be done:   6%|▌         | 170/3000 [00:00<00:01, 1697.71it/s]warmup should be done:   5%|▌         | 163/3000 [00:00<00:01, 1627.53it/s]warmup should be done:   6%|▌         | 167/3000 [00:00<00:01, 1666.51it/s]warmup should be done:   6%|▌         | 166/3000 [00:00<00:01, 1655.38it/s]warmup should be done:   6%|▌         | 167/3000 [00:00<00:01, 1665.62it/s]warmup should be done:   6%|▌         | 166/3000 [00:00<00:01, 1653.84it/s]warmup should be done:   6%|▌         | 171/3000 [00:00<00:01, 1701.56it/s]warmup should be done:  11%|█         | 327/3000 [00:00<00:01, 1633.78it/s]warmup should be done:  11%|█▏        | 340/3000 [00:00<00:01, 1695.74it/s]warmup should be done:  11%|█         | 334/3000 [00:00<00:01, 1665.34it/s]warmup should be done:  11%|█▏        | 341/3000 [00:00<00:01, 1699.83it/s]warmup should be done:  11%|█         | 332/3000 [00:00<00:01, 1653.48it/s]warmup should be done:  11%|█         | 334/3000 [00:00<00:01, 1663.65it/s]warmup should be done:  11%|█▏        | 342/3000 [00:00<00:01, 1701.67it/s]warmup should be done:  11%|█         | 332/3000 [00:00<00:01, 1649.51it/s]warmup should be done:  16%|█▋        | 492/3000 [00:00<00:01, 1639.77it/s]warmup should be done:  17%|█▋        | 498/3000 [00:00<00:01, 1655.22it/s]warmup should be done:  17%|█▋        | 502/3000 [00:00<00:01, 1667.75it/s]warmup should be done:  17%|█▋        | 512/3000 [00:00<00:01, 1700.33it/s]warmup should be done:  17%|█▋        | 510/3000 [00:00<00:01, 1690.96it/s]warmup should be done:  17%|█▋        | 501/3000 [00:00<00:01, 1661.73it/s]warmup should be done:  17%|█▋        | 499/3000 [00:00<00:01, 1654.32it/s]warmup should be done:  17%|█▋        | 513/3000 [00:00<00:01, 1692.85it/s]warmup should be done:  22%|██▏       | 657/3000 [00:00<00:01, 1639.86it/s]warmup should be done:  22%|██▏       | 670/3000 [00:00<00:01, 1671.93it/s]warmup should be done:  23%|██▎       | 683/3000 [00:00<00:01, 1702.49it/s]warmup should be done:  22%|██▏       | 665/3000 [00:00<00:01, 1657.27it/s]warmup should be done:  22%|██▏       | 666/3000 [00:00<00:01, 1659.25it/s]warmup should be done:  22%|██▏       | 669/3000 [00:00<00:01, 1665.44it/s]warmup should be done:  23%|██▎       | 680/3000 [00:00<00:01, 1687.47it/s]warmup should be done:  23%|██▎       | 683/3000 [00:00<00:01, 1685.94it/s]warmup should be done:  27%|██▋       | 821/3000 [00:00<00:01, 1639.23it/s]warmup should be done:  28%|██▊       | 838/3000 [00:00<00:01, 1674.07it/s]warmup should be done:  28%|██▊       | 854/3000 [00:00<00:01, 1703.12it/s]warmup should be done:  28%|██▊       | 832/3000 [00:00<00:01, 1660.29it/s]warmup should be done:  28%|██▊       | 837/3000 [00:00<00:01, 1668.12it/s]warmup should be done:  28%|██▊       | 834/3000 [00:00<00:01, 1663.68it/s]warmup should be done:  28%|██▊       | 849/3000 [00:00<00:01, 1686.83it/s]warmup should be done:  28%|██▊       | 852/3000 [00:00<00:01, 1683.71it/s]warmup should be done:  33%|███▎      | 986/3000 [00:00<00:01, 1640.83it/s]warmup should be done:  34%|███▎      | 1006/3000 [00:00<00:01, 1672.40it/s]warmup should be done:  34%|███▍      | 1025/3000 [00:00<00:01, 1703.08it/s]warmup should be done:  33%|███▎      | 1001/3000 [00:00<00:01, 1663.97it/s]warmup should be done:  33%|███▎      | 999/3000 [00:00<00:01, 1657.70it/s]warmup should be done:  33%|███▎      | 1004/3000 [00:00<00:01, 1665.50it/s]warmup should be done:  34%|███▍      | 1018/3000 [00:00<00:01, 1678.13it/s]warmup should be done:  34%|███▍      | 1021/3000 [00:00<00:01, 1680.34it/s]warmup should be done:  38%|███▊      | 1151/3000 [00:00<00:01, 1642.22it/s]warmup should be done:  39%|███▉      | 1174/3000 [00:00<00:01, 1670.27it/s]warmup should be done:  40%|███▉      | 1196/3000 [00:00<00:01, 1699.39it/s]warmup should be done:  39%|███▉      | 1165/3000 [00:00<00:01, 1655.94it/s]warmup should be done:  39%|███▉      | 1172/3000 [00:00<00:01, 1667.15it/s]warmup should be done:  39%|███▉      | 1168/3000 [00:00<00:01, 1661.74it/s]warmup should be done:  40%|███▉      | 1186/3000 [00:00<00:01, 1671.11it/s]warmup should be done:  40%|███▉      | 1190/3000 [00:00<00:01, 1676.88it/s]warmup should be done:  45%|████▍     | 1342/3000 [00:00<00:00, 1672.13it/s]warmup should be done:  44%|████▍     | 1316/3000 [00:00<00:01, 1638.35it/s]warmup should be done:  44%|████▍     | 1335/3000 [00:00<00:01, 1662.72it/s]warmup should be done:  44%|████▍     | 1331/3000 [00:00<00:01, 1654.14it/s]warmup should be done:  45%|████▍     | 1341/3000 [00:00<00:00, 1672.01it/s]warmup should be done:  46%|████▌     | 1366/3000 [00:00<00:00, 1694.99it/s]warmup should be done:  45%|████▌     | 1354/3000 [00:00<00:00, 1670.09it/s]warmup should be done:  45%|████▌     | 1359/3000 [00:00<00:00, 1677.90it/s]warmup should be done:  50%|█████     | 1510/3000 [00:00<00:00, 1672.53it/s]warmup should be done:  49%|████▉     | 1481/3000 [00:00<00:00, 1640.11it/s]warmup should be done:  50%|████▉     | 1497/3000 [00:00<00:00, 1655.34it/s]warmup should be done:  50%|█████     | 1502/3000 [00:00<00:00, 1663.02it/s]warmup should be done:  50%|█████     | 1510/3000 [00:00<00:00, 1675.62it/s]warmup should be done:  51%|█████     | 1536/3000 [00:00<00:00, 1689.00it/s]warmup should be done:  51%|█████     | 1527/3000 [00:00<00:00, 1678.40it/s]warmup should be done:  51%|█████     | 1522/3000 [00:00<00:00, 1667.54it/s]warmup should be done:  55%|█████▍    | 1647/3000 [00:01<00:00, 1645.44it/s]warmup should be done:  56%|█████▌    | 1679/3000 [00:01<00:00, 1675.19it/s]warmup should be done:  55%|█████▌    | 1664/3000 [00:01<00:00, 1657.46it/s]warmup should be done:  56%|█████▌    | 1679/3000 [00:01<00:00, 1678.97it/s]warmup should be done:  56%|█████▌    | 1669/3000 [00:01<00:00, 1662.77it/s]warmup should be done:  57%|█████▋    | 1705/3000 [00:01<00:00, 1684.71it/s]warmup should be done:  57%|█████▋    | 1696/3000 [00:01<00:00, 1679.40it/s]warmup should be done:  56%|█████▋    | 1690/3000 [00:01<00:00, 1668.58it/s]warmup should be done:  60%|██████    | 1814/3000 [00:01<00:00, 1650.37it/s]warmup should be done:  62%|██████▏   | 1848/3000 [00:01<00:00, 1677.35it/s]warmup should be done:  62%|██████▏   | 1848/3000 [00:01<00:00, 1682.05it/s]warmup should be done:  61%|██████    | 1831/3000 [00:01<00:00, 1658.93it/s]warmup should be done:  61%|██████    | 1836/3000 [00:01<00:00, 1663.50it/s]warmup should be done:  62%|██████▏   | 1874/3000 [00:01<00:00, 1683.03it/s]warmup should be done:  62%|██████▏   | 1858/3000 [00:01<00:00, 1669.74it/s]warmup should be done:  62%|██████▏   | 1865/3000 [00:01<00:00, 1679.69it/s]warmup should be done:  66%|██████▌   | 1980/3000 [00:01<00:00, 1652.26it/s]warmup should be done:  67%|██████▋   | 2016/3000 [00:01<00:00, 1676.95it/s]warmup should be done:  67%|██████▋   | 2017/3000 [00:01<00:00, 1683.65it/s]warmup should be done:  67%|██████▋   | 1997/3000 [00:01<00:00, 1656.52it/s]warmup should be done:  67%|██████▋   | 2003/3000 [00:01<00:00, 1662.36it/s]warmup should be done:  68%|██████▊   | 2043/3000 [00:01<00:00, 1683.11it/s]warmup should be done:  68%|██████▊   | 2026/3000 [00:01<00:00, 1671.18it/s]warmup should be done:  68%|██████▊   | 2034/3000 [00:01<00:00, 1681.15it/s]warmup should be done:  72%|███████▏  | 2147/3000 [00:01<00:00, 1655.49it/s]warmup should be done:  73%|███████▎  | 2186/3000 [00:01<00:00, 1681.02it/s]warmup should be done:  73%|███████▎  | 2186/3000 [00:01<00:00, 1682.18it/s]warmup should be done:  72%|███████▏  | 2163/3000 [00:01<00:00, 1654.32it/s]warmup should be done:  72%|███████▏  | 2170/3000 [00:01<00:00, 1661.16it/s]warmup should be done:  74%|███████▎  | 2212/3000 [00:01<00:00, 1679.72it/s]warmup should be done:  73%|███████▎  | 2203/3000 [00:01<00:00, 1680.91it/s]warmup should be done:  73%|███████▎  | 2194/3000 [00:01<00:00, 1670.61it/s]warmup should be done:  77%|███████▋  | 2313/3000 [00:01<00:00, 1656.00it/s]warmup should be done:  79%|███████▊  | 2358/3000 [00:01<00:00, 1691.15it/s]warmup should be done:  78%|███████▊  | 2355/3000 [00:01<00:00, 1683.24it/s]warmup should be done:  78%|███████▊  | 2330/3000 [00:01<00:00, 1658.77it/s]warmup should be done:  78%|███████▊  | 2338/3000 [00:01<00:00, 1663.80it/s]warmup should be done:  79%|███████▉  | 2380/3000 [00:01<00:00, 1678.57it/s]warmup should be done:  79%|███████▉  | 2372/3000 [00:01<00:00, 1680.26it/s]warmup should be done:  79%|███████▊  | 2362/3000 [00:01<00:00, 1670.20it/s]warmup should be done:  83%|████████▎ | 2479/3000 [00:01<00:00, 1652.75it/s]warmup should be done:  84%|████████▍ | 2524/3000 [00:01<00:00, 1683.89it/s]warmup should be done:  84%|████████▍ | 2530/3000 [00:01<00:00, 1697.63it/s]warmup should be done:  83%|████████▎ | 2498/3000 [00:01<00:00, 1663.54it/s]warmup should be done:  84%|████████▎ | 2506/3000 [00:01<00:00, 1666.30it/s]warmup should be done:  85%|████████▍ | 2549/3000 [00:01<00:00, 1681.34it/s]warmup should be done:  84%|████████▍ | 2531/3000 [00:01<00:00, 1673.72it/s]warmup should be done:  85%|████████▍ | 2541/3000 [00:01<00:00, 1679.88it/s]warmup should be done:  90%|█████████ | 2702/3000 [00:01<00:00, 1703.30it/s]warmup should be done:  90%|████████▉ | 2693/3000 [00:01<00:00, 1684.36it/s]warmup should be done:  89%|████████▉ | 2665/3000 [00:01<00:00, 1664.31it/s]warmup should be done:  88%|████████▊ | 2645/3000 [00:01<00:00, 1647.23it/s]warmup should be done:  89%|████████▉ | 2673/3000 [00:01<00:00, 1664.40it/s]warmup should be done:  91%|█████████ | 2718/3000 [00:01<00:00, 1682.49it/s]warmup should be done:  90%|█████████ | 2702/3000 [00:01<00:00, 1682.04it/s]warmup should be done:  90%|█████████ | 2710/3000 [00:01<00:00, 1681.10it/s]warmup should be done:  96%|█████████▌| 2874/3000 [00:01<00:00, 1707.30it/s]warmup should be done:  95%|█████████▌| 2862/3000 [00:01<00:00, 1683.12it/s]warmup should be done:  94%|█████████▍| 2832/3000 [00:01<00:00, 1664.57it/s]warmup should be done:  94%|█████████▎| 2810/3000 [00:01<00:00, 1646.81it/s]warmup should be done:  95%|█████████▍| 2840/3000 [00:01<00:00, 1662.85it/s]warmup should be done:  96%|█████████▌| 2887/3000 [00:01<00:00, 1679.41it/s]warmup should be done:  96%|█████████▌| 2873/3000 [00:01<00:00, 1689.10it/s]warmup should be done:  96%|█████████▌| 2879/3000 [00:01<00:00, 1680.11it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1687.61it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1685.67it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1681.81it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1679.74it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1677.17it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1662.34it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1666.95it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1659.57it/s]warmup should be done:  99%|█████████▉| 2976/3000 [00:01<00:00, 1647.90it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1645.53it/s]
WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f96a7b6b190>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f96a7b5b1f0>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f96a7b6a280>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f96a7b6a1c0>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f96a7b6a0d0>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f96a7e4eb80>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f96a7e4f730>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f96a7e51d30>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
2022-12-12 08:25:10.600605: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f91e282bcb0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 08:25:10.600664: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 08:25:10.608806: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 08:25:10.661437: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f91e302dc70 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 08:25:10.661495: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 08:25:10.669978: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 08:25:10.815810: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f91d7031360 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 08:25:10.815869: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 08:25:10.825313: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 08:25:11.315156: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f91e28338b0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 08:25:11.315223: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 08:25:11.323110: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 08:25:11.442261: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f91db02d320 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 08:25:11.442325: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 08:25:11.443615: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f91db02cdc0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 08:25:11.443678: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 08:25:11.450418: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 08:25:11.451563: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 08:25:11.487382: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f91db031770 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 08:25:11.487447: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 08:25:11.490747: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f91da7957f0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 08:25:11.490798: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 08:25:11.495098: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 08:25:11.500060: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 08:25:17.794281: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 08:25:17.930990: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 08:25:18.128654: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 08:25:18.128823: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 08:25:18.280558: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 08:25:18.359405: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 08:25:18.403179: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 08:25:18.404964: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
[HCTR][08:26:07.115][ERROR][RK0][main]: replica 5 reaches 1000, calling init pre replica
[HCTR][08:26:07.115][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][08:26:07.121][ERROR][RK0][main]: coll ps creation done
[HCTR][08:26:07.121][ERROR][RK0][main]: replica 5 waits for coll ps creation barrier
[HCTR][08:26:07.287][ERROR][RK0][tid #140264706406144]: replica 7 reaches 1000, calling init pre replica
[HCTR][08:26:07.287][ERROR][RK0][tid #140264706406144]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][08:26:07.295][ERROR][RK0][tid #140264706406144]: coll ps creation done
[HCTR][08:26:07.295][ERROR][RK0][tid #140264706406144]: replica 7 waits for coll ps creation barrier
[HCTR][08:26:07.415][ERROR][RK0][main]: replica 4 reaches 1000, calling init pre replica
[HCTR][08:26:07.415][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][08:26:07.422][ERROR][RK0][main]: coll ps creation done
[HCTR][08:26:07.423][ERROR][RK0][main]: replica 4 waits for coll ps creation barrier
[HCTR][08:26:07.525][ERROR][RK0][tid #140264706406144]: replica 1 reaches 1000, calling init pre replica
[HCTR][08:26:07.525][ERROR][RK0][tid #140264706406144]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][08:26:07.532][ERROR][RK0][tid #140264706406144]: coll ps creation done
[HCTR][08:26:07.532][ERROR][RK0][tid #140264706406144]: replica 1 waits for coll ps creation barrier
[HCTR][08:26:07.598][ERROR][RK0][main]: replica 3 reaches 1000, calling init pre replica
[HCTR][08:26:07.598][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][08:26:07.603][ERROR][RK0][main]: coll ps creation done
[HCTR][08:26:07.603][ERROR][RK0][main]: replica 3 waits for coll ps creation barrier
[HCTR][08:26:07.615][ERROR][RK0][main]: replica 0 reaches 1000, calling init pre replica
[HCTR][08:26:07.615][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][08:26:07.622][ERROR][RK0][main]: coll ps creation done
[HCTR][08:26:07.622][ERROR][RK0][main]: replica 0 waits for coll ps creation barrier
[HCTR][08:26:07.634][ERROR][RK0][tid #140264706406144]: replica 6 reaches 1000, calling init pre replica
[HCTR][08:26:07.634][ERROR][RK0][tid #140264706406144]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][08:26:07.639][ERROR][RK0][tid #140264706406144]: coll ps creation done
[HCTR][08:26:07.639][ERROR][RK0][tid #140264706406144]: replica 6 waits for coll ps creation barrier
[HCTR][08:26:07.689][ERROR][RK0][tid #140265780147968]: replica 2 reaches 1000, calling init pre replica
[HCTR][08:26:07.690][ERROR][RK0][tid #140265780147968]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][08:26:07.697][ERROR][RK0][tid #140265780147968]: coll ps creation done
[HCTR][08:26:07.697][ERROR][RK0][tid #140265780147968]: replica 2 waits for coll ps creation barrier
[HCTR][08:26:07.697][ERROR][RK0][main]: replica 0 preparing frequency
[HCTR][08:26:08.569][ERROR][RK0][main]: replica 0 preparing frequency done
[HCTR][08:26:08.617][ERROR][RK0][main]: replica 0 calling init per replica
[HCTR][08:26:08.617][ERROR][RK0][main]: replica 5 calling init per replica
[HCTR][08:26:08.617][ERROR][RK0][tid #140264706406144]: replica 7 calling init per replica
[HCTR][08:26:08.617][ERROR][RK0][main]: replica 4 calling init per replica
[HCTR][08:26:08.617][ERROR][RK0][tid #140264706406144]: replica 1 calling init per replica
[HCTR][08:26:08.617][ERROR][RK0][tid #140264706406144]: replica 6 calling init per replica
[HCTR][08:26:08.617][ERROR][RK0][main]: replica 3 calling init per replica
[HCTR][08:26:08.617][ERROR][RK0][tid #140265780147968]: replica 2 calling init per replica
[HCTR][08:26:08.617][ERROR][RK0][main]: Calling build_v2
[HCTR][08:26:08.617][ERROR][RK0][main]: Calling build_v2
[HCTR][08:26:08.617][ERROR][RK0][tid #140264706406144]: Calling build_v2
[HCTR][08:26:08.617][ERROR][RK0][main]: Calling build_v2
[HCTR][08:26:08.617][ERROR][RK0][tid #140264706406144]: Calling build_v2
[HCTR][08:26:08.617][ERROR][RK0][tid #140264706406144]: Calling build_v2
[HCTR][08:26:08.617][ERROR][RK0][main]: Calling build_v2
[HCTR][08:26:08.617][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][08:26:08.617][ERROR][RK0][tid #140265780147968]: Calling build_v2
[HCTR][08:26:08.617][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][08:26:08.617][ERROR][RK0][tid #140264706406144]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][08:26:08.617][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][08:26:08.617][ERROR][RK0][tid #140264706406144]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][08:26:08.617][ERROR][RK0][tid #140264706406144]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][08:26:08.617][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][08:26:08.617][ERROR][RK0][tid #140265780147968]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[[[[[[[2022-12-12 08:26:08[2022-12-12 08:26:082022-12-12 08:26:082022-12-12 08:26:08.2022-12-12 08:26:082022-12-12 08:26:08...2022-12-12 08:26:086178032022-12-12 08:26:08..617804617803617803.: 617823.617817617825: : : E: 617832: : EEE E: EE   /hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc E  /hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc:/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc /hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc:::136:/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc::136136136] 136:136136] ] ] using concurrent impl MPSPhase] 136] ] using concurrent impl MPSPhaseusing concurrent impl MPSPhaseusing concurrent impl MPSPhase
using concurrent impl MPSPhase] using concurrent impl MPSPhaseusing concurrent impl MPSPhase



using concurrent impl MPSPhase


[2022-12-12 08:26:08.622054: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:178] v100x8, slow pcie
[2022-12-12 08:26:08.622091: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[:2022-12-12 08:26:08196.] 622099assigning 8 to cpu: 
E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:178] v100x8, slow pcie
[2022-12-12 08:26:08.622145[: 2022-12-12 08:26:08E. 622144/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: [:E2022-12-12 08:26:08196 .] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc622165assigning 8 to cpu:: 
178E[]  2022-12-12 08:26:08v100x8, slow pcie/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.
:622190[212: [2022-12-12 08:26:08] E2022-12-12 08:26:08.build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8[ .622223
2022-12-12 08:26:08/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc622230: .:: E622235178E : []  /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE2022-12-12 08:26:08v100x8, slow pcie/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: [.
:212/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 08:26:08622273196] [:.: ] build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8[2022-12-12 08:26:08178622286Eassigning 8 to cpu
2022-12-12 08:26:08.] : [ 
[.622319v100x8, slow pcieE2022-12-12 08:26:08/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 08:26:08622328: 
 .[:.: E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc622388[2022-12-12 08:26:08213622371E :: 2022-12-12 08:26:08.] :  /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc178E.622422remote time is 8.68421E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:]  622444: 
 :196v100x8, slow pcie/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc178[] 
:E [:] 2022-12-12 08:26:08assigning 8 to cpu213 /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 08:26:08178v100x8, slow pcie.
] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:.] 
622554remote time is 8.68421:212622592v100x8, slow pcie: 
196[] : 
E] [[2022-12-12 08:26:08build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8E[ assigning 8 to cpu2022-12-12 08:26:082022-12-12 08:26:08.
 2022-12-12 08:26:08/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
.[.622649/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.:6226702022-12-12 08:26:08622687: :622679214: .: E[196: ] E622733E 2022-12-12 08:26:08] Ecpu time is 97.0588 :  /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.assigning 8 to cpu 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:622765
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: :196: :214/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc196] E212] :] assigning 8 to cpu[ ] cpu time is 97.0588213assigning 8 to cpu
2022-12-12 08:26:08/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccbuild asymm link desc with 8X Tesla V100-SXM2-32GB out of 8
] 
.:
remote time is 8.68421622892212
: ] [Ebuild asymm link desc with 8X Tesla V100-SXM2-32GB out of 8[2022-12-12 08:26:08 [
2022-12-12 08:26:08[./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 08:26:08.2022-12-12 08:26:08622966[:.622974.: 2022-12-12 08:26:08212622981: 622988E.] : E:  623011build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8E E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: 
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc :E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc213 [:214:] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 08:26:08212] 212remote time is 8.68421:.] cpu time is 97.0588] 
213623148build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8
build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8] : [

remote time is 8.68421E2022-12-12 08:26:08
 .[[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc623236[2022-12-12 08:26:082022-12-12 08:26:08:: 2022-12-12 08:26:08..213E.623274623276]  623286: : remote time is 8.68421/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: EE
:E  214 [/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 08:26:08::cpu time is 97.0588:.213213
214623392] ] ] : remote time is 8.68421remote time is 8.68421cpu time is 97.0588E


 [/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[2022-12-12 08:26:08:2022-12-12 08:26:08.214.623485] 623488: cpu time is 97.0588: E
E  /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc::214214] ] cpu time is 97.0588cpu time is 97.0588

[2022-12-12 08:27:28.819730: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:118] solver created. now build & solve
[2022-12-12 08:27:28.860104: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 381.47 MB
[2022-12-12 08:27:28.860189: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 381.47 MB
[2022-12-12 08:27:28.861481: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:74] mapping nid to rank...
[2022-12-12 08:27:28.928667: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:91] counting slots...
[2022-12-12 08:27:29.327385: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:105] Final num slot is 49
[2022-12-12 08:27:29.327493: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:109] counting blocks...
[2022-12-12 08:27:36.117740: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:118] Final num block is 1023
[2022-12-12 08:27:36.117835: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:123] counting freq and density...
[2022-12-12 08:27:37.852550: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:154] averaging freq and density...
[2022-12-12 08:27:37.852691: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:155] 1023
[2022-12-12 08:27:37.855849: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:123] solver built. now solve
[2022-12-12 08:27:37.855936: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:354] constructing optimal solver, device=8, stream=1
1023 blocks, 8 devices
[2022-12-12 08:27:38. 62028: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:527] Add Var...
[2022-12-12 08:27:38. 90444: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:545] Capacity...
[2022-12-12 08:27:38. 91836: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:548] Connect CPU...
[2022-12-12 08:27:38.111744: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:550] Connect Access To Storage...
[2022-12-12 08:27:38.688186: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:554] Time...
[2022-12-12 08:27:38.690438: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:516] dst 0, total sm is 80
[2022-12-12 08:27:38.693386: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:516] dst 1, total sm is 80
[2022-12-12 08:27:38.696249: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:516] dst 2, total sm is 80
[2022-12-12 08:27:38.699054: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:516] dst 3, total sm is 80
[2022-12-12 08:27:38.701879: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:516] dst 4, total sm is 80
[2022-12-12 08:27:38.704679: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:516] dst 5, total sm is 80
[2022-12-12 08:27:38.707477: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:516] dst 6, total sm is 80
[2022-12-12 08:27:38.710250: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:516] dst 7, total sm is 80
[2022-12-12 08:28:39.775214: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:569] Coll Cache init block placement array
[2022-12-12 08:28:39.785159: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:645] Coll Cache init block placement array done
[2022-12-12 08:28:39.788881: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:647] Coll Cache model reset done
[2022-12-12 08:28:39.832090: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:127] solver solved
[2022-12-12 08:28:39.832202: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:195] 0 solved master
[2022-12-12 08:28:39.832236: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 0 solved
[2022-12-12 08:28:39.832267: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 0 initing device 0
[2022-12-12 08:28:39.832830: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-12 08:28:39.832883: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 08:28:39.834083: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 08:28:39.834747: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 08:28:39.847587: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 3 solved
[2022-12-12 08:28:39.847662: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 3 initing device 3
[[2022-12-12 08:28:392022-12-12 08:28:39..847902847910: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc::202202] ] 7 solved6 solved

[[2022-12-12 08:28:392022-12-12 08:28:39..847994847995: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc::205205] ] worker 0 thread 7 initing device 7worker 0 thread 6 initing device 6

[2022-12-12 08:28:39.848108: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-12 08:28:39.848167: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 08:28:39.848235: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 2 solved
[2022-12-12 08:28:39.848305: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 2 initing device 2
[[2022-12-12 08:28:392022-12-12 08:28:39..848448848450: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::18151815] ] Building Coll Cache with ... num gpu device is 8Building Coll Cache with ... num gpu device is 8

[[2022-12-12 08:28:39[2022-12-12 08:28:39.2022-12-12 08:28:39.848522.848523: 848510: E: E E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:1980:1980] 202] eager alloc mem 381.47 MB] eager alloc mem 381.47 MB

5 solved
[2022-12-12 08:28:39.848620: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 5 initing device 5
[2022-12-12 08:28:39.848758: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-12 08:28:39.848807: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 08:28:39.849118: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-12 08:28:39.849178: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 08:28:39.849373: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 1 solved
[2022-12-12 08:28:39[.2022-12-12 08:28:39849425.: 849412E:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc :/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc205:] 202worker 0 thread 1 initing device 1] 
4 solved
[2022-12-12 08:28:39.849519: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 4 initing device 4
[2022-12-12 08:28:39.849888: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-12 08:28:39.849931: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 08:28:39.849967: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-12 08:28:39.850012: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 08:28:39.852368: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 08:28:39.852635: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 08:28:39.852768: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 08:28:39.853439: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 08:28:39.853497: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 08:28:39.853994: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 08:28:39.854548: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 08:28:39.856811: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 08:28:39.857056: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 08:28:39.857168: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 08:28:39.857736: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 08:28:39.857787: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 08:28:39.857837: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 08:28:39.858342: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 08:28:39.911989: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1023.00 Bytes
[2022-12-12 08:28:39.917173: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1023
[2022-12-12 08:28:39.917284: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 08:28:39.918100: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 114.82 MB
[2022-12-12 08:28:39.918800: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 08:28:39.919935: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 08:28:39.919985: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 114.34 MB
[2022-12-12 08:28:39.937529: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1023.00 Bytes
[2022-12-12 08:28:39.[9375972022-12-12 08:28:39[: [.2022-12-12 08:28:39E2022-12-12 08:28:39937629. .: 937656/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu937655E: ::  E1980E/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu ]  :/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cueager alloc mem 1023.00 Bytes/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu1980:
:] 19801980eager alloc mem 1023.00 Bytes] ] 
eager alloc mem 1023.00 Byteseager alloc mem 1023.00 Bytes

[2022-12-12 08:28:39.941982: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1023.00 Bytes
[2022-12-12 08:28:39.942204: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1023.00 Bytes
[2022-12-12 08:28:39.950409: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1023
[2022-12-12 08:28:39.950496: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 08:28:39.950646: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1023
[2022-12-12 08:28:39.950731: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 08:28:39.951310: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 114.82 MB
[2022-12-12 08:28:39.951390: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1023
[2022-12-12 08:28:39.951470: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 08:28:39.951578: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1023
[2022-12-12 08:28:39.951654: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 08:28:39.951748: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1023
[2022-12-12 08:28:39.951827: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 08:28:39.951838: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1023
[2022-12-12 08:28:39.951913: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000[
2022-12-12 08:28:39.951922: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1023
[2022-12-12 08:28:39.951999: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 08:28:39.952288: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 114.82 MB
[2022-12-12 08:28:39.953435: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 114.82 MB
[2022-12-12 08:28:39.953680: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 08:28:39.954336: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 114.82 MB
[2022-12-12 08:28:39.954774: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 08:28:39.954821: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 114.43 MB
[2022-12-12 08:28:39.955005: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 114.82 MB
[2022-12-12 08:28:39.955563: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 114.82 MB
[2022-12-12 08:28:39.956074: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 114.82 MB
[2022-12-12 08:28:39.957094: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 08:28:39.957485: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 08:28:39.957716: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 08:28:39.957769: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 08:28:39.957829: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 08:28:39.957869: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 08:28:39.958190: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 08:28:39.958235: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 114.41 MB
[2022-12-12 08:28:39.958577: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 08:28:39.958623: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 114.42 MB
[2022-12-12 08:28:39.958798: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 08:28:39.958844: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 114.22 MB
[2022-12-12 08:28:39.958881: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 08:28:39.[9589172022-12-12 08:28:39: .E958926 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccW: 638/hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc[] :2022-12-12 08:28:39eager release cuda mem 62566343.
] 958975WORKER[0] alloc host memory 114.39 MB: 
E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663[
2022-12-12 08:28:39.959032: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 113.83 MB[
2022-12-12 08:28:39.959062: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 114.09 MB
[2022-12-12 08:28:39.995497: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 08:28:39.996133: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 08:28:39.996184: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 14.30 GB
[2022-12-12 08:28:40. 28759: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 08:28:40. 29422: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 08:28:40. 29464: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 14.31 GB
[2022-12-12 08:28:40. 32717: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 08:28:40. 33353: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 08:28:40. 33398: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 14.31 GB
[2022-12-12 08:28:40. 33601: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 08:28:40. 34232: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 08:28:40. 34275: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 14.27 GB
[2022-12-12 08:28:40. 35191: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 08:28:40. 35534: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 08:28:40. 35796: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 08:28:40. 35837: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 14.28 GB
[2022-12-12 08:28:40. 36146: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 08:28:40. 36189: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 14.31 GB
[2022-12-12 08:28:40. 36520: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 08:28:40. 37154: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 08:28:40. 37196: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 14.23 GB
[2022-12-12 08:28:40. 38088: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 08:28:40. 38724: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 08:28:40. 38766: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 14.30 GB
[[[2022-12-12 08:28:442022-12-12 08:28:44[..[2022-12-12 08:28:44544106[5441062022-12-12 08:28:44[.: [: 2022-12-12 08:28:442022-12-12 08:28:44.2022-12-12 08:28:44544106EE2022-12-12 08:28:44..544106.:   .544106544106: 544106E/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu544107: : E:  ::: EE E/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu19261926E  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu :] ]  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu1926Device 4 init p2p of link 5Device 0 init p2p of link 3/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::1926:] 

:19261926] 1926Device 5 init p2p of link 61926] ] Device 2 init p2p of link 1] 
] Device 6 init p2p of link 0Device 1 init p2p of link 7
Device 7 init p2p of link 4Device 3 init p2p of link 2



[2022-12-12 08:28:44.[5450212022-12-12 08:28:44: .E545029 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE: 1980[[/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu] [[2022-12-12 08:28:442022-12-12 08:28:44[:[eager alloc mem 611.00 KB2022-12-12 08:28:442022-12-12 08:28:44..2022-12-12 08:28:4419802022-12-12 08:28:44
..545052545054.] .545060545060: : 545068eager alloc mem 611.00 KB545071: : EE: 
: EE  EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::19801980::19801980] ] 19801980] ] eager alloc mem 611.00 KBeager alloc mem 611.00 KB] ] eager alloc mem 611.00 KBeager alloc mem 611.00 KB

eager alloc mem 611.00 KBeager alloc mem 611.00 KB



[2022-12-12 08:28:44.546033: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 08:28:44.546101: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 08:28:44.546189: [[E2022-12-12 08:28:442022-12-12 08:28:44 ../hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc546201[546201:: 2022-12-12 08:28:44: 638E[.[E]  2022-12-12 08:28:445462232022-12-12 08:28:44 eager release cuda mem 625663/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc.: ./hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc
:546231E546234:638:  : 638] E/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccE] eager release cuda mem 625663 : eager release cuda mem 625663
/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc638/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc
:] :638eager release cuda mem 625663638] 
] eager release cuda mem 625663eager release cuda mem 625663

[2022-12-12 08:28:44.568281: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 6 init p2p of link 5
[2022-12-12 08:28:44.568438: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 08:28:44.568967: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 4 init p2p of link 7
[2022-12-12 08:28:44.569129: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 08:28:44.569182: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 2
[2022-12-12 08:28:44.569337: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 08:28:44.569373: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 08:28:44.569540: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 5 init p2p of link 4
[2022-12-12 08:28:44.569621: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 0 init p2p of link 6
[2022-12-12 08:28:44.569695: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 08:28:44.569788: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 08:28:44.570067: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 08:28:44.570265: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 08:28:44.[5705872022-12-12 08:28:44: .E570606 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE: 1926/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc] :Device 3 init p2p of link 0638
] eager release cuda mem 625663
[2022-12-12 08:28:44.570671: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[[2022-12-12 08:28:442022-12-12 08:28:44..570745570759: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::19261980] ] Device 7 init p2p of link 1eager alloc mem 611.00 KB

[2022-12-12 08:28:44.570922: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 08:28:44.570976: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 2 init p2p of link 3
[2022-12-12 08:28:44.571144: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 08:28:44.571688: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 08:28:44.571797: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 08:28:44.572049: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 08:28:44.590394: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 6 init p2p of link 4
[2022-12-12 08:28:44.590520: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 08:28:44.591474: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 08:28:44.591965: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 4 init p2p of link 2
[2022-12-12 08:28:44.592065: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926[] 2022-12-12 08:28:44Device 7 init p2p of link 6.
592097: E [/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu2022-12-12 08:28:44[:.2022-12-12 08:28:441980592115.] : 592126eager alloc mem 611.00 KBE: 
 E[/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu 2022-12-12 08:28:44:/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu.1926:592195] 1926: Device 1 init p2p of link 3] E
Device 0 init p2p of link 1 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 08:28:44.592343: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB[
2022-12-12 08:28:44.592363: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 08:28:44.592829: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 5 init p2p of link 7
[2022-12-12 08:28:44.592960: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 08:28:44.593020: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 3 init p2p of link 5
[2022-12-12 08:28:44.593059: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 08:28:44.593140: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:[19802022-12-12 08:28:44] .eager alloc mem 611.00 KB593156
: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 08:28:44.593251: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 08:28:44.593281: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 08:28:44.593314: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 2 init p2p of link 0
[2022-12-12 08:28:44.593429: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 08:28:44.593862: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 08:28:44.594033: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 08:28:44.594337: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 08:28:44.620567: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 6 init p2p of link 7
[2022-12-12 08:28:44.620684: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 08:28:44.620888: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 4 init p2p of link 6
[2022-12-12 08:28:44.621013: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 08:28:44.621602: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 08:28:44.621930: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 08:28:44.623440: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 0 init p2p of link 2
[[2022-12-12 08:28:442022-12-12 08:28:44.[.6235812022-12-12 08:28:44623574: .: E623589E :  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu: :1980/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu1926] [:] eager alloc mem 611.00 KB2022-12-12 08:28:441926Device 3 init p2p of link 1
.] 
623680Device 7 init p2p of link 5: 
E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 0
[2022-12-12 08:28:44.[6238122022-12-12 08:28:44: .E623818 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE: [1980/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu2022-12-12 08:28:44] :.eager alloc mem 611.00 KB1980623840
] : eager alloc mem 611.00 KBE
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 08:28:44.624295: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 5 init p2p of link 3
[2022-12-12 08:28:44.624423: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 08:28:44.624638: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[[2022-12-12 08:28:442022-12-12 08:28:44..624751624750: : EE [ /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc2022-12-12 08:28:44/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:.:638624776638] : ] eager release cuda mem 625663Eeager release cuda mem 625663
 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 08:28:44.625322: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 08:28:44.625930: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 2 init p2p of link 4
[2022-12-12 08:28:44.626048: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 08:28:44.626911: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 08:28:44.649518: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 120400004
[2022-12-12 08:28:44.649997: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 29993551 / 100000000 nodes ( 29.99 %~30.00 %) | remote 70006449 / 100000000 nodes ( 70.01 %) | cpu 0 / 100000000 nodes ( 0.00 %) | 14.31 GB | 4.80148 secs 
[2022-12-12 08:28:44.650515: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 120400004
[2022-12-12 08:28:44.650991: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 29997029 / 100000000 nodes ( 30.00 %~30.00 %) | remote 70002971 / 100000000 nodes ( 70.00 %) | cpu 0 / 100000000 nodes ( 0.00 %) | 14.31 GB | 4.80099 secs 
[2022-12-12 08:28:44.652009: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 120400004
[2022-12-12 08:28:44.653513: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 120400004
[2022-12-12 08:28:44.653918: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:[19552022-12-12 08:28:44] .Asymm Coll cache (policy: coll_cache_asymm_link) | local 29839921 / 100000000 nodes ( 29.84 %~30.00 %) | remote 70160079 / 100000000 nodes ( 70.16 %) | cpu 0 / 100000000 nodes ( 0.00 %) | 14.23 GB | 4.80512 secs 653931
: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 120400004
[2022-12-12 08:28:44.654146: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 120400004
[2022-12-12 08:28:44.654346: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 120400004
[2022-12-12 08:28:44.655148: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 120400004
[2022-12-12 08:28:44.655303: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 29943301 / 100000000 nodes ( 29.94 %~30.00 %) | remote 70056699 / 100000000 nodes ( 70.06 %) | cpu 0 / 100000000 nodes ( 0.00 %) | 14.28 GB | 4.80679 secs 
[2022-12-12 08:28:44.656096: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 29972565 / 100000000 nodes ( 29.97 %~30.00 %) | remote 70027435 / 100000000 nodes ( 70.03 %) | cpu 0 / 100000000 nodes ( 0.00 %) | 14.30 GB | 4.82323 secs 
[2022-12-12 08:28:44.656557: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 29986743 / 100000000 nodes ( 29.99 %~30.00 %) | remote 70013257 / 100000000 nodes ( 70.01 %) | cpu 0 / 100000000 nodes ( 0.00 %) | 14.30 GB | 4.80739 secs 
[2022-12-12 08:28:44.657337: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 29908916 / 100000000 nodes ( 29.91 %~30.00 %) | remote 70091084 / 100000000 nodes ( 70.09 %) | cpu 0 / 100000000 nodes ( 0.00 %) | 14.27 GB | 4.80919 secs 
[2022-12-12 08:28:44.657590: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 29991105 / 100000000 nodes ( 29.99 %~30.00 %) | remote 70008895 / 100000000 nodes ( 70.01 %) | cpu 0 / 100000000 nodes ( 0.00 %) | 14.31 GB | 4.80767 secs 
[2022-12-12 08:28:44.657636: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2213] before create ctx, mem is 22.62 GB
[2022-12-12 08:28:46. 38074: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2219] after create ctx, mem is 22.88 GB
[2022-12-12 08:28:46. 39414: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2226] after create stream, mem is 22.88 GB
[2022-12-12 08:28:46. 43596: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2213] before create ctx, mem is 22.88 GB
[2022-12-12 08:28:47.364262: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2219] after create ctx, mem is 23.14 GB
[2022-12-12 08:28:47.364382: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2226] after create stream, mem is 23.14 GB
[2022-12-12 08:28:47.366089: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2213] before create ctx, mem is 23.14 GB
[2022-12-12 08:28:48.691082: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2219] after create ctx, mem is 23.36 GB
[2022-12-12 08:28:48.692044: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2226] after create stream, mem is 23.36 GB
[2022-12-12 08:28:48.693108: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2213] before create ctx, mem is 23.36 GB
[2022-12-12 08:28:50. 43726: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2219] after create ctx, mem is 23.57 GB
[2022-12-12 08:28:50. 44049: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2226] after create stream, mem is 23.57 GB
[2022-12-12 08:28:50. 44662: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2243] before create stream, mem is 23.57 GB
[2022-12-12 08:28:50. 45164: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2249] after create stream, mem is 23.57 GB
[2022-12-12 08:28:50. 46789: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2213] before create ctx, mem is 23.57 GB
[2022-12-12 08:28:51.373049: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2219] after create ctx, mem is 23.77 GB
[2022-12-12 08:28:51.373875: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2226] after create stream, mem is 23.77 GB
[HCTR][08:28:52.536][ERROR][RK0][main]: replica 4 calling init per replica done, doing barrier
[HCTR][08:28:52.536][ERROR][RK0][tid #140264706406144]: replica 1 calling init per replica done, doing barrier
[HCTR][08:28:52.536][ERROR][RK0][tid #140265780147968]: replica 2 calling init per replica done, doing barrier
[HCTR][08:28:52.536][ERROR][RK0][main]: replica 0 calling init per replica done, doing barrier
[HCTR][08:28:52.536][ERROR][RK0][tid #140264706406144]: replica 6 calling init per replica done, doing barrier
[HCTR][08:28:52.536][ERROR][RK0][main]: replica 3 calling init per replica done, doing barrier
[HCTR][08:28:52.536][ERROR][RK0][main]: replica 5 calling init per replica done, doing barrier
[HCTR][08:28:52.536][ERROR][RK0][tid #140264706406144]: replica 7 calling init per replica done, doing barrier
[HCTR][08:28:52.536][ERROR][RK0][tid #140265780147968]: replica 2 calling init per replica done, doing barrier done
[HCTR][08:28:52.536][ERROR][RK0][main]: replica 4 calling init per replica done, doing barrier done
[HCTR][08:28:52.536][ERROR][RK0][tid #140264706406144]: replica 6 calling init per replica done, doing barrier done
[HCTR][08:28:52.536][ERROR][RK0][tid #140264706406144]: replica 1 calling init per replica done, doing barrier done
[HCTR][08:28:52.536][ERROR][RK0][main]: replica 0 calling init per replica done, doing barrier done
[HCTR][08:28:52.536][ERROR][RK0][main]: replica 5 calling init per replica done, doing barrier done
[HCTR][08:28:52.536][ERROR][RK0][tid #140264706406144]: replica 7 calling init per replica done, doing barrier done
[HCTR][08:28:52.536][ERROR][RK0][main]: replica 3 calling init per replica done, doing barrier done
[HCTR][08:28:52.537][ERROR][RK0][tid #140265780147968]: init per replica done
[HCTR][08:28:52.537][ERROR][RK0][main]: init per replica done
[HCTR][08:28:52.537][ERROR][RK0][tid #140264706406144]: init per replica done
[HCTR][08:28:52.537][ERROR][RK0][tid #140264706406144]: init per replica done
[HCTR][08:28:52.537][ERROR][RK0][main]: init per replica done
[HCTR][08:28:52.537][ERROR][RK0][tid #140264706406144]: init per replica done
[HCTR][08:28:52.537][ERROR][RK0][main]: init per replica done
[HCTR][08:28:52.540][ERROR][RK0][main]: init per replica done
[HCTR][08:28:52.575][ERROR][RK0][tid #140264706406144]: 7 allocated 3276800 at 0x7f7308238400
[HCTR][08:28:52.575][ERROR][RK0][tid #140264706406144]: 7 allocated 6553600 at 0x7f7308558400
[HCTR][08:28:52.575][ERROR][RK0][tid #140264706406144]: 7 allocated 3276800 at 0x7f7308b98400
[HCTR][08:28:52.575][ERROR][RK0][tid #140264706406144]: 7 allocated 6553600 at 0x7f7308eb8400
[HCTR][08:28:52.575][ERROR][RK0][main]: 4 allocated 3276800 at 0x7f7312238400
[HCTR][08:28:52.575][ERROR][RK0][main]: 4 allocated 6553600 at 0x7f7312558400
[HCTR][08:28:52.575][ERROR][RK0][main]: 4 allocated 3276800 at 0x7f7312b98400
[HCTR][08:28:52.575][ERROR][RK0][main]: 4 allocated 6553600 at 0x7f7312eb8400
[HCTR][08:28:52.575][ERROR][RK0][tid #140265780147968]: 2 allocated 3276800 at 0x7f71cc238400
[HCTR][08:28:52.575][ERROR][RK0][tid #140264706406144]: 1 allocated 3276800 at 0x7f7310238400
[HCTR][08:28:52.575][ERROR][RK0][tid #140265780147968]: 2 allocated 6553600 at 0x7f71cc558400
[HCTR][08:28:52.576][ERROR][RK0][tid #140265780147968]: 2 allocated 3276800 at 0x7f71ccb98400
[HCTR][08:28:52.575][ERROR][RK0][tid #140264706406144]: 1 allocated 6553600 at 0x7f7310558400
[HCTR][08:28:52.576][ERROR][RK0][tid #140265780147968]: 2 allocated 6553600 at 0x7f71cceb8400
[HCTR][08:28:52.576][ERROR][RK0][tid #140264706406144]: 1 allocated 3276800 at 0x7f7310b98400
[HCTR][08:28:52.576][ERROR][RK0][tid #140264706406144]: 1 allocated 6553600 at 0x7f7310eb8400
[HCTR][08:28:52.576][ERROR][RK0][main]: 5 allocated 3276800 at 0x7f72fc238400
[HCTR][08:28:52.576][ERROR][RK0][main]: 5 allocated 6553600 at 0x7f72fc558400
[HCTR][08:28:52.576][ERROR][RK0][main]: 5 allocated 3276800 at 0x7f72fcb98400
[HCTR][08:28:52.576][ERROR][RK0][tid #140264706406144]: 6 allocated 3276800 at 0x7f730c238400
[HCTR][08:28:52.576][ERROR][RK0][tid #140264698013440]: 3 allocated 3276800 at 0x7f7310238400
[HCTR][08:28:52.576][ERROR][RK0][main]: 5 allocated 6553600 at 0x7f72fceb8400
[HCTR][08:28:52.576][ERROR][RK0][tid #140264706406144]: 6 allocated 6553600 at 0x7f730c558400
[HCTR][08:28:52.576][ERROR][RK0][tid #140264698013440]: 3 allocated 6553600 at 0x7f7310558400
[HCTR][08:28:52.576][ERROR][RK0][tid #140264706406144]: 6 allocated 3276800 at 0x7f730cb98400
[HCTR][08:28:52.576][ERROR][RK0][tid #140264698013440]: 3 allocated 3276800 at 0x7f7310b98400
[HCTR][08:28:52.576][ERROR][RK0][tid #140264706406144]: 6 allocated 6553600 at 0x7f730ceb8400
[HCTR][08:28:52.576][ERROR][RK0][tid #140264698013440]: 3 allocated 6553600 at 0x7f7310eb8400
[HCTR][08:28:52.578][ERROR][RK0][tid #140264706406144]: 0 allocated 3276800 at 0x7f72a0320000
[HCTR][08:28:52.578][ERROR][RK0][tid #140264706406144]: 0 allocated 6553600 at 0x7f72a0640000
[HCTR][08:28:52.578][ERROR][RK0][tid #140264706406144]: 0 allocated 3276800 at 0x7f72a0c80000
[HCTR][08:28:52.578][ERROR][RK0][tid #140264706406144]: 0 allocated 6553600 at 0x7f72a0fa0000
