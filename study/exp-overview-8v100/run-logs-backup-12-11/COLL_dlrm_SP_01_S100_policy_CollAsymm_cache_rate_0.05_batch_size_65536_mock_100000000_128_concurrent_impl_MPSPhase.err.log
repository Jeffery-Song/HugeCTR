2022-12-12 01:27:27.274913: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.279190: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.286929: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.300514: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.306901: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.311637: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.316650: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.328859: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.379465: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.383627: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.386721: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.387731: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.388705: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.389713: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.390694: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.391649: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.392670: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.393964: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.395658: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.396637: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.396804: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.398381: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.398390: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.399836: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.399988: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.401255: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.401506: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.402789: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.403107: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.404722: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.405136: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.406135: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.406665: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.407997: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.408314: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.410277: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.411437: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.413579: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.415417: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.417796: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.421365: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.422598: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.423594: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.424360: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 01:27:27.424577: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.425743: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.427151: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.428288: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.429430: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.434159: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.435410: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.437228: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.437938: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.440155: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.440275: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.442888: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.443072: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.445590: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.445692: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.448429: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.448618: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.449521: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.451427: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.451646: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.453088: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.454595: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.455177: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.455405: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.457200: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.458433: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.458495: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.459229: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.459553: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.461163: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.462337: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.462874: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.463517: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.463802: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.465251: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.465807: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.466424: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.467229: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.474336: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.474482: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.475770: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.476793: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.477563: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.478600: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.478853: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.478908: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.480637: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.481522: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.482355: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.482719: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.482873: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.508242: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.511055: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.518560: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.519492: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.520247: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.520624: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.522569: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.522613: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.523608: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.524931: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.525109: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.525662: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.526950: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.527045: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.529175: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.529904: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.530001: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.530399: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.532196: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.532429: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.534439: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.534763: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.534866: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.536276: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.536569: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.538896: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.539372: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.540422: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.540653: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.542770: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.543296: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.544261: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.544405: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.545682: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.546284: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.547244: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.547332: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.548771: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.549017: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.550080: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.550215: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.551846: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.551984: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.553132: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.553517: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.555237: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.556240: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.557046: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.557573: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.558840: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.559878: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.560565: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.560737: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.562345: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.563655: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.564245: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.564296: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.565417: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.565739: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.567192: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.568092: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.568128: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.568979: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.570406: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.570670: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.571216: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.571260: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.572404: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.573818: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.573929: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.574463: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.574548: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.575763: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.577896: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.577966: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.577986: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.579862: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.580371: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.580408: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.582007: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.582074: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.582229: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.583723: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.583784: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.584397: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.584407: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.586238: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.586374: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.588318: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.588417: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.588845: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.588896: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.590836: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.590962: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.590966: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 01:27:27.592343: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.592500: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.593052: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.593100: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.594755: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.594955: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.596753: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.597322: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.597740: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.598564: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.600762: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.600925: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.600994: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.602001: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.602618: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.603071: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.603283: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.605271: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.605415: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.606429: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.607217: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.607327: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.607529: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.608961: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.609149: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.609802: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 01:27:27.610084: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.611422: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.611429: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.611609: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.613204: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.614955: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.616294: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.618278: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.618858: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.620009: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 01:27:27.620103: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 01:27:27.620366: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.621866: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.623269: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.623684: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.624546: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.625856: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.628499: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.628832: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.629875: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.630020: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.630202: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.631200: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.633198: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.635175: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.635332: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.635555: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.638035: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.638275: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.640853: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.641109: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.641310: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.645260: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.645426: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.648096: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.650824: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.651338: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.653737: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.656384: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.657459: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.658610: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.661394: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.691912: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.693427: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.698667: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.700749: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.703007: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.704966: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.707042: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.709711: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.710973: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.713842: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.745015: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.748565: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.750386: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 01:27:27.751919: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.758524: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.759770: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.761457: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.765727: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.766108: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.767571: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.780359: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 01:27:27.790156: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.805841: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.808157: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.811366: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.818462: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.819607: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 01:27:27.829058: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.879825: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:27.887517: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:28.793412: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:28.794051: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:28.794593: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:28.797983: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 01:27:28.798053: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-12 01:27:28.816793: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:28.817437: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:28.817964: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:28.819362: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:28.819907: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:28.820380: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-12 01:27:28.866281: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 01:27:28.866502: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 01:27:28.923851: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12340
2022-12-12 01:27:29.050723: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:29.052178: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:29.054483: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:29.056861: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 01:27:29.056929: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-12 01:27:29.071820: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:29.074315: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:29.075140: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:29.075756: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:29.076612: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:29.077228: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 01:27:29.077280: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-12 01:27:29.078111: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:29.079259: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:29.080251: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:29.081724: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:1/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-12 01:27:29.095405: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:29.096713: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:29.097670: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:29.098969: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:29.100021: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:29.101087: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:2/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-12 01:27:29.102174: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:29.104631: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:29.105688: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:29.106625: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 01:27:29.106677: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-12 01:27:29.120442: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:29.122839: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:29.123890: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:29.124891: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 01:27:29.124968: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-12 01:27:29.125597: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:29.126512: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:29.127285: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:29.128052: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:29.128565: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:29.129035: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:3/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-12 01:27:29.142912: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:29.143553: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:29.144279: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:29.144869: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:29.145386: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:29.145859: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:4/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-12 01:27:29.160924: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 01:27:29.161145: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 01:27:29.162960: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12341
2022-12-12 01:27:29.173621: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 01:27:29.173836: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 01:27:29.175646: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12343
2022-12-12 01:27:29.175886: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:29.177066: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:29.178235: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:29.179263: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 01:27:29.179321: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-12 01:27:29.188077: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:29.189655: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:29.190667: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:29.190944: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 01:27:29.191159: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 01:27:29.191596: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 01:27:29.191652: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-12 01:27:29.192899: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12344
2022-12-12 01:27:29.193635: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:29.194808: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:29.196068: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:29.196415: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 01:27:29.196578: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 01:27:29.197096: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 01:27:29.197151: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-12 01:27:29.198224: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12342
2022-12-12 01:27:29.198527: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:29.199557: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:29.200762: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:29.201971: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:29.203087: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:29.204172: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:6/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-12 01:27:29.209603: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:29.210684: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:29.211681: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:29.212751: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:29.213926: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:29.214819: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:29.214927: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:5/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-12 01:27:29.216449: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:29.217594: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:29.218866: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:29.220015: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:27:29.221035: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:7/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-12 01:27:29.250920: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 01:27:29.251125: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 01:27:29.252149: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12346
2022-12-12 01:27:29.258651: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 01:27:29.258826: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 01:27:29.260598: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12345
2022-12-12 01:27:29.265222: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 01:27:29.265377: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 01:27:29.266939: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12347
[HCTR][01:27:30.523][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][01:27:30.523][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][01:27:30.531][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][01:27:30.534][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][01:27:30.534][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][01:27:30.534][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][01:27:30.534][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][01:27:30.534][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 1it [00:01,  1.53s/it]warmup run: 1it [00:01,  1.55s/it]warmup run: 1it [00:01,  1.56s/it]warmup run: 1it [00:01,  1.57s/it]warmup run: 1it [00:01,  1.59s/it]warmup run: 102it [00:01, 86.78it/s]warmup run: 1it [00:01,  1.58s/it]warmup run: 1it [00:01,  1.60s/it]warmup run: 100it [00:01, 83.94it/s]warmup run: 1it [00:01,  1.48s/it]warmup run: 99it [00:01, 82.63it/s]warmup run: 96it [00:01, 79.57it/s]warmup run: 92it [00:01, 75.75it/s]warmup run: 201it [00:01, 184.81it/s]warmup run: 96it [00:01, 79.30it/s]warmup run: 101it [00:01, 82.48it/s]warmup run: 199it [00:01, 181.02it/s]warmup run: 93it [00:01, 81.60it/s]warmup run: 198it [00:01, 179.41it/s]warmup run: 188it [00:01, 168.73it/s]warmup run: 191it [00:01, 172.19it/s]warmup run: 192it [00:01, 172.44it/s]warmup run: 299it [00:01, 291.22it/s]warmup run: 201it [00:01, 178.50it/s]warmup run: 299it [00:01, 289.68it/s]warmup run: 187it [00:01, 177.47it/s]warmup run: 296it [00:01, 285.16it/s]warmup run: 284it [00:01, 272.52it/s]warmup run: 291it [00:01, 280.49it/s]warmup run: 397it [00:01, 401.77it/s]warmup run: 289it [00:01, 276.95it/s]warmup run: 303it [00:01, 287.69it/s]warmup run: 400it [00:01, 404.16it/s]warmup run: 395it [00:01, 397.13it/s]warmup run: 279it [00:01, 279.46it/s]warmup run: 377it [00:01, 375.99it/s]warmup run: 391it [00:01, 393.69it/s]warmup run: 494it [00:02, 507.01it/s]warmup run: 387it [00:01, 387.41it/s]warmup run: 405it [00:02, 401.82it/s]warmup run: 501it [00:02, 516.09it/s]warmup run: 496it [00:02, 509.91it/s]warmup run: 371it [00:01, 384.25it/s]warmup run: 470it [00:02, 477.34it/s]warmup run: 490it [00:02, 502.30it/s]warmup run: 475it [00:02, 476.38it/s]warmup run: 504it [00:02, 508.68it/s]warmup run: 586it [00:02, 578.43it/s]warmup run: 604it [00:02, 622.76it/s]warmup run: 597it [00:02, 613.36it/s]warmup run: 466it [00:01, 490.93it/s]warmup run: 564it [00:02, 573.12it/s]warmup run: 587it [00:02, 598.90it/s]warmup run: 605it [00:02, 612.12it/s]warmup run: 563it [00:02, 549.19it/s]warmup run: 675it [00:02, 641.26it/s]warmup run: 705it [00:02, 710.13it/s]warmup run: 696it [00:02, 699.70it/s]warmup run: 561it [00:02, 588.64it/s]warmup run: 658it [00:02, 656.49it/s]warmup run: 684it [00:02, 683.19it/s]warmup run: 706it [00:02, 702.56it/s]warmup run: 664it [00:02, 652.52it/s]warmup run: 777it [00:02, 732.40it/s]warmup run: 807it [00:02, 784.81it/s]warmup run: 658it [00:02, 676.93it/s]warmup run: 798it [00:02, 777.23it/s]warmup run: 752it [00:02, 726.19it/s]warmup run: 783it [00:02, 756.91it/s]warmup run: 808it [00:02, 779.11it/s]warmup run: 765it [00:02, 738.82it/s]warmup run: 878it [00:02, 802.40it/s]warmup run: 906it [00:02, 834.14it/s]warmup run: 899it [00:02, 837.77it/s]warmup run: 755it [00:02, 748.30it/s]warmup run: 845it [00:02, 778.80it/s]warmup run: 881it [00:02, 814.06it/s]warmup run: 910it [00:02, 839.55it/s]warmup run: 865it [00:02, 804.43it/s]warmup run: 980it [00:02, 859.16it/s]warmup run: 1005it [00:02, 871.90it/s]warmup run: 998it [00:02, 878.54it/s]warmup run: 851it [00:02, 802.82it/s]warmup run: 939it [00:02, 820.73it/s]warmup run: 978it [00:02, 855.64it/s]warmup run: 1012it [00:02, 886.92it/s]warmup run: 964it [00:02, 853.68it/s]warmup run: 1082it [00:02, 902.66it/s]warmup run: 1106it [00:02, 908.38it/s]warmup run: 1099it [00:02, 914.77it/s]warmup run: 945it [00:02, 837.94it/s]warmup run: 1033it [00:02, 852.35it/s]warmup run: 1075it [00:02, 887.34it/s]warmup run: 1113it [00:02, 909.41it/s]warmup run: 1064it [00:02, 891.86it/s]warmup run: 1184it [00:02, 933.41it/s]warmup run: 1207it [00:02, 936.14it/s]warmup run: 1200it [00:02, 939.71it/s]warmup run: 1039it [00:02, 861.53it/s]warmup run: 1126it [00:02, 872.16it/s]warmup run: 1172it [00:02, 909.63it/s]warmup run: 1164it [00:02, 920.61it/s]warmup run: 1213it [00:02, 928.09it/s]warmup run: 1285it [00:02, 955.20it/s]warmup run: 1309it [00:02, 960.13it/s]warmup run: 1302it [00:02, 961.85it/s]warmup run: 1133it [00:02, 872.86it/s]warmup run: 1219it [00:02, 888.72it/s]warmup run: 1269it [00:02, 924.70it/s]warmup run: 1263it [00:02, 938.67it/s]warmup run: 1313it [00:02, 946.30it/s]warmup run: 1385it [00:02, 965.64it/s]warmup run: 1410it [00:02, 971.18it/s]warmup run: 1403it [00:02, 970.62it/s]warmup run: 1226it [00:02, 887.47it/s]warmup run: 1312it [00:02, 893.25it/s]warmup run: 1366it [00:02, 937.16it/s]warmup run: 1362it [00:02, 951.66it/s]warmup run: 1416it [00:03, 969.79it/s]warmup run: 1485it [00:03, 961.49it/s]warmup run: 1512it [00:03, 985.16it/s]warmup run: 1503it [00:03, 977.91it/s]warmup run: 1322it [00:02, 907.12it/s]warmup run: 1404it [00:03, 895.36it/s]warmup run: 1464it [00:03, 948.03it/s]warmup run: 1461it [00:03, 962.70it/s]warmup run: 1517it [00:03, 981.02it/s]warmup run: 1584it [00:03, 961.35it/s]warmup run: 1613it [00:03, 989.10it/s]warmup run: 1603it [00:03, 981.12it/s]warmup run: 1418it [00:02, 921.90it/s]warmup run: 1497it [00:03, 903.72it/s]warmup run: 1563it [00:03, 959.54it/s]warmup run: 1620it [00:03, 994.16it/s]warmup run: 1560it [00:03, 966.84it/s]warmup run: 1682it [00:03, 958.46it/s]warmup run: 1714it [00:03, 991.46it/s]warmup run: 1704it [00:03, 987.17it/s]warmup run: 1515it [00:03, 935.87it/s]warmup run: 1589it [00:03, 906.42it/s]warmup run: 1661it [00:03, 963.38it/s]warmup run: 1723it [00:03, 1003.74it/s]warmup run: 1659it [00:03, 964.19it/s]warmup run: 1779it [00:03, 955.07it/s]warmup run: 1815it [00:03, 996.57it/s]warmup run: 1806it [00:03, 995.59it/s]warmup run: 1613it [00:03, 946.29it/s]warmup run: 1684it [00:03, 918.63it/s]warmup run: 1762it [00:03, 975.55it/s]warmup run: 1826it [00:03, 1009.63it/s]warmup run: 1758it [00:03, 969.39it/s]warmup run: 1876it [00:03, 950.40it/s]warmup run: 1916it [00:03, 999.23it/s]warmup run: 1907it [00:03, 997.78it/s]warmup run: 1711it [00:03, 953.41it/s]warmup run: 1786it [00:03, 948.10it/s]warmup run: 1865it [00:03, 990.39it/s]warmup run: 1928it [00:03, 1012.26it/s]warmup run: 1859it [00:03, 979.45it/s]warmup run: 1972it [00:03, 951.64it/s]warmup run: 2020it [00:03, 1010.57it/s]warmup run: 2009it [00:03, 1003.71it/s]warmup run: 1887it [00:03, 966.27it/s]warmup run: 1807it [00:03, 927.22it/s]warmup run: 1968it [00:03, 1000.09it/s]warmup run: 2036it [00:03, 1030.76it/s]warmup run: 1963it [00:03, 996.47it/s]warmup run: 2085it [00:03, 1003.04it/s]warmup run: 2139it [00:03, 1062.02it/s]warmup run: 2132it [00:03, 1069.42it/s]warmup run: 1986it [00:03, 971.30it/s]warmup run: 2082it [00:03, 1039.49it/s]warmup run: 1901it [00:03, 923.23it/s]warmup run: 2158it [00:03, 1084.56it/s]warmup run: 2079it [00:03, 1044.18it/s]warmup run: 2207it [00:03, 1064.77it/s]warmup run: 2258it [00:03, 1098.16it/s]warmup run: 2255it [00:03, 1115.85it/s]warmup run: 2103it [00:03, 1029.27it/s]warmup run: 2203it [00:03, 1089.67it/s]warmup run: 1994it [00:03, 919.39it/s]warmup run: 2280it [00:03, 1122.25it/s]warmup run: 2202it [00:03, 1096.97it/s]warmup run: 2329it [00:03, 1108.60it/s]warmup run: 2377it [00:03, 1124.75it/s]warmup run: 2378it [00:03, 1148.48it/s]warmup run: 2224it [00:03, 1082.88it/s]warmup run: 2324it [00:03, 1125.03it/s]warmup run: 2106it [00:03, 976.63it/s]warmup run: 2402it [00:03, 1150.02it/s]warmup run: 2325it [00:03, 1134.14it/s]warmup run: 2451it [00:03, 1140.45it/s]warmup run: 2496it [00:03, 1143.35it/s]warmup run: 2501it [00:03, 1171.75it/s]warmup run: 2347it [00:03, 1126.50it/s]warmup run: 2446it [00:03, 1151.30it/s]warmup run: 2220it [00:03, 1023.88it/s]warmup run: 2524it [00:04, 1169.14it/s]warmup run: 2449it [00:04, 1162.93it/s]warmup run: 2573it [00:04, 1163.33it/s]warmup run: 2615it [00:04, 1155.32it/s]warmup run: 2623it [00:04, 1185.36it/s]warmup run: 2470it [00:04, 1156.45it/s]warmup run: 2568it [00:04, 1169.78it/s]warmup run: 2334it [00:03, 1057.25it/s]warmup run: 2645it [00:04, 1178.64it/s]warmup run: 2573it [00:04, 1184.04it/s]warmup run: 2694it [00:04, 1174.67it/s]warmup run: 2733it [00:04, 1159.99it/s]warmup run: 2742it [00:04, 1185.03it/s]warmup run: 2594it [00:04, 1178.64it/s]warmup run: 2688it [00:04, 1178.15it/s]warmup run: 2448it [00:04, 1080.17it/s]warmup run: 2767it [00:04, 1188.35it/s]warmup run: 2697it [00:04, 1199.06it/s]warmup run: 2816it [00:04, 1186.03it/s]warmup run: 2852it [00:04, 1168.34it/s]warmup run: 2863it [00:04, 1189.90it/s]warmup run: 2717it [00:04, 1191.69it/s]warmup run: 2810it [00:04, 1188.11it/s]warmup run: 2562it [00:04, 1097.12it/s]warmup run: 2889it [00:04, 1195.97it/s]warmup run: 2819it [00:04, 1205.18it/s]warmup run: 2938it [00:04, 1194.17it/s]warmup run: 2971it [00:04, 1174.03it/s]warmup run: 2984it [00:04, 1193.31it/s]warmup run: 2841it [00:04, 1203.48it/s]warmup run: 2931it [00:04, 1192.13it/s]warmup run: 3000it [00:04, 681.42it/s] warmup run: 2673it [00:04, 1100.45it/s]warmup run: 3000it [00:04, 682.02it/s] warmup run: 3000it [00:04, 676.20it/s] warmup run: 3000it [00:04, 678.51it/s] warmup run: 2940it [00:04, 1203.22it/s]warmup run: 3000it [00:04, 671.97it/s] warmup run: 3000it [00:04, 671.64it/s] warmup run: 2962it [00:04, 1201.47it/s]warmup run: 2787it [00:04, 1109.99it/s]warmup run: 3000it [00:04, 662.00it/s] warmup run: 2900it [00:04, 1115.42it/s]warmup run: 3000it [00:04, 665.45it/s] 
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]






warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   6%|▌         | 165/3000 [00:00<00:01, 1649.33it/s]warmup should be done:   5%|▌         | 156/3000 [00:00<00:01, 1554.69it/s]warmup should be done:   6%|▌         | 167/3000 [00:00<00:01, 1665.62it/s]warmup should be done:   5%|▌         | 163/3000 [00:00<00:01, 1625.32it/s]warmup should be done:   5%|▌         | 161/3000 [00:00<00:01, 1603.53it/s]warmup should be done:   5%|▌         | 162/3000 [00:00<00:01, 1611.89it/s]warmup should be done:   6%|▌         | 165/3000 [00:00<00:01, 1641.47it/s]warmup should be done:   6%|▌         | 165/3000 [00:00<00:01, 1639.75it/s]warmup should be done:  10%|█         | 312/3000 [00:00<00:01, 1556.80it/s]warmup should be done:  11%|█         | 332/3000 [00:00<00:01, 1659.01it/s]warmup should be done:  11%|█         | 329/3000 [00:00<00:01, 1643.52it/s]warmup should be done:  11%|█         | 325/3000 [00:00<00:01, 1620.29it/s]warmup should be done:  11%|█         | 336/3000 [00:00<00:01, 1674.40it/s]warmup should be done:  11%|█         | 324/3000 [00:00<00:01, 1613.94it/s]warmup should be done:  11%|█         | 334/3000 [00:00<00:01, 1665.38it/s]warmup should be done:  11%|█         | 331/3000 [00:00<00:01, 1648.29it/s]warmup should be done:  17%|█▋        | 498/3000 [00:00<00:01, 1658.59it/s]warmup should be done:  16%|█▌        | 468/3000 [00:00<00:01, 1553.77it/s]warmup should be done:  16%|█▋        | 494/3000 [00:00<00:01, 1642.04it/s]warmup should be done:  17%|█▋        | 502/3000 [00:00<00:01, 1668.65it/s]warmup should be done:  17%|█▋        | 496/3000 [00:00<00:01, 1644.66it/s]warmup should be done:  16%|█▋        | 488/3000 [00:00<00:01, 1618.09it/s]warmup should be done:  16%|█▌        | 486/3000 [00:00<00:01, 1610.23it/s]warmup should be done:  17%|█▋        | 504/3000 [00:00<00:01, 1667.15it/s]warmup should be done:  22%|██▏       | 664/3000 [00:00<00:01, 1657.76it/s]warmup should be done:  22%|██▏       | 659/3000 [00:00<00:01, 1641.86it/s]warmup should be done:  22%|██▏       | 670/3000 [00:00<00:01, 1671.60it/s]warmup should be done:  22%|██▏       | 650/3000 [00:00<00:01, 1615.08it/s]warmup should be done:  22%|██▏       | 661/3000 [00:00<00:01, 1642.29it/s]warmup should be done:  22%|██▏       | 648/3000 [00:00<00:01, 1610.40it/s]warmup should be done:  21%|██        | 624/3000 [00:00<00:01, 1544.87it/s]warmup should be done:  22%|██▏       | 671/3000 [00:00<00:01, 1662.61it/s]warmup should be done:  28%|██▊       | 830/3000 [00:00<00:01, 1655.82it/s]warmup should be done:  28%|██▊       | 838/3000 [00:00<00:01, 1672.15it/s]warmup should be done:  27%|██▋       | 824/3000 [00:00<00:01, 1640.70it/s]warmup should be done:  27%|██▋       | 812/3000 [00:00<00:01, 1619.02it/s]warmup should be done:  27%|██▋       | 812/3000 [00:00<00:01, 1612.63it/s]warmup should be done:  28%|██▊       | 826/3000 [00:00<00:01, 1639.18it/s]warmup should be done:  26%|██▌       | 780/3000 [00:00<00:01, 1547.26it/s]warmup should be done:  28%|██▊       | 838/3000 [00:00<00:01, 1663.39it/s]warmup should be done:  33%|███▎      | 996/3000 [00:00<00:01, 1650.57it/s]warmup should be done:  32%|███▏      | 974/3000 [00:00<00:01, 1619.30it/s]warmup should be done:  34%|███▎      | 1006/3000 [00:00<00:01, 1666.90it/s]warmup should be done:  33%|███▎      | 989/3000 [00:00<00:01, 1635.29it/s]warmup should be done:  34%|███▎      | 1005/3000 [00:00<00:01, 1661.70it/s]warmup should be done:  32%|███▏      | 974/3000 [00:00<00:01, 1605.80it/s]warmup should be done:  33%|███▎      | 990/3000 [00:00<00:01, 1607.40it/s]warmup should be done:  31%|███       | 935/3000 [00:00<00:01, 1515.46it/s]warmup should be done:  39%|███▊      | 1162/3000 [00:00<00:01, 1651.28it/s]warmup should be done:  38%|███▊      | 1138/3000 [00:00<00:01, 1623.75it/s]warmup should be done:  38%|███▊      | 1153/3000 [00:00<00:01, 1635.71it/s]warmup should be done:  39%|███▉      | 1173/3000 [00:00<00:01, 1660.33it/s]warmup should be done:  38%|███▊      | 1135/3000 [00:00<00:01, 1605.55it/s]warmup should be done:  39%|███▉      | 1172/3000 [00:00<00:01, 1658.47it/s]warmup should be done:  38%|███▊      | 1154/3000 [00:00<00:01, 1617.50it/s]warmup should be done:  36%|███▌      | 1087/3000 [00:00<00:01, 1494.26it/s]warmup should be done:  43%|████▎     | 1302/3000 [00:00<00:01, 1626.11it/s]warmup should be done:  44%|████▍     | 1317/3000 [00:00<00:01, 1633.82it/s]warmup should be done:  44%|████▍     | 1328/3000 [00:00<00:01, 1643.21it/s]warmup should be done:  43%|████▎     | 1296/3000 [00:00<00:01, 1605.70it/s]warmup should be done:  45%|████▍     | 1340/3000 [00:00<00:01, 1655.24it/s]warmup should be done:  45%|████▍     | 1338/3000 [00:00<00:01, 1644.89it/s]warmup should be done:  44%|████▍     | 1316/3000 [00:00<00:01, 1596.66it/s]warmup should be done:  41%|████      | 1237/3000 [00:00<00:01, 1468.12it/s]warmup should be done:  49%|████▉     | 1466/3000 [00:00<00:00, 1627.57it/s]warmup should be done:  49%|████▉     | 1481/3000 [00:00<00:00, 1635.34it/s]warmup should be done:  49%|████▊     | 1457/3000 [00:00<00:00, 1605.60it/s]warmup should be done:  50%|████▉     | 1493/3000 [00:00<00:00, 1637.40it/s]warmup should be done:  50%|█████     | 1506/3000 [00:00<00:00, 1650.65it/s]warmup should be done:  50%|█████     | 1504/3000 [00:00<00:00, 1646.74it/s]warmup should be done:  49%|████▉     | 1476/3000 [00:00<00:00, 1567.25it/s]warmup should be done:  46%|████▌     | 1384/3000 [00:00<00:01, 1447.46it/s]warmup should be done:  54%|█████▍    | 1630/3000 [00:01<00:00, 1628.85it/s]warmup should be done:  55%|█████▍    | 1645/3000 [00:01<00:00, 1636.74it/s]warmup should be done:  54%|█████▍    | 1618/3000 [00:01<00:00, 1604.62it/s]warmup should be done:  55%|█████▌    | 1657/3000 [00:01<00:00, 1630.36it/s]warmup should be done:  56%|█████▌    | 1672/3000 [00:01<00:00, 1650.19it/s]warmup should be done:  56%|█████▌    | 1669/3000 [00:01<00:00, 1631.81it/s]warmup should be done:  54%|█████▍    | 1633/3000 [00:01<00:00, 1551.19it/s]warmup should be done:  51%|█████     | 1529/3000 [00:01<00:01, 1434.38it/s]warmup should be done:  60%|██████    | 1810/3000 [00:01<00:00, 1638.63it/s]warmup should be done:  60%|█████▉    | 1794/3000 [00:01<00:00, 1629.72it/s]warmup should be done:  59%|█████▉    | 1779/3000 [00:01<00:00, 1604.44it/s]warmup should be done:  61%|██████▏   | 1838/3000 [00:01<00:00, 1649.86it/s]warmup should be done:  61%|██████    | 1821/3000 [00:01<00:00, 1624.45it/s]warmup should be done:  61%|██████    | 1833/3000 [00:01<00:00, 1634.18it/s]warmup should be done:  60%|█████▉    | 1789/3000 [00:01<00:00, 1540.76it/s]warmup should be done:  56%|█████▌    | 1673/3000 [00:01<00:00, 1427.06it/s]warmup should be done:  65%|██████▌   | 1957/3000 [00:01<00:00, 1628.80it/s]warmup should be done:  66%|██████▌   | 1975/3000 [00:01<00:00, 1639.76it/s]warmup should be done:  65%|██████▍   | 1940/3000 [00:01<00:00, 1604.04it/s]warmup should be done:  67%|██████▋   | 2004/3000 [00:01<00:00, 1650.05it/s]warmup should be done:  66%|██████▌   | 1984/3000 [00:01<00:00, 1621.06it/s]warmup should be done:  67%|██████▋   | 1998/3000 [00:01<00:00, 1636.68it/s]warmup should be done:  65%|██████▍   | 1944/3000 [00:01<00:00, 1533.84it/s]warmup should be done:  61%|██████    | 1816/3000 [00:01<00:00, 1422.14it/s]warmup should be done:  71%|███████▏  | 2140/3000 [00:01<00:00, 1639.95it/s]warmup should be done:  71%|███████   | 2120/3000 [00:01<00:00, 1622.68it/s]warmup should be done:  70%|███████   | 2101/3000 [00:01<00:00, 1604.35it/s]warmup should be done:  72%|███████▏  | 2170/3000 [00:01<00:00, 1649.48it/s]warmup should be done:  72%|███████▏  | 2147/3000 [00:01<00:00, 1618.72it/s]warmup should be done:  72%|███████▏  | 2162/3000 [00:01<00:00, 1636.79it/s]warmup should be done:  70%|██████▉   | 2098/3000 [00:01<00:00, 1531.68it/s]warmup should be done:  65%|██████▌   | 1959/3000 [00:01<00:00, 1421.25it/s]warmup should be done:  77%|███████▋  | 2304/3000 [00:01<00:00, 1636.10it/s]warmup should be done:  76%|███████▌  | 2283/3000 [00:01<00:00, 1622.88it/s]warmup should be done:  75%|███████▌  | 2262/3000 [00:01<00:00, 1602.46it/s]warmup should be done:  78%|███████▊  | 2335/3000 [00:01<00:00, 1645.17it/s]warmup should be done:  77%|███████▋  | 2309/3000 [00:01<00:00, 1615.54it/s]warmup should be done:  78%|███████▊  | 2326/3000 [00:01<00:00, 1635.72it/s]warmup should be done:  75%|███████▌  | 2254/3000 [00:01<00:00, 1539.07it/s]warmup should be done:  70%|███████   | 2106/3000 [00:01<00:00, 1433.45it/s]warmup should be done:  82%|████████▏ | 2446/3000 [00:01<00:00, 1624.58it/s]warmup should be done:  82%|████████▏ | 2469/3000 [00:01<00:00, 1637.72it/s]warmup should be done:  81%|████████  | 2423/3000 [00:01<00:00, 1602.83it/s]warmup should be done:  83%|████████▎ | 2500/3000 [00:01<00:00, 1638.71it/s]warmup should be done:  82%|████████▏ | 2471/3000 [00:01<00:00, 1614.48it/s]warmup should be done:  83%|████████▎ | 2490/3000 [00:01<00:00, 1628.03it/s]warmup should be done:  80%|████████  | 2408/3000 [00:01<00:00, 1533.09it/s]warmup should be done:  75%|███████▌  | 2250/3000 [00:01<00:00, 1422.19it/s]warmup should be done:  87%|████████▋ | 2609/3000 [00:01<00:00, 1624.06it/s]warmup should be done:  88%|████████▊ | 2634/3000 [00:01<00:00, 1639.79it/s]warmup should be done:  86%|████████▌ | 2584/3000 [00:01<00:00, 1602.77it/s]warmup should be done:  88%|████████▊ | 2633/3000 [00:01<00:00, 1614.43it/s]warmup should be done:  89%|████████▉ | 2664/3000 [00:01<00:00, 1635.84it/s]warmup should be done:  88%|████████▊ | 2653/3000 [00:01<00:00, 1627.93it/s]warmup should be done:  85%|████████▌ | 2562/3000 [00:01<00:00, 1527.94it/s]warmup should be done:  80%|███████▉  | 2393/3000 [00:01<00:00, 1417.15it/s]warmup should be done:  93%|█████████▎| 2799/3000 [00:01<00:00, 1641.58it/s]warmup should be done:  92%|█████████▏| 2773/3000 [00:01<00:00, 1626.11it/s]warmup should be done:  92%|█████████▏| 2745/3000 [00:01<00:00, 1602.57it/s]warmup should be done:  93%|█████████▎| 2795/3000 [00:01<00:00, 1612.93it/s]warmup should be done:  94%|█████████▍| 2828/3000 [00:01<00:00, 1631.81it/s]warmup should be done:  94%|█████████▍| 2818/3000 [00:01<00:00, 1632.72it/s]warmup should be done:  91%|█████████ | 2718/3000 [00:01<00:00, 1536.13it/s]warmup should be done:  85%|████████▍ | 2543/3000 [00:01<00:00, 1439.63it/s]warmup should be done:  99%|█████████▉| 2965/3000 [00:01<00:00, 1646.34it/s]warmup should be done:  98%|█████████▊| 2936/3000 [00:01<00:00, 1627.13it/s]warmup should be done:  97%|█████████▋| 2907/3000 [00:01<00:00, 1606.02it/s]warmup should be done:  99%|█████████▊| 2958/3000 [00:01<00:00, 1617.99it/s]warmup should be done: 100%|█████████▉| 2992/3000 [00:01<00:00, 1629.73it/s]warmup should be done: 100%|█████████▉| 2986/3000 [00:01<00:00, 1644.04it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1647.78it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1644.55it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1639.65it/s]warmup should be done:  96%|█████████▌| 2878/3000 [00:01<00:00, 1554.26it/s]warmup should be done:  90%|████████▉ | 2694/3000 [00:01<00:00, 1459.49it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1630.42it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1623.50it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1606.83it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1571.26it/s]warmup should be done:  95%|█████████▍| 2846/3000 [00:01<00:00, 1476.99it/s]warmup should be done: 100%|█████████▉| 2999/3000 [00:02<00:00, 1491.60it/s]warmup should be done: 100%|██████████| 3000/3000 [00:02<00:00, 1470.24it/s]


warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]

warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]

warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   5%|▌         | 155/3000 [00:00<00:01, 1549.45it/s]warmup should be done:   6%|▌         | 168/3000 [00:00<00:01, 1677.51it/s]warmup should be done:   6%|▌         | 165/3000 [00:00<00:01, 1645.53it/s]warmup should be done:   6%|▌         | 171/3000 [00:00<00:01, 1701.91it/s]warmup should be done:   5%|▌         | 163/3000 [00:00<00:01, 1624.95it/s]warmup should be done:   6%|▌         | 165/3000 [00:00<00:01, 1640.95it/s]warmup should be done:   5%|▌         | 164/3000 [00:00<00:01, 1632.13it/s]warmup should be done:   6%|▌         | 166/3000 [00:00<00:01, 1650.03it/s]warmup should be done:  10%|█         | 312/3000 [00:00<00:01, 1556.14it/s]warmup should be done:  11%|█         | 328/3000 [00:00<00:01, 1638.85it/s]warmup should be done:  11%|█         | 332/3000 [00:00<00:01, 1656.83it/s]warmup should be done:  11%|█▏        | 338/3000 [00:00<00:01, 1685.30it/s]warmup should be done:  11%|█         | 331/3000 [00:00<00:01, 1649.64it/s]warmup should be done:  11%|█▏        | 343/3000 [00:00<00:01, 1708.25it/s]warmup should be done:  11%|█         | 330/3000 [00:00<00:01, 1644.72it/s]warmup should be done:  11%|█         | 334/3000 [00:00<00:01, 1662.52it/s]warmup should be done:  16%|█▋        | 495/3000 [00:00<00:01, 1650.20it/s]warmup should be done:  16%|█▌        | 469/3000 [00:00<00:01, 1559.92it/s]warmup should be done:  17%|█▋        | 497/3000 [00:00<00:01, 1652.74it/s]warmup should be done:  17%|█▋        | 497/3000 [00:00<00:01, 1655.70it/s]warmup should be done:  17%|█▋        | 500/3000 [00:00<00:01, 1663.03it/s]warmup should be done:  17%|█▋        | 508/3000 [00:00<00:01, 1687.68it/s]warmup should be done:  17%|█▋        | 515/3000 [00:00<00:01, 1709.91it/s]warmup should be done:  17%|█▋        | 505/3000 [00:00<00:01, 1680.60it/s]warmup should be done:  22%|██▏       | 661/3000 [00:00<00:01, 1653.34it/s]warmup should be done:  21%|██        | 625/3000 [00:00<00:01, 1558.35it/s]warmup should be done:  23%|██▎       | 677/3000 [00:00<00:01, 1688.05it/s]warmup should be done:  22%|██▏       | 664/3000 [00:00<00:01, 1659.61it/s]warmup should be done:  22%|██▏       | 663/3000 [00:00<00:01, 1652.22it/s]warmup should be done:  22%|██▎       | 675/3000 [00:00<00:01, 1688.05it/s]warmup should be done:  22%|██▏       | 667/3000 [00:00<00:01, 1662.23it/s]warmup should be done:  23%|██▎       | 686/3000 [00:00<00:01, 1704.54it/s]warmup should be done:  28%|██▊       | 827/3000 [00:00<00:01, 1653.78it/s]warmup should be done:  26%|██▌       | 781/3000 [00:00<00:01, 1556.85it/s]warmup should be done:  28%|██▊       | 831/3000 [00:00<00:01, 1661.33it/s]warmup should be done:  28%|██▊       | 834/3000 [00:00<00:01, 1661.79it/s]warmup should be done:  28%|██▊       | 829/3000 [00:00<00:01, 1650.44it/s]warmup should be done:  28%|██▊       | 844/3000 [00:00<00:01, 1684.23it/s]warmup should be done:  28%|██▊       | 846/3000 [00:00<00:01, 1676.34it/s]warmup should be done:  29%|██▊       | 857/3000 [00:00<00:01, 1692.21it/s]warmup should be done:  33%|███▎      | 998/3000 [00:00<00:01, 1663.38it/s]warmup should be done:  33%|███▎      | 994/3000 [00:00<00:01, 1655.96it/s]warmup should be done:  31%|███▏      | 938/3000 [00:00<00:01, 1558.18it/s]warmup should be done:  34%|███▍      | 1015/3000 [00:00<00:01, 1690.20it/s]warmup should be done:  34%|███▍      | 1014/3000 [00:00<00:01, 1668.05it/s]warmup should be done:  33%|███▎      | 1001/3000 [00:00<00:01, 1647.77it/s]warmup should be done:  34%|███▍      | 1027/3000 [00:00<00:01, 1681.61it/s]warmup should be done:  33%|███▎      | 995/3000 [00:00<00:01, 1615.58it/s]warmup should be done:  39%|███▊      | 1161/3000 [00:00<00:01, 1659.22it/s]warmup should be done:  39%|███▉      | 1166/3000 [00:00<00:01, 1666.01it/s]warmup should be done:  36%|███▋      | 1095/3000 [00:00<00:01, 1558.88it/s]warmup should be done:  40%|███▉      | 1185/3000 [00:00<00:01, 1685.50it/s]warmup should be done:  39%|███▉      | 1181/3000 [00:00<00:01, 1664.90it/s]warmup should be done:  39%|███▉      | 1166/3000 [00:00<00:01, 1637.28it/s]warmup should be done:  40%|███▉      | 1196/3000 [00:00<00:01, 1673.06it/s]warmup should be done:  39%|███▊      | 1157/3000 [00:00<00:01, 1604.94it/s]warmup should be done:  44%|████▍     | 1327/3000 [00:00<00:01, 1654.96it/s]warmup should be done:  44%|████▍     | 1333/3000 [00:00<00:01, 1663.13it/s]warmup should be done:  42%|████▏     | 1251/3000 [00:00<00:01, 1547.44it/s]warmup should be done:  45%|████▍     | 1349/3000 [00:00<00:00, 1668.15it/s]warmup should be done:  45%|████▌     | 1354/3000 [00:00<00:00, 1677.70it/s]warmup should be done:  44%|████▍     | 1335/3000 [00:00<00:01, 1651.87it/s]warmup should be done:  45%|████▌     | 1364/3000 [00:00<00:00, 1673.17it/s]warmup should be done:  44%|████▍     | 1320/3000 [00:00<00:01, 1610.43it/s]warmup should be done:  50%|████▉     | 1496/3000 [00:00<00:00, 1663.70it/s]warmup should be done:  50%|█████     | 1500/3000 [00:00<00:00, 1663.15it/s]warmup should be done:  47%|████▋     | 1409/3000 [00:00<00:01, 1556.33it/s]warmup should be done:  51%|█████     | 1517/3000 [00:00<00:00, 1669.38it/s]warmup should be done:  51%|█████     | 1522/3000 [00:00<00:00, 1671.05it/s]warmup should be done:  50%|█████     | 1504/3000 [00:00<00:00, 1661.95it/s]warmup should be done:  51%|█████     | 1532/3000 [00:00<00:00, 1670.29it/s]warmup should be done:  50%|████▉     | 1486/3000 [00:00<00:00, 1623.44it/s]warmup should be done:  56%|█████▌    | 1668/3000 [00:01<00:00, 1666.45it/s]warmup should be done:  55%|█████▌    | 1663/3000 [00:01<00:00, 1656.79it/s]warmup should be done:  52%|█████▏    | 1566/3000 [00:01<00:00, 1559.60it/s]warmup should be done:  56%|█████▌    | 1685/3000 [00:01<00:00, 1670.36it/s]warmup should be done:  56%|█████▌    | 1673/3000 [00:01<00:00, 1669.35it/s]warmup should be done:  56%|█████▋    | 1690/3000 [00:01<00:00, 1668.83it/s]warmup should be done:  57%|█████▋    | 1700/3000 [00:01<00:00, 1667.88it/s]warmup should be done:  55%|█████▌    | 1652/3000 [00:01<00:00, 1632.05it/s]warmup should be done:  61%|██████    | 1836/3000 [00:01<00:00, 1669.40it/s]warmup should be done:  61%|██████    | 1833/3000 [00:01<00:00, 1668.18it/s]warmup should be done:  57%|█████▊    | 1725/3000 [00:01<00:00, 1566.83it/s]warmup should be done:  62%|██████▏   | 1853/3000 [00:01<00:00, 1671.91it/s]warmup should be done:  61%|██████▏   | 1842/3000 [00:01<00:00, 1675.07it/s]warmup should be done:  62%|██████▏   | 1859/3000 [00:01<00:00, 1674.27it/s]warmup should be done:  62%|██████▏   | 1867/3000 [00:01<00:00, 1667.32it/s]warmup should be done:  61%|██████    | 1817/3000 [00:01<00:00, 1636.97it/s]warmup should be done:  67%|██████▋   | 2005/3000 [00:01<00:00, 1673.86it/s]warmup should be done:  67%|██████▋   | 2001/3000 [00:01<00:00, 1671.11it/s]warmup should be done:  67%|██████▋   | 2021/3000 [00:01<00:00, 1670.27it/s]warmup should be done:  63%|██████▎   | 1882/3000 [00:01<00:00, 1557.54it/s]warmup should be done:  67%|██████▋   | 2011/3000 [00:01<00:00, 1678.03it/s]warmup should be done:  68%|██████▊   | 2028/3000 [00:01<00:00, 1678.23it/s]warmup should be done:  68%|██████▊   | 2034/3000 [00:01<00:00, 1667.36it/s]warmup should be done:  66%|██████▌   | 1982/3000 [00:01<00:00, 1640.23it/s]warmup should be done:  72%|███████▏  | 2174/3000 [00:01<00:00, 1677.94it/s]warmup should be done:  72%|███████▏  | 2170/3000 [00:01<00:00, 1676.34it/s]warmup should be done:  68%|██████▊   | 2040/3000 [00:01<00:00, 1562.78it/s]warmup should be done:  73%|███████▎  | 2189/3000 [00:01<00:00, 1669.08it/s]warmup should be done:  73%|███████▎  | 2197/3000 [00:01<00:00, 1680.12it/s]warmup should be done:  73%|███████▎  | 2180/3000 [00:01<00:00, 1679.68it/s]warmup should be done:  73%|███████▎  | 2201/3000 [00:01<00:00, 1665.58it/s]warmup should be done:  72%|███████▏  | 2148/3000 [00:01<00:00, 1643.21it/s]warmup should be done:  78%|███████▊  | 2344/3000 [00:01<00:00, 1682.68it/s]warmup should be done:  78%|███████▊  | 2339/3000 [00:01<00:00, 1678.74it/s]warmup should be done:  79%|███████▊  | 2356/3000 [00:01<00:00, 1667.89it/s]warmup should be done:  78%|███████▊  | 2349/3000 [00:01<00:00, 1681.41it/s]warmup should be done:  73%|███████▎  | 2197/3000 [00:01<00:00, 1559.73it/s]warmup should be done:  79%|███████▉  | 2366/3000 [00:01<00:00, 1677.91it/s]warmup should be done:  79%|███████▉  | 2368/3000 [00:01<00:00, 1664.47it/s]warmup should be done:  77%|███████▋  | 2314/3000 [00:01<00:00, 1647.38it/s]warmup should be done:  84%|████████▍ | 2514/3000 [00:01<00:00, 1685.95it/s]warmup should be done:  84%|████████▎ | 2508/3000 [00:01<00:00, 1681.27it/s]warmup should be done:  84%|████████▍ | 2524/3000 [00:01<00:00, 1670.14it/s]warmup should be done:  78%|███████▊  | 2354/3000 [00:01<00:00, 1562.40it/s]warmup should be done:  84%|████████▍ | 2534/3000 [00:01<00:00, 1678.37it/s]warmup should be done:  84%|████████▍ | 2518/3000 [00:01<00:00, 1679.72it/s]warmup should be done:  84%|████████▍ | 2535/3000 [00:01<00:00, 1665.55it/s]warmup should be done:  83%|████████▎ | 2480/3000 [00:01<00:00, 1649.41it/s]warmup should be done:  89%|████████▉ | 2684/3000 [00:01<00:00, 1688.29it/s]warmup should be done:  89%|████████▉ | 2677/3000 [00:01<00:00, 1678.51it/s]warmup should be done:  84%|████████▍ | 2514/3000 [00:01<00:00, 1572.33it/s]warmup should be done:  90%|█████████ | 2702/3000 [00:01<00:00, 1678.61it/s]warmup should be done:  90%|████████▉ | 2692/3000 [00:01<00:00, 1666.46it/s]warmup should be done:  90%|████████▉ | 2686/3000 [00:01<00:00, 1671.23it/s]warmup should be done:  90%|█████████ | 2703/3000 [00:01<00:00, 1668.27it/s]warmup should be done:  88%|████████▊ | 2645/3000 [00:01<00:00, 1648.08it/s]warmup should be done:  95%|█████████▌| 2854/3000 [00:01<00:00, 1689.65it/s]warmup should be done:  95%|█████████▍| 2846/3000 [00:01<00:00, 1680.50it/s]warmup should be done:  89%|████████▉ | 2673/3000 [00:01<00:00, 1577.54it/s]warmup should be done:  96%|█████████▌| 2870/3000 [00:01<00:00, 1677.79it/s]warmup should be done:  95%|█████████▌| 2859/3000 [00:01<00:00, 1665.72it/s]warmup should be done:  96%|█████████▌| 2872/3000 [00:01<00:00, 1674.32it/s]warmup should be done:  95%|█████████▌| 2854/3000 [00:01<00:00, 1663.69it/s]warmup should be done:  94%|█████████▎| 2811/3000 [00:01<00:00, 1649.74it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1677.90it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1676.74it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1673.94it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1670.89it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1667.92it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1664.69it/s]warmup should be done:  94%|█████████▍| 2833/3000 [00:01<00:00, 1581.91it/s]warmup should be done:  99%|█████████▉| 2977/3000 [00:01<00:00, 1652.48it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1639.87it/s]warmup should be done: 100%|█████████▉| 2992/3000 [00:01<00:00, 1571.74it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1563.47it/s]2022-12-12 01:29:06.004434: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fd9e782f110 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 01:29:06.004496: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 01:29:07.438523: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fbe1002cc60 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 01:29:07.438588: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 01:29:07.679213: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fbdcc028440 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 01:29:07.679277: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 01:29:07.696566: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fbdd0028470 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 01:29:07.696630: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 01:29:07.872854: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fd9e38330d0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 01:29:07.872916: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 01:29:07.888712: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fd9df8325f0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 01:29:07.888790: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 01:29:07.888944: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fd9d3796ac0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 01:29:07.889003: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 01:29:07.911426: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fbd1802cd00 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 01:29:07.911509: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 01:29:08.243716: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 01:29:09.684710: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 01:29:09.923175: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 01:29:09.945479: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 01:29:10.177906: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 01:29:10.196809: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 01:29:10.203675: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 01:29:10.212756: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 01:29:11.102666: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 01:29:12.509600: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 01:29:12.745937: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 01:29:12.790999: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 01:29:13.066020: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 01:29:13.080446: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 01:29:13.115754: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 01:29:13.145990: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
[HCTR][01:29:46.115][ERROR][RK0][main]: replica 4 reaches 1000, calling init pre replica
[HCTR][01:29:46.115][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][01:29:46.122][ERROR][RK0][main]: coll ps creation done
[HCTR][01:29:46.122][ERROR][RK0][main]: replica 4 waits for coll ps creation barrier
[HCTR][01:29:46.144][ERROR][RK0][main]: replica 5 reaches 1000, calling init pre replica
[HCTR][01:29:46.144][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][01:29:46.149][ERROR][RK0][main]: coll ps creation done
[HCTR][01:29:46.149][ERROR][RK0][main]: replica 5 waits for coll ps creation barrier
[HCTR][01:29:46.258][ERROR][RK0][main]: replica 6 reaches 1000, calling init pre replica
[HCTR][01:29:46.258][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][01:29:46.263][ERROR][RK0][main]: coll ps creation done
[HCTR][01:29:46.263][ERROR][RK0][main]: replica 6 waits for coll ps creation barrier
[HCTR][01:29:46.265][ERROR][RK0][main]: replica 7 reaches 1000, calling init pre replica
[HCTR][01:29:46.265][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][01:29:46.270][ERROR][RK0][main]: coll ps creation done
[HCTR][01:29:46.270][ERROR][RK0][main]: replica 7 waits for coll ps creation barrier
[HCTR][01:29:46.309][ERROR][RK0][main]: replica 3 reaches 1000, calling init pre replica
[HCTR][01:29:46.309][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][01:29:46.310][ERROR][RK0][main]: replica 2 reaches 1000, calling init pre replica
[HCTR][01:29:46.310][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][01:29:46.317][ERROR][RK0][main]: coll ps creation done
[HCTR][01:29:46.317][ERROR][RK0][main]: replica 2 waits for coll ps creation barrier
[HCTR][01:29:46.318][ERROR][RK0][main]: coll ps creation done
[HCTR][01:29:46.318][ERROR][RK0][main]: replica 3 waits for coll ps creation barrier
[HCTR][01:29:46.388][ERROR][RK0][main]: replica 1 reaches 1000, calling init pre replica
[HCTR][01:29:46.388][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][01:29:46.395][ERROR][RK0][main]: coll ps creation done
[HCTR][01:29:46.395][ERROR][RK0][main]: replica 1 waits for coll ps creation barrier
[HCTR][01:29:46.412][ERROR][RK0][tid #140574522853120]: replica 0 reaches 1000, calling init pre replica
[HCTR][01:29:46.412][ERROR][RK0][tid #140574522853120]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][01:29:46.417][ERROR][RK0][tid #140574522853120]: coll ps creation done
[HCTR][01:29:46.417][ERROR][RK0][tid #140574522853120]: replica 0 waits for coll ps creation barrier
[HCTR][01:29:46.417][ERROR][RK0][tid #140574522853120]: replica 0 preparing frequency
[HCTR][01:29:47.341][ERROR][RK0][tid #140574522853120]: replica 0 preparing frequency done
[HCTR][01:29:47.372][ERROR][RK0][tid #140574522853120]: replica 0 calling init per replica
[HCTR][01:29:47.373][ERROR][RK0][main]: replica 7 calling init per replica
[HCTR][01:29:47.373][ERROR][RK0][main]: replica 6 calling init per replica
[HCTR][01:29:47.373][ERROR][RK0][main]: replica 2 calling init per replica
[HCTR][01:29:47.373][ERROR][RK0][main]: replica 4 calling init per replica
[HCTR][01:29:47.373][ERROR][RK0][main]: replica 3 calling init per replica
[HCTR][01:29:47.373][ERROR][RK0][main]: replica 5 calling init per replica
[HCTR][01:29:47.373][ERROR][RK0][main]: replica 1 calling init per replica
[HCTR][01:29:47.373][ERROR][RK0][tid #140574522853120]: Calling build_v2
[HCTR][01:29:47.373][ERROR][RK0][main]: Calling build_v2
[HCTR][01:29:47.373][ERROR][RK0][main]: Calling build_v2
[HCTR][01:29:47.373][ERROR][RK0][main]: Calling build_v2
[HCTR][01:29:47.373][ERROR][RK0][main]: Calling build_v2
[HCTR][01:29:47.373][ERROR][RK0][main]: Calling build_v2
[HCTR][01:29:47.373][ERROR][RK0][main]: Calling build_v2
[HCTR][01:29:47.373][ERROR][RK0][tid #140574522853120]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][01:29:47.373][ERROR][RK0][main]: Calling build_v2
[HCTR][01:29:47.373][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][01:29:47.373][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][01:29:47.373][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][01:29:47.373][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][01:29:47.373][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][01:29:47.373][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][01:29:47.373][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[[[[[[[[2022-12-12 01:29:472022-12-12 01:29:472022-12-12 01:29:472022-12-12 01:29:472022-12-12 01:29:472022-12-12 01:29:47.2022-12-12 01:29:47.....373197.3731973731972022-12-12 01:29:47373197373198373197: 373197: : .: : : E: EE373201EEE E  :    /hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc /hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.ccE/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc:/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc:: :::136:136136/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc136136136] 136] ] :] ] ] using concurrent impl MPSPhase] using concurrent impl MPSPhaseusing concurrent impl MPSPhase136using concurrent impl MPSPhaseusing concurrent impl MPSPhaseusing concurrent impl MPSPhase
using concurrent impl MPSPhase

] 



using concurrent impl MPSPhase
[2022-12-12 01:29:47.377443: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:178] v100x8, slow pcie
[2022-12-12 01:29:47.377482: E [/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 01:29:47:.196377488] : assigning 8 to cpuE
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:178] v100x8, slow pcie
[2022-12-12 01:29:47[.2022-12-12 01:29:47377536.: 377533E:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc [:/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 01:29:47196:.] 178377562assigning 8 to cpu] [: 
v100x8, slow pcie2022-12-12 01:29:47E
. 377578/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[: :2022-12-12 01:29:47E212. ] 377608/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccbuild asymm link desc with 8X Tesla V100-SXM2-32GB out of 8: :
[E178[2022-12-12 01:29:47 ] 2022-12-12 01:29:47./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccv100x8, slow pcie.[377625:
3776362022-12-12 01:29:47: 196: .E[[] E377653 2022-12-12 01:29:472022-12-12 01:29:47assigning 8 to cpu : /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc..
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE:377679377676: 178: : 212/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] EE[] :v100x8, slow pcie  2022-12-12 01:29:47[build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8213
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.2022-12-12 01:29:47
] [::[377727.remote time is 8.684212022-12-12 01:29:471961782022-12-12 01:29:47[: 377749
.] ] .[2022-12-12 01:29:47E: 377771assigning 8 to cpuv100x8, slow pcie3777822022-12-12 01:29:47. E: 

: .377802/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc EE377837[: :/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc  : 2022-12-12 01:29:47E178[:/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE. ] 2022-12-12 01:29:47212:: 377881/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccv100x8, slow pcie.] 178196/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: :
377906build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8] ] :E213: 
v100x8, slow pcie[assigning 8 to cpu214 ] E
2022-12-12 01:29:47
[] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccremote time is 8.68421 .[2022-12-12 01:29:47cpu time is 97.0588:
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc3779882022-12-12 01:29:47.
196:: [.378010] 212[E2022-12-12 01:29:47378026: assigning 8 to cpu] 2022-12-12 01:29:47 .: E
build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc378060E 
378072::  /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: 196E[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[:E]  2022-12-12 01:29:47:2022-12-12 01:29:47213 assigning 8 to cpu/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.196.] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
:378149] 378150remote time is 8.68421:214: assigning 8 to cpu: 
212] E
E] cpu time is 97.0588 [ build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 01:29:47/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
:.:[213378249212[2022-12-12 01:29:47] : ] [2022-12-12 01:29:47.remote time is 8.68421Ebuild asymm link desc with 8X Tesla V100-SXM2-32GB out of 82022-12-12 01:29:47.378273
 
.378284: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[378293: E[:2022-12-12 01:29:47: E 2022-12-12 01:29:47214.E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.] 378341 /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:378350cpu time is 97.0588: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:212: 
E:213] E 212] build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8 /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] remote time is 8.68421
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8
:214
213] [] [cpu time is 97.05882022-12-12 01:29:47remote time is 8.68421[2022-12-12 01:29:47
.
2022-12-12 01:29:47.378488.[378488: 3784982022-12-12 01:29:47: E: .E E378529 /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc : /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE:214: 213] 213/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] cpu time is 97.0588] :remote time is 8.68421
remote time is 8.68421214

] cpu time is 97.0588
[[2022-12-12 01:29:472022-12-12 01:29:47..378665378667: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc::214214] ] cpu time is 97.0588cpu time is 97.0588

[2022-12-12 01:31:05.388767: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:118] solver created. now build & solve
[2022-12-12 01:31:05.428797: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 381.47 MB
[2022-12-12 01:31:05.428883: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 381.47 MB
[2022-12-12 01:31:05.429875: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:74] mapping nid to rank...
[2022-12-12 01:31:05.512263: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:91] counting slots...
[2022-12-12 01:31:05.905095: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:105] Final num slot is 49
[2022-12-12 01:31:05.905183: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:109] counting blocks...
[2022-12-12 01:31:12.772903: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:118] Final num block is 1024
[2022-12-12 01:31:12.772993: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:123] counting freq and density...
[2022-12-12 01:31:14.472105: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:154] averaging freq and density...
[2022-12-12 01:31:14.472216: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:155] 1024
[2022-12-12 01:31:14.475047: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:123] solver built. now solve
[2022-12-12 01:31:14.475103: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:354] constructing optimal solver, device=8, stream=1
1024 blocks, 8 devices
[2022-12-12 01:31:14.747478: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:527] Add Var...
[2022-12-12 01:31:14.775457: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:545] Capacity...
[2022-12-12 01:31:14.776908: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:548] Connect CPU...
[2022-12-12 01:31:14.797196: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:550] Connect Access To Storage...
[2022-12-12 01:31:15.319320: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:554] Time...
[2022-12-12 01:31:15.321597: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:516] dst 0, total sm is 80
[2022-12-12 01:31:15.324615: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:516] dst 1, total sm is 80
[2022-12-12 01:31:15.327555: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:516] dst 2, total sm is 80
[2022-12-12 01:31:15.330476: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:516] dst 3, total sm is 80
[2022-12-12 01:31:15.333397: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:516] dst 4, total sm is 80
[2022-12-12 01:31:15.336318: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:516] dst 5, total sm is 80
[2022-12-12 01:31:15.339208: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:516] dst 6, total sm is 80
[2022-12-12 01:31:15.342111: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:516] dst 7, total sm is 80
[2022-12-12 01:32:05. 50224: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:569] Coll Cache init block placement array
[2022-12-12 01:32:05. 58662: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:645] Coll Cache init block placement array done
[2022-12-12 01:32:05. 61842: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:647] Coll Cache model reset done
[2022-12-12 01:32:05.106166: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:127] solver solved
[2022-12-12 01:32:05.106267: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:195] 0 solved master
[2022-12-12 01:32:05.106302: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 0 solved
[2022-12-12 01:32:05.106336: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 0 initing device 0
[2022-12-12 01:32:05.106871: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-12 01:32:05.106926: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 01:32:05.107865: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 01:32:05.108533: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 01:32:05.121666: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 1 solved
[2022-12-12 01:32:05.121748: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 1 initing device 1
[2022-12-12 01:32:05.121864: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 7 solved
[2022-12-12 01:32:05.121938: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 7 initing device 7
[2022-12-12 01:32:05.121995: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:[2022022-12-12 01:32:05] .3 solved122021[
: 2022-12-12 01:32:05E. [122040/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc2022-12-12 01:32:05: :.E202122072 ] : /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.ccE4 solved: 
202/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc] :[6 solved2052022-12-12 01:32:05
] .worker 0 thread 3 initing device 3122119
[: 2022-12-12 01:32:05E. 122137/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc: :E205 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.ccworker 0 thread 4 initing device 4:
205] worker 0 thread 6 initing device 6
[2022-12-12 01:32:05.122191: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-12 01:32:05.122242: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 01:32:05.122365: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-12 01:32:05.122407: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 01:32:05.122535: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[[2022-12-12 01:32:052022-12-12 01:32:05[..2022-12-12 01:32:05122574122577.: : 122586EE:   E/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu ::/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu18151980:] ] 1815Building Coll Cache with ... num gpu device is 8eager alloc mem 381.47 MB] 

Building Coll Cache with ... num gpu device is 8
[2022-12-12 01:32:05.[1226712022-12-12 01:32:05: .E122676 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE: 1980/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu] :eager alloc mem 381.47 MB1980
] eager alloc mem 381.47 MB
[2022-12-12 01:32:05.123144: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 5 solved
[[2022-12-12 01:32:052022-12-12 01:32:05..123173123197: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc::202205] ] 2 solvedworker 0 thread 5 initing device 5

[2022-12-12 01:32:05.123297: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 2 initing device 2
[2022-12-12 01:32:05.123688: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-12 01:32:05.123733: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 01:32:05.123757: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-12 01:32:05.123807: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 01:32:05.126674: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 01:32:05.126719: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 01:32:05.126847: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 01:32:05.127000: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 01:32:05.127056: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 01:32:05.127572: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 01:32:05.128513: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 01:32:05.130969: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 01:32:05.131072: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 01:32:05.131126: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 01:32:05.131252: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 01:32:05.131312: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 01:32:05.131363: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 01:32:05.132298: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 01:32:05.187142: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1024.00 Bytes
[2022-12-12 01:32:05.192631: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-12 01:32:05.192765: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 01:32:05.193604: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 19.45 MB
[2022-12-12 01:32:05.194229: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 01:32:05.195238: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 01:32:05.195286: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 19.07 MB
[2022-12-12 01:32:05.200505: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 01:32:05.201293: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 01:32:05.201339: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 2.39 GB
[2022-12-12 01:32:05.211117: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1024.00 Bytes
[2022-12-12 01:32:05.211275: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1024.00 Bytes
[[[[2022-12-12 01:32:052022-12-12 01:32:052022-12-12 01:32:052022-12-12 01:32:05....211326211326211326211326: : : : EEEE    /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::::1980198019801980] ] ] ] eager alloc mem 1024.00 Byteseager alloc mem 1024.00 Byteseager alloc mem 1024.00 Byteseager alloc mem 1024.00 Bytes



[2022-12-12 01:32:05.211458: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1024.00 Bytes
[2022-12-12 01:32:05.217276: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-12 01:32:05.217361: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 01:32:05.217653: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-12 01:32:05.217750: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 01:32:05.217909: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-12 01:32:05.217996: E[ 2022-12-12 01:32:05/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc.:218004638: ] Eeager release cuda mem 400000000 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-12 01:32:05.218068: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc[:2022-12-12 01:32:05638.] 218100eager release cuda mem 1024: 
E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 01:32:05.218140: E[ [2022-12-12 01:32:05/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu2022-12-12 01:32:05.:.2181581980218148: ] : Eeager alloc mem 19.45 MBE 
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc[/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:2022-12-12 01:32:05:638.638] 218228] eager release cuda mem 400000000eager release cuda mem 1024: 

E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-12 01:32:05.218320: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:[6382022-12-12 01:32:05] .eager release cuda mem 400000000218335
: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 01:32:05.219349: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 19.45 MB
[2022-12-12 01:32:05.219918: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 19.45 MB
[2022-12-12 01:32:05.220531: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 19.45 MB
[2022-12-12 01:32:05.221455: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 19.45 MB
[2022-12-12 01:32:05.222029: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 19.45 MB
[2022-12-12 01:32:05.222542: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 19.45 MB
[2022-12-12 01:32:05.222795: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 01:32:05.223482: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 01:32:05.223528: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 01:32:05.223580: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 01:32:05.223785: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 01:32:05.223809: E[ 2022-12-12 01:32:05/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu.:2238331980: ] W[eager alloc mem 611.00 KB 2022-12-12 01:32:05
/hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc.:22385543: [] E2022-12-12 01:32:05WORKER[0] alloc host memory 19.06 MB .
/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu223899:: 1980E]  eager alloc mem 611.00 KB/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu
:1980] eager alloc mem 611.00 KB
[2022-12-12 01:32:05.224461: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 01:32:05[.2022-12-12 01:32:05224505.: 224505W:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc :/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc43:] 638WORKER[0] alloc host memory 18.90 MB] 
eager release cuda mem 625663
[2022-12-12 01:32:05.[2245612022-12-12 01:32:05: .E224571 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccW: 638/hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc] :eager release cuda mem 62566343
] WORKER[0] alloc host memory 18.93 MB
[2022-12-12 01:32:05.224630: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 19.06 MB
[2022-12-12 01:32:05.224829: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 01:32:05.224883: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43[] 2022-12-12 01:32:05WORKER[0] alloc host memory 19.07 MB.
224898: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] [eager release cuda mem 6256632022-12-12 01:32:05
.224924: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663[
2022-12-12 01:32:05.224953: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] [WORKER[0] alloc host memory 19.07 MB2022-12-12 01:32:05
.224976: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 18.98 MB
[2022-12-12 01:32:05.237301: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 01:32:05.237903: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 01:32:05.237946: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 2.39 GB
[2022-12-12 01:32:05.237990: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[[2022-12-12 01:32:052022-12-12 01:32:05..238248238256: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::19801980] ] eager alloc mem 25.25 KBeager alloc mem 25.25 KB

[2022-12-12 01:32:05.238609: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 01:32:05.238653: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 2.37 GB
[[2022-12-12 01:32:052022-12-12 01:32:05..238880238880: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::638638] ] eager release cuda mem 25855eager release cuda mem 25855

[[2022-12-12 01:32:052022-12-12 01:32:05..238946238948: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::19801980] ] eager alloc mem 2.39 GBeager alloc mem 2.37 GB

[2022-12-12 01:32:05.239730: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 01:32:05.239802: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980[] 2022-12-12 01:32:05eager alloc mem 25.25 KB.
239823: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 01:32:05.240328: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 01:32:05.240369: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 2.39 GB
[2022-12-12 01:32:05.240405: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] [eager release cuda mem 258552022-12-12 01:32:05
.240425: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] [eager release cuda mem 258552022-12-12 01:32:05
.240451: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] [eager alloc mem 2.38 GB2022-12-12 01:32:05
.240475: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 2.39 GB
[[[[[[[[2022-12-12 01:32:062022-12-12 01:32:062022-12-12 01:32:062022-12-12 01:32:062022-12-12 01:32:062022-12-12 01:32:062022-12-12 01:32:062022-12-12 01:32:06........ 34207 34207 34208 34209 34207 34207 34208 34208: : : : : : : : EEEEEEEE        /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::::::::19261926192619261926192619261926] ] ] ] ] ] ] ] Device 7 init p2p of link 4Device 6 init p2p of link 0Device 3 init p2p of link 2Device 0 init p2p of link 3Device 4 init p2p of link 5Device 5 init p2p of link 6Device 1 init p2p of link 7Device 2 init p2p of link 1







[[[[[[2022-12-12 01:32:062022-12-12 01:32:062022-12-12 01:32:06[2022-12-12 01:32:062022-12-12 01:32:062022-12-12 01:32:06.[..2022-12-12 01:32:06... 347442022-12-12 01:32:06 34748 34744. 34744 34744 34745: .: :  34755: : : E 34764EE: EEE :   E   /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu: ::/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:::1980/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu19801980:198019801980] :] ] 1980] ] ] eager alloc mem 611.00 KB1980eager alloc mem 611.00 KBeager alloc mem 611.00 KB] eager alloc mem 611.00 KBeager alloc mem 611.00 KBeager alloc mem 611.00 KB
] 

eager alloc mem 611.00 KB


eager alloc mem 611.00 KB

[2022-12-12 01:32:06. 35788[: [2022-12-12 01:32:06[E[[2022-12-12 01:32:06.2022-12-12 01:32:06 [2022-12-12 01:32:06[2022-12-12 01:32:06. 35795./hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc2022-12-12 01:32:06.2022-12-12 01:32:06. 35800:  35802:. 35804. 35813: E: 638 35812:  35816: E E] : E: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc eager release cuda mem 625663E E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638:/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] 638:638:638] eager release cuda mem 625663] 638] 638] eager release cuda mem 625663
eager release cuda mem 625663] eager release cuda mem 625663] eager release cuda mem 625663

eager release cuda mem 625663
eager release cuda mem 625663


[2022-12-12 01:32:06. 49877: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 6 init p2p of link 5
[[2022-12-12 01:32:062022-12-12 01:32:06.. 50035 50047: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::19261980] ] Device 3 init p2p of link 0eager alloc mem 611.00 KB

[2022-12-12 01:32:06. 50217: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 01:32:06. 50610: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 5 init p2p of link 4
[2022-12-12 01:32:06. 50742: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 7 init p2p of link 1[
2022-12-12 01:32:06. 50779: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[[2022-12-12 01:32:06[2022-12-12 01:32:06.2022-12-12 01:32:06. 50890. 50902:  50908: E: E E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926:638] 1980] Device 1 init p2p of link 2] eager release cuda mem 625663
eager alloc mem 611.00 KB

[2022-12-12 01:32:06. 50999: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 4 init p2p of link 7
[2022-12-12 01:32:06. 51049: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663[
2022-12-12 01:32:06. 51093: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 01:32:06. 51171: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 01:32:06. 51421: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 2 init p2p of link 3
[2022-12-12 01:32:06. 51595: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc[:2022-12-12 01:32:06638.]  51611eager release cuda mem 625663: 
E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 01:32:06. 51646: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 0 init p2p of link 6
[2022-12-12 01:32:06. 51747: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 01:32:06. 51819: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 01:32:06. 51927: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 01:32:06. 51988: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 01:32:06. 52445: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 01:32:06. 52610: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 01:32:06. 64141: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 6 init p2p of link 4
[2022-12-12 01:32:06. 64255: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 01:32:06. 64638: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 7 init p2p of link 6
[2022-12-12 01:32:06. 64709: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 3 init p2p of link 5
[2022-12-12 01:32:06. 64752: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 01:32:06. 64831: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[[2022-12-12 01:32:062022-12-12 01:32:06.. 65054 65069: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::1926638] ] Device 5 init p2p of link 7eager release cuda mem 625663

[2022-12-12 01:32:06. 65213: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 01:32:06. 65374: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 0 init p2p of link 1
[2022-12-12 01:32:06. 65482: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 3
[2022-12-12 01:32:06. 65519: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 01:32:06. 65576: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 01:32:06. 65602: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 01:32:06. 65639: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 01:32:06. 65823: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 4 init p2p of link 2
[2022-12-12 01:32:06[.2022-12-12 01:32:06 65922.:  65939E:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu :/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu1926:] 1980Device 2 init p2p of link 0] 
eager alloc mem 611.00 KB[
2022-12-12 01:32:06. 66021: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 01:32:06. 66096: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 01:32:06. 66331: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 01:32:06. 66402: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 01:32:06. 66801: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 01:32:06. 66900: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 01:32:06. 81249: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 3 init p2p of link 1
[2022-12-12 01:32:06. 81332: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 6 init p2p of link 7[
2022-12-12 01:32:06. 81368: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 01:32:06. 81447: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 01:32:06. 82154: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] [Device 7 init p2p of link 52022-12-12 01:32:06
. 82187: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[[2022-12-12 01:32:062022-12-12 01:32:06.. 82269 82270: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::6381980] ] eager release cuda mem 625663eager alloc mem 611.00 KB

[2022-12-12 01:32:06. 82792: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 4 init p2p of link 6
[2022-12-12 01:32:06. 82912: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 01:32:06. 83049: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 0
[2022-12-12 01:32:06. 83112: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 01:32:06. 83171: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 01:32:06. 83297: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 5 init p2p of link 3
[2022-12-12 01:32:06. 83338: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 0 init p2p of link 2
[2022-12-12 01:32:06. 83414: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 01:32:06. 83466: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 01:32:06. 83708: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 01:32:06. 83976: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 01:32:06. 84147: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 2 init p2p of link 4
[2022-12-12 01:32:06. 84190: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 01:32:06. 84265: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu[:2022-12-12 01:32:061980.]  84280eager alloc mem 611.00 KB: 
E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 01:32:06. 85078: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 01:32:06. 98237: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 20400000
[2022-12-12 01:32:06. 98590: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 4954914 / 100000000 nodes ( 4.95 %~5.00 %) | remote 14879801 / 100000000 nodes ( 14.88 %) | cpu 80165285 / 100000000 nodes ( 80.17 %) | 2.37 GB | 0.976019 secs 
[2022-12-12 01:32:06. 98634: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 20400000
[2022-12-12 01:32:06. 98964: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 4976066 / 100000000 nodes ( 4.98 %~5.00 %) | remote 14858649 / 100000000 nodes ( 14.86 %) | cpu 80165285 / 100000000 nodes ( 80.17 %) | 2.38 GB | 0.975239 secs 
[2022-12-12 01:32:06. 99150: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 20400000
[2022-12-12 01:32:06. 99428: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 20400000
[2022-12-12 01:32:06. 99485: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 4999233 / 100000000 nodes ( 5.00 %~5.00 %) | remote 14835482 / 100000000 nodes ( 14.84 %) | cpu 80165285 / 100000000 nodes ( 80.17 %) | 2.39 GB | 0.976816 secs 
[2022-12-12 01:32:06. 99686: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 20400000
[2022-12-12 01:32:06. 99938: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 20400000
[2022-12-12 01:32:06.100200: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 20400000
[2022-12-12 01:32:06.100999: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 4998590 / 100000000 nodes ( 5.00 %~5.00 %) | remote 14836125 / 100000000 nodes ( 14.84 %) | cpu 80165285 / 100000000 nodes ( 80.17 %) | 2.39 GB | 0.978336 secs 
[2022-12-12 01:32:06.101044: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 20400000
[2022-12-12 01:32:06.102198: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 4961731 / 100000000 nodes ( 4.96 %~5.00 %) | remote 14872984 / 100000000 nodes ( 14.87 %) | cpu 80165285 / 100000000 nodes ( 80.17 %) | 2.37 GB | 0.979798 secs 
[2022-12-12 01:32:06.102307: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 4996952 / 100000000 nodes ( 5.00 %~5.00 %) | remote 14837763 / 100000000 nodes ( 14.84 %) | cpu 80165285 / 100000000 nodes ( 80.17 %) | 2.39 GB | 0.97851 secs 
[2022-12-12 01:32:06.102467: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 4996204 / 100000000 nodes ( 5.00 %~5.00 %) | remote 14838511 / 100000000 nodes ( 14.84 %) | cpu 80165285 / 100000000 nodes ( 80.17 %) | 2.39 GB | 0.98024 secs 
[2022-12-12 01:32:06.104023: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 4999075 / 100000000 nodes ( 5.00 %~5.00 %) | remote 14835640 / 100000000 nodes ( 14.84 %) | cpu 80165285 / 100000000 nodes ( 80.17 %) | 2.39 GB | 0.99711 secs 
[2022-12-12 01:32:06.105897: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2213] before create ctx, mem is 9.03 GB
[2022-12-12 01:32:07.447757: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2219] after create ctx, mem is 9.29 GB
[2022-12-12 01:32:07.448611: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2226] after create stream, mem is 9.29 GB
[2022-12-12 01:32:07.452169: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2213] before create ctx, mem is 9.29 GB
[2022-12-12 01:32:09. 65019: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2219] after create ctx, mem is 9.55 GB
[2022-12-12 01:32:09. 65590: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2226] after create stream, mem is 9.55 GB
[2022-12-12 01:32:09. 66580: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2213] before create ctx, mem is 9.55 GB
[2022-12-12 01:32:10.335797: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2219] after create ctx, mem is 9.77 GB
[2022-12-12 01:32:10.336000: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2226] after create stream, mem is 9.77 GB
[2022-12-12 01:32:10.336387: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2213] before create ctx, mem is 9.77 GB
[2022-12-12 01:32:11.951667: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2219] after create ctx, mem is 9.98 GB
[2022-12-12 01:32:11.951830: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2226] after create stream, mem is 9.98 GB
[2022-12-12 01:32:11.952217: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2243] before create stream, mem is 9.98 GB
[2022-12-12 01:32:11.952382: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2249] after create stream, mem is 9.98 GB
[2022-12-12 01:32:11.953348: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2213] before create ctx, mem is 9.98 GB
[2022-12-12 01:32:13.474213: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2219] after create ctx, mem is 10.18 GB
[2022-12-12 01:32:13.474387: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2226] after create stream, mem is 10.18 GB
[HCTR][01:32:13.867][ERROR][RK0][main]: replica 7 calling init per replica done, doing barrier
[HCTR][01:32:13.867][ERROR][RK0][main]: replica 3 calling init per replica done, doing barrier
[HCTR][01:32:13.867][ERROR][RK0][main]: replica 2 calling init per replica done, doing barrier
[HCTR][01:32:13.867][ERROR][RK0][main]: replica 5 calling init per replica done, doing barrier
[HCTR][01:32:13.867][ERROR][RK0][main]: replica 4 calling init per replica done, doing barrier
[HCTR][01:32:13.867][ERROR][RK0][tid #140574522853120]: replica 0 calling init per replica done, doing barrier
[HCTR][01:32:13.867][ERROR][RK0][main]: replica 1 calling init per replica done, doing barrier
[HCTR][01:32:13.867][ERROR][RK0][main]: replica 6 calling init per replica done, doing barrier
[HCTR][01:32:13.867][ERROR][RK0][main]: replica 4 calling init per replica done, doing barrier done
[HCTR][01:32:13.867][ERROR][RK0][main]: replica 3 calling init per replica done, doing barrier done
[HCTR][01:32:13.867][ERROR][RK0][main]: replica 6 calling init per replica done, doing barrier done
[HCTR][01:32:13.867][ERROR][RK0][main]: replica 7 calling init per replica done, doing barrier done
[HCTR][01:32:13.867][ERROR][RK0][main]: replica 1 calling init per replica done, doing barrier done
[HCTR][01:32:13.867][ERROR][RK0][main]: replica 2 calling init per replica done, doing barrier done
[HCTR][01:32:13.867][ERROR][RK0][main]: init per replica done
[HCTR][01:32:13.867][ERROR][RK0][main]: replica 5 calling init per replica done, doing barrier done
[HCTR][01:32:13.867][ERROR][RK0][tid #140574522853120]: replica 0 calling init per replica done, doing barrier done
[HCTR][01:32:13.867][ERROR][RK0][main]: init per replica done
[HCTR][01:32:13.867][ERROR][RK0][main]: init per replica done
[HCTR][01:32:13.867][ERROR][RK0][main]: init per replica done
[HCTR][01:32:13.867][ERROR][RK0][main]: init per replica done
[HCTR][01:32:13.867][ERROR][RK0][main]: init per replica done
[HCTR][01:32:13.867][ERROR][RK0][main]: init per replica done
[HCTR][01:32:13.870][ERROR][RK0][tid #140574522853120]: init per replica done
[HCTR][01:32:13.873][ERROR][RK0][main]: 5 allocated 3276800 at 0x7fdbd1b20000
[HCTR][01:32:13.873][ERROR][RK0][main]: 3 allocated 3276800 at 0x7fdbd1b20000
[HCTR][01:32:13.873][ERROR][RK0][main]: 5 allocated 6553600 at 0x7fdbd2000000
[HCTR][01:32:13.873][ERROR][RK0][main]: 3 allocated 6553600 at 0x7fdbd2000000
[HCTR][01:32:13.873][ERROR][RK0][main]: 5 allocated 3276800 at 0x7fdbd2640000
[HCTR][01:32:13.873][ERROR][RK0][main]: 3 allocated 3276800 at 0x7fdbd2640000
[HCTR][01:32:13.873][ERROR][RK0][main]: 5 allocated 6553600 at 0x7fdbd2960000
[HCTR][01:32:13.873][ERROR][RK0][main]: 3 allocated 6553600 at 0x7fdbd2960000
[HCTR][01:32:13.873][ERROR][RK0][tid #140574061483776]: 4 allocated 3276800 at 0x7fdbd1b20000
[HCTR][01:32:13.873][ERROR][RK0][tid #140574061483776]: 4 allocated 6553600 at 0x7fdbd2000000
[HCTR][01:32:13.873][ERROR][RK0][tid #140574061483776]: 4 allocated 3276800 at 0x7fdbd2640000
[HCTR][01:32:13.873][ERROR][RK0][tid #140574061483776]: 4 allocated 6553600 at 0x7fdbd2960000
[HCTR][01:32:13.873][ERROR][RK0][tid #140574128592640]: 2 allocated 3276800 at 0x7fdbd1b20000
[HCTR][01:32:13.873][ERROR][RK0][tid #140574128592640]: 2 allocated 6553600 at 0x7fdbd2000000
[HCTR][01:32:13.873][ERROR][RK0][tid #140574128592640]: 2 allocated 3276800 at 0x7fdbd2640000
[HCTR][01:32:13.873][ERROR][RK0][tid #140574128592640]: 2 allocated 6553600 at 0x7fdbd2960000
[HCTR][01:32:13.874][ERROR][RK0][tid #140574120199936]: 7 allocated 3276800 at 0x7fdbd1b20000
[HCTR][01:32:13.874][ERROR][RK0][tid #140574120199936]: 7 allocated 6553600 at 0x7fdbd2000000
[HCTR][01:32:13.874][ERROR][RK0][tid #140574120199936]: 7 allocated 3276800 at 0x7fdbd2640000
[HCTR][01:32:13.874][ERROR][RK0][tid #140574120199936]: 7 allocated 6553600 at 0x7fdbd2960000
[HCTR][01:32:13.874][ERROR][RK0][tid #140574447351552]: 6 allocated 3276800 at 0x7fdbcdb20000
[HCTR][01:32:13.874][ERROR][RK0][tid #140574447351552]: 6 allocated 6553600 at 0x7fdbce000000
[HCTR][01:32:13.874][ERROR][RK0][tid #140574447351552]: 6 allocated 3276800 at 0x7fdbce640000
[HCTR][01:32:13.874][ERROR][RK0][tid #140574447351552]: 6 allocated 6553600 at 0x7fdbce960000
[HCTR][01:32:13.875][ERROR][RK0][main]: 1 allocated 3276800 at 0x7fdbd1b20000
[HCTR][01:32:13.875][ERROR][RK0][main]: 1 allocated 6553600 at 0x7fdbd2000000
[HCTR][01:32:13.875][ERROR][RK0][main]: 1 allocated 3276800 at 0x7fdbd2640000
[HCTR][01:32:13.875][ERROR][RK0][main]: 1 allocated 6553600 at 0x7fdbd2960000
[HCTR][01:32:13.877][ERROR][RK0][tid #140574522853120]: 0 allocated 3276800 at 0x7fdbd4120000
[HCTR][01:32:13.877][ERROR][RK0][tid #140574522853120]: 0 allocated 6553600 at 0x7fdbd4600000
[HCTR][01:32:13.877][ERROR][RK0][tid #140574522853120]: 0 allocated 3276800 at 0x7fdbd530e800
[HCTR][01:32:13.877][ERROR][RK0][tid #140574522853120]: 0 allocated 6553600 at 0x7fdbd562e800








