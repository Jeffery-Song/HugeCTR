2022-12-11 23:22:18.683603: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:18.688770: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:18.697297: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:18.701759: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:18.708513: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:18.718908: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:18.726932: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:18.738518: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:18.790246: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:18.796488: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:18.800144: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:18.801862: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:18.803082: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:18.803244: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:18.804810: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:18.804913: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:18.806267: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:18.806654: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:18.807625: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:18.808125: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:18.809127: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:18.809555: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:18.810702: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:18.811301: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:18.812241: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:18.813067: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:18.813634: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:18.815002: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:18.815361: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:18.816896: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:18.817947: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:18.818987: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:18.820797: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:18.821800: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:18.822774: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:18.823817: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:18.824798: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:18.825815: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:18.826838: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:18.827788: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:18.832192: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:18.833006: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 23:22:18.833350: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:18.834296: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:18.835416: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:18.836435: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:18.837459: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:18.838507: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:18.839563: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:18.841946: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:18.843841: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:18.845115: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:18.846148: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:18.846794: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:18.847644: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:18.849538: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:18.849777: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:18.850068: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:18.852307: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:18.852529: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:18.852699: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:18.854981: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:18.855326: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:18.855418: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:18.857668: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:18.858124: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:18.858216: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:18.860777: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:18.861243: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:18.861307: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:18.861520: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:18.863505: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:18.864427: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:18.865004: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:18.865127: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:18.865258: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:18.867666: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:18.868183: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:18.868491: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:18.869176: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:18.869190: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:18.871368: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:18.871909: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:18.872147: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:18.872376: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:18.874046: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:18.874613: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:18.874670: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:18.878577: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:18.879775: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:18.879975: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:18.881673: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:18.881810: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:18.882086: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:18.883814: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:18.884020: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:18.884359: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:18.885511: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:18.913372: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:18.916988: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:18.918778: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:18.921688: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:18.921792: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:18.922684: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:18.923241: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:18.923347: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:18.923797: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:18.923837: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:18.926794: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:18.928061: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:18.928170: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:18.928360: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:18.928430: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:18.928476: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:18.931613: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:18.933862: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:18.934040: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:18.934167: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:18.934225: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:18.936066: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:18.937365: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:18.937529: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:18.937578: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:18.937668: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:18.940174: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:18.941081: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:18.941266: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:18.941316: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:18.941454: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:18.944296: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:18.945195: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:18.945333: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:18.945384: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:18.945453: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:18.948014: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:18.948790: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:18.948899: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:18.949044: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:18.949080: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:18.951426: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:18.952170: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:18.952309: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:18.952449: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:18.952487: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:18.955423: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:18.955510: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:18.955563: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:18.955601: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:18.955650: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:18.960005: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:18.960093: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:18.960247: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:18.960375: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:18.960391: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:18.963693: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:18.963875: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:18.963956: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:18.964145: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:18.964229: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:18.967820: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:18.968008: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:18.968157: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:18.968217: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:18.968314: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:18.971117: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:18.971472: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:18.971765: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:18.971954: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:18.971996: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:18.972134: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:18.975237: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:18.975430: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:18.975669: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:18.975886: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:18.975928: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:18.976168: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:18.977789: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:18.979506: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:18.979902: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:18.980298: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:18.980341: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:18.980563: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:18.980980: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:18.982655: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:18.985041: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:18.986053: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:18.986596: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:18.986934: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:18.987122: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:18.987474: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:18.988140: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:18.989930: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:18.990787: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:18.991400: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:18.991621: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:18.992189: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:18.992545: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:18.993945: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:18.994679: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:18.995436: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:18.995520: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:18.995562: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 23:22:18.995806: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:18.996321: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:18.997571: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:18.998571: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:18.999088: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:18.999313: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:18.999568: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:19.000216: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:19.001580: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:19.002579: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:19.003564: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:19.003873: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:19.004010: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:19.004596: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:19.004891: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:19.007124: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:19.008304: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:19.008605: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:19.008981: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:19.009208: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:19.009755: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:19.010071: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:19.011212: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:19.012887: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:19.013494: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:19.013621: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:19.013736: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:19.014052: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:19.014250: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:19.015647: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:19.017113: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:19.017880: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:19.017930: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:19.018215: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:19.018772: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:19.020086: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:19.023218: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:19.024255: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:19.026056: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 23:22:19.026056: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 23:22:19.026061: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 23:22:19.026587: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 23:22:19.026859: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:19.027365: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:19.029307: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:19.030114: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:19.031622: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:19.032388: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:19.033940: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:19.035232: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:19.035468: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:19.035499: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:19.036094: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:19.036282: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:19.037382: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:19.040747: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:19.041278: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:19.041301: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:19.041540: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:19.041705: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:19.043111: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:19.045206: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:19.045717: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:19.045775: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:19.046026: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:19.046109: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:19.047450: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:19.050173: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:19.051574: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:19.089922: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:19.092636: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:19.095723: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:19.098080: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:19.102353: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:19.103712: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:19.136183: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:19.138255: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:19.144331: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:19.146566: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 23:22:19.153722: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 23:22:19.155653: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:19.159067: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:19.162523: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:19.165805: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:19.174988: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:19.179885: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:20.209725: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:20.210347: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:20.210866: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:20.211485: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 23:22:20.211542: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-11 23:22:20.230692: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:20.231353: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:20.231866: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:20.232571: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:20.233216: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:20.233690: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-11 23:22:20.281150: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 23:22:20.281360: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 23:22:20.316391: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12340
2022-12-11 23:22:20.441593: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:20.442227: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:20.442751: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:20.443770: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 23:22:20.443828: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-11 23:22:20.462261: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:20.462882: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:20.463419: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:20.463991: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:20.464531: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:20.465206: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:1/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-11 23:22:20.506899: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:20.507733: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:20.508418: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:20.508895: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 23:22:20.508947: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-11 23:22:20.526948: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:20.527796: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:20.528321: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:20.529049: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:20.529842: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:20.530520: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:2/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-11 23:22:20.532875: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:20.533463: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:20.534134: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:20.534610: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 23:22:20.534654: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-11 23:22:20.543326: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:20.544162: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:20.544729: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:20.544792: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:20.545809: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 23:22:20.545865: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-11 23:22:20.546094: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:20.546659: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:20.547148: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 23:22:20.547201: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-11 23:22:20.549304: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:20.549895: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:20.550426: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:20.550894: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 23:22:20.550940: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-11 23:22:20.553868: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:20.554491: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:20.555364: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:20.555989: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:20.556521: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:20.556997: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:5/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-11 23:22:20.560209: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:20.560785: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:20.561311: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:20.561783: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 23:22:20.561828: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-11 23:22:20.563328: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:20.563956: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:20.564467: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:20.565026: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:20.565035: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:20.566318: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:20.566321: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:20.567482: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:7/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-11 23:22:20.567490: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:20.568078: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:20.568633: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:20.569113: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:6/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-11 23:22:20.569800: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:20.570030: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 23:22:20.570181: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 23:22:20.570417: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:20.570922: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:20.571510: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:20.571900: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12341
2022-12-11 23:22:20.572024: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:20.572498: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:3/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-11 23:22:20.579275: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:20.579913: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:20.580666: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:20.581296: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:20.581814: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:22:20.582294: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:4/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-11 23:22:20.603456: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 23:22:20.603679: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 23:22:20.605566: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12345
2022-12-11 23:22:20.613125: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 23:22:20.613332: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 23:22:20.615276: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12347
2022-12-11 23:22:20.616077: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 23:22:20.616259: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 23:22:20.618034: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12346
2022-12-11 23:22:20.619438: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 23:22:20.619603: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 23:22:20.621370: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12343
2022-12-11 23:22:20.629745: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 23:22:20.629907: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 23:22:20.631775: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12344
2022-12-11 23:22:20.637053: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 23:22:20.637266: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 23:22:20.639185: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12342
[HCTR][23:22:21.885][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][23:22:21.885][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][23:22:21.886][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][23:22:21.913][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][23:22:21.913][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][23:22:21.913][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][23:22:21.916][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][23:22:21.916][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 1it [00:01,  1.57s/it]warmup run: 1it [00:01,  1.53s/it]warmup run: 1it [00:01,  1.54s/it]warmup run: 97it [00:01, 80.62it/s]warmup run: 103it [00:01, 87.57it/s]warmup run: 1it [00:01,  1.50s/it]warmup run: 97it [00:01, 82.24it/s]warmup run: 196it [00:01, 177.37it/s]warmup run: 206it [00:01, 189.83it/s]warmup run: 1it [00:01,  1.53s/it]warmup run: 100it [00:01, 86.58it/s]warmup run: 194it [00:01, 178.37it/s]warmup run: 295it [00:01, 284.47it/s]warmup run: 1it [00:01,  1.49s/it]warmup run: 308it [00:01, 301.22it/s]warmup run: 100it [00:01, 85.17it/s]warmup run: 201it [00:01, 188.30it/s]warmup run: 292it [00:01, 285.50it/s]warmup run: 1it [00:01,  1.48s/it]warmup run: 394it [00:01, 396.26it/s]warmup run: 1it [00:01,  1.51s/it]warmup run: 101it [00:01, 87.89it/s]warmup run: 409it [00:01, 415.17it/s]warmup run: 199it [00:01, 183.43it/s]warmup run: 390it [00:01, 396.86it/s]warmup run: 302it [00:01, 299.83it/s]warmup run: 101it [00:01, 88.64it/s]warmup run: 494it [00:02, 507.37it/s]warmup run: 99it [00:01, 85.39it/s]warmup run: 202it [00:01, 190.09it/s]warmup run: 510it [00:02, 526.31it/s]warmup run: 297it [00:01, 290.31it/s]warmup run: 486it [00:02, 501.58it/s]warmup run: 403it [00:01, 414.73it/s]warmup run: 202it [00:01, 191.48it/s]warmup run: 595it [00:02, 611.44it/s]warmup run: 199it [00:01, 185.93it/s]warmup run: 304it [00:01, 303.19it/s]warmup run: 609it [00:02, 622.75it/s]warmup run: 396it [00:01, 402.57it/s]warmup run: 584it [00:02, 601.42it/s]warmup run: 505it [00:02, 527.70it/s]warmup run: 304it [00:01, 305.07it/s]warmup run: 697it [00:02, 704.62it/s]warmup run: 299it [00:01, 296.43it/s]warmup run: 406it [00:01, 420.05it/s]warmup run: 711it [00:02, 713.02it/s]warmup run: 496it [00:02, 513.97it/s]warmup run: 682it [00:02, 687.53it/s]warmup run: 606it [00:02, 629.65it/s]warmup run: 405it [00:01, 420.20it/s]warmup run: 797it [00:02, 776.67it/s]warmup run: 399it [00:01, 410.20it/s]warmup run: 505it [00:01, 527.33it/s]warmup run: 812it [00:02, 785.93it/s]warmup run: 597it [00:02, 618.00it/s]warmup run: 778it [00:02, 753.69it/s]warmup run: 708it [00:02, 719.43it/s]warmup run: 507it [00:01, 532.99it/s]warmup run: 896it [00:02, 831.26it/s]warmup run: 500it [00:02, 522.14it/s]warmup run: 608it [00:02, 633.68it/s]warmup run: 913it [00:02, 842.80it/s]warmup run: 698it [00:02, 708.56it/s]warmup run: 874it [00:02, 805.56it/s]warmup run: 810it [00:02, 792.52it/s]warmup run: 610it [00:02, 637.79it/s]warmup run: 602it [00:02, 626.15it/s]warmup run: 995it [00:02, 864.89it/s]warmup run: 710it [00:02, 723.48it/s]warmup run: 1015it [00:02, 890.62it/s]warmup run: 801it [00:02, 786.89it/s]warmup run: 911it [00:02, 849.07it/s]warmup run: 970it [00:02, 845.51it/s]warmup run: 713it [00:02, 727.69it/s]warmup run: 703it [00:02, 714.60it/s]warmup run: 1098it [00:02, 910.09it/s]warmup run: 812it [00:02, 797.41it/s]warmup run: 1119it [00:02, 929.92it/s]warmup run: 903it [00:02, 846.46it/s]warmup run: 1012it [00:02, 892.60it/s]warmup run: 1066it [00:02, 877.18it/s]warmup run: 815it [00:02, 798.60it/s]warmup run: 802it [00:02, 782.39it/s]warmup run: 1198it [00:02, 935.07it/s]warmup run: 915it [00:02, 857.22it/s]warmup run: 1221it [00:02, 953.71it/s]warmup run: 1004it [00:02, 888.56it/s]warmup run: 1114it [00:02, 927.87it/s]warmup run: 1162it [00:02, 899.83it/s]warmup run: 917it [00:02, 854.76it/s]warmup run: 903it [00:02, 840.85it/s]warmup run: 1298it [00:02, 951.02it/s]warmup run: 1018it [00:02, 901.97it/s]warmup run: 1323it [00:02, 967.01it/s]warmup run: 1106it [00:02, 923.54it/s]warmup run: 1215it [00:02, 948.63it/s]warmup run: 1258it [00:02, 913.12it/s]warmup run: 1018it [00:02, 896.02it/s]warmup run: 1004it [00:02, 886.68it/s]warmup run: 1400it [00:02, 969.55it/s]warmup run: 1424it [00:02, 978.06it/s]warmup run: 1316it [00:02, 963.24it/s]warmup run: 1355it [00:02, 927.13it/s]warmup run: 1119it [00:02, 865.03it/s]warmup run: 1207it [00:02, 883.39it/s]warmup run: 1121it [00:02, 932.23it/s]warmup run: 1107it [00:02, 924.83it/s]warmup run: 1500it [00:03, 970.08it/s]warmup run: 1526it [00:03, 989.60it/s]warmup run: 1451it [00:03, 934.71it/s]warmup run: 1221it [00:02, 906.17it/s]warmup run: 1417it [00:02, 962.53it/s]warmup run: 1308it [00:02, 917.84it/s]warmup run: 1223it [00:02, 955.68it/s]warmup run: 1208it [00:02, 948.81it/s]warmup run: 1600it [00:03, 969.05it/s]warmup run: 1629it [00:03, 1001.44it/s]warmup run: 1547it [00:03, 940.89it/s]warmup run: 1323it [00:02, 936.53it/s]warmup run: 1516it [00:03, 956.66it/s]warmup run: 1410it [00:02, 944.17it/s]warmup run: 1325it [00:02, 968.78it/s]warmup run: 1310it [00:02, 967.61it/s]warmup run: 1699it [00:03, 964.91it/s]warmup run: 1733it [00:03, 1011.84it/s]warmup run: 1643it [00:03, 942.00it/s]warmup run: 1424it [00:02, 957.06it/s]warmup run: 1512it [00:03, 963.15it/s]warmup run: 1614it [00:03, 951.36it/s]warmup run: 1426it [00:02, 971.17it/s]warmup run: 1413it [00:02, 985.05it/s]warmup run: 1801it [00:03, 978.67it/s]warmup run: 1837it [00:03, 1017.68it/s]warmup run: 1525it [00:03, 971.41it/s]warmup run: 1740it [00:03, 948.55it/s]warmup run: 1612it [00:03, 973.16it/s]warmup run: 1711it [00:03, 954.81it/s]warmup run: 1526it [00:02, 973.04it/s]warmup run: 1517it [00:03, 1001.16it/s]warmup run: 1903it [00:03, 988.64it/s]warmup run: 1941it [00:03, 1021.82it/s]warmup run: 1627it [00:03, 983.16it/s]warmup run: 1836it [00:03, 948.38it/s]warmup run: 1713it [00:03, 983.46it/s]warmup run: 1808it [00:03, 956.93it/s]warmup run: 1626it [00:03, 976.22it/s]warmup run: 1621it [00:03, 1009.88it/s]warmup run: 2006it [00:03, 999.90it/s]warmup run: 2052it [00:03, 1047.66it/s]warmup run: 1729it [00:03, 992.07it/s]warmup run: 1938it [00:03, 966.98it/s]warmup run: 1814it [00:03, 989.08it/s]warmup run: 1905it [00:03, 960.07it/s]warmup run: 1725it [00:03, 979.10it/s]warmup run: 1724it [00:03, 1010.43it/s]warmup run: 2125it [00:03, 1054.33it/s]warmup run: 2174it [00:03, 1097.31it/s]warmup run: 1832it [00:03, 1001.48it/s]warmup run: 2046it [00:03, 1000.48it/s]warmup run: 1914it [00:03, 989.12it/s]warmup run: 2004it [00:03, 966.99it/s]warmup run: 1824it [00:03, 979.97it/s]warmup run: 1827it [00:03, 1011.34it/s]warmup run: 2244it [00:03, 1094.05it/s]warmup run: 2296it [00:03, 1132.53it/s]warmup run: 1935it [00:03, 1009.24it/s]warmup run: 2165it [00:03, 1056.76it/s]warmup run: 2019it [00:03, 1005.39it/s]warmup run: 2125it [00:03, 1037.99it/s]warmup run: 1924it [00:03, 983.08it/s]warmup run: 1929it [00:03, 1013.21it/s]warmup run: 2363it [00:03, 1122.40it/s]warmup run: 2418it [00:03, 1157.15it/s]warmup run: 2043it [00:03, 1029.51it/s]warmup run: 2285it [00:03, 1096.91it/s]warmup run: 2137it [00:03, 1056.90it/s]warmup run: 2246it [00:03, 1088.25it/s]warmup run: 2027it [00:03, 994.89it/s]warmup run: 2036it [00:03, 1028.97it/s]warmup run: 2482it [00:03, 1141.63it/s]warmup run: 2534it [00:03, 1157.52it/s]warmup run: 2165it [00:03, 1084.12it/s]warmup run: 2404it [00:03, 1124.57it/s]warmup run: 2260it [00:03, 1106.04it/s]warmup run: 2366it [00:03, 1121.12it/s]warmup run: 2148it [00:03, 1058.09it/s]warmup run: 2158it [00:03, 1083.59it/s]warmup run: 2601it [00:04, 1155.51it/s]warmup run: 2650it [00:04, 1153.65it/s]warmup run: 2287it [00:03, 1122.51it/s]warmup run: 2524it [00:04, 1144.31it/s]warmup run: 2383it [00:03, 1142.69it/s]warmup run: 2487it [00:03, 1146.80it/s]warmup run: 2269it [00:03, 1102.34it/s]warmup run: 2280it [00:03, 1121.95it/s]warmup run: 2720it [00:04, 1165.37it/s]warmup run: 2771it [00:04, 1168.05it/s]warmup run: 2639it [00:04, 1144.90it/s]warmup run: 2409it [00:03, 1149.29it/s]warmup run: 2506it [00:03, 1168.46it/s]warmup run: 2608it [00:04, 1164.60it/s]warmup run: 2390it [00:03, 1132.31it/s]warmup run: 2402it [00:03, 1149.03it/s]warmup run: 2838it [00:04, 1168.65it/s]warmup run: 2888it [00:04, 1156.74it/s]warmup run: 2530it [00:03, 1166.72it/s]warmup run: 2758it [00:04, 1156.14it/s]warmup run: 2630it [00:04, 1187.60it/s]warmup run: 2729it [00:04, 1177.41it/s]warmup run: 2511it [00:03, 1153.78it/s]warmup run: 2524it [00:03, 1167.57it/s]warmup run: 2957it [00:04, 1174.39it/s]warmup run: 3000it [00:04, 688.91it/s] warmup run: 2651it [00:04, 1179.07it/s]warmup run: 2877it [00:04, 1166.19it/s]warmup run: 2751it [00:04, 1193.53it/s]warmup run: 2848it [00:04, 1180.33it/s]warmup run: 3000it [00:04, 676.18it/s] warmup run: 2632it [00:04, 1167.80it/s]warmup run: 2645it [00:04, 1178.00it/s]warmup run: 2996it [00:04, 1172.27it/s]warmup run: 2770it [00:04, 1180.06it/s]warmup run: 2873it [00:04, 1199.07it/s]warmup run: 2969it [00:04, 1188.54it/s]warmup run: 3000it [00:04, 671.98it/s] warmup run: 2753it [00:04, 1179.82it/s]warmup run: 3000it [00:04, 688.01it/s] warmup run: 2764it [00:04, 1179.91it/s]warmup run: 2889it [00:04, 1175.56it/s]warmup run: 2995it [00:04, 1204.46it/s]warmup run: 3000it [00:04, 684.48it/s] warmup run: 2873it [00:04, 1182.96it/s]warmup run: 2883it [00:04, 1180.71it/s]warmup run: 3000it [00:04, 692.13it/s] warmup run: 2994it [00:04, 1190.51it/s]warmup run: 3000it [00:04, 695.33it/s] warmup run: 3000it [00:04, 692.16it/s] 
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]


warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]


warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   6%|         | 165/3000 [00:00<00:01, 1647.13it/s]warmup should be done:   5%|         | 163/3000 [00:00<00:01, 1628.87it/s]warmup should be done:   4%|         | 114/3000 [00:00<00:02, 1139.34it/s]warmup should be done:   5%|         | 162/3000 [00:00<00:01, 1614.26it/s]warmup should be done:   6%|         | 165/3000 [00:00<00:01, 1641.43it/s]warmup should be done:   5%|         | 164/3000 [00:00<00:01, 1630.96it/s]warmup should be done:   5%|         | 158/3000 [00:00<00:01, 1572.39it/s]warmup should be done:   5%|         | 163/3000 [00:00<00:01, 1619.62it/s]warmup should be done:  11%|         | 332/3000 [00:00<00:01, 1657.61it/s]warmup should be done:   9%|         | 282/3000 [00:00<00:01, 1455.90it/s]warmup should be done:  11%|         | 328/3000 [00:00<00:01, 1635.64it/s]warmup should be done:  11%|         | 321/3000 [00:00<00:01, 1606.05it/s]warmup should be done:  11%|         | 329/3000 [00:00<00:01, 1640.34it/s]warmup should be done:  11%|         | 327/3000 [00:00<00:01, 1628.89it/s]warmup should be done:  11%|         | 331/3000 [00:00<00:01, 1648.03it/s]warmup should be done:  11%|         | 324/3000 [00:00<00:01, 1600.57it/s]warmup should be done:  17%|        | 498/3000 [00:00<00:01, 1657.53it/s]warmup should be done:  15%|        | 449/3000 [00:00<00:01, 1552.85it/s]warmup should be done:  16%|        | 492/3000 [00:00<00:01, 1634.12it/s]warmup should be done:  16%|        | 486/3000 [00:00<00:01, 1622.54it/s]warmup should be done:  16%|        | 490/3000 [00:00<00:01, 1627.05it/s]warmup should be done:  17%|        | 497/3000 [00:00<00:01, 1650.90it/s]warmup should be done:  16%|        | 494/3000 [00:00<00:01, 1638.78it/s]warmup should be done:  16%|        | 488/3000 [00:00<00:01, 1616.13it/s]warmup should be done:  22%|       | 664/3000 [00:00<00:01, 1655.85it/s]warmup should be done:  20%|        | 613/3000 [00:00<00:01, 1583.84it/s]warmup should be done:  22%|       | 656/3000 [00:00<00:01, 1634.46it/s]warmup should be done:  22%|       | 653/3000 [00:00<00:01, 1627.38it/s]warmup should be done:  22%|       | 651/3000 [00:00<00:01, 1630.37it/s]warmup should be done:  22%|       | 663/3000 [00:00<00:01, 1652.56it/s]warmup should be done:  22%|       | 658/3000 [00:00<00:01, 1637.85it/s]warmup should be done:  22%|       | 654/3000 [00:00<00:01, 1629.70it/s]warmup should be done:  28%|       | 830/3000 [00:00<00:01, 1656.03it/s]warmup should be done:  26%|       | 781/3000 [00:00<00:01, 1616.07it/s]warmup should be done:  27%|       | 816/3000 [00:00<00:01, 1627.80it/s]warmup should be done:  27%|       | 821/3000 [00:00<00:01, 1638.62it/s]warmup should be done:  27%|       | 816/3000 [00:00<00:01, 1636.57it/s]warmup should be done:  27%|       | 822/3000 [00:00<00:01, 1637.76it/s]warmup should be done:  28%|       | 829/3000 [00:00<00:01, 1652.05it/s]warmup should be done:  27%|       | 820/3000 [00:00<00:01, 1637.41it/s]warmup should be done:  33%|      | 996/3000 [00:00<00:01, 1654.11it/s]warmup should be done:  33%|      | 985/3000 [00:00<00:01, 1638.03it/s]warmup should be done:  32%|      | 948/3000 [00:00<00:01, 1632.33it/s]warmup should be done:  33%|      | 979/3000 [00:00<00:01, 1626.44it/s]warmup should be done:  33%|      | 981/3000 [00:00<00:01, 1640.47it/s]warmup should be done:  33%|      | 986/3000 [00:00<00:01, 1635.70it/s]warmup should be done:  33%|      | 995/3000 [00:00<00:01, 1651.77it/s]warmup should be done:  33%|      | 985/3000 [00:00<00:01, 1639.27it/s]warmup should be done:  37%|      | 1114/3000 [00:00<00:01, 1640.79it/s]warmup should be done:  38%|      | 1149/3000 [00:00<00:01, 1635.34it/s]warmup should be done:  38%|      | 1146/3000 [00:00<00:01, 1640.40it/s]warmup should be done:  38%|      | 1142/3000 [00:00<00:01, 1623.30it/s]warmup should be done:  39%|      | 1162/3000 [00:00<00:01, 1650.27it/s]warmup should be done:  39%|      | 1161/3000 [00:00<00:01, 1649.12it/s]warmup should be done:  38%|      | 1150/3000 [00:00<00:01, 1630.02it/s]warmup should be done:  38%|      | 1149/3000 [00:00<00:01, 1637.91it/s]warmup should be done:  43%|     | 1282/3000 [00:00<00:01, 1650.45it/s]warmup should be done:  44%|     | 1313/3000 [00:00<00:01, 1636.04it/s]warmup should be done:  44%|     | 1311/3000 [00:00<00:01, 1642.86it/s]warmup should be done:  44%|     | 1305/3000 [00:00<00:01, 1623.71it/s]warmup should be done:  44%|     | 1328/3000 [00:00<00:01, 1650.39it/s]warmup should be done:  44%|     | 1327/3000 [00:00<00:01, 1650.00it/s]warmup should be done:  44%|     | 1314/3000 [00:00<00:01, 1630.31it/s]warmup should be done:  44%|     | 1314/3000 [00:00<00:01, 1640.52it/s]warmup should be done:  48%|     | 1449/3000 [00:00<00:00, 1656.36it/s]warmup should be done:  49%|     | 1476/3000 [00:00<00:00, 1642.94it/s]warmup should be done:  49%|     | 1478/3000 [00:00<00:00, 1637.83it/s]warmup should be done:  49%|     | 1468/3000 [00:00<00:00, 1622.44it/s]warmup should be done:  50%|     | 1494/3000 [00:00<00:00, 1649.70it/s]warmup should be done:  49%|     | 1481/3000 [00:00<00:00, 1647.46it/s]warmup should be done:  49%|     | 1478/3000 [00:00<00:00, 1629.44it/s]warmup should be done:  50%|     | 1493/3000 [00:00<00:00, 1648.15it/s]warmup should be done:  55%|    | 1644/3000 [00:01<00:00, 1642.35it/s]warmup should be done:  54%|    | 1615/3000 [00:01<00:00, 1651.80it/s]warmup should be done:  54%|    | 1631/3000 [00:01<00:00, 1623.59it/s]warmup should be done:  55%|    | 1641/3000 [00:01<00:00, 1638.95it/s]warmup should be done:  55%|    | 1659/3000 [00:01<00:00, 1648.07it/s]warmup should be done:  55%|    | 1649/3000 [00:01<00:00, 1655.52it/s]warmup should be done:  55%|    | 1641/3000 [00:01<00:00, 1627.71it/s]warmup should be done:  55%|    | 1658/3000 [00:01<00:00, 1644.29it/s]warmup should be done:  60%|    | 1810/3000 [00:01<00:00, 1645.69it/s]warmup should be done:  60%|    | 1794/3000 [00:01<00:00, 1622.94it/s]warmup should be done:  60%|    | 1805/3000 [00:01<00:00, 1636.36it/s]warmup should be done:  61%|    | 1824/3000 [00:01<00:00, 1645.35it/s]warmup should be done:  59%|    | 1781/3000 [00:01<00:00, 1645.50it/s]warmup should be done:  61%|    | 1817/3000 [00:01<00:00, 1659.99it/s]warmup should be done:  60%|    | 1805/3000 [00:01<00:00, 1628.57it/s]warmup should be done:  61%|    | 1823/3000 [00:01<00:00, 1641.44it/s]warmup should be done:  66%|   | 1975/3000 [00:01<00:00, 1645.51it/s]warmup should be done:  65%|   | 1957/3000 [00:01<00:00, 1622.02it/s]warmup should be done:  66%|   | 1989/3000 [00:01<00:00, 1644.72it/s]warmup should be done:  66%|   | 1983/3000 [00:01<00:00, 1659.95it/s]warmup should be done:  65%|   | 1946/3000 [00:01<00:00, 1641.67it/s]warmup should be done:  66%|   | 1968/3000 [00:01<00:00, 1624.25it/s]warmup should be done:  66%|   | 1969/3000 [00:01<00:00, 1624.31it/s]warmup should be done:  66%|   | 1988/3000 [00:01<00:00, 1632.01it/s]warmup should be done:  71%|  | 2140/3000 [00:01<00:00, 1645.77it/s]warmup should be done:  72%|  | 2154/3000 [00:01<00:00, 1645.49it/s]warmup should be done:  71%|   | 2120/3000 [00:01<00:00, 1621.28it/s]warmup should be done:  72%|  | 2149/3000 [00:01<00:00, 1656.97it/s]warmup should be done:  70%|   | 2111/3000 [00:01<00:00, 1639.80it/s]warmup should be done:  72%|  | 2153/3000 [00:01<00:00, 1635.88it/s]warmup should be done:  71%|   | 2132/3000 [00:01<00:00, 1604.36it/s]warmup should be done:  71%|   | 2131/3000 [00:01<00:00, 1599.04it/s]warmup should be done:  77%|  | 2305/3000 [00:01<00:00, 1645.29it/s]warmup should be done:  77%|  | 2319/3000 [00:01<00:00, 1644.87it/s]warmup should be done:  76%|  | 2283/3000 [00:01<00:00, 1619.89it/s]warmup should be done:  77%|  | 2315/3000 [00:01<00:00, 1656.24it/s]warmup should be done:  76%|  | 2276/3000 [00:01<00:00, 1640.53it/s]warmup should be done:  77%|  | 2318/3000 [00:01<00:00, 1638.07it/s]warmup should be done:  76%|  | 2293/3000 [00:01<00:00, 1602.33it/s]warmup should be done:  76%|  | 2294/3000 [00:01<00:00, 1607.36it/s]warmup should be done:  82%| | 2470/3000 [00:01<00:00, 1639.37it/s]warmup should be done:  83%| | 2484/3000 [00:01<00:00, 1643.34it/s]warmup should be done:  82%| | 2445/3000 [00:01<00:00, 1618.08it/s]warmup should be done:  83%| | 2481/3000 [00:01<00:00, 1653.79it/s]warmup should be done:  81%| | 2441/3000 [00:01<00:00, 1638.13it/s]warmup should be done:  83%| | 2482/3000 [00:01<00:00, 1637.28it/s]warmup should be done:  82%| | 2455/3000 [00:01<00:00, 1607.39it/s]warmup should be done:  82%| | 2457/3000 [00:01<00:00, 1611.79it/s]warmup should be done:  88%| | 2634/3000 [00:01<00:00, 1639.47it/s]warmup should be done:  88%| | 2649/3000 [00:01<00:00, 1644.80it/s]warmup should be done:  87%| | 2608/3000 [00:01<00:00, 1619.89it/s]warmup should be done:  88%| | 2648/3000 [00:01<00:00, 1655.71it/s]warmup should be done:  87%| | 2605/3000 [00:01<00:00, 1637.94it/s]warmup should be done:  88%| | 2647/3000 [00:01<00:00, 1639.77it/s]warmup should be done:  87%| | 2618/3000 [00:01<00:00, 1614.03it/s]warmup should be done:  87%| | 2621/3000 [00:01<00:00, 1618.38it/s]warmup should be done:  93%|| 2799/3000 [00:01<00:00, 1640.94it/s]warmup should be done:  94%|| 2815/3000 [00:01<00:00, 1646.38it/s]warmup should be done:  92%|| 2771/3000 [00:01<00:00, 1621.57it/s]warmup should be done:  94%|| 2814/3000 [00:01<00:00, 1651.44it/s]warmup should be done:  92%|| 2769/3000 [00:01<00:00, 1636.13it/s]warmup should be done:  94%|| 2811/3000 [00:01<00:00, 1631.70it/s]warmup should be done:  93%|| 2781/3000 [00:01<00:00, 1618.31it/s]warmup should be done:  93%|| 2785/3000 [00:01<00:00, 1622.21it/s]warmup should be done:  99%|| 2966/3000 [00:01<00:00, 1647.61it/s]warmup should be done:  99%|| 2981/3000 [00:01<00:00, 1649.74it/s]warmup should be done:  98%|| 2935/3000 [00:01<00:00, 1626.25it/s]warmup should be done:  98%|| 2933/3000 [00:01<00:00, 1632.71it/s]warmup should be done:  99%|| 2976/3000 [00:01<00:00, 1636.15it/s]warmup should be done:  98%|| 2947/3000 [00:01<00:00, 1629.18it/s]warmup should be done:  98%|| 2951/3000 [00:01<00:00, 1630.78it/s]warmup should be done:  99%|| 2980/3000 [00:01<00:00, 1614.22it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1648.92it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1640.97it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1640.91it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1638.41it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1625.35it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1625.26it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1623.12it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1620.82it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]






warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   6%|         | 168/3000 [00:00<00:01, 1677.77it/s]warmup should be done:   6%|         | 168/3000 [00:00<00:01, 1678.37it/s]warmup should be done:   5%|         | 157/3000 [00:00<00:01, 1569.17it/s]warmup should be done:   5%|         | 162/3000 [00:00<00:01, 1618.85it/s]warmup should be done:   5%|         | 163/3000 [00:00<00:01, 1626.83it/s]warmup should be done:   6%|         | 170/3000 [00:00<00:01, 1693.07it/s]warmup should be done:   6%|         | 168/3000 [00:00<00:01, 1672.82it/s]warmup should be done:   5%|         | 155/3000 [00:00<00:01, 1543.38it/s]warmup should be done:  11%|         | 337/3000 [00:00<00:01, 1683.59it/s]warmup should be done:  11%|         | 336/3000 [00:00<00:01, 1678.72it/s]warmup should be done:  11%|         | 327/3000 [00:00<00:01, 1633.69it/s]warmup should be done:  11%|         | 318/3000 [00:00<00:01, 1593.48it/s]warmup should be done:  11%|         | 336/3000 [00:00<00:01, 1674.43it/s]warmup should be done:  11%|        | 340/3000 [00:00<00:01, 1689.87it/s]warmup should be done:  10%|         | 314/3000 [00:00<00:01, 1557.87it/s]warmup should be done:  11%|         | 324/3000 [00:00<00:01, 1602.52it/s]warmup should be done:  17%|        | 507/3000 [00:00<00:01, 1688.26it/s]warmup should be done:  17%|        | 505/3000 [00:00<00:01, 1680.60it/s]warmup should be done:  16%|        | 483/3000 [00:00<00:01, 1618.32it/s]warmup should be done:  16%|        | 493/3000 [00:00<00:01, 1641.10it/s]warmup should be done:  17%|        | 505/3000 [00:00<00:01, 1679.63it/s]warmup should be done:  17%|        | 509/3000 [00:00<00:01, 1688.66it/s]warmup should be done:  16%|        | 470/3000 [00:00<00:01, 1558.48it/s]warmup should be done:  16%|        | 487/3000 [00:00<00:01, 1613.89it/s]warmup should be done:  22%|       | 674/3000 [00:00<00:01, 1683.56it/s]warmup should be done:  23%|       | 677/3000 [00:00<00:01, 1690.52it/s]warmup should be done:  22%|       | 659/3000 [00:00<00:01, 1647.83it/s]warmup should be done:  22%|       | 647/3000 [00:00<00:01, 1625.20it/s]warmup should be done:  22%|       | 675/3000 [00:00<00:01, 1684.67it/s]warmup should be done:  23%|       | 679/3000 [00:00<00:01, 1691.55it/s]warmup should be done:  21%|        | 627/3000 [00:00<00:01, 1561.56it/s]warmup should be done:  22%|       | 649/3000 [00:00<00:01, 1611.64it/s]warmup should be done:  28%|       | 843/3000 [00:00<00:01, 1685.47it/s]warmup should be done:  28%|       | 847/3000 [00:00<00:01, 1689.08it/s]warmup should be done:  28%|       | 850/3000 [00:00<00:01, 1694.73it/s]warmup should be done:  27%|       | 810/3000 [00:00<00:01, 1619.53it/s]warmup should be done:  26%|       | 784/3000 [00:00<00:01, 1560.82it/s]warmup should be done:  27%|       | 824/3000 [00:00<00:01, 1640.49it/s]warmup should be done:  28%|       | 844/3000 [00:00<00:01, 1677.60it/s]warmup should be done:  27%|       | 813/3000 [00:00<00:01, 1618.44it/s]warmup should be done:  34%|      | 1012/3000 [00:00<00:01, 1685.32it/s]warmup should be done:  34%|      | 1016/3000 [00:00<00:01, 1688.50it/s]warmup should be done:  34%|      | 1020/3000 [00:00<00:01, 1694.94it/s]warmup should be done:  33%|      | 989/3000 [00:00<00:01, 1642.57it/s]warmup should be done:  33%|      | 976/3000 [00:00<00:01, 1630.04it/s]warmup should be done:  34%|      | 1014/3000 [00:00<00:01, 1684.34it/s]warmup should be done:  31%|      | 941/3000 [00:00<00:01, 1558.51it/s]warmup should be done:  32%|      | 975/3000 [00:00<00:01, 1612.64it/s]warmup should be done:  40%|      | 1185/3000 [00:00<00:01, 1688.45it/s]warmup should be done:  39%|      | 1181/3000 [00:00<00:01, 1681.77it/s]warmup should be done:  38%|      | 1154/3000 [00:00<00:01, 1644.81it/s]warmup should be done:  40%|      | 1190/3000 [00:00<00:01, 1692.32it/s]warmup should be done:  37%|      | 1098/3000 [00:00<00:01, 1561.34it/s]warmup should be done:  39%|      | 1184/3000 [00:00<00:01, 1686.62it/s]warmup should be done:  38%|      | 1140/3000 [00:00<00:01, 1628.60it/s]warmup should be done:  38%|      | 1137/3000 [00:00<00:01, 1601.86it/s]warmup should be done:  45%|     | 1355/3000 [00:00<00:00, 1689.60it/s]warmup should be done:  45%|     | 1350/3000 [00:00<00:00, 1681.97it/s]warmup should be done:  44%|     | 1319/3000 [00:00<00:01, 1641.73it/s]warmup should be done:  45%|     | 1361/3000 [00:00<00:00, 1695.13it/s]warmup should be done:  45%|     | 1354/3000 [00:00<00:00, 1688.40it/s]warmup should be done:  43%|     | 1304/3000 [00:00<00:01, 1630.13it/s]warmup should be done:  42%|     | 1255/3000 [00:00<00:01, 1556.24it/s]warmup should be done:  43%|     | 1302/3000 [00:00<00:01, 1615.22it/s]warmup should be done:  51%|     | 1524/3000 [00:00<00:00, 1689.21it/s]warmup should be done:  51%|     | 1519/3000 [00:00<00:00, 1683.01it/s]warmup should be done:  51%|     | 1531/3000 [00:00<00:00, 1695.41it/s]warmup should be done:  49%|     | 1484/3000 [00:00<00:00, 1640.80it/s]warmup should be done:  49%|     | 1469/3000 [00:00<00:00, 1635.39it/s]warmup should be done:  51%|     | 1523/3000 [00:00<00:00, 1686.82it/s]warmup should be done:  47%|     | 1412/3000 [00:00<00:01, 1560.35it/s]warmup should be done:  49%|     | 1468/3000 [00:00<00:00, 1628.91it/s]warmup should be done:  56%|    | 1693/3000 [00:01<00:00, 1688.39it/s]warmup should be done:  56%|    | 1688/3000 [00:01<00:00, 1683.96it/s]warmup should be done:  57%|    | 1701/3000 [00:01<00:00, 1696.52it/s]warmup should be done:  55%|    | 1649/3000 [00:01<00:00, 1643.07it/s]warmup should be done:  56%|    | 1694/3000 [00:01<00:00, 1691.15it/s]warmup should be done:  52%|    | 1569/3000 [00:01<00:00, 1560.32it/s]warmup should be done:  54%|    | 1633/3000 [00:01<00:00, 1627.36it/s]warmup should be done:  54%|    | 1631/3000 [00:01<00:00, 1623.56it/s]warmup should be done:  62%|   | 1862/3000 [00:01<00:00, 1688.16it/s]warmup should be done:  62%|   | 1857/3000 [00:01<00:00, 1685.29it/s]warmup should be done:  62%|   | 1872/3000 [00:01<00:00, 1698.41it/s]warmup should be done:  60%|    | 1814/3000 [00:01<00:00, 1638.57it/s]warmup should be done:  58%|    | 1726/3000 [00:01<00:00, 1559.24it/s]warmup should be done:  62%|   | 1864/3000 [00:01<00:00, 1683.90it/s]warmup should be done:  60%|    | 1796/3000 [00:01<00:00, 1622.92it/s]warmup should be done:  60%|    | 1797/3000 [00:01<00:00, 1634.34it/s]warmup should be done:  68%|   | 2031/3000 [00:01<00:00, 1688.54it/s]warmup should be done:  68%|   | 2026/3000 [00:01<00:00, 1684.83it/s]warmup should be done:  68%|   | 2043/3000 [00:01<00:00, 1699.90it/s]warmup should be done:  66%|   | 1978/3000 [00:01<00:00, 1635.61it/s]warmup should be done:  63%|   | 1884/3000 [00:01<00:00, 1562.71it/s]warmup should be done:  65%|   | 1959/3000 [00:01<00:00, 1624.00it/s]warmup should be done:  68%|   | 2033/3000 [00:01<00:00, 1680.17it/s]warmup should be done:  65%|   | 1961/3000 [00:01<00:00, 1625.91it/s]warmup should be done:  73%|  | 2200/3000 [00:01<00:00, 1686.89it/s]warmup should be done:  73%|  | 2195/3000 [00:01<00:00, 1683.31it/s]warmup should be done:  74%|  | 2213/3000 [00:01<00:00, 1697.49it/s]warmup should be done:  71%|  | 2143/3000 [00:01<00:00, 1639.60it/s]warmup should be done:  68%|   | 2041/3000 [00:01<00:00, 1564.68it/s]warmup should be done:  71%|   | 2124/3000 [00:01<00:00, 1630.12it/s]warmup should be done:  73%|  | 2203/3000 [00:01<00:00, 1683.34it/s]warmup should be done:  71%|   | 2124/3000 [00:01<00:00, 1621.96it/s]warmup should be done:  79%|  | 2369/3000 [00:01<00:00, 1685.98it/s]warmup should be done:  79%|  | 2364/3000 [00:01<00:00, 1683.43it/s]warmup should be done:  79%|  | 2383/3000 [00:01<00:00, 1696.02it/s]warmup should be done:  77%|  | 2309/3000 [00:01<00:00, 1644.18it/s]warmup should be done:  73%|  | 2198/3000 [00:01<00:00, 1563.80it/s]warmup should be done:  76%|  | 2289/3000 [00:01<00:00, 1635.17it/s]warmup should be done:  79%|  | 2373/3000 [00:01<00:00, 1685.70it/s]warmup should be done:  76%|  | 2288/3000 [00:01<00:00, 1626.96it/s]warmup should be done:  85%| | 2539/3000 [00:01<00:00, 1688.31it/s]warmup should be done:  84%| | 2533/3000 [00:01<00:00, 1685.21it/s]warmup should be done:  85%| | 2554/3000 [00:01<00:00, 1697.81it/s]warmup should be done:  82%| | 2474/3000 [00:01<00:00, 1642.32it/s]warmup should be done:  78%|  | 2355/3000 [00:01<00:00, 1564.85it/s]warmup should be done:  82%| | 2453/3000 [00:01<00:00, 1630.67it/s]warmup should be done:  85%| | 2543/3000 [00:01<00:00, 1687.06it/s]warmup should be done:  82%| | 2451/3000 [00:01<00:00, 1621.05it/s]warmup should be done:  90%| | 2709/3000 [00:01<00:00, 1690.00it/s]warmup should be done:  90%| | 2702/3000 [00:01<00:00, 1685.28it/s]warmup should be done:  91%| | 2725/3000 [00:01<00:00, 1700.43it/s]warmup should be done:  84%| | 2513/3000 [00:01<00:00, 1567.55it/s]warmup should be done:  88%| | 2639/3000 [00:01<00:00, 1638.91it/s]warmup should be done:  87%| | 2617/3000 [00:01<00:00, 1631.58it/s]warmup should be done:  90%| | 2712/3000 [00:01<00:00, 1687.52it/s]warmup should be done:  87%| | 2615/3000 [00:01<00:00, 1624.87it/s]warmup should be done:  96%|| 2878/3000 [00:01<00:00, 1689.50it/s]warmup should be done:  96%|| 2871/3000 [00:01<00:00, 1683.65it/s]warmup should be done:  97%|| 2896/3000 [00:01<00:00, 1700.30it/s]warmup should be done:  89%| | 2670/3000 [00:01<00:00, 1566.67it/s]warmup should be done:  93%|| 2804/3000 [00:01<00:00, 1640.62it/s]warmup should be done:  96%|| 2882/3000 [00:01<00:00, 1691.11it/s]warmup should be done:  93%|| 2782/3000 [00:01<00:00, 1636.69it/s]warmup should be done:  93%|| 2778/3000 [00:01<00:00, 1622.55it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1696.55it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1688.40it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1685.02it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1683.35it/s]warmup should be done:  99%|| 2969/3000 [00:01<00:00, 1642.45it/s]warmup should be done:  94%|| 2827/3000 [00:01<00:00, 1564.72it/s]warmup should be done:  98%|| 2947/3000 [00:01<00:00, 1638.58it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1641.28it/s]warmup should be done:  98%|| 2941/3000 [00:01<00:00, 1512.70it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1627.96it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1598.10it/s]warmup should be done: 100%|| 2985/3000 [00:01<00:00, 1566.67it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1562.67it/s]
WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f5b919221f0>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f5b91931040>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f5b91c34e80>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f5b91923250>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f5b91921190>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f5b91c35730>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f5b919301c0>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f5b91c37d30>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
2022-12-11 23:23:51.939924: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f56c302a270 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 23:23:51.940001: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 23:23:51.942376: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f56c3031b10 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 23:23:51.942424: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 23:23:51.949391: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 23:23:51.949737: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 23:23:52.981234: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f56cb029090 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 23:23:52.981295: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 23:23:52.988820: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f56ca830b80 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 23:23:52.988885: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 23:23:52.989400: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f56ca831030 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 23:23:52.989451: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 23:23:52.991896: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 23:23:52.996862: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 23:23:52.999670: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 23:23:53.064583: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f56ca7966e0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 23:23:53.064648: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 23:23:53.066074: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f56ce838050 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 23:23:53.066119: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 23:23:53.074941: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 23:23:53.074984: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 23:23:53.089952: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f56c70311b0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 23:23:53.090016: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 23:23:53.099436: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 23:23:59.245548: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 23:23:59.401010: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 23:23:59.651727: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 23:23:59.722732: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 23:23:59.877410: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 23:23:59.900849: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 23:23:59.904734: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 23:23:59.985113: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
[HCTR][23:24:59.242][ERROR][RK0][main]: replica 7 reaches 1000, calling init pre replica
[HCTR][23:24:59.242][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][23:24:59.248][ERROR][RK0][main]: coll ps creation done
[HCTR][23:24:59.248][ERROR][RK0][main]: replica 7 waits for coll ps creation barrier
[HCTR][23:24:59.390][ERROR][RK0][main]: replica 5 reaches 1000, calling init pre replica
[HCTR][23:24:59.390][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][23:24:59.397][ERROR][RK0][main]: coll ps creation done
[HCTR][23:24:59.397][ERROR][RK0][main]: replica 5 waits for coll ps creation barrier
[HCTR][23:24:59.444][ERROR][RK0][tid #140010892289792]: replica 0 reaches 1000, calling init pre replica
[HCTR][23:24:59.444][ERROR][RK0][tid #140010892289792]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][23:24:59.449][ERROR][RK0][tid #140010892289792]: coll ps creation done
[HCTR][23:24:59.449][ERROR][RK0][tid #140010892289792]: replica 0 waits for coll ps creation barrier
[HCTR][23:24:59.482][ERROR][RK0][tid #140011454306048]: replica 6 reaches 1000, calling init pre replica
[HCTR][23:24:59.482][ERROR][RK0][tid #140011454306048]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][23:24:59.489][ERROR][RK0][tid #140010967791360]: replica 1 reaches 1000, calling init pre replica
[HCTR][23:24:59.489][ERROR][RK0][tid #140010967791360]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][23:24:59.490][ERROR][RK0][tid #140011454306048]: coll ps creation done
[HCTR][23:24:59.490][ERROR][RK0][tid #140011454306048]: replica 6 waits for coll ps creation barrier
[HCTR][23:24:59.496][ERROR][RK0][tid #140010967791360]: coll ps creation done
[HCTR][23:24:59.496][ERROR][RK0][tid #140010967791360]: replica 1 waits for coll ps creation barrier
[HCTR][23:24:59.571][ERROR][RK0][main]: replica 4 reaches 1000, calling init pre replica
[HCTR][23:24:59.571][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][23:24:59.576][ERROR][RK0][main]: coll ps creation done
[HCTR][23:24:59.576][ERROR][RK0][main]: replica 4 waits for coll ps creation barrier
[HCTR][23:24:59.970][ERROR][RK0][tid #140011856959232]: replica 2 reaches 1000, calling init pre replica
[HCTR][23:24:59.970][ERROR][RK0][tid #140011856959232]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][23:24:59.975][ERROR][RK0][tid #140011856959232]: coll ps creation done
[HCTR][23:24:59.975][ERROR][RK0][tid #140011856959232]: replica 2 waits for coll ps creation barrier
[HCTR][23:24:59.988][ERROR][RK0][main]: replica 3 reaches 1000, calling init pre replica
[HCTR][23:24:59.988][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][23:24:59.996][ERROR][RK0][main]: coll ps creation done
[HCTR][23:24:59.996][ERROR][RK0][main]: replica 3 waits for coll ps creation barrier
[HCTR][23:24:59.997][ERROR][RK0][tid #140010892289792]: replica 0 preparing frequency
[HCTR][23:25:00.857][ERROR][RK0][tid #140010892289792]: replica 0 preparing frequency done
[HCTR][23:25:00.891][ERROR][RK0][tid #140010892289792]: replica 0 calling init per replica
[HCTR][23:25:00.891][ERROR][RK0][tid #140010967791360]: replica 1 calling init per replica
[HCTR][23:25:00.891][ERROR][RK0][main]: replica 3 calling init per replica
[HCTR][23:25:00.891][ERROR][RK0][main]: replica 7 calling init per replica
[HCTR][23:25:00.891][ERROR][RK0][tid #140011856959232]: replica 2 calling init per replica
[HCTR][23:25:00.891][ERROR][RK0][tid #140011454306048]: replica 6 calling init per replica
[HCTR][23:25:00.891][ERROR][RK0][main]: replica 5 calling init per replica
[HCTR][23:25:00.891][ERROR][RK0][main]: replica 4 calling init per replica
[HCTR][23:25:00.891][ERROR][RK0][tid #140010892289792]: Calling build_v2
[HCTR][23:25:00.891][ERROR][RK0][tid #140010967791360]: Calling build_v2
[HCTR][23:25:00.891][ERROR][RK0][main]: Calling build_v2
[HCTR][23:25:00.891][ERROR][RK0][main]: Calling build_v2
[HCTR][23:25:00.891][ERROR][RK0][tid #140011856959232]: Calling build_v2
[HCTR][23:25:00.891][ERROR][RK0][tid #140011454306048]: Calling build_v2
[HCTR][23:25:00.891][ERROR][RK0][main]: Calling build_v2
[HCTR][23:25:00.891][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][23:25:00.891][ERROR][RK0][tid #140010892289792]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][23:25:00.891][ERROR][RK0][main]: Calling build_v2
[HCTR][23:25:00.891][ERROR][RK0][tid #140010967791360]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][23:25:00.891][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][23:25:00.891][ERROR][RK0][tid #140011856959232]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][23:25:00.891][ERROR][RK0][tid #140011454306048]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][23:25:00.891][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][23:25:00.891][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[2022-12-11 23:25:00.895630: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:178] v100x8, slow pcie
[[2022-12-11 23:25:00.895705: E2022-12-11 23:25:00 ./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc895678:: [196E]  assigning 0 to cpu/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
:178] v100x8, slow pcie
2022-12-11 23:25:00.[895725[[: 2022-12-11 23:25:002022-12-11 23:25:002022-12-11 23:25:00E... [895790: 895773895790/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE: 2022-12-11 23:25:00: : E[.E178/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc 895815 ] 2022-12-11 23:25:00:/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[v100x8, slow pcie.212:E:
895863] 2022-12-11 23:25:00178 196: [build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8[.] 2022-12-11 23:25:00/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] E
2022-12-11 23:25:00895908v100x8, slow pcie.:assigning 0 to cpu .: 
895952178
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[895967E: ] :[2022-12-11 23:25:00:  Ev100x8, slow pcie1782022-12-11 23:25:00.E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc 
] .896027 [:/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccv100x8, slow pcie896041: [/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-11 23:25:00178:
: E2022-12-11 23:25:00:.] 178E [.196896089v100x8, slow pcie]  /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-11 23:25:00896100] : 
v100x8, slow pcie/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:.: assigning 0 to cpuE
:[213896153E
 1962022-12-11 23:25:00] [:  /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] .remote time is 8.684212022-12-11 23:25:00E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:assigning 0 to cpu896220
. :212
: [896243/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc196[] E2022-12-11 23:25:00: :] 2022-12-11 23:25:00build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8 .E196[assigning 0 to cpu.
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc896297 ] 2022-12-11 23:25:00
896307:: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccassigning 0 to cpu.[: 196E:
8963442022-12-11 23:25:00E]  196[: . assigning 0 to cpu/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] 2022-12-11 23:25:00E896393/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
[:assigning 0 to cpu. : :2022-12-11 23:25:00212
896431/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE[214.] : : 2022-12-11 23:25:00] 896464build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8E212/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.cpu time is 97.0588: 
 ] :[896501
E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccbuild asymm link desc with 8X Tesla V100-SXM2-32GB out of 82132022-12-11 23:25:00: [ :
] .E2022-12-11 23:25:00/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc212remote time is 8.68421896560 .[:] 
: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc8965872022-12-11 23:25:00212build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8E:[: .] 
 2122022-12-11 23:25:00E896627build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] .[ : 
:build asymm link desc with 8X Tesla V100-SXM2-32GB out of 88966552022-12-11 23:25:00/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE212
: .[: ] E8966912022-12-11 23:25:00[213/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccbuild asymm link desc with 8X Tesla V100-SXM2-32GB out of 8 : .2022-12-11 23:25:00] :
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE896724.remote time is 8.68421213: : 896743[
] 214/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE: 2022-12-11 23:25:00remote time is 8.68421] :[ E.
cpu time is 97.05882132022-12-11 23:25:00[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc 896796
] .2022-12-11 23:25:00:/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: remote time is 8.68421896822.213:E
: 896860] 213 E: [remote time is 8.68421] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc E2022-12-11 23:25:00
remote time is 8.68421:/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc .
213[:/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc896915] 2022-12-11 23:25:00[214:: remote time is 8.68421.2022-12-11 23:25:00] 214E
896955.cpu time is 97.0588]  : 896970[
cpu time is 97.0588/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE: 2022-12-11 23:25:00
: E.214/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc 897016] :/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: cpu time is 97.0588214:E
] 214 cpu time is 97.0588] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
cpu time is 97.0588:
214] cpu time is 97.0588
[2022-12-11 23:26:19.843696: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:118] solver created. now build & solve
[2022-12-11 23:26:19.883855: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:123] solver built. now solve
[2022-12-11 23:26:19.883921: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:919] num_cached_nodes = 2999999
[2022-12-11 23:26:20.  2326: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:127] solver solved
[2022-12-11 23:26:20.  2416: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:195] 0 solved master
[2022-12-11 23:26:20.139452: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 0 solved
[2022-12-11 23:26:20.139492: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 0 initing device 0
[2022-12-11 23:26:20.139936: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 23:26:20.140957: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 23:26:20.141816: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 23:26:20.154727: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 5 solved
[[2022-12-11 23:26:202022-12-11 23:26:20..154790154771: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc::205202] ] worker 0 thread 5 initing device 52 solved

[2022-12-11 23:26:20.154893: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 2 initing device 2
[2022-12-11 23:26:20.155261: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 23:26:20.155316: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 23:26:20.[1556532022-12-11 23:26:20: .E155673 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.ccE: 202/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc] :6 solved202
] 3 solved
[2022-12-11 23:26:20.155740[: 2022-12-11 23:26:20E. 155747/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc: :E205 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.ccworker 0 thread 6 initing device 6:
205] worker 0 thread 3 initing device 3
[[2022-12-11 23:26:202022-12-11 23:26:20..156156156156: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::19801980] ] eager alloc mem 381.47 MBeager alloc mem 381.47 MB

[2022-12-11 23:26:20.157509: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc[:2022-12-11 23:26:20202.] 1575354 solved: 
E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:[2022022-12-11 23:26:20] .1575967 solved: 
E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:[2052022-12-11 23:26:20] .worker 0 thread 4 initing device 4157634
: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 7 initing device 7
[2022-12-11 23:26:20.157904: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 23:26:20.158034: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 23:26:20.158059: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 23:26:20.158165: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 23:26:20.158706: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 23:26:20.158770: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 23:26:20.159081: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 1 solved
[2022-12-11 23:26:20.159144: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 1 initing device 1
[2022-12-11 23:26:20.159547: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 23:26:20.162354: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 23:26:20.162560: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 23:26:20.162688: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 23:26:20.162760: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 23:26:20.162811: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 23:26:20.163312: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 23:26:20.163401: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 23:26:20.167033: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 23:26:20.167144: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 23:26:20.167359: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 23:26:20.218146: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 2.00 Bytes
[2022-12-11 23:26:20.218542: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1024.00 Bytes
[2022-12-11 23:26:20.236200: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-11 23:26:20.236277: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 2
[2022-12-11 23:26:20.236321: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-11 23:26:20.239986: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 11.83 MB
[2022-12-11 23:26:20.240642: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:26:20.241562: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:26:20.241650: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 23:26:20.242341: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 23:26:20.242381: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1.43 GB
[2022-12-11 23:26:20.245426: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 2.00 Bytes
[2022-12-11 23:26:20.245742: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1024.00 Bytes
[2022-12-11 23:26:20.250477: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 2.00 Bytes
[2022-12-11 23:26:20.250581: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-11 23:26:20.250644: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 2
[2022-12-11 23:26:20.250685: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-11 23:26:20.250798: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1024.00 Bytes
[2022-12-11 23:26:20.251440: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 11.83 MB
[2022-12-11 23:26:20.251857: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:26:20.252801: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:26:20.252883: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 23:26:20.253544: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 23:26:20.253581: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1.43 GB
[[[[[2022-12-11 23:26:202022-12-11 23:26:202022-12-11 23:26:202022-12-11 23:26:202022-12-11 23:26:20.....255299255299255299255299255299: : : : : EEEEE     /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:::::19801980198019801980] ] ] ] ] eager alloc mem 2.00 Byteseager alloc mem 2.00 Byteseager alloc mem 2.00 Byteseager alloc mem 2.00 Byteseager alloc mem 2.00 Bytes




[2022-12-11 23:26:20[.[2022-12-11 23:26:202557372022-12-11 23:26:20[.[: .2022-12-11 23:26:202557402022-12-11 23:26:20E255741.: . : 255755E255757/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE:  : : E/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE1980/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu : ] :/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu1980/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cueager alloc mem 1024.00 Bytes1980:] :
] 1980eager alloc mem 1024.00 Bytes1980eager alloc mem 1024.00 Bytes] 
] 
eager alloc mem 1024.00 Byteseager alloc mem 1024.00 Bytes

[2022-12-11 23:26:20.257111: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-11 23:26:20.257173: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 2
[2022-12-11 23:26:20.257213: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-11 23:26:20.257968: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 11.83 MB
[2022-12-11 23:26:20.258350: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:26:20.259295: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:26:20.259383: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 23:26:20.260057: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 23:26:20.260096: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1.43 GB
[2022-12-11 23:26:20.262030: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-11 23:26:20.262104: [E2022-12-11 23:26:20 ./hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc262098:: 638E]  eager release cuda mem 2/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc
:638] eager release cuda mem 1024
[2022-12-11 23:26:20.262168: E [/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc2022-12-11 23:26:20:.638262180] : [eager release cuda mem 400000000E2022-12-11 23:26:20
 ./hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc262184:: 638E]  eager release cuda mem 2/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc
:638] eager release cuda mem 1024
[2022-12-11 23:26:20[[.2022-12-11 23:26:202022-12-11 23:26:20262255..: 262267262257E: :  EE/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc  :/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc638::] 638638eager release cuda mem 400000000] ] 
eager release cuda mem 1024eager release cuda mem 2

[[2022-12-11 23:26:202022-12-11 23:26:20..262362262364: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::638638] ] eager release cuda mem 2eager release cuda mem 400000000

[2022-12-11 23:26:20.262427: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-11 23:26:20.263286: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 11.83 MB
[2022-12-11 23:26:20.263985: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-11 23:26:20.264048: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 2
[2022-12-11 23:26:20.264089: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-11 23:26:20.264138: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 11.83 MB
[2022-12-11 23:26:20.264638: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 11.83 MB
[2022-12-11 23:26:20.265143: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 11.83 MB
[2022-12-11 23:26:20.265847: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:26:20.266256: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 11.83 MB
[2022-12-11 23:26:20.266678: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:26:20.266814: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:26:20.266889: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:26:20.[2669302022-12-11 23:26:20: .E266931 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE: 1980/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu] :eager alloc mem 25.25 KB1980
] eager alloc mem 611.00 KB
[2022-12-11 23:26:20.267223: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:26:20.267629: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:26:20.267658: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 23:26:20.267704: [E2022-12-11 23:26:20 ./hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu267712:: 1980E]  eager alloc mem 1.43 GB/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu
:1980] eager alloc mem 25.25 KB
[2022-12-11 23:26:20.267842: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:26:20.267895: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:26:20.267926: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 23:26:20.267976: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 23:26:20.268176: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:26:20.268259: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 23:26:20.268410: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 23:26:20.268457: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1.43 GB
[2022-12-11 23:26:20.268595: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 23:26:20.268634: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu[:2022-12-11 23:26:201980.] 268645eager alloc mem 1.43 GB: 
E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 23:26:20.268695: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1.43 GB
[2022-12-11 23:26:20.268930: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 23:26:20.268971: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1.43 GB
[[2022-12-11 23:26:20[[[2022-12-11 23:26:20[.2022-12-11 23:26:20[2022-12-11 23:26:202022-12-11 23:26:20.[2022-12-11 23:26:20550867.2022-12-11 23:26:20..5508802022-12-11 23:26:20.: 550891.550886550891: .550923E: 550912: : E550931:  E: EE : E/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu E  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE :/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu: /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu1980:/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::1980/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:] 1980:19801980] :1980eager alloc mem 611.00 KB] 1980] ] eager alloc mem 611.00 KB1980] 
eager alloc mem 611.00 KB] eager alloc mem 611.00 KBeager alloc mem 611.00 KB
] eager alloc mem 611.00 KB
eager alloc mem 611.00 KB

eager alloc mem 611.00 KB


[[2022-12-11 23:26:202022-12-11 23:26:20..552032552034[: [: 2022-12-11 23:26:20E2022-12-11 23:26:20E[[. .[[ 2022-12-11 23:26:202022-12-11 23:26:20552046/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc5520482022-12-11 23:26:202022-12-11 23:26:20/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc..: :: ..:552073552068E638E552076552077638: :  ]  : : ] EE/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cceager release cuda mem 625663/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccEEeager release cuda mem 625663  :
:  
/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc638638/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::] ] ::638638eager release cuda mem 625663eager release cuda mem 625663638638] ] 
[
] ] eager release cuda mem 625663[eager release cuda mem 6256632022-12-11 23:26:20eager release cuda mem 625663eager release cuda mem 625663
2022-12-11 23:26:20
.

.552295[552306: [2022-12-11 23:26:20: E2022-12-11 23:26:20.E .552362 /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu552373[[: /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu[:[: 2022-12-11 23:26:202022-12-11 23:26:20E:2022-12-11 23:26:2019802022-12-11 23:26:20E.. 1980.] . 552412552413/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu] 552419eager alloc mem 611.00 KB552421/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu: : :eager alloc mem 611.00 KB: 
: :EE1980
EE1980  ]   ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cueager alloc mem 611.00 KB/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cueager alloc mem 611.00 KB::
::
1980198019801980] ] ] ] eager alloc mem 611.00 KBeager alloc mem 611.00 KBeager alloc mem 611.00 KBeager alloc mem 611.00 KB



[2022-12-11 23:26:20.[5533112022-12-11 23:26:20: .E553319 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccE: 638/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc] :eager release cuda mem 625663638
] eager release cuda mem 625663
[2022-12-11 23:26:20.553362: E[ 2022-12-11 23:26:20/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc.:553375638: [] E[2022-12-11 23:26:20eager release cuda mem 625663[ [2022-12-11 23:26:20.
2022-12-11 23:26:20/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc2022-12-11 23:26:20[.553402.:[.2022-12-11 23:26:20553408: 5534146382022-12-11 23:26:20553418.: [E: ] .: 553441E2022-12-11 23:26:20 Eeager release cuda mem 625663553451E:  ./hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu 
:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu553492:/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccE/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc :: 1980: :/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc1980E] 638/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc[638:]  eager alloc mem 611.00 KB] :2022-12-11 23:26:20] 638eager alloc mem 611.00 KB/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu
eager release cuda mem 625663638.eager release cuda mem 625663] 
:
] 553630
eager release cuda mem 6256631980eager release cuda mem 625663: 
] 
Eeager alloc mem 611.00 KB 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:[19802022-12-11 23:26:20] [.eager alloc mem 611.00 KB2022-12-11 23:26:20553780[
.: 2022-12-11 23:26:20553789E.:  553804E/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:  [:E/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu2022-12-11 23:26:201980 :.] /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu1980553834eager alloc mem 611.00 KB:] : 
1980eager alloc mem 611.00 KBE] 
 eager alloc mem 611.00 KB/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu
:1980] eager alloc mem 611.00 KB
[2022-12-11 23:26:20.554426: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:26:20.554482: E [/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc2022-12-11 23:26:20:.638554494] [: eager release cuda mem 6256632022-12-11 23:26:20E
. 554506/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu: :E[1980 2022-12-11 23:26:20] /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc.eager alloc mem 611.00 KB[:554549
2022-12-11 23:26:20638: .] E[554577[eager release cuda mem 625663 2022-12-11 23:26:20: 2022-12-11 23:26:20
/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc.E.[:554606[ 5546142022-12-11 23:26:20638: 2022-12-11 23:26:20/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu: .[] E.:1980E5546492022-12-11 23:26:20eager release cuda mem 625663 554657]  : .
/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc: eager alloc mem 611.00 KB/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccE554694:E
: : 638 638/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccE] /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc] : eager release cuda mem 625663[:eager release cuda mem 625663638/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu
2022-12-11 23:26:20638
] :.] eager release cuda mem 6256631980554816eager release cuda mem 625663
] : 
eager alloc mem 611.00 KBE[
[ 2022-12-11 23:26:202022-12-11 23:26:20/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu..:[55488955489819802022-12-11 23:26:20: : ] .EEeager alloc mem 611.00 KB[554926  
2022-12-11 23:26:20: /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu.E::554957 19801980: /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu] ] :eager alloc mem 611.00 KBEeager alloc mem 611.00 KB1980
 
] /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cueager alloc mem 611.00 KB:
1980] eager alloc mem 611.00 KB
[2022-12-11 23:26:20.555322: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:26:20.555392: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:26:20.555526: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:26:20.555594: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:26:20.555636: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:26:20.555704: E [/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu2022-12-11 23:26:20:1980.] 555714eager alloc mem 611.00 KB: 
E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663[
2022-12-11 23:26:20.555768: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:26:20.[5558082022-12-11 23:26:20[: [.2022-12-11 23:26:20E2022-12-11 23:26:20555805.[ .: 5558172022-12-11 23:26:20/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc555822E: .::  E555843638E/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc : ]  :/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccEeager release cuda mem 625663/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu638: 
:] 638/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu1980eager release cuda mem 625663] :] 
eager release cuda mem 6256631980eager alloc mem 611.00 KB
] [
eager alloc mem 611.00 KB2022-12-11 23:26:20
.555995: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:26:20.556050: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] [eager alloc mem 611.00 KB2022-12-11 23:26:20
.556065: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:26:20.556181: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:26:20.556263: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:26:20.556342: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:26:20.556409: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:26:20.556484: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:26:20.556551: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:26:20.556731: [E2022-12-11 23:26:20 ./hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc556744:: 638E] [ eager release cuda mem 6256632022-12-11 23:26:20/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc
.:[5567846382022-12-11 23:26:20: ] .Eeager release cuda mem 625663556809 
: /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccE: 638/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc] :eager release cuda mem 625663638[
] 2022-12-11 23:26:20eager release cuda mem 625663.
556852[[: 2022-12-11 23:26:202022-12-11 23:26:20E..[ 5568795568722022-12-11 23:26:20/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu[: : .:2022-12-11 23:26:20EE5569051980.  : ] 556921/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccEeager alloc mem 611.00 KB: :: 
E1980638/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu ] ] :/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu[eager alloc mem 611.00 KBeager release cuda mem 6256631980:2022-12-11 23:26:20

] 1980.eager alloc mem 611.00 KB] 557015
eager alloc mem 611.00 KB: 
E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[[2022-12-11 23:26:202022-12-11 23:26:20[..2022-12-11 23:26:20557138557145.: : 557155EE:   E/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu ::/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc19801980:] ] 638eager alloc mem 611.00 KBeager alloc mem 611.00 KB] 

eager release cuda mem 625663
[2022-12-11 23:26:20.557295: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:26:20.557330: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:26:20.557362: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:26:20.557729: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:26:20.557784: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:[6382022-12-11 23:26:20] .[eager release cuda mem 6256635578032022-12-11 23:26:20
: [.E2022-12-11 23:26:20557814 .: /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu557833E::  [1980E/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc2022-12-11 23:26:20]  :.eager alloc mem 611.00 KB/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc638557880
:] : 638eager release cuda mem 625663E] 
 eager release cuda mem 625663/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu
:1980] eager alloc mem 611.00 KB
[2022-12-11 23:26:20.557989[: 2022-12-11 23:26:20E. 557996/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu: [:E2022-12-11 23:26:201980 .] /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu558008eager alloc mem 611.00 KB:: 
1980E] [ eager alloc mem 611.00 KB[2022-12-11 23:26:20/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc
2022-12-11 23:26:20.:.558056638558078[: ] : 2022-12-11 23:26:20Eeager release cuda mem 625663E. 
 558113/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc: :[:E6382022-12-11 23:26:20638 ] .] /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cceager release cuda mem 625663558174eager release cuda mem 625663:
: 
638E]  eager release cuda mem 625663[/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc
2022-12-11 23:26:20:.638558254] : [eager release cuda mem 12399996E2022-12-11 23:26:20
 ./hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc558279:: 638[E] 2022-12-11 23:26:20 eager release cuda mem 12399996./hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc
558311:: 638E]  eager release cuda mem 12399996/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu
:1980] eager alloc mem 611.00 KB
[2022-12-11 23:26:20.558652: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[[2022-12-11 23:26:202022-12-11 23:26:20..558689558688: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::638638] ] eager release cuda mem 12399996eager release cuda mem 625663

[[2022-12-11 23:26:202022-12-11 23:26:20..558786558789: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::638638] ] eager release cuda mem 625663eager release cuda mem 12399996

[[2022-12-11 23:26:202022-12-11 23:26:20..558844558844: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::638638] ] eager release cuda mem 12399996eager release cuda mem 625663

[2022-12-11 23:26:20.558901: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 12399996
[2022-12-11 23:26:20.559169: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:26:20.559226: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 12399996
[2022-12-11 23:26:20.564875: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 2999999 / 100000000 nodes ( 3.00 %~3.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 97000001 / 100000000 nodes ( 97.00 %) | 1.43 GB | 0.409619 secs 
[2022-12-11 23:26:20.565281: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 2999999 / 100000000 nodes ( 3.00 %~3.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 97000001 / 100000000 nodes ( 97.00 %) | 1.43 GB | 0.407253 secs 
[2022-12-11 23:26:20.565691: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 2999999 / 100000000 nodes ( 3.00 %~3.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 97000001 / 100000000 nodes ( 97.00 %) | 1.43 GB | 0.409541 secs 
[2022-12-11 23:26:20.566149: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 2999999 / 100000000 nodes ( 3.00 %~3.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 97000001 / 100000000 nodes ( 97.00 %) | 1.43 GB | 0.426221 secs 
[2022-12-11 23:26:20.566699: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 2999999 / 100000000 nodes ( 3.00 %~3.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 97000001 / 100000000 nodes ( 97.00 %) | 1.43 GB | 0.408646 secs 
[2022-12-11 23:26:20.571318: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 2999999 / 100000000 nodes ( 3.00 %~3.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 97000001 / 100000000 nodes ( 97.00 %) | 1.43 GB | 0.416013 secs 
[2022-12-11 23:26:20.571858: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 2999999 / 100000000 nodes ( 3.00 %~3.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 97000001 / 100000000 nodes ( 97.00 %) | 1.43 GB | 0.415709 secs 
[2022-12-11 23:26:20.572279: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 2999999 / 100000000 nodes ( 3.00 %~3.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 97000001 / 100000000 nodes ( 97.00 %) | 1.43 GB | 0.412739 secs 
[HCTR][23:26:20.572][ERROR][RK0][tid #140011856959232]: replica 2 calling init per replica done, doing barrier
[HCTR][23:26:20.572][ERROR][RK0][main]: replica 3 calling init per replica done, doing barrier
[HCTR][23:26:20.572][ERROR][RK0][main]: replica 5 calling init per replica done, doing barrier
[HCTR][23:26:20.572][ERROR][RK0][main]: replica 7 calling init per replica done, doing barrier
[HCTR][23:26:20.572][ERROR][RK0][tid #140010892289792]: replica 0 calling init per replica done, doing barrier
[HCTR][23:26:20.572][ERROR][RK0][main]: replica 4 calling init per replica done, doing barrier
[HCTR][23:26:20.572][ERROR][RK0][tid #140011454306048]: replica 6 calling init per replica done, doing barrier
[HCTR][23:26:20.572][ERROR][RK0][tid #140010967791360]: replica 1 calling init per replica done, doing barrier
[HCTR][23:26:20.572][ERROR][RK0][main]: replica 4 calling init per replica done, doing barrier done
[HCTR][23:26:20.572][ERROR][RK0][tid #140011454306048]: replica 6 calling init per replica done, doing barrier done
[HCTR][23:26:20.572][ERROR][RK0][tid #140011856959232]: replica 2 calling init per replica done, doing barrier done
[HCTR][23:26:20.572][ERROR][RK0][tid #140010892289792]: replica 0 calling init per replica done, doing barrier done
[HCTR][23:26:20.572][ERROR][RK0][tid #140010967791360]: replica 1 calling init per replica done, doing barrier done
[HCTR][23:26:20.572][ERROR][RK0][main]: replica 5 calling init per replica done, doing barrier done
[HCTR][23:26:20.572][ERROR][RK0][main]: replica 3 calling init per replica done, doing barrier done
[HCTR][23:26:20.572][ERROR][RK0][main]: replica 7 calling init per replica done, doing barrier done
[HCTR][23:26:20.572][ERROR][RK0][main]: init per replica done
[HCTR][23:26:20.572][ERROR][RK0][tid #140011454306048]: init per replica done
[HCTR][23:26:20.572][ERROR][RK0][tid #140011856959232]: init per replica done
[HCTR][23:26:20.572][ERROR][RK0][tid #140010967791360]: init per replica done
[HCTR][23:26:20.572][ERROR][RK0][main]: init per replica done
[HCTR][23:26:20.572][ERROR][RK0][main]: init per replica done
[HCTR][23:26:20.572][ERROR][RK0][main]: init per replica done
[HCTR][23:26:20.575][ERROR][RK0][tid #140010892289792]: init per replica done
