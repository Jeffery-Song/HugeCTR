2022-12-12 01:00:42.710333: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:42.718039: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:42.724951: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:42.730291: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:42.736447: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:42.748244: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:42.754476: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:42.769410: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:42.818351: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:42.828483: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:42.832180: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:42.833228: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:42.834586: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:42.836941: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:42.837605: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:42.838029: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:42.839243: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:42.839388: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:42.840731: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:42.840791: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:42.842150: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:42.842222: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:42.843678: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:42.843773: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:42.845082: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:42.845353: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:42.846359: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:42.846872: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:42.847783: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:42.848406: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:42.849152: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:42.850501: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:42.852246: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:42.853228: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:42.854290: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:42.855405: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:42.856535: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:42.858168: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:42.860329: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:42.862500: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:42.868927: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 01:00:42.877864: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:42.878703: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:42.880284: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:42.881222: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:42.882880: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:42.883057: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:42.883651: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:42.885697: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:42.885873: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:42.888428: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:42.888693: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:42.891203: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:42.891531: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:42.893797: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:42.893967: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:42.894566: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:42.897100: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:42.897573: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:42.897988: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:42.900338: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:42.901293: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:42.903044: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:42.904008: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:42.905521: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:42.908371: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:42.910448: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:42.924021: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:42.925361: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:42.926449: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:42.928843: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:42.928976: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:42.929332: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:42.938038: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:42.938229: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:42.938291: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:42.938664: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:42.942366: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:42.942404: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:42.942634: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:42.943168: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:42.943521: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:42.944601: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:42.950924: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:42.979616: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:42.979920: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:42.980295: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:42.980889: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:42.981333: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:42.982192: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:42.983078: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:42.984712: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:42.985515: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:42.986154: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:42.986878: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:42.987469: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:42.987852: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:42.988308: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:42.990838: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:42.991935: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:42.992405: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:42.993062: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:42.994414: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:42.994510: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:42.994797: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:42.996538: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:42.998291: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:42.999089: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:42.999541: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:42.999577: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:43.000105: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:43.000242: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:43.002910: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:43.005882: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:43.005952: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:43.006228: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:43.006451: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:43.006919: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:43.008371: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:43.010660: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:43.011091: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:43.011191: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:43.011934: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:43.012664: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:43.014667: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:43.014980: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:43.015888: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:43.016326: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:43.017968: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:43.018347: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:43.019743: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:43.020899: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:43.021281: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:43.022523: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:43.024863: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:43.025138: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:43.026602: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:43.027858: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:43.028078: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:43.029677: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:43.030971: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:43.031167: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:43.033629: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:43.033930: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:43.035840: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:43.036118: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:43.037152: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 01:00:43.038133: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:43.038331: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:43.041969: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:43.042488: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:43.044585: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:43.044738: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:43.046808: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:43.047089: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:43.047284: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:43.049583: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:43.049641: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:43.049872: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:43.050109: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:43.052933: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:43.053145: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:43.053392: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:43.054083: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:43.054879: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:43.058482: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:43.058480: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:43.058560: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:43.058810: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:43.059574: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:43.062672: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:43.063177: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:43.063347: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:43.063684: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:43.064118: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:43.064303: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:43.067737: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:43.068163: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:43.068312: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:43.068640: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:43.069194: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:43.069411: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:43.072645: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:43.073028: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:43.073244: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:43.074424: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:43.077561: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:43.077842: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 01:00:43.077842: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 01:00:43.078514: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:43.079047: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:43.079525: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:43.082111: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:43.082741: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:43.083049: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:43.083689: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:43.086142: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:43.087162: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:43.087438: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:43.087446: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:43.087451: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:43.087702: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:43.091676: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:43.092822: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:43.093050: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:43.093089: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:43.093130: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:43.093265: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:43.096535: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:43.098175: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:43.098382: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:43.098475: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:43.098562: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:43.098615: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:43.102884: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:43.103748: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:43.104115: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:43.104231: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:43.110437: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:43.112116: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:43.112448: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:43.112689: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:43.116195: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:43.117150: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:43.117385: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:43.117601: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:43.149664: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:43.150536: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:43.150805: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:43.151046: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:43.182701: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:43.184398: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:43.184561: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:43.184854: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:43.190102: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:43.191526: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:43.191965: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:43.193071: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:43.198014: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:43.199020: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:43.199845: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:43.200558: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:43.203506: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:43.204958: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:43.206457: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:43.207750: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:43.210161: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:43.210438: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:43.211667: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:43.213987: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:43.216220: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:43.216647: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:43.217645: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:43.218613: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:43.221509: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:43.230918: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:43.231712: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:43.234546: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:43.237010: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:43.237453: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:43.238564: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:43.240337: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:43.243218: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:43.243423: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:43.244271: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:43.245829: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:43.305201: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:43.305562: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:43.307995: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:43.310684: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 01:00:43.310970: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:43.311293: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:43.317382: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 01:00:43.317531: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:43.321318: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 01:00:43.321345: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:43.325534: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 01:00:43.326325: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:43.327089: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:43.330961: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:43.332977: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:43.333509: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:43.334134: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:43.339306: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:43.343110: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:43.344150: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:43.347714: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:43.349411: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:44.297886: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:44.299190: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:44.300699: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:44.301748: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 01:00:44.301805: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-12 01:00:44.320378: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:44.321790: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:44.322862: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:44.324296: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:44.325392: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:44.326845: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-12 01:00:44.369306: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 01:00:44.369506: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 01:00:44.399729: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12340
2022-12-12 01:00:44.500542: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:44.501869: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:44.503192: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:44.504443: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 01:00:44.504497: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-12 01:00:44.522186: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:44.523378: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:44.524485: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:44.525965: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:44.527292: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:44.528863: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:1/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-12 01:00:44.583186: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:44.583852: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:44.584388: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:44.585151: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 01:00:44.585214: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-12 01:00:44.593305: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:44.594433: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:44.594994: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:44.595509: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 01:00:44.595567: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-12 01:00:44.603095: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:44.604703: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 01:00:44.604862: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 01:00:44.605439: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:44.606468: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:44.606633: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12341
2022-12-12 01:00:44.607595: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:44.608825: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:44.609746: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:2/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-12 01:00:44.612517: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:44.613952: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:44.615104: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:44.616272: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:44.617393: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:44.618491: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:3/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-12 01:00:44.659211: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 01:00:44.659409: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 01:00:44.661191: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12343
2022-12-12 01:00:44.696321: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 01:00:44.696526: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 01:00:44.697590: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12342
2022-12-12 01:00:44.701327: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:44.701932: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:44.702474: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:44.702811: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:44.703467: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:44.703642: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 01:00:44.703696: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-12 01:00:44.704274: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:44.704735: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 01:00:44.704775: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-12 01:00:44.704955: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:44.705629: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:44.706439: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:44.707814: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 01:00:44.707874: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-12 01:00:44.718393: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:44.719476: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:44.720494: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:44.721067: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:44.721600: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:44.721648: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 01:00:44.721703: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-12 01:00:44.723216: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:44.723563: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:44.724688: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:44.724830: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:44.725242: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:44.726890: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:44.727035: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:44.727394: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:44.728856: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:44.729029: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:44.729392: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:44.731025: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:4/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-12 01:00:44.731052: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:44.731349: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:5/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-12 01:00:44.732367: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:44.733435: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:7/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-12 01:00:44.738781: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:44.739887: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:44.740861: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:44.741922: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:44.742914: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:00:44.743879: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:6/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-12 01:00:44.772376: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 01:00:44.772456: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 01:00:44.772577: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 01:00:44.772611: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 01:00:44.773467: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12344
2022-12-12 01:00:44.774148: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 01:00:44.774262: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 01:00:44.774837: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12345
2022-12-12 01:00:44.775092: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12347
2022-12-12 01:00:44.786822: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 01:00:44.787039: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 01:00:44.788069: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12346
[HCTR][01:00:46.057][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][01:00:46.057][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][01:00:46.057][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][01:00:46.057][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][01:00:46.058][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][01:00:46.058][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][01:00:46.058][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][01:00:46.059][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 1it [00:01,  1.59s/it]warmup run: 98it [00:01, 80.46it/s]warmup run: 1it [00:01,  1.51s/it]warmup run: 196it [00:01, 175.13it/s]warmup run: 1it [00:01,  1.53s/it]warmup run: 101it [00:01, 86.95it/s]warmup run: 1it [00:01,  1.53s/it]warmup run: 1it [00:01,  1.50s/it]warmup run: 1it [00:01,  1.57s/it]warmup run: 1it [00:01,  1.50s/it]warmup run: 296it [00:01, 282.64it/s]warmup run: 98it [00:01, 83.37it/s]warmup run: 199it [00:01, 184.92it/s]warmup run: 92it [00:01, 78.04it/s]warmup run: 1it [00:01,  1.48s/it]warmup run: 101it [00:01, 87.76it/s]warmup run: 97it [00:01, 80.49it/s]warmup run: 97it [00:01, 84.06it/s]warmup run: 395it [00:01, 393.63it/s]warmup run: 198it [00:01, 182.85it/s]warmup run: 297it [00:01, 292.63it/s]warmup run: 184it [00:01, 169.22it/s]warmup run: 90it [00:01, 78.90it/s]warmup run: 203it [00:01, 190.77it/s]warmup run: 195it [00:01, 176.00it/s]warmup run: 193it [00:01, 180.76it/s]warmup run: 495it [00:02, 504.08it/s]warmup run: 297it [00:01, 291.16it/s]warmup run: 395it [00:01, 403.61it/s]warmup run: 276it [00:01, 269.63it/s]warmup run: 181it [00:01, 171.52it/s]warmup run: 305it [00:01, 303.77it/s]warmup run: 292it [00:01, 280.48it/s]warmup run: 290it [00:01, 287.96it/s]warmup run: 595it [00:02, 605.71it/s]warmup run: 396it [00:01, 403.25it/s]warmup run: 491it [00:02, 507.84it/s]warmup run: 368it [00:01, 373.69it/s]warmup run: 274it [00:01, 275.58it/s]warmup run: 407it [00:01, 420.00it/s]warmup run: 390it [00:01, 390.87it/s]warmup run: 388it [00:01, 400.20it/s]warmup run: 695it [00:02, 694.46it/s]warmup run: 495it [00:02, 512.71it/s]warmup run: 590it [00:02, 608.59it/s]warmup run: 465it [00:02, 484.60it/s]warmup run: 369it [00:01, 385.90it/s]warmup run: 510it [00:01, 534.11it/s]warmup run: 487it [00:02, 497.33it/s]warmup run: 486it [00:02, 508.81it/s]warmup run: 795it [00:02, 768.42it/s]warmup run: 594it [00:02, 613.12it/s]warmup run: 688it [00:02, 694.25it/s]warmup run: 564it [00:02, 590.79it/s]warmup run: 470it [00:01, 504.00it/s]warmup run: 614it [00:02, 641.27it/s]warmup run: 587it [00:02, 600.87it/s]warmup run: 584it [00:02, 608.24it/s]warmup run: 894it [00:02, 825.75it/s]warmup run: 693it [00:02, 699.92it/s]warmup run: 786it [00:02, 764.42it/s]warmup run: 665it [00:02, 686.39it/s]warmup run: 569it [00:02, 607.08it/s]warmup run: 718it [00:02, 733.25it/s]warmup run: 686it [00:02, 689.94it/s]warmup run: 684it [00:02, 697.68it/s]warmup run: 993it [00:02, 868.62it/s]warmup run: 792it [00:02, 771.05it/s]warmup run: 884it [00:02, 818.72it/s]warmup run: 763it [00:02, 757.93it/s]warmup run: 665it [00:02, 687.86it/s]warmup run: 823it [00:02, 810.31it/s]warmup run: 785it [00:02, 763.82it/s]warmup run: 783it [00:02, 769.26it/s]warmup run: 1093it [00:02, 903.32it/s]warmup run: 891it [00:02, 826.32it/s]warmup run: 982it [00:02, 861.30it/s]warmup run: 861it [00:02, 814.77it/s]warmup run: 760it [00:02, 752.34it/s]warmup run: 926it [00:02, 866.55it/s]warmup run: 884it [00:02, 822.10it/s]warmup run: 882it [00:02, 825.51it/s]warmup run: 1193it [00:02, 928.56it/s]warmup run: 990it [00:02, 868.72it/s]warmup run: 1079it [00:02, 891.30it/s]warmup run: 959it [00:02, 857.96it/s]warmup run: 856it [00:02, 805.62it/s]warmup run: 1028it [00:02, 906.47it/s]warmup run: 982it [00:02, 860.16it/s]warmup run: 980it [00:02, 867.39it/s]warmup run: 1292it [00:02, 946.02it/s]warmup run: 1088it [00:02, 898.84it/s]warmup run: 1176it [00:02, 913.24it/s]warmup run: 1057it [00:02, 889.63it/s]warmup run: 951it [00:02, 843.91it/s]warmup run: 1078it [00:02, 896.50it/s]warmup run: 1130it [00:02, 880.41it/s]warmup run: 1079it [00:02, 838.08it/s]warmup run: 1391it [00:02, 956.89it/s]warmup run: 1186it [00:02, 920.57it/s]warmup run: 1274it [00:02, 930.66it/s]warmup run: 1154it [00:02, 909.17it/s]warmup run: 1046it [00:02, 872.65it/s]warmup run: 1177it [00:02, 922.39it/s]warmup run: 1233it [00:02, 919.30it/s]warmup run: 1179it [00:02, 880.28it/s]warmup run: 1491it [00:03, 966.63it/s]warmup run: 1286it [00:02, 941.11it/s]warmup run: 1374it [00:02, 950.20it/s]warmup run: 1254it [00:02, 932.56it/s]warmup run: 1144it [00:02, 903.16it/s]warmup run: 1275it [00:02, 936.32it/s]warmup run: 1336it [00:02, 947.70it/s]warmup run: 1279it [00:02, 911.09it/s]warmup run: 1591it [00:03, 974.49it/s]warmup run: 1385it [00:02, 954.95it/s]warmup run: 1474it [00:03, 963.99it/s]warmup run: 1354it [00:02, 951.73it/s]warmup run: 1240it [00:02, 917.87it/s]warmup run: 1439it [00:02, 970.83it/s]warmup run: 1373it [00:02, 940.23it/s]warmup run: 1377it [00:03, 928.43it/s]warmup run: 1690it [00:03, 977.44it/s]warmup run: 1484it [00:03, 960.46it/s]warmup run: 1576it [00:03, 978.87it/s]warmup run: 1455it [00:03, 966.56it/s]warmup run: 1338it [00:02, 935.59it/s]warmup run: 1474it [00:03, 958.78it/s]warmup run: 1540it [00:03, 974.92it/s]warmup run: 1476it [00:03, 944.54it/s]warmup run: 1789it [00:03, 977.06it/s]warmup run: 1582it [00:03, 964.71it/s]warmup run: 1676it [00:03, 984.69it/s]warmup run: 1555it [00:03, 973.47it/s]warmup run: 1439it [00:02, 955.07it/s]warmup run: 1576it [00:03, 976.33it/s]warmup run: 1641it [00:03, 983.69it/s]warmup run: 1574it [00:03, 953.21it/s]warmup run: 1890it [00:03, 984.60it/s]warmup run: 1680it [00:03, 968.05it/s]warmup run: 1779it [00:03, 995.49it/s]warmup run: 1655it [00:03, 981.03it/s]warmup run: 1539it [00:03, 968.16it/s]warmup run: 1678it [00:03, 987.41it/s]warmup run: 1672it [00:03, 958.96it/s]warmup run: 1741it [00:03, 974.84it/s]warmup run: 1990it [00:03, 988.67it/s]warmup run: 1779it [00:03, 972.45it/s]warmup run: 1882it [00:03, 1003.05it/s]warmup run: 1755it [00:03, 952.96it/s]warmup run: 1639it [00:03, 976.42it/s]warmup run: 1780it [00:03, 995.30it/s]warmup run: 1770it [00:03, 964.27it/s]warmup run: 2109it [00:03, 1048.10it/s]warmup run: 1843it [00:03, 985.29it/s]warmup run: 1879it [00:03, 979.45it/s]warmup run: 1983it [00:03, 1001.84it/s]warmup run: 1855it [00:03, 964.39it/s]warmup run: 1740it [00:03, 984.72it/s]warmup run: 1881it [00:03, 996.26it/s]warmup run: 1868it [00:03, 965.63it/s]warmup run: 1944it [00:03, 991.51it/s]warmup run: 2231it [00:03, 1097.79it/s]warmup run: 1979it [00:03, 982.87it/s]warmup run: 2099it [00:03, 1048.67it/s]warmup run: 1953it [00:03, 967.55it/s]warmup run: 1841it [00:03, 989.64it/s]warmup run: 1982it [00:03, 995.59it/s]warmup run: 2052it [00:03, 1016.74it/s]warmup run: 2353it [00:03, 1132.09it/s]warmup run: 1966it [00:03, 959.57it/s]warmup run: 2094it [00:03, 1031.27it/s]warmup run: 2220it [00:03, 1094.43it/s]warmup run: 2061it [00:03, 998.57it/s]warmup run: 1942it [00:03, 994.46it/s]warmup run: 2099it [00:03, 1045.65it/s]warmup run: 2174it [00:03, 1075.96it/s]warmup run: 2475it [00:04, 1156.35it/s]warmup run: 2078it [00:03, 1005.50it/s]warmup run: 2214it [00:03, 1078.97it/s]warmup run: 2340it [00:03, 1125.03it/s]warmup run: 2178it [00:03, 1047.45it/s]warmup run: 2050it [00:03, 1019.83it/s]warmup run: 2220it [00:03, 1094.33it/s]warmup run: 2297it [00:03, 1119.25it/s]warmup run: 2597it [00:04, 1174.39it/s]warmup run: 2198it [00:03, 1063.10it/s]warmup run: 2334it [00:03, 1113.47it/s]warmup run: 2460it [00:03, 1146.85it/s]warmup run: 2295it [00:03, 1083.16it/s]warmup run: 2170it [00:03, 1071.36it/s]warmup run: 2341it [00:03, 1127.73it/s]warmup run: 2419it [00:03, 1149.13it/s]warmup run: 2718it [00:04, 1183.41it/s]warmup run: 2318it [00:03, 1103.79it/s]warmup run: 2454it [00:03, 1137.89it/s]warmup run: 2580it [00:04, 1162.65it/s]warmup run: 2406it [00:03, 1088.98it/s]warmup run: 2290it [00:03, 1107.07it/s]warmup run: 2461it [00:03, 1148.25it/s]warmup run: 2541it [00:03, 1169.74it/s]warmup run: 2840it [00:04, 1192.85it/s]warmup run: 2439it [00:04, 1132.79it/s]warmup run: 2574it [00:04, 1154.57it/s]warmup run: 2698it [00:04, 1166.55it/s]warmup run: 2523it [00:04, 1112.75it/s]warmup run: 2409it [00:03, 1131.34it/s]warmup run: 2582it [00:04, 1166.61it/s]warmup run: 2660it [00:04, 1174.98it/s]warmup run: 2962it [00:04, 1198.07it/s]warmup run: 2559it [00:04, 1152.75it/s]warmup run: 2691it [00:04, 1158.73it/s]warmup run: 2819it [00:04, 1177.06it/s]warmup run: 3000it [00:04, 674.72it/s] warmup run: 2639it [00:04, 1124.25it/s]warmup run: 2528it [00:03, 1147.89it/s]warmup run: 2701it [00:04, 1172.52it/s]warmup run: 2778it [00:04, 1173.35it/s]warmup run: 2677it [00:04, 1160.01it/s]warmup run: 2810it [00:04, 1167.49it/s]warmup run: 2940it [00:04, 1185.69it/s]warmup run: 2756it [00:04, 1135.35it/s]warmup run: 2646it [00:04, 1157.33it/s]warmup run: 3000it [00:04, 684.88it/s] warmup run: 2821it [00:04, 1177.78it/s]warmup run: 2898it [00:04, 1178.44it/s]warmup run: 2797it [00:04, 1169.98it/s]warmup run: 2930it [00:04, 1174.67it/s]warmup run: 2872it [00:04, 1141.45it/s]warmup run: 2765it [00:04, 1166.62it/s]warmup run: 3000it [00:04, 680.18it/s] warmup run: 3000it [00:04, 692.43it/s] warmup run: 2940it [00:04, 1178.85it/s]warmup run: 2917it [00:04, 1178.07it/s]warmup run: 2988it [00:04, 1146.42it/s]warmup run: 3000it [00:04, 685.59it/s] warmup run: 2884it [00:04, 1172.13it/s]warmup run: 3000it [00:04, 669.67it/s] warmup run: 3000it [00:04, 668.24it/s] warmup run: 3000it [00:04, 682.29it/s] 
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]





warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   5%|         | 164/3000 [00:00<00:01, 1638.73it/s]warmup should be done:   6%|         | 165/3000 [00:00<00:01, 1648.30it/s]warmup should be done:   5%|         | 163/3000 [00:00<00:01, 1624.42it/s]warmup should be done:   5%|         | 162/3000 [00:00<00:01, 1616.24it/s]warmup should be done:   5%|         | 163/3000 [00:00<00:01, 1625.53it/s]warmup should be done:   5%|         | 162/3000 [00:00<00:01, 1616.31it/s]warmup should be done:   5%|         | 162/3000 [00:00<00:01, 1611.66it/s]warmup should be done:   5%|         | 163/3000 [00:00<00:01, 1622.10it/s]warmup should be done:  11%|         | 328/3000 [00:00<00:01, 1638.74it/s]warmup should be done:  11%|         | 331/3000 [00:00<00:01, 1652.84it/s]warmup should be done:  11%|         | 327/3000 [00:00<00:01, 1632.52it/s]warmup should be done:  11%|         | 327/3000 [00:00<00:01, 1629.64it/s]warmup should be done:  11%|         | 326/3000 [00:00<00:01, 1627.26it/s]warmup should be done:  11%|         | 326/3000 [00:00<00:01, 1627.81it/s]warmup should be done:  11%|         | 326/3000 [00:00<00:01, 1626.20it/s]warmup should be done:  11%|         | 328/3000 [00:00<00:01, 1635.25it/s]warmup should be done:  16%|        | 489/3000 [00:00<00:01, 1627.86it/s]warmup should be done:  16%|        | 490/3000 [00:00<00:01, 1627.63it/s]warmup should be done:  16%|        | 489/3000 [00:00<00:01, 1625.98it/s]warmup should be done:  16%|        | 489/3000 [00:00<00:01, 1624.40it/s]warmup should be done:  16%|        | 492/3000 [00:00<00:01, 1635.10it/s]warmup should be done:  16%|        | 491/3000 [00:00<00:01, 1628.82it/s]warmup should be done:  16%|        | 492/3000 [00:00<00:01, 1628.65it/s]warmup should be done:  17%|        | 497/3000 [00:00<00:01, 1632.17it/s]warmup should be done:  22%|       | 652/3000 [00:00<00:01, 1626.95it/s]warmup should be done:  22%|       | 653/3000 [00:00<00:01, 1625.60it/s]warmup should be done:  22%|       | 652/3000 [00:00<00:01, 1623.78it/s]warmup should be done:  22%|       | 654/3000 [00:00<00:01, 1627.76it/s]warmup should be done:  22%|       | 656/3000 [00:00<00:01, 1634.32it/s]warmup should be done:  22%|       | 655/3000 [00:00<00:01, 1625.11it/s]warmup should be done:  22%|       | 662/3000 [00:00<00:01, 1638.70it/s]warmup should be done:  22%|       | 652/3000 [00:00<00:01, 1592.44it/s]warmup should be done:  27%|       | 815/3000 [00:00<00:01, 1625.34it/s]warmup should be done:  27%|       | 816/3000 [00:00<00:01, 1623.41it/s]warmup should be done:  27%|       | 815/3000 [00:00<00:01, 1623.04it/s]warmup should be done:  27%|       | 820/3000 [00:00<00:01, 1632.63it/s]warmup should be done:  28%|       | 828/3000 [00:00<00:01, 1643.65it/s]warmup should be done:  27%|       | 818/3000 [00:00<00:01, 1609.66it/s]warmup should be done:  27%|       | 812/3000 [00:00<00:01, 1594.06it/s]warmup should be done:  27%|       | 817/3000 [00:00<00:01, 1595.85it/s]warmup should be done:  33%|      | 978/3000 [00:00<00:01, 1619.85it/s]warmup should be done:  33%|      | 979/3000 [00:00<00:01, 1616.57it/s]warmup should be done:  33%|      | 984/3000 [00:00<00:01, 1626.73it/s]warmup should be done:  33%|      | 993/3000 [00:00<00:01, 1641.05it/s]warmup should be done:  33%|      | 978/3000 [00:00<00:01, 1609.61it/s]warmup should be done:  32%|      | 974/3000 [00:00<00:01, 1601.50it/s]warmup should be done:  33%|      | 977/3000 [00:00<00:01, 1591.67it/s]warmup should be done:  33%|      | 979/3000 [00:00<00:01, 1585.41it/s]warmup should be done:  38%|      | 1140/3000 [00:00<00:01, 1619.01it/s]warmup should be done:  38%|      | 1141/3000 [00:00<00:01, 1615.04it/s]warmup should be done:  38%|      | 1148/3000 [00:00<00:01, 1627.97it/s]warmup should be done:  39%|      | 1158/3000 [00:00<00:01, 1642.98it/s]warmup should be done:  38%|      | 1140/3000 [00:00<00:01, 1611.37it/s]warmup should be done:  38%|      | 1135/3000 [00:00<00:01, 1602.05it/s]warmup should be done:  38%|      | 1139/3000 [00:00<00:01, 1600.05it/s]warmup should be done:  38%|      | 1140/3000 [00:00<00:01, 1592.18it/s]warmup should be done:  43%|     | 1302/3000 [00:00<00:01, 1617.42it/s]warmup should be done:  43%|     | 1304/3000 [00:00<00:01, 1617.66it/s]warmup should be done:  44%|     | 1311/3000 [00:00<00:01, 1627.45it/s]warmup should be done:  44%|     | 1323/3000 [00:00<00:01, 1642.03it/s]warmup should be done:  43%|     | 1302/3000 [00:00<00:01, 1609.52it/s]warmup should be done:  43%|     | 1300/3000 [00:00<00:01, 1601.82it/s]warmup should be done:  43%|     | 1296/3000 [00:00<00:01, 1595.96it/s]warmup should be done:  43%|     | 1301/3000 [00:00<00:01, 1595.90it/s]warmup should be done:  49%|     | 1464/3000 [00:00<00:00, 1617.50it/s]warmup should be done:  49%|     | 1468/3000 [00:00<00:00, 1623.27it/s]warmup should be done:  49%|     | 1475/3000 [00:00<00:00, 1628.27it/s]warmup should be done:  50%|     | 1488/3000 [00:00<00:00, 1642.73it/s]warmup should be done:  49%|     | 1463/3000 [00:00<00:00, 1609.59it/s]warmup should be done:  49%|     | 1463/3000 [00:00<00:00, 1608.72it/s]warmup should be done:  49%|     | 1456/3000 [00:00<00:00, 1596.56it/s]warmup should be done:  49%|     | 1464/3000 [00:00<00:00, 1604.86it/s]warmup should be done:  54%|    | 1626/3000 [00:01<00:00, 1617.73it/s]warmup should be done:  54%|    | 1632/3000 [00:01<00:00, 1627.33it/s]warmup should be done:  55%|    | 1639/3000 [00:01<00:00, 1628.83it/s]warmup should be done:  55%|    | 1653/3000 [00:01<00:00, 1643.09it/s]warmup should be done:  54%|    | 1627/3000 [00:01<00:00, 1618.24it/s]warmup should be done:  54%|    | 1618/3000 [00:01<00:00, 1603.39it/s]warmup should be done:  54%|    | 1628/3000 [00:01<00:00, 1614.26it/s]warmup should be done:  54%|    | 1624/3000 [00:01<00:00, 1586.58it/s]warmup should be done:  60%|    | 1789/3000 [00:01<00:00, 1619.06it/s]warmup should be done:  60%|    | 1796/3000 [00:01<00:00, 1630.59it/s]warmup should be done:  60%|    | 1803/3000 [00:01<00:00, 1629.94it/s]warmup should be done:  61%|    | 1818/3000 [00:01<00:00, 1641.88it/s]warmup should be done:  60%|    | 1791/3000 [00:01<00:00, 1624.15it/s]warmup should be done:  59%|    | 1780/3000 [00:01<00:00, 1607.90it/s]warmup should be done:  59%|    | 1783/3000 [00:01<00:00, 1583.92it/s]warmup should be done:  60%|    | 1790/3000 [00:01<00:00, 1607.85it/s]warmup should be done:  65%|   | 1952/3000 [00:01<00:00, 1619.99it/s]warmup should be done:  65%|   | 1961/3000 [00:01<00:00, 1633.53it/s]warmup should be done:  66%|   | 1967/3000 [00:01<00:00, 1631.33it/s]warmup should be done:  66%|   | 1983/3000 [00:01<00:00, 1641.05it/s]warmup should be done:  65%|   | 1955/3000 [00:01<00:00, 1627.69it/s]warmup should be done:  65%|   | 1942/3000 [00:01<00:00, 1609.28it/s]warmup should be done:  65%|   | 1942/3000 [00:01<00:00, 1582.06it/s]warmup should be done:  65%|   | 1951/3000 [00:01<00:00, 1605.28it/s]warmup should be done:  70%|   | 2115/3000 [00:01<00:00, 1620.31it/s]warmup should be done:  71%|   | 2126/3000 [00:01<00:00, 1635.62it/s]warmup should be done:  71%|   | 2131/3000 [00:01<00:00, 1631.93it/s]warmup should be done:  72%|  | 2148/3000 [00:01<00:00, 1640.01it/s]warmup should be done:  70%|   | 2104/3000 [00:01<00:00, 1612.01it/s]warmup should be done:  71%|   | 2120/3000 [00:01<00:00, 1631.56it/s]warmup should be done:  70%|   | 2101/3000 [00:01<00:00, 1576.62it/s]warmup should be done:  70%|   | 2112/3000 [00:01<00:00, 1601.43it/s]warmup should be done:  76%|  | 2278/3000 [00:01<00:00, 1617.52it/s]warmup should be done:  76%|  | 2290/3000 [00:01<00:00, 1634.14it/s]warmup should be done:  77%|  | 2313/3000 [00:01<00:00, 1639.67it/s]warmup should be done:  76%|  | 2266/3000 [00:01<00:00, 1613.56it/s]warmup should be done:  76%|  | 2284/3000 [00:01<00:00, 1628.01it/s]warmup should be done:  75%|  | 2262/3000 [00:01<00:00, 1584.99it/s]warmup should be done:  76%|  | 2295/3000 [00:01<00:00, 1595.22it/s]warmup should be done:  76%|  | 2273/3000 [00:01<00:00, 1597.85it/s]warmup should be done:  81%| | 2440/3000 [00:01<00:00, 1617.48it/s]warmup should be done:  82%| | 2454/3000 [00:01<00:00, 1633.19it/s]warmup should be done:  83%| | 2478/3000 [00:01<00:00, 1641.21it/s]warmup should be done:  81%|  | 2428/3000 [00:01<00:00, 1613.50it/s]warmup should be done:  82%| | 2449/3000 [00:01<00:00, 1631.98it/s]warmup should be done:  81%|  | 2424/3000 [00:01<00:00, 1594.38it/s]warmup should be done:  81%|  | 2433/3000 [00:01<00:00, 1597.29it/s]warmup should be done:  82%| | 2455/3000 [00:01<00:00, 1583.41it/s]warmup should be done:  87%| | 2603/3000 [00:01<00:00, 1619.19it/s]warmup should be done:  87%| | 2620/3000 [00:01<00:00, 1638.91it/s]warmup should be done:  88%| | 2643/3000 [00:01<00:00, 1640.95it/s]warmup should be done:  86%| | 2590/3000 [00:01<00:00, 1614.63it/s]warmup should be done:  87%| | 2614/3000 [00:01<00:00, 1634.90it/s]warmup should be done:  86%| | 2586/3000 [00:01<00:00, 1599.64it/s]warmup should be done:  86%| | 2593/3000 [00:01<00:00, 1596.19it/s]warmup should be done:  87%| | 2617/3000 [00:01<00:00, 1592.18it/s]warmup should be done:  92%|| 2766/3000 [00:01<00:00, 1621.02it/s]warmup should be done:  93%|| 2784/3000 [00:01<00:00, 1639.00it/s]warmup should be done:  94%|| 2808/3000 [00:01<00:00, 1641.18it/s]warmup should be done:  92%|| 2753/3000 [00:01<00:00, 1616.60it/s]warmup should be done:  93%|| 2779/3000 [00:01<00:00, 1636.77it/s]warmup should be done:  92%|| 2748/3000 [00:01<00:00, 1604.08it/s]warmup should be done:  92%|| 2753/3000 [00:01<00:00, 1596.39it/s]warmup should be done:  93%|| 2777/3000 [00:01<00:00, 1590.62it/s]warmup should be done:  98%|| 2930/3000 [00:01<00:00, 1624.89it/s]warmup should be done:  99%|| 2974/3000 [00:01<00:00, 1645.25it/s]warmup should be done:  97%|| 2917/3000 [00:01<00:00, 1620.90it/s]warmup should be done:  98%|| 2948/3000 [00:01<00:00, 1618.39it/s]warmup should be done:  98%|| 2945/3000 [00:01<00:00, 1642.64it/s]warmup should be done:  97%|| 2911/3000 [00:01<00:00, 1610.22it/s]warmup should be done:  97%|| 2915/3000 [00:01<00:00, 1603.43it/s]warmup should be done:  98%|| 2939/3000 [00:01<00:00, 1599.31it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1642.19it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1625.42it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1624.54it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1621.22it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1612.94it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1610.26it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1604.40it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1603.40it/s]






warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   6%|         | 166/3000 [00:00<00:01, 1655.68it/s]warmup should be done:   5%|         | 163/3000 [00:00<00:01, 1626.34it/s]warmup should be done:   5%|         | 163/3000 [00:00<00:01, 1625.30it/s]warmup should be done:   6%|         | 169/3000 [00:00<00:01, 1684.58it/s]warmup should be done:   6%|         | 166/3000 [00:00<00:01, 1654.79it/s]warmup should be done:   6%|         | 169/3000 [00:00<00:01, 1683.60it/s]warmup should be done:   6%|         | 167/3000 [00:00<00:01, 1662.32it/s]warmup should be done:   6%|         | 167/3000 [00:00<00:01, 1661.64it/s]warmup should be done:  11%|         | 329/3000 [00:00<00:01, 1643.44it/s]warmup should be done:  11%|        | 339/3000 [00:00<00:01, 1690.65it/s]warmup should be done:  11%|        | 339/3000 [00:00<00:01, 1690.19it/s]warmup should be done:  11%|         | 328/3000 [00:00<00:01, 1635.40it/s]warmup should be done:  11%|         | 334/3000 [00:00<00:01, 1664.58it/s]warmup should be done:  11%|         | 334/3000 [00:00<00:01, 1663.10it/s]warmup should be done:  11%|         | 335/3000 [00:00<00:01, 1668.59it/s]warmup should be done:  11%|         | 332/3000 [00:00<00:01, 1649.13it/s]warmup should be done:  17%|        | 509/3000 [00:00<00:01, 1692.98it/s]warmup should be done:  16%|        | 492/3000 [00:00<00:01, 1634.83it/s]warmup should be done:  17%|        | 503/3000 [00:00<00:01, 1671.96it/s]warmup should be done:  17%|        | 510/3000 [00:00<00:01, 1695.28it/s]warmup should be done:  17%|        | 502/3000 [00:00<00:01, 1667.69it/s]warmup should be done:  17%|        | 502/3000 [00:00<00:01, 1667.69it/s]warmup should be done:  16%|        | 494/3000 [00:00<00:01, 1640.22it/s]warmup should be done:  17%|        | 497/3000 [00:00<00:01, 1648.61it/s]warmup should be done:  23%|       | 679/3000 [00:00<00:01, 1694.41it/s]warmup should be done:  22%|       | 663/3000 [00:00<00:01, 1651.42it/s]warmup should be done:  22%|       | 656/3000 [00:00<00:01, 1633.08it/s]warmup should be done:  22%|       | 670/3000 [00:00<00:01, 1670.12it/s]warmup should be done:  22%|       | 669/3000 [00:00<00:01, 1665.65it/s]warmup should be done:  23%|       | 681/3000 [00:00<00:01, 1697.23it/s]warmup should be done:  22%|       | 671/3000 [00:00<00:01, 1670.60it/s]warmup should be done:  22%|       | 659/3000 [00:00<00:01, 1638.35it/s]warmup should be done:  28%|       | 849/3000 [00:00<00:01, 1694.93it/s]warmup should be done:  28%|       | 829/3000 [00:00<00:01, 1653.99it/s]warmup should be done:  28%|       | 851/3000 [00:00<00:01, 1696.83it/s]warmup should be done:  28%|       | 836/3000 [00:00<00:01, 1665.44it/s]warmup should be done:  28%|       | 838/3000 [00:00<00:01, 1669.68it/s]warmup should be done:  27%|       | 820/3000 [00:00<00:01, 1630.88it/s]warmup should be done:  28%|       | 825/3000 [00:00<00:01, 1644.51it/s]warmup should be done:  28%|       | 839/3000 [00:00<00:01, 1669.31it/s]warmup should be done:  34%|      | 1019/3000 [00:00<00:01, 1696.54it/s]warmup should be done:  34%|      | 1021/3000 [00:00<00:01, 1696.86it/s]warmup should be done:  33%|      | 995/3000 [00:00<00:01, 1651.11it/s]warmup should be done:  34%|      | 1005/3000 [00:00<00:01, 1668.24it/s]warmup should be done:  33%|      | 1004/3000 [00:00<00:01, 1667.00it/s]warmup should be done:  33%|      | 984/3000 [00:00<00:01, 1631.81it/s]warmup should be done:  34%|      | 1007/3000 [00:00<00:01, 1670.77it/s]warmup should be done:  33%|      | 990/3000 [00:00<00:01, 1641.35it/s]warmup should be done:  40%|      | 1189/3000 [00:00<00:01, 1693.58it/s]warmup should be done:  39%|      | 1171/3000 [00:00<00:01, 1665.83it/s]warmup should be done:  39%|      | 1172/3000 [00:00<00:01, 1665.81it/s]warmup should be done:  39%|      | 1175/3000 [00:00<00:01, 1672.13it/s]warmup should be done:  38%|      | 1148/3000 [00:00<00:01, 1631.81it/s]warmup should be done:  39%|      | 1161/3000 [00:00<00:01, 1649.47it/s]warmup should be done:  38%|      | 1155/3000 [00:00<00:01, 1641.59it/s]warmup should be done:  40%|      | 1191/3000 [00:00<00:01, 1669.62it/s]warmup should be done:  45%|     | 1359/3000 [00:00<00:00, 1695.13it/s]warmup should be done:  45%|     | 1343/3000 [00:00<00:00, 1674.23it/s]warmup should be done:  45%|     | 1339/3000 [00:00<00:00, 1665.27it/s]warmup should be done:  45%|     | 1338/3000 [00:00<00:00, 1664.36it/s]warmup should be done:  44%|     | 1326/3000 [00:00<00:01, 1646.93it/s]warmup should be done:  44%|     | 1321/3000 [00:00<00:01, 1646.19it/s]warmup should be done:  44%|     | 1312/3000 [00:00<00:01, 1626.92it/s]warmup should be done:  45%|     | 1359/3000 [00:00<00:00, 1652.21it/s]warmup should be done:  51%|     | 1529/3000 [00:00<00:00, 1689.76it/s]warmup should be done:  50%|     | 1506/3000 [00:00<00:00, 1666.24it/s]warmup should be done:  50%|     | 1511/3000 [00:00<00:00, 1674.14it/s]warmup should be done:  50%|     | 1492/3000 [00:00<00:00, 1648.64it/s]warmup should be done:  50%|     | 1505/3000 [00:00<00:00, 1658.49it/s]warmup should be done:  49%|     | 1476/3000 [00:00<00:00, 1628.95it/s]warmup should be done:  50%|     | 1486/3000 [00:00<00:00, 1633.79it/s]warmup should be done:  51%|     | 1525/3000 [00:00<00:00, 1639.13it/s]warmup should be done:  56%|    | 1673/3000 [00:01<00:00, 1666.80it/s]warmup should be done:  56%|    | 1679/3000 [00:01<00:00, 1675.00it/s]warmup should be done:  57%|    | 1698/3000 [00:01<00:00, 1685.80it/s]warmup should be done:  55%|    | 1658/3000 [00:01<00:00, 1651.36it/s]warmup should be done:  55%|    | 1640/3000 [00:01<00:00, 1630.73it/s]warmup should be done:  56%|    | 1673/3000 [00:01<00:00, 1662.14it/s]warmup should be done:  55%|    | 1650/3000 [00:01<00:00, 1628.98it/s]warmup should be done:  56%|    | 1689/3000 [00:01<00:00, 1632.66it/s]warmup should be done:  61%|   | 1841/3000 [00:01<00:00, 1669.26it/s]warmup should be done:  62%|   | 1847/3000 [00:01<00:00, 1675.09it/s]warmup should be done:  62%|   | 1867/3000 [00:01<00:00, 1685.29it/s]warmup should be done:  61%|    | 1824/3000 [00:01<00:00, 1652.85it/s]warmup should be done:  60%|    | 1804/3000 [00:01<00:00, 1630.93it/s]warmup should be done:  61%|   | 1840/3000 [00:01<00:00, 1661.39it/s]warmup should be done:  60%|    | 1813/3000 [00:01<00:00, 1625.02it/s]warmup should be done:  62%|   | 1853/3000 [00:01<00:00, 1628.88it/s]warmup should be done:  67%|   | 2008/3000 [00:01<00:00, 1669.00it/s]warmup should be done:  67%|   | 2015/3000 [00:01<00:00, 1674.00it/s]warmup should be done:  68%|   | 2036/3000 [00:01<00:00, 1684.04it/s]warmup should be done:  66%|   | 1990/3000 [00:01<00:00, 1651.21it/s]warmup should be done:  66%|   | 1969/3000 [00:01<00:00, 1634.89it/s]warmup should be done:  67%|   | 2007/3000 [00:01<00:00, 1659.31it/s]warmup should be done:  66%|   | 1976/3000 [00:01<00:00, 1621.97it/s]warmup should be done:  67%|   | 2016/3000 [00:01<00:00, 1625.03it/s]warmup should be done:  72%|  | 2175/3000 [00:01<00:00, 1667.57it/s]warmup should be done:  73%|  | 2183/3000 [00:01<00:00, 1673.20it/s]warmup should be done:  72%|  | 2156/3000 [00:01<00:00, 1650.25it/s]warmup should be done:  71%|   | 2135/3000 [00:01<00:00, 1641.06it/s]warmup should be done:  74%|  | 2205/3000 [00:01<00:00, 1677.45it/s]warmup should be done:  72%|  | 2173/3000 [00:01<00:00, 1656.38it/s]warmup should be done:  71%|  | 2139/3000 [00:01<00:00, 1620.78it/s]warmup should be done:  73%|  | 2179/3000 [00:01<00:00, 1620.39it/s]warmup should be done:  78%|  | 2343/3000 [00:01<00:00, 1668.68it/s]warmup should be done:  78%|  | 2351/3000 [00:01<00:00, 1674.55it/s]warmup should be done:  77%|  | 2322/3000 [00:01<00:00, 1652.74it/s]warmup should be done:  77%|  | 2301/3000 [00:01<00:00, 1645.57it/s]warmup should be done:  78%|  | 2341/3000 [00:01<00:00, 1661.18it/s]warmup should be done:  79%|  | 2373/3000 [00:01<00:00, 1670.60it/s]warmup should be done:  77%|  | 2305/3000 [00:01<00:00, 1631.20it/s]warmup should be done:  78%|  | 2342/3000 [00:01<00:00, 1620.82it/s]warmup should be done:  84%| | 2519/3000 [00:01<00:00, 1676.00it/s]warmup should be done:  84%| | 2511/3000 [00:01<00:00, 1670.21it/s]warmup should be done:  83%| | 2488/3000 [00:01<00:00, 1653.09it/s]warmup should be done:  82%| | 2467/3000 [00:01<00:00, 1647.72it/s]warmup should be done:  84%| | 2509/3000 [00:01<00:00, 1666.57it/s]warmup should be done:  85%| | 2541/3000 [00:01<00:00, 1667.92it/s]warmup should be done:  82%| | 2469/3000 [00:01<00:00, 1615.09it/s]warmup should be done:  84%| | 2505/3000 [00:01<00:00, 1621.52it/s]warmup should be done:  90%| | 2687/3000 [00:01<00:00, 1676.60it/s]warmup should be done:  89%| | 2679/3000 [00:01<00:00, 1667.16it/s]warmup should be done:  88%| | 2632/3000 [00:01<00:00, 1647.84it/s]warmup should be done:  88%| | 2654/3000 [00:01<00:00, 1650.25it/s]warmup should be done:  89%| | 2676/3000 [00:01<00:00, 1666.47it/s]warmup should be done:  90%| | 2709/3000 [00:01<00:00, 1669.31it/s]warmup should be done:  88%| | 2631/3000 [00:01<00:00, 1615.34it/s]warmup should be done:  89%| | 2668/3000 [00:01<00:00, 1618.53it/s]warmup should be done:  95%|| 2855/3000 [00:01<00:00, 1676.58it/s]warmup should be done:  95%|| 2846/3000 [00:01<00:00, 1665.51it/s]warmup should be done:  93%|| 2798/3000 [00:01<00:00, 1649.39it/s]warmup should be done:  94%|| 2820/3000 [00:01<00:00, 1649.50it/s]warmup should be done:  96%|| 2877/3000 [00:01<00:00, 1669.72it/s]warmup should be done:  95%|| 2843/3000 [00:01<00:00, 1655.57it/s]warmup should be done:  93%|| 2794/3000 [00:01<00:00, 1617.31it/s]warmup should be done:  94%|| 2831/3000 [00:01<00:00, 1620.52it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1681.39it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1674.11it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1667.18it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1662.27it/s]warmup should be done:  99%|| 2964/3000 [00:01<00:00, 1651.42it/s]warmup should be done: 100%|| 2986/3000 [00:01<00:00, 1651.68it/s]warmup should be done:  99%|| 2959/3000 [00:01<00:00, 1626.22it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1650.64it/s]warmup should be done: 100%|| 2995/3000 [00:01<00:00, 1625.49it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1643.78it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1639.08it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1629.99it/s]2022-12-12 01:02:21.722449: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f8eb7832dc0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 01:02:21.722510: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 01:02:21.752345: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f8eab82ed20 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 01:02:21.752406: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 01:02:22.307960: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f72a00282e0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 01:02:22.308023: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 01:02:22.314176: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f8eb7795c50 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 01:02:22.314239: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 01:02:22.480844: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f8eb782fcb0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 01:02:22.480912: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 01:02:22.482347: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x1ccd3ba0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 01:02:22.482391: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 01:02:22.543917: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f7238030eb0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 01:02:22.544010: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 01:02:22.549618: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f72a402c720 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 01:02:22.549665: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 01:02:24.015900: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 01:02:24.029190: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 01:02:24.551781: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 01:02:24.676564: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 01:02:24.723246: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 01:02:24.724727: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 01:02:24.843049: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 01:02:24.861607: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 01:02:26.891377: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 01:02:26.990650: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 01:02:27.368462: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 01:02:27.579812: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 01:02:27.587321: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 01:02:27.746679: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 01:02:27.757968: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 01:02:27.791665: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
[HCTR][01:03:00.817][ERROR][RK0][tid #140251309778688]: replica 2 reaches 1000, calling init pre replica
[HCTR][01:03:00.820][ERROR][RK0][tid #140251309778688]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][01:03:00.826][ERROR][RK0][tid #140251309778688]: coll ps creation done
[HCTR][01:03:00.826][ERROR][RK0][tid #140251309778688]: replica 2 waits for coll ps creation barrier
[HCTR][01:03:00.876][ERROR][RK0][tid #140251309778688]: replica 3 reaches 1000, calling init pre replica
[HCTR][01:03:00.876][ERROR][RK0][tid #140251309778688]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][01:03:00.881][ERROR][RK0][tid #140251309778688]: coll ps creation done
[HCTR][01:03:00.881][ERROR][RK0][tid #140251309778688]: replica 3 waits for coll ps creation barrier
[HCTR][01:03:00.909][ERROR][RK0][tid #140251318171392]: replica 7 reaches 1000, calling init pre replica
[HCTR][01:03:00.909][ERROR][RK0][tid #140251318171392]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][01:03:00.913][ERROR][RK0][tid #140251318171392]: coll ps creation done
[HCTR][01:03:00.913][ERROR][RK0][tid #140251318171392]: replica 7 waits for coll ps creation barrier
[HCTR][01:03:00.921][ERROR][RK0][tid #140251251062528]: replica 6 reaches 1000, calling init pre replica
[HCTR][01:03:00.921][ERROR][RK0][tid #140251251062528]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][01:03:00.929][ERROR][RK0][tid #140251251062528]: coll ps creation done
[HCTR][01:03:00.929][ERROR][RK0][tid #140251251062528]: replica 6 waits for coll ps creation barrier
[HCTR][01:03:01.047][ERROR][RK0][main]: replica 4 reaches 1000, calling init pre replica
[HCTR][01:03:01.047][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][01:03:01.052][ERROR][RK0][main]: coll ps creation done
[HCTR][01:03:01.052][ERROR][RK0][main]: replica 4 waits for coll ps creation barrier
[HCTR][01:03:01.118][ERROR][RK0][main]: replica 5 reaches 1000, calling init pre replica
[HCTR][01:03:01.118][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][01:03:01.126][ERROR][RK0][main]: coll ps creation done
[HCTR][01:03:01.126][ERROR][RK0][main]: replica 5 waits for coll ps creation barrier
[HCTR][01:03:01.132][ERROR][RK0][main]: replica 1 reaches 1000, calling init pre replica
[HCTR][01:03:01.132][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][01:03:01.137][ERROR][RK0][main]: coll ps creation done
[HCTR][01:03:01.137][ERROR][RK0][main]: replica 1 waits for coll ps creation barrier
[HCTR][01:03:01.140][ERROR][RK0][tid #140252517738240]: replica 0 reaches 1000, calling init pre replica
[HCTR][01:03:01.140][ERROR][RK0][tid #140252517738240]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][01:03:01.145][ERROR][RK0][tid #140252517738240]: coll ps creation done
[HCTR][01:03:01.145][ERROR][RK0][tid #140252517738240]: replica 0 waits for coll ps creation barrier
[HCTR][01:03:01.145][ERROR][RK0][tid #140252517738240]: replica 0 preparing frequency
[HCTR][01:03:01.989][ERROR][RK0][tid #140252517738240]: replica 0 preparing frequency done
[HCTR][01:03:02.023][ERROR][RK0][tid #140252517738240]: replica 0 calling init per replica
[HCTR][01:03:02.023][ERROR][RK0][tid #140251309778688]: replica 3 calling init per replica
[HCTR][01:03:02.023][ERROR][RK0][tid #140251318171392]: replica 7 calling init per replica
[HCTR][01:03:02.023][ERROR][RK0][main]: replica 4 calling init per replica
[HCTR][01:03:02.023][ERROR][RK0][main]: replica 1 calling init per replica
[HCTR][01:03:02.023][ERROR][RK0][main]: replica 5 calling init per replica
[HCTR][01:03:02.023][ERROR][RK0][tid #140251251062528]: replica 6 calling init per replica
[HCTR][01:03:02.023][ERROR][RK0][tid #140251309778688]: replica 2 calling init per replica
[HCTR][01:03:02.024][ERROR][RK0][tid #140252517738240]: Calling build_v2
[HCTR][01:03:02.024][ERROR][RK0][tid #140251309778688]: Calling build_v2
[HCTR][01:03:02.024][ERROR][RK0][tid #140251318171392]: Calling build_v2
[HCTR][01:03:02.024][ERROR][RK0][tid #140252517738240]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][01:03:02.024][ERROR][RK0][tid #140251309778688]: Calling build_v2
[HCTR][01:03:02.024][ERROR][RK0][main]: Calling build_v2
[HCTR][01:03:02.024][ERROR][RK0][main]: Calling build_v2
[HCTR][01:03:02.024][ERROR][RK0][main]: Calling build_v2
[HCTR][01:03:02.024][ERROR][RK0][tid #140251251062528]: Calling build_v2
[HCTR][01:03:02.024][ERROR][RK0][tid #140251309778688]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][01:03:02.024][ERROR][RK0][tid #140251318171392]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][01:03:02.024][ERROR][RK0][tid #140251309778688]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][01:03:02.024][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][01:03:02.024][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][01:03:02.024][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][01:03:02.024][ERROR][RK0][tid #140251251062528]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[2022-12-12 01:03:02. 28046: E[ /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:178] 2022-12-12 01:03:02v100x8, slow pcie.
 28089: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[:[2022-12-12 01:03:02178.]  28144v100x8, slow pcie: 
E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:196] [assigning 0 to cpu2022-12-12 01:03:02
2022-12-12 01:03:02.. 28176 28135: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc::196[178] [] assigning 0 to cpu2022-12-12 01:03:02v100x8, slow pcie
2022-12-12 01:03:02.
.[ 28226 28192[: : 2022-12-12 01:03:022022-12-12 01:03:02EE[.[.  2022-12-12 01:03:022022-12-12 01:03:02 28261 28232[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc..: : ::2022-12-12 01:03:02 28284 28274EE212178[.: : 2022-12-12 01:03:02  ] ]  28323EE./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccbuild asymm link desc with 8X Tesla V100-SXM2-32GB out of 8v100x8, slow pcie:    28362::

E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: 196178 :[:E[] ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2122022-12-12 01:03:02178 2022-12-12 01:03:02assigning 0 to cpuv100x8, slow pcie:] .] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.

178build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8 28494v100x8, slow pcie: 28501] 
: [
178: v100x8, slow pcieE2022-12-12 01:03:02] E[[
 .[v100x8, slow pcie 2022-12-12 01:03:022022-12-12 01:03:02/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc 285892022-12-12 01:03:02[
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc..:: .2022-12-12 01:03:02: 28622 28622[196E 28629.213: : 2022-12-12 01:03:02]  :  28650] EE.assigning 0 to cpu/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE: remote time is 8.68421   28689
: E
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: 196/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc ::[E] :/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2131962022-12-12 01:03:02[ assigning 0 to cpu212:] ] .2022-12-12 01:03:02/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
] 196remote time is 8.68421assigning 0 to cpu 28810.:build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8] 

: [ 28835196
assigning 0 to cpu[E2022-12-12 01:03:02: ] 
2022-12-12 01:03:02 [.Eassigning 0 to cpu.[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 01:03:02 28919 
 289422022-12-12 01:03:02:.: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: [.214 28967E:E2022-12-12 01:03:02 28979] :  [212 .: cpu time is 97.0588E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 01:03:02] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc 29022E
 :.build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8::  /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc212 29059
214E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:] : ]  :213[build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8Ecpu time is 97.0588/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc212] 2022-12-12 01:03:02
 
:] remote time is 8.68421./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc212build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8[
 29173:] 
2022-12-12 01:03:02: 212build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8[.E] 
[2022-12-12 01:03:02 29233 build asymm link desc with 8X Tesla V100-SXM2-32GB out of 82022-12-12 01:03:02.: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[
. 29270E:2022-12-12 01:03:02 29283:  [213.: E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 01:03:02]  29306E :.remote time is 8.68421:  /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc213 29335
E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:] :  :[214remote time is 8.68421E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2132022-12-12 01:03:02] 
 :] .cpu time is 97.0588/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc213remote time is 8.68421[ 29404
:] 
2022-12-12 01:03:02: 213remote time is 8.68421.E] [
 29448 remote time is 8.684212022-12-12 01:03:02: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
[.E:2022-12-12 01:03:02 29483 [214.: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 01:03:02]  29500E:.cpu time is 97.0588:  214 29513
E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] :  :cpu time is 97.0588E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc214
 :] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc214cpu time is 97.0588:] 
214cpu time is 97.0588] 
cpu time is 97.0588
[2022-12-12 01:04:21.441697: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:118] solver created. now build & solve
[2022-12-12 01:04:21.481828: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:123] solver built. now solve
block 0 storage is 00010001
	access is	0	0	0	0	4	4	4	4	
block 1 storage is 00100010
	access is	1	1	1	1	5	5	5	5	
block 2 storage is 01000100
	access is	2	2	2	2	6	6	6	6	
block 3 storage is 10001000
	access is	3	3	3	3	7	7	7	7	
block 4 storage is 00000000
	access is	8	8	8	8	8	8	8	8	
[2022-12-12 01:04:21.595391: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:127] solver solved
[2022-12-12 01:04:21.595454: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:195] 0 solved master
[2022-12-12 01:04:21.595487: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 0 solved
[2022-12-12 01:04:21.595518: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 0 initing device 0
[2022-12-12 01:04:21.595993: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-12 01:04:21.596049: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 01:04:21.597072: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 01:04:21.597920: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 01:04:21.610761: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 5 solved
[2022-12-12 01:04:21.610822: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 5 initing device 5
[2022-12-12 01:04:21.611001: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 6 solved
[[2022-12-12 01:04:212022-12-12 01:04:21..611062611041: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc::205202] ] worker 0 thread 6 initing device 6
2 solved
[2022-12-12 01:04:21.611179: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 2 initing device 2
[2022-12-12 01:04:21.611225: E[ 2022-12-12 01:04:21/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc.:611263202: ] E4 solved 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8[
2022-12-12 01:04:21.611336: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 4 initing device 4
[2022-12-12 01:04:21.611368: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 01:04:21.611561: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-12 01:04:21.611609: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB[
2022-12-12 01:04:21.611628: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-12 01:04:21.611687: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 01:04:21.611770: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-12 01:04:21.611819: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 01:04:21.614251: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 01:04:21.614362: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 01:04:21.614428: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 01:04:21.614480: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 01:04:21.615080: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 1 solved
[2022-12-12 01:04:21.615155: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 1 initing device 1
[2022-12-12 01:04:21.615530: E[ 2022-12-12 01:04:21/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc.:615550202: ] E7 solved 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc[:[2022-12-12 01:04:212022022-12-12 01:04:21.] .6156073 solved615619: 
: EE  [/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc2022-12-12 01:04:21::.1815205615671] ] : Building Coll Cache with ... num gpu device is 8worker 0 thread 7 initing device 7E

 /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 3 initing device 3
[2022-12-12 01:04:21.615758: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 01:04:21.616142: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815[] 2022-12-12 01:04:21Building Coll Cache with ... num gpu device is 8.
616160: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-12 01:04:21.616204: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980[] 2022-12-12 01:04:21eager alloc mem 381.47 MB.
616222: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 01:04:21.618552: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 01:04:21.618599: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 01:04:21.618774: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 01:04:21.618994: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 01:04:21.620006: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 01:04:21.620477: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 01:04:21.620545: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 01:04:21.623586: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 01:04:21.623626: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 01:04:21.623696: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 01:04:21.676123: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.00 Bytes
[2022-12-12 01:04:21.681401: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5
[2022-12-12 01:04:21.681523: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 01:04:21.682338: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 19.45 MB
[2022-12-12 01:04:21.682901: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 01:04:21.683900: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 01:04:21.683949: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 19.07 MB
[2022-12-12 01:04:21.695265: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 01:04:21.695952: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 01:04:21.695996: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 2.39 GB
[[[2022-12-12 01:04:212022-12-12 01:04:212022-12-12 01:04:21...701615701615701615: : : EEE   /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:::198019801980] ] ] eager alloc mem 5.00 Byteseager alloc mem 5.00 Byteseager alloc mem 5.00 Bytes


[2022-12-12 01:04:21.707085: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5
[2022-12-12 01:04:21.[7071582022-12-12 01:04:21: .E707180 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccE: 638/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc] :eager release cuda mem 5638
] eager release cuda mem 400000000
[[2022-12-12 01:04:212022-12-12 01:04:21..707241707257: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::638638] ] eager release cuda mem 5eager release cuda mem 400000000

[2022-12-12 01:04:21.707345: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 01:04:21.708066: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 19.45 MB
[2022-12-12 01:04:21.708819: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 19.45 MB
[2022-12-12 01:04:21.709331: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 19.45 MB
[[[2022-12-12 01:04:212022-12-12 01:04:212022-12-12 01:04:21...709475709475709475: : : EEE   /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:::198019801980] ] [] eager alloc mem 5.00 Byteseager alloc mem 5.00 Bytes2022-12-12 01:04:21eager alloc mem 5.00 Bytes

.
709575: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.00 Bytes
[2022-12-12 01:04:21.709833: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 01:04:21.710045: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 01:04:21.710083: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 01:04:21.710808: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 01:04:21.710852: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 19.07 MB
[2022-12-12 01:04:21.711031: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 01:04:21.711062: [E2022-12-12 01:04:21 ./hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc711074:: 638W]  eager release cuda mem 625663/hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc
:43] WORKER[0] alloc host memory 19.07 MB
[2022-12-12 01:04:21.711122: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 19.07 MB
[2022-12-12 01:04:21.716076: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5
[[2022-12-12 01:04:212022-12-12 01:04:21..716150716166: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::638638] ] eager release cuda mem 5eager release cuda mem 400000000

[2022-12-12 01:04:21.716236: [E2022-12-12 01:04:21 ./hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc716260:: 638E]  eager release cuda mem 5/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc
:638] eager release cuda mem 400000000
[2022-12-12 01:04:21.716335: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000[
2022-12-12 01:04:21.716343: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5
[2022-12-12 01:04:21.716435: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 01:04:21.717814: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 19.45 MB
[2022-12-12 01:04:21.718340: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 19.45 MB
[2022-12-12 01:04:21.718850: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 19.45 MB
[2022-12-12 01:04:21.719398: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 19.45 MB
[2022-12-12 01:04:21.720495: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 01:04:21.720545: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 01:04:21.720583: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 01:04:21.720706: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 01:04:21.721473: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 01:04:21.721516: W[ 2022-12-12 01:04:21/hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc.:72152343: ] EWORKER[0] alloc host memory 19.07 MB 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 01:04:21.721564: E [/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc2022-12-12 01:04:21:.638721581] : eager release cuda mem 625663W
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 19.07 MB
[2022-12-12 01:04:21.721620: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 19.07 MB
[2022-12-12 01:04:21.721678: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 01:04:21.721723: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 19.07 MB
[2022-12-12 01:04:21.723508: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 01:04:21.724084: E [/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu2022-12-12 01:04:21:.1980724114] : eager alloc mem 25.25 KBE
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 01:04:21.724170: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 2.39 GB
[2022-12-12 01:04:21.724703: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 01:04:21.724744: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 2.39 GB
[2022-12-12 01:04:21.726026: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 01:04:21.726634: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 01:04:21.726674: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 2.39 GB
[2022-12-12 01:04:21.735852: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 01:04:21.736020: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 01:04:21.736081: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 01:04:21.736186: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 01:04:21.736461: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 01:04:21.736505: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 2.39 GB
[2022-12-12 01:04:21.736625: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 01:04:21.736666: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980[] 2022-12-12 01:04:21eager alloc mem 2.39 GB.
736680: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 01:04:21.736728: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 2.39 GB
[2022-12-12 01:04:21.736786: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 01:04:21.736827: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 2.39 GB
[[[[[[[[2022-12-12 01:04:222022-12-12 01:04:222022-12-12 01:04:222022-12-12 01:04:222022-12-12 01:04:222022-12-12 01:04:222022-12-12 01:04:222022-12-12 01:04:22........596896596897596897596900596896596897596897596896: : : : : : : : EEEEEEEE        /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::::::::19261926192619261926192619261926] ] ] ] ] Device 4 init p2p of link 5] ] ] Device 2 init p2p of link 1Device 1 init p2p of link 7Device 7 init p2p of link 4Device 3 init p2p of link 2
Device 0 init p2p of link 3Device 6 init p2p of link 0Device 5 init p2p of link 6






[[2022-12-12 01:04:22[[2022-12-12 01:04:22[.[[2022-12-12 01:04:222022-12-12 01:04:22.2022-12-12 01:04:225974542022-12-12 01:04:22[2022-12-12 01:04:22..597455.: .2022-12-12 01:04:22.597461597462: 597465E597465.597464: : E:  : 597482: EE E/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE: E  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu : E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu1980/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::1980:] :/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:19801980] 1980eager alloc mem 611.00 KB1980:1980] ] eager alloc mem 611.00 KB] 
] 1980] eager alloc mem 611.00 KBeager alloc mem 611.00 KB
eager alloc mem 611.00 KBeager alloc mem 611.00 KB] eager alloc mem 611.00 KB



eager alloc mem 611.00 KB

[2022-12-12 01:04:22.598589[[: 2022-12-12 01:04:222022-12-12 01:04:22[E..2022-12-12 01:04:22 598596598597[[./hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc: : 2022-12-12 01:04:22[2022-12-12 01:04:22598605:EE.2022-12-12 01:04:22.: 638  598638.598628E] /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc: 598645[:  eager release cuda mem 625663::E: 2022-12-12 01:04:22E/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc
638638 E. :] ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc 598728/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc638eager release cuda mem 625663eager release cuda mem 625663:/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc: :] 

638:E638eager release cuda mem 625663] 638 ] 
eager release cuda mem 625663] /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cceager release cuda mem 625663
eager release cuda mem 625663:

638] eager release cuda mem 625663
[2022-12-12 01:04:22.612244: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 2
[2022-12-12 01:04:22.612425: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu[:2022-12-12 01:04:221980.] 612427eager alloc mem 611.00 KB: 
E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 6 init p2p of link 5
[2022-12-12 01:04:22.612606: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 01:04:22.613278: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 01:04:22.613432: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 01:04:22.614034: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 7 init p2p of link 1
[2022-12-12 01:04:22.614215: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 01:04:22.614759: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 3 init p2p of link 0
[2022-12-12 01:04:22.614922: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB[
2022-12-12 01:04:22.614936: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 0 init p2p of link 6
[2022-12-12 01:04:22.614989: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 01:04:22.615098: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 01:04:22.615188: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 2 init p2p of link 3
[2022-12-12 01:04:22.615370[: 2022-12-12 01:04:22E. 615367/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu: :E1980 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cueager alloc mem 611.00 KB:
1926] Device 5 init p2p of link 4
[2022-12-12 01:04:22.615577: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 01:04:22.615670: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 4 init p2p of link 7
[2022-12-12 01:04:22.615748: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 01:04:22.615884: [E2022-12-12 01:04:22 ./hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc615892:: 638E]  eager release cuda mem 625663/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu
:1980] eager alloc mem 611.00 KB
[2022-12-12 01:04:22.616246: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 01:04:22.616387: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 01:04:22.616821: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 01:04:22.626142: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 3
[2022-12-12 01:04:22.626272: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 01:04:22.626741: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 7 init p2p of link 6
[2022-12-12 01:04:22.626875: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 01:04:22.626894: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 6 init p2p of link 4
[2022-12-12 01:04:22.627017: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 01:04:22.627080: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 01:04:22.627518: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 0 init p2p of link 1
[2022-12-12 01:04:22.627645: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 01:04:22.627693: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 01:04:22.627826: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 01:04:22.628491: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 01:04:22.629303: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 3 init p2p of link 5
[2022-12-12 01:04:22.629417: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 01:04:22.629805: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 2 init p2p of link 0
[2022-12-12 01:04:22.629944: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 01:04:22.630036: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 5 init p2p of link 7
[2022-12-12 01:04:22.630154: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 01:04:22.630188: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 01:04:22.630757: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 01:04:22.630968: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 01:04:22.631044: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 4 init p2p of link 2
[2022-12-12 01:04:22.631260: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 01:04:22.632086: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 01:04:22.644037: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 3 init p2p of link 1
[2022-12-12 01:04:22.644154: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 01:04:22.644991: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 01:04:22.645250: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 4 init p2p of link 6
[2022-12-12 01:04:22.645417: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 01:04:22.646299: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 01:04:22.646520: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 0
[2022-12-12 01:04:22.646641: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 01:04:22.646972: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 7 init p2p of link 5
[2022-12-12 01:04:22.647095: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 01:04:22.647322: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 6 init p2p of link 7
[2022-12-12 01:04:22.647435: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 01:04:22.647465: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 01:04:22.647876: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 0 init p2p of link 2
[2022-12-12 01:04:22.647923: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 01:04:22.648003: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 01:04:22.648243: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 01:04:22.648637: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 2 init p2p of link 4
[2022-12-12 01:04:22.648769: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 01:04:22.648828: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 01:04:22.648924: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 5 init p2p of link 3
[2022-12-12 01:04:22.649049: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 01:04:22.649539: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 01:04:22.649812: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 01:04:22.658250: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 20400000
[2022-12-12 01:04:22.661321: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 5000000 / 100000000 nodes ( 5.00 %~5.00 %) | remote 15000000 / 100000000 nodes ( 15.00 %) | cpu 80000000 / 100000000 nodes ( 80.00 %) | 2.39 GB | 1.04511 secs 
[2022-12-12 01:04:22.662217: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 20400000
[2022-12-12 01:04:22.662553: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 20400000
[2022-12-12 01:04:22.662859: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 5000000 / 100000000 nodes ( 5.00 %~5.00 %) | remote 15000000 / 100000000 nodes ( 15.00 %) | cpu 80000000 / 100000000 nodes ( 80.00 %) | 2.39 GB | 1.05119 secs 
[2022-12-12 01:04:22.663005: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 20400000
[2022-12-12 01:04:22.663055: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 5000000 / 100000000 nodes ( 5.00 %~5.00 %) | remote 15000000 / 100000000 nodes ( 15.00 %) | cpu 80000000 / 100000000 nodes ( 80.00 %) | 2.39 GB | 1.0517 secs 
[2022-12-12 01:04:22.663474: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 5000000 / 100000000 nodes ( 5.00 %~5.00 %) | remote 15000000 / 100000000 nodes ( 15.00 %) | cpu 80000000 / 100000000 nodes ( 80.00 %) | 2.39 GB | 1.05167 secs 
[2022-12-12 01:04:22.664665: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 20400000
[2022-12-12 01:04:22.665007: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 5000000 / 100000000 nodes ( 5.00 %~5.00 %) | remote 15000000 / 100000000 nodes ( 15.00 %) | cpu 80000000 / 100000000 nodes ( 80.00 %) | 2.39 GB | 1.04926 secs 
[2022-12-12 01:04:22.665294: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 20400000
[2022-12-12 01:04:22.665622: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 5000000 / 100000000 nodes ( 5.00 %~5.00 %) | remote 15000000 / 100000000 nodes ( 15.00 %) | cpu 80000000 / 100000000 nodes ( 80.00 %) | 2.39 GB | 1.04943 secs 
[2022-12-12 01:04:22.665780: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 20400000
[2022-12-12 01:04:22.666107[: 2022-12-12 01:04:22E. 666125/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc: :E638 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cueager release cuda mem 20400000:
1955] Asymm Coll cache (policy: clique_part) | local 5000000 / 100000000 nodes ( 5.00 %~5.00 %) | remote 15000000 / 100000000 nodes ( 15.00 %) | cpu 80000000 / 100000000 nodes ( 80.00 %) | 2.39 GB | 1.05453 secs 
[2022-12-12 01:04:22.666458: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 5000000 / 100000000 nodes ( 5.00 %~5.00 %) | remote 15000000 / 100000000 nodes ( 15.00 %) | cpu 80000000 / 100000000 nodes ( 80.00 %) | 2.39 GB | 1.07042 secs 
[HCTR][01:04:22.666][ERROR][RK0][main]: replica 1 calling init per replica done, doing barrier
[HCTR][01:04:22.666][ERROR][RK0][tid #140252517738240]: replica 0 calling init per replica done, doing barrier
[HCTR][01:04:22.666][ERROR][RK0][tid #140251318171392]: replica 7 calling init per replica done, doing barrier
[HCTR][01:04:22.666][ERROR][RK0][tid #140251251062528]: replica 6 calling init per replica done, doing barrier
[HCTR][01:04:22.666][ERROR][RK0][main]: replica 5 calling init per replica done, doing barrier
[HCTR][01:04:22.666][ERROR][RK0][main]: replica 4 calling init per replica done, doing barrier
[HCTR][01:04:22.666][ERROR][RK0][tid #140251309778688]: replica 3 calling init per replica done, doing barrier
[HCTR][01:04:22.666][ERROR][RK0][tid #140251309778688]: replica 2 calling init per replica done, doing barrier
[HCTR][01:04:22.666][ERROR][RK0][tid #140251309778688]: replica 2 calling init per replica done, doing barrier done
[HCTR][01:04:22.666][ERROR][RK0][tid #140251309778688]: replica 3 calling init per replica done, doing barrier done
[HCTR][01:04:22.666][ERROR][RK0][main]: replica 1 calling init per replica done, doing barrier done
[HCTR][01:04:22.666][ERROR][RK0][tid #140251318171392]: replica 7 calling init per replica done, doing barrier done
[HCTR][01:04:22.666][ERROR][RK0][tid #140252517738240]: replica 0 calling init per replica done, doing barrier done
[HCTR][01:04:22.666][ERROR][RK0][main]: replica 5 calling init per replica done, doing barrier done
[HCTR][01:04:22.666][ERROR][RK0][tid #140251251062528]: replica 6 calling init per replica done, doing barrier done
[HCTR][01:04:22.666][ERROR][RK0][main]: replica 4 calling init per replica done, doing barrier done
[HCTR][01:04:22.666][ERROR][RK0][tid #140251309778688]: init per replica done
[HCTR][01:04:22.666][ERROR][RK0][tid #140251309778688]: init per replica done
[HCTR][01:04:22.666][ERROR][RK0][main]: init per replica done
[HCTR][01:04:22.666][ERROR][RK0][tid #140251318171392]: init per replica done
[HCTR][01:04:22.666][ERROR][RK0][main]: init per replica done
[HCTR][01:04:22.666][ERROR][RK0][tid #140251251062528]: init per replica done
[HCTR][01:04:22.666][ERROR][RK0][main]: init per replica done
[HCTR][01:04:22.669][ERROR][RK0][tid #140252517738240]: init per replica done








