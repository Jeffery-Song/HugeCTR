2022-12-11 23:54:00.875973: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:00.887139: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:00.892514: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:00.896783: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:00.900741: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:00.914600: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:00.928457: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:00.934701: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:00.986368: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:00.990327: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:00.992216: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:00.993319: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:00.993534: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:00.995251: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:00.995261: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:00.996738: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:00.996937: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:00.998241: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:00.998786: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.000049: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.000546: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.001643: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.002078: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.003044: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.003787: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.004545: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.005562: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.005987: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.007408: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.008348: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.009281: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.010292: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.012119: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.013263: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.014186: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.015221: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.016161: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.017098: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.018039: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.018953: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.022966: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.024097: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.024762: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 23:54:01.025119: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.026122: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.027201: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.028259: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.029318: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.030630: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.033414: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.034541: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.035186: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.035788: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.036713: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.037441: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.038144: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.039072: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.040842: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.042761: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.045033: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.046698: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.049497: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.049900: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.052071: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.052582: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.052780: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.052825: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.055066: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.055870: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.055973: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.056221: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.056631: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.058535: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.059241: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.059283: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.059696: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.060144: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.061685: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.061920: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.062429: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.062987: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.066112: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.068414: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.068545: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.069456: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.069774: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.070363: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.071559: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.071724: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.072786: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.073065: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.073840: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.077676: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.092383: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.107590: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.109179: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.109422: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.110209: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.110496: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.110876: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.110929: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.114094: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.114717: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.114891: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.114984: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.115211: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.119156: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.120221: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.120281: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.120381: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.122529: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.122917: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.123169: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.125513: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.125694: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.125874: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.128358: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.128474: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.128614: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.130708: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.130749: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.131061: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.132898: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.133048: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.133381: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.135222: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.135263: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.135603: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.137581: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.137619: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.138878: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.139666: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.139710: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.141303: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.142032: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.142292: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.143676: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.144177: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.144358: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.145928: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.146760: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.146853: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.148205: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.148998: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.149138: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.150691: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.152026: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.152738: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.152889: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.153058: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.154376: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.155451: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.155989: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.156186: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.157604: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.158730: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.158784: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.158827: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.160264: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.161409: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.161568: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.162003: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.162743: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 23:54:01.162958: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.163229: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.164879: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.164887: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.165244: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.166547: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.166636: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.167601: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.168494: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.168541: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.168748: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.170683: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.170698: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.171578: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.172570: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.172742: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.172780: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.173481: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.174703: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.174918: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.175624: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.177058: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.177170: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.177273: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.177971: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.179068: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.180087: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.181367: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.181491: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.182046: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.182955: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.183785: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.183937: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 23:54:01.184691: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.184813: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.185008: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 23:54:01.186200: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.187178: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.188196: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.188353: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.190005: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.190746: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.191766: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.191887: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.193992: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.194499: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.194846: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.195532: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.195804: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.196346: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.198172: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.198932: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.199407: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.200458: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.200769: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.201099: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.202867: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.203495: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.204259: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.205222: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.205403: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.205700: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.208921: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.210074: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.211324: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.211608: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.213234: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.214745: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.216291: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.217346: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.246175: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.247310: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.248630: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.249837: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.250898: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.252438: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.254701: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.254761: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.255883: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.257255: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.259182: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.259254: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.260918: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.274190: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.275876: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.276093: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.277732: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.280074: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.282215: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.282563: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.283594: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.285500: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.314706: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.315732: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.316765: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.319033: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.320445: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.320696: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.321631: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.323592: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.325654: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.328842: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.329219: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 23:54:01.334726: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.336645: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.338007: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.339340: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.339910: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.342292: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.343970: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 23:54:01.354139: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.373300: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.374612: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.407658: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.408555: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.409223: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.410095: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 23:54:01.413762: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.417924: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 23:54:01.421229: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.425861: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.428396: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.431264: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.434130: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:01.439782: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:02.316181: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:02.316799: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:02.317503: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:02.318355: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 23:54:02.318415: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-11 23:54:02.336341: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:02.336966: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:02.337709: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:02.338430: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:02.338942: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:02.339431: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-11 23:54:02.382467: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 23:54:02.382665: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 23:54:02.432538: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12340
2022-12-11 23:54:02.529033: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:02.530082: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:02.530614: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:02.531080: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 23:54:02.531148: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-11 23:54:02.548976: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:02.550440: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:02.550955: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:02.551557: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:02.552270: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:02.552742: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:1/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-11 23:54:02.561511: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:02.562116: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:02.562641: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:02.563108: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 23:54:02.563175: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-11 23:54:02.581503: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:02.582142: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:02.582652: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:02.583472: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:02.583988: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:02.584672: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:2/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-11 23:54:02.614362: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:02.614964: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:02.615512: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:02.615975: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 23:54:02.616029: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-11 23:54:02.633898: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:02.633897: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:02.635023: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:02.635109: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:02.635950: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:02.636075: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:02.636728: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:02.636848: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 23:54:02.636901: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-11 23:54:02.637449: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:02.638190: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:02.638322: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:02.639258: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:02.639297: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:4/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-11 23:54:02.639964: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 23:54:02.640025: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-11 23:54:02.647911: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 23:54:02.648111: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 23:54:02.650825: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12341
2022-12-11 23:54:02.655187: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:02.655846: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:02.656370: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:02.656892: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:02.657173: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:02.657635: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:02.658326: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:02.658884: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:02.659250: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:02.659850: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:02.660389: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:7/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-11 23:54:02.660691: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:02.660922: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 23:54:02.660974: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-11 23:54:02.661591: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:02.662125: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:02.662611: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:5/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-11 23:54:02.664291: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 23:54:02.664445: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 23:54:02.666440: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12342
2022-12-11 23:54:02.678847: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:02.679534: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:02.680051: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:02.680657: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:02.681192: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:02.681673: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:6/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-11 23:54:02.681658: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 23:54:02.681823: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 23:54:02.683607: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12344
2022-12-11 23:54:02.688469: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:02.689082: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:02.689606: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:02.690074: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 23:54:02.690127: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-11 23:54:02.702437: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 23:54:02.702610: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 23:54:02.704377: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12347
2022-12-11 23:54:02.705156: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 23:54:02.705329: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 23:54:02.707212: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12345
2022-12-11 23:54:02.707660: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:02.708322: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:02.708830: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:02.709418: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:02.709927: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:54:02.710399: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:3/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-11 23:54:02.724826: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 23:54:02.725000: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 23:54:02.726874: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12346
2022-12-11 23:54:02.752184: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 23:54:02.752370: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 23:54:02.754095: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12343
[HCTR][23:54:04.009][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][23:54:04.014][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][23:54:04.015][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][23:54:04.015][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][23:54:04.015][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][23:54:04.015][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][23:54:04.059][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][23:54:04.059][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 1it [00:01,  1.55s/it]warmup run: 95it [00:01, 79.85it/s]warmup run: 1it [00:01,  1.55s/it]warmup run: 1it [00:01,  1.54s/it]warmup run: 192it [00:01, 175.42it/s]warmup run: 93it [00:01, 78.26it/s]warmup run: 96it [00:01, 81.06it/s]warmup run: 289it [00:01, 280.89it/s]warmup run: 1it [00:01,  1.50s/it]warmup run: 188it [00:01, 172.02it/s]warmup run: 192it [00:01, 175.88it/s]warmup run: 386it [00:01, 390.62it/s]warmup run: 1it [00:01,  1.49s/it]warmup run: 101it [00:01, 87.28it/s]warmup run: 1it [00:01,  1.49s/it]warmup run: 1it [00:01,  1.48s/it]warmup run: 283it [00:01, 275.43it/s]warmup run: 289it [00:01, 281.64it/s]warmup run: 483it [00:02, 497.98it/s]warmup run: 93it [00:01, 80.80it/s]warmup run: 1it [00:01,  1.47s/it]warmup run: 201it [00:01, 187.81it/s]warmup run: 74it [00:01, 64.62it/s]warmup run: 101it [00:01, 88.41it/s]warmup run: 377it [00:01, 381.47it/s]warmup run: 385it [00:01, 390.02it/s]warmup run: 581it [00:02, 598.10it/s]warmup run: 186it [00:01, 174.70it/s]warmup run: 96it [00:01, 84.50it/s]warmup run: 301it [00:01, 297.81it/s]warmup run: 173it [00:01, 167.58it/s]warmup run: 201it [00:01, 189.92it/s]warmup run: 470it [00:02, 483.22it/s]warmup run: 479it [00:02, 492.16it/s]warmup run: 680it [00:02, 687.75it/s]warmup run: 280it [00:01, 278.80it/s]warmup run: 197it [00:01, 187.99it/s]warmup run: 400it [00:01, 410.06it/s]warmup run: 273it [00:01, 281.51it/s]warmup run: 302it [00:01, 302.35it/s]warmup run: 566it [00:02, 581.87it/s]warmup run: 576it [00:02, 592.45it/s]warmup run: 778it [00:02, 758.92it/s]warmup run: 373it [00:01, 384.68it/s]warmup run: 296it [00:01, 298.52it/s]warmup run: 500it [00:02, 520.49it/s]warmup run: 373it [00:01, 398.96it/s]warmup run: 404it [00:01, 419.56it/s]warmup run: 662it [00:02, 667.68it/s]warmup run: 674it [00:02, 681.32it/s]warmup run: 876it [00:02, 816.42it/s]warmup run: 466it [00:01, 487.30it/s]warmup run: 394it [00:01, 410.56it/s]warmup run: 601it [00:02, 623.44it/s]warmup run: 473it [00:01, 512.71it/s]warmup run: 507it [00:01, 534.34it/s]warmup run: 758it [00:02, 739.45it/s]warmup run: 771it [00:02, 751.76it/s]warmup run: 974it [00:02, 859.37it/s]warmup run: 554it [00:02, 569.61it/s]warmup run: 492it [00:01, 518.64it/s]warmup run: 702it [00:02, 712.39it/s]warmup run: 575it [00:02, 619.55it/s]warmup run: 611it [00:02, 640.96it/s]warmup run: 852it [00:02, 788.37it/s]warmup run: 868it [00:02, 807.64it/s]warmup run: 1072it [00:02, 892.19it/s]warmup run: 651it [00:02, 661.38it/s]warmup run: 591it [00:02, 619.25it/s]warmup run: 804it [00:02, 787.38it/s]warmup run: 676it [00:02, 709.04it/s]warmup run: 715it [00:02, 732.95it/s]warmup run: 945it [00:02, 822.89it/s]warmup run: 964it [00:02, 848.95it/s]warmup run: 1170it [00:02, 915.91it/s]warmup run: 747it [00:02, 734.28it/s]warmup run: 691it [00:02, 706.95it/s]warmup run: 905it [00:02, 843.96it/s]warmup run: 777it [00:02, 783.80it/s]warmup run: 818it [00:02, 805.87it/s]warmup run: 1066it [00:02, 895.43it/s]warmup run: 1038it [00:02, 844.63it/s]warmup run: 840it [00:02, 783.78it/s]warmup run: 788it [00:02, 758.82it/s]warmup run: 1268it [00:02, 846.09it/s]warmup run: 1007it [00:02, 889.75it/s]warmup run: 875it [00:02, 805.30it/s]warmup run: 919it [00:02, 858.51it/s]warmup run: 1168it [00:02, 928.62it/s]warmup run: 1130it [00:02, 857.02it/s]warmup run: 932it [00:02, 814.95it/s]warmup run: 1358it [00:02, 860.24it/s]warmup run: 883it [00:02, 797.20it/s]warmup run: 1110it [00:02, 927.83it/s]warmup run: 1020it [00:02, 894.06it/s]warmup run: 1269it [00:02, 952.04it/s]warmup run: 970it [00:02, 820.24it/s]warmup run: 1222it [00:02, 872.52it/s]warmup run: 1030it [00:02, 860.47it/s]warmup run: 1462it [00:03, 909.03it/s]warmup run: 979it [00:02, 838.72it/s]warmup run: 1212it [00:02, 953.36it/s]warmup run: 1069it [00:02, 865.64it/s]warmup run: 1369it [00:02, 960.25it/s]warmup run: 1313it [00:02, 882.71it/s]warmup run: 1120it [00:02, 895.25it/s]warmup run: 1128it [00:02, 893.85it/s]warmup run: 1558it [00:03, 923.47it/s]warmup run: 1074it [00:02, 867.71it/s]warmup run: 1316it [00:02, 976.69it/s]warmup run: 1469it [00:03, 970.96it/s]warmup run: 1168it [00:02, 898.27it/s]warmup run: 1405it [00:03, 890.74it/s]warmup run: 1219it [00:02, 920.66it/s]warmup run: 1230it [00:02, 928.45it/s]warmup run: 1656it [00:03, 937.35it/s]warmup run: 1170it [00:02, 893.40it/s]warmup run: 1418it [00:02, 985.03it/s]warmup run: 1266it [00:02, 920.53it/s]warmup run: 1571it [00:03, 983.03it/s]warmup run: 1317it [00:02, 936.64it/s]warmup run: 1499it [00:03, 902.20it/s]warmup run: 1332it [00:02, 952.55it/s]warmup run: 1270it [00:02, 923.90it/s]warmup run: 1752it [00:03, 936.85it/s]warmup run: 1521it [00:03, 997.77it/s]warmup run: 1672it [00:03, 989.89it/s]warmup run: 1366it [00:02, 939.83it/s]warmup run: 1418it [00:02, 957.45it/s]warmup run: 1593it [00:03, 910.62it/s]warmup run: 1436it [00:03, 976.36it/s]warmup run: 1850it [00:03, 948.84it/s]warmup run: 1368it [00:02, 937.84it/s]warmup run: 1623it [00:03, 1004.32it/s]warmup run: 1773it [00:03, 994.79it/s]warmup run: 1463it [00:03, 944.13it/s]warmup run: 1522it [00:03, 981.27it/s]warmup run: 1685it [00:03, 913.15it/s]warmup run: 1538it [00:03, 987.31it/s]warmup run: 1954it [00:03, 974.88it/s]warmup run: 1465it [00:02, 946.50it/s]warmup run: 1727it [00:03, 1012.13it/s]warmup run: 1874it [00:03, 998.47it/s]warmup run: 1563it [00:03, 959.70it/s]warmup run: 1624it [00:03, 992.59it/s]warmup run: 1777it [00:03, 908.90it/s]warmup run: 1640it [00:03, 996.92it/s]warmup run: 2061it [00:03, 1001.65it/s]warmup run: 1562it [00:03, 942.84it/s]warmup run: 1830it [00:03, 1013.89it/s]warmup run: 1975it [00:03, 1000.18it/s]warmup run: 1661it [00:03, 961.84it/s]warmup run: 1875it [00:03, 927.38it/s]warmup run: 1725it [00:03, 978.76it/s]warmup run: 1743it [00:03, 1004.23it/s]warmup run: 2178it [00:03, 1050.38it/s]warmup run: 1661it [00:03, 956.51it/s]warmup run: 1933it [00:03, 1015.51it/s]warmup run: 2088it [00:03, 1038.81it/s]warmup run: 1759it [00:03, 967.11it/s]warmup run: 1974it [00:03, 945.02it/s]warmup run: 1824it [00:03, 979.17it/s]warmup run: 1845it [00:03, 1007.10it/s]warmup run: 2291it [00:03, 1072.10it/s]warmup run: 1761it [00:03, 968.72it/s]warmup run: 2042it [00:03, 1037.60it/s]warmup run: 2207it [00:03, 1081.41it/s]warmup run: 1857it [00:03, 965.53it/s]warmup run: 2086it [00:03, 995.16it/s]warmup run: 1926it [00:03, 990.60it/s]warmup run: 1947it [00:03, 1006.18it/s]warmup run: 2404it [00:03, 1087.65it/s]warmup run: 1861it [00:03, 977.77it/s]warmup run: 2162it [00:03, 1085.12it/s]warmup run: 2325it [00:03, 1110.12it/s]warmup run: 1957it [00:03, 974.03it/s]warmup run: 2204it [00:03, 1049.60it/s]warmup run: 2034it [00:03, 1016.23it/s]warmup run: 2056it [00:03, 1030.06it/s]warmup run: 2515it [00:04, 1092.90it/s]warmup run: 1961it [00:03, 981.74it/s]warmup run: 2282it [00:03, 1118.27it/s]warmup run: 2443it [00:03, 1130.47it/s]warmup run: 2063it [00:03, 997.92it/s]warmup run: 2323it [00:03, 1089.32it/s]warmup run: 2156it [00:03, 1075.85it/s]warmup run: 2176it [00:03, 1080.04it/s]warmup run: 2626it [00:04, 1096.19it/s]warmup run: 2071it [00:03, 1016.76it/s]warmup run: 2403it [00:03, 1143.28it/s]warmup run: 2562it [00:04, 1146.17it/s]warmup run: 2181it [00:03, 1051.00it/s]warmup run: 2442it [00:04, 1118.04it/s]warmup run: 2278it [00:03, 1118.29it/s]warmup run: 2297it [00:03, 1116.33it/s]warmup run: 2736it [00:04, 1095.86it/s]warmup run: 2189it [00:03, 1065.28it/s]warmup run: 2524it [00:03, 1160.82it/s]warmup run: 2680it [00:04, 1154.76it/s]warmup run: 2300it [00:03, 1090.13it/s]warmup run: 2561it [00:04, 1138.10it/s]warmup run: 2391it [00:03, 1062.85it/s]warmup run: 2417it [00:03, 1140.65it/s]warmup run: 2847it [00:04, 1099.78it/s]warmup run: 2307it [00:03, 1097.76it/s]warmup run: 2643it [00:04, 1169.28it/s]warmup run: 2798it [00:04, 1161.80it/s]warmup run: 2417it [00:03, 1113.31it/s]warmup run: 2677it [00:04, 1142.52it/s]warmup run: 2509it [00:03, 1095.78it/s]warmup run: 2537it [00:04, 1157.51it/s]warmup run: 2958it [00:04, 1099.98it/s]warmup run: 2425it [00:03, 1120.40it/s]warmup run: 2763it [00:04, 1178.22it/s]warmup run: 3000it [00:04, 659.77it/s] warmup run: 2916it [00:04, 1164.35it/s]warmup run: 2534it [00:04, 1127.56it/s]warmup run: 2795it [00:04, 1152.52it/s]warmup run: 2628it [00:04, 1121.80it/s]warmup run: 2655it [00:04, 1163.80it/s]warmup run: 2544it [00:03, 1140.60it/s]warmup run: 2883it [00:04, 1182.65it/s]warmup run: 3000it [00:04, 676.62it/s] warmup run: 2651it [00:04, 1138.24it/s]warmup run: 2914it [00:04, 1162.29it/s]warmup run: 2750it [00:04, 1149.32it/s]warmup run: 2773it [00:04, 1168.28it/s]warmup run: 2664it [00:04, 1157.34it/s]warmup run: 3000it [00:04, 693.84it/s] warmup run: 3000it [00:04, 658.57it/s] warmup run: 2769it [00:04, 1150.35it/s]warmup run: 2872it [00:04, 1169.49it/s]warmup run: 2891it [00:04, 1170.94it/s]warmup run: 2783it [00:04, 1165.32it/s]warmup run: 2887it [00:04, 1157.02it/s]warmup run: 2994it [00:04, 1182.73it/s]warmup run: 3000it [00:04, 680.10it/s] warmup run: 3000it [00:04, 690.66it/s] warmup run: 2903it [00:04, 1174.61it/s]warmup run: 3000it [00:04, 677.42it/s] warmup run: 3000it [00:04, 683.57it/s] 

warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]



warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   6%|▌         | 167/3000 [00:00<00:01, 1666.76it/s]warmup should be done:   5%|▌         | 162/3000 [00:00<00:01, 1618.59it/s]warmup should be done:   5%|▌         | 162/3000 [00:00<00:01, 1615.20it/s]warmup should be done:   5%|▌         | 162/3000 [00:00<00:01, 1613.65it/s]warmup should be done:   5%|▌         | 154/3000 [00:00<00:01, 1530.26it/s]warmup should be done:   5%|▌         | 151/3000 [00:00<00:01, 1504.31it/s]warmup should be done:   5%|▌         | 157/3000 [00:00<00:01, 1560.33it/s]warmup should be done:   5%|▌         | 157/3000 [00:00<00:01, 1560.36it/s]warmup should be done:  11%|█         | 325/3000 [00:00<00:01, 1623.64it/s]warmup should be done:  11%|█         | 326/3000 [00:00<00:01, 1627.27it/s]warmup should be done:  11%|█         | 327/3000 [00:00<00:01, 1630.61it/s]warmup should be done:  11%|█         | 316/3000 [00:00<00:01, 1575.25it/s]warmup should be done:  10%|█         | 303/3000 [00:00<00:01, 1510.17it/s]warmup should be done:  10%|█         | 309/3000 [00:00<00:01, 1536.62it/s]warmup should be done:  10%|█         | 315/3000 [00:00<00:01, 1567.59it/s]warmup should be done:  11%|█         | 334/3000 [00:00<00:01, 1648.69it/s]warmup should be done:  16%|█▋        | 488/3000 [00:00<00:01, 1623.54it/s]warmup should be done:  16%|█▋        | 489/3000 [00:00<00:01, 1626.74it/s]warmup should be done:  16%|█▌        | 479/3000 [00:00<00:01, 1599.95it/s]warmup should be done:  15%|█▌        | 463/3000 [00:00<00:01, 1537.69it/s]warmup should be done:  16%|█▋        | 491/3000 [00:00<00:01, 1631.88it/s]warmup should be done:  16%|█▌        | 480/3000 [00:00<00:01, 1601.95it/s]warmup should be done:  15%|█▌        | 455/3000 [00:00<00:01, 1507.71it/s]warmup should be done:  17%|█▋        | 499/3000 [00:00<00:01, 1633.80it/s]warmup should be done:  22%|██▏       | 651/3000 [00:00<00:01, 1625.81it/s]warmup should be done:  22%|██▏       | 652/3000 [00:00<00:01, 1626.41it/s]warmup should be done:  21%|██▏       | 642/3000 [00:00<00:01, 1609.85it/s]warmup should be done:  21%|██▏       | 644/3000 [00:00<00:01, 1616.85it/s]warmup should be done:  22%|██▏       | 655/3000 [00:00<00:01, 1631.07it/s]warmup should be done:  21%|██        | 617/3000 [00:00<00:01, 1532.49it/s]warmup should be done:  20%|██        | 606/3000 [00:00<00:01, 1499.43it/s]warmup should be done:  22%|██▏       | 663/3000 [00:00<00:01, 1627.72it/s]warmup should be done:  27%|██▋       | 814/3000 [00:00<00:01, 1625.39it/s]warmup should be done:  27%|██▋       | 805/3000 [00:00<00:01, 1615.45it/s]warmup should be done:  27%|██▋       | 816/3000 [00:00<00:01, 1627.82it/s]warmup should be done:  27%|██▋       | 808/3000 [00:00<00:01, 1624.76it/s]warmup should be done:  26%|██▌       | 776/3000 [00:00<00:01, 1551.12it/s]warmup should be done:  27%|██▋       | 819/3000 [00:00<00:01, 1629.94it/s]warmup should be done:  26%|██▌       | 765/3000 [00:00<00:01, 1531.11it/s]warmup should be done:  28%|██▊       | 826/3000 [00:00<00:01, 1623.50it/s]warmup should be done:  32%|███▏      | 971/3000 [00:00<00:01, 1626.24it/s]warmup should be done:  32%|███▏      | 967/3000 [00:00<00:01, 1612.60it/s]warmup should be done:  31%|███       | 937/3000 [00:00<00:01, 1570.51it/s]warmup should be done:  33%|███▎      | 979/3000 [00:00<00:01, 1622.68it/s]warmup should be done:  33%|███▎      | 977/3000 [00:00<00:01, 1612.66it/s]warmup should be done:  31%|███       | 928/3000 [00:00<00:01, 1564.08it/s]warmup should be done:  33%|███▎      | 982/3000 [00:00<00:01, 1617.85it/s]warmup should be done:  33%|███▎      | 989/3000 [00:00<00:01, 1616.62it/s]warmup should be done:  38%|███▊      | 1136/3000 [00:00<00:01, 1631.25it/s]warmup should be done:  37%|███▋      | 1097/3000 [00:00<00:01, 1578.06it/s]warmup should be done:  38%|███▊      | 1129/3000 [00:00<00:01, 1611.51it/s]warmup should be done:  38%|███▊      | 1142/3000 [00:00<00:01, 1623.59it/s]warmup should be done:  36%|███▋      | 1090/3000 [00:00<00:01, 1579.10it/s]warmup should be done:  38%|███▊      | 1144/3000 [00:00<00:01, 1617.64it/s]warmup should be done:  38%|███▊      | 1139/3000 [00:00<00:01, 1600.49it/s]warmup should be done:  38%|███▊      | 1151/3000 [00:00<00:01, 1613.48it/s]warmup should be done:  43%|████▎     | 1300/3000 [00:00<00:01, 1633.89it/s]warmup should be done:  43%|████▎     | 1291/3000 [00:00<00:01, 1611.73it/s]warmup should be done:  44%|████▎     | 1308/3000 [00:00<00:01, 1632.61it/s]warmup should be done:  42%|████▏     | 1258/3000 [00:00<00:01, 1585.63it/s]warmup should be done:  42%|████▏     | 1251/3000 [00:00<00:01, 1587.44it/s]warmup should be done:  43%|████▎     | 1300/3000 [00:00<00:01, 1600.56it/s]warmup should be done:  44%|████▎     | 1306/3000 [00:00<00:01, 1606.60it/s]warmup should be done:  44%|████▍     | 1313/3000 [00:00<00:01, 1605.59it/s]warmup should be done:  49%|████▉     | 1464/3000 [00:00<00:00, 1634.92it/s]warmup should be done:  49%|████▉     | 1474/3000 [00:00<00:00, 1639.49it/s]warmup should be done:  47%|████▋     | 1419/3000 [00:00<00:00, 1591.35it/s]warmup should be done:  48%|████▊     | 1453/3000 [00:00<00:00, 1611.28it/s]warmup should be done:  47%|████▋     | 1413/3000 [00:00<00:00, 1595.92it/s]warmup should be done:  49%|████▉     | 1463/3000 [00:00<00:00, 1607.88it/s]warmup should be done:  49%|████▉     | 1467/3000 [00:00<00:00, 1603.03it/s]warmup should be done:  49%|████▉     | 1474/3000 [00:00<00:00, 1605.39it/s]warmup should be done:  54%|█████▍    | 1628/3000 [00:01<00:00, 1635.82it/s]warmup should be done:  55%|█████▍    | 1640/3000 [00:01<00:00, 1644.65it/s]warmup should be done:  53%|█████▎    | 1580/3000 [00:01<00:00, 1596.12it/s]warmup should be done:  54%|█████▍    | 1615/3000 [00:01<00:00, 1610.86it/s]warmup should be done:  52%|█████▎    | 1575/3000 [00:01<00:00, 1602.15it/s]warmup should be done:  54%|█████▍    | 1626/3000 [00:01<00:00, 1612.26it/s]warmup should be done:  55%|█████▍    | 1635/3000 [00:01<00:00, 1605.74it/s]warmup should be done:  54%|█████▍    | 1628/3000 [00:01<00:00, 1577.29it/s]warmup should be done:  60%|█████▉    | 1792/3000 [00:01<00:00, 1636.56it/s]warmup should be done:  60%|██████    | 1806/3000 [00:01<00:00, 1648.83it/s]warmup should be done:  58%|█████▊    | 1741/3000 [00:01<00:00, 1598.43it/s]warmup should be done:  59%|█████▉    | 1777/3000 [00:01<00:00, 1612.33it/s]warmup should be done:  58%|█████▊    | 1737/3000 [00:01<00:00, 1606.51it/s]warmup should be done:  60%|█████▉    | 1789/3000 [00:01<00:00, 1616.53it/s]warmup should be done:  60%|█████▉    | 1796/3000 [00:01<00:00, 1592.04it/s]warmup should be done:  60%|█████▉    | 1786/3000 [00:01<00:00, 1567.60it/s]warmup should be done:  65%|██████▌   | 1956/3000 [00:01<00:00, 1637.31it/s]warmup should be done:  66%|██████▌   | 1972/3000 [00:01<00:00, 1651.37it/s]warmup should be done:  65%|██████▍   | 1939/3000 [00:01<00:00, 1613.37it/s]warmup should be done:  63%|██████▎   | 1899/3000 [00:01<00:00, 1609.70it/s]warmup should be done:  65%|██████▌   | 1952/3000 [00:01<00:00, 1618.00it/s]warmup should be done:  65%|██████▌   | 1957/3000 [00:01<00:00, 1596.15it/s]warmup should be done:  65%|██████▍   | 1943/3000 [00:01<00:00, 1559.15it/s]warmup should be done:  63%|██████▎   | 1901/3000 [00:01<00:00, 1508.46it/s]warmup should be done:  71%|███████   | 2121/3000 [00:01<00:00, 1638.75it/s]warmup should be done:  71%|███████▏  | 2138/3000 [00:01<00:00, 1653.11it/s]warmup should be done:  70%|███████   | 2101/3000 [00:01<00:00, 1613.76it/s]warmup should be done:  69%|██████▊   | 2061/3000 [00:01<00:00, 1610.98it/s]warmup should be done:  70%|███████   | 2115/3000 [00:01<00:00, 1620.26it/s]warmup should be done:  71%|███████   | 2118/3000 [00:01<00:00, 1598.95it/s]warmup should be done:  70%|██████▉   | 2099/3000 [00:01<00:00, 1551.74it/s]warmup should be done:  68%|██████▊   | 2053/3000 [00:01<00:00, 1465.75it/s]warmup should be done:  76%|███████▌  | 2285/3000 [00:01<00:00, 1636.54it/s]warmup should be done:  77%|███████▋  | 2304/3000 [00:01<00:00, 1650.75it/s]warmup should be done:  75%|███████▌  | 2263/3000 [00:01<00:00, 1611.11it/s]warmup should be done:  74%|███████▍  | 2223/3000 [00:01<00:00, 1609.76it/s]warmup should be done:  76%|███████▌  | 2278/3000 [00:01<00:00, 1618.17it/s]warmup should be done:  76%|███████▌  | 2278/3000 [00:01<00:00, 1598.04it/s]warmup should be done:  75%|███████▌  | 2255/3000 [00:01<00:00, 1533.33it/s]warmup should be done:  74%|███████▍  | 2216/3000 [00:01<00:00, 1510.34it/s]warmup should be done:  82%|████████▏ | 2449/3000 [00:01<00:00, 1636.81it/s]warmup should be done:  82%|████████▏ | 2470/3000 [00:01<00:00, 1651.81it/s]warmup should be done:  81%|████████  | 2425/3000 [00:01<00:00, 1611.85it/s]warmup should be done:  79%|███████▉  | 2384/3000 [00:01<00:00, 1607.11it/s]warmup should be done:  81%|████████▏ | 2440/3000 [00:01<00:00, 1615.51it/s]warmup should be done:  81%|████████▏ | 2439/3000 [00:01<00:00, 1600.65it/s]warmup should be done:  80%|████████  | 2410/3000 [00:01<00:00, 1536.40it/s]warmup should be done:  79%|███████▉  | 2379/3000 [00:01<00:00, 1542.39it/s]warmup should be done:  87%|████████▋ | 2614/3000 [00:01<00:00, 1638.09it/s]warmup should be done:  88%|████████▊ | 2636/3000 [00:01<00:00, 1652.55it/s]warmup should be done:  86%|████████▌ | 2587/3000 [00:01<00:00, 1611.46it/s]warmup should be done:  85%|████████▍ | 2546/3000 [00:01<00:00, 1609.06it/s]warmup should be done:  87%|████████▋ | 2603/3000 [00:01<00:00, 1617.77it/s]warmup should be done:  87%|████████▋ | 2600/3000 [00:01<00:00, 1602.14it/s]warmup should be done:  85%|████████▌ | 2564/3000 [00:01<00:00, 1531.36it/s]warmup should be done:  85%|████████▍ | 2542/3000 [00:01<00:00, 1567.25it/s]warmup should be done:  93%|█████████▎| 2778/3000 [00:01<00:00, 1638.49it/s]warmup should be done:  93%|█████████▎| 2802/3000 [00:01<00:00, 1654.38it/s]warmup should be done:  92%|█████████▏| 2749/3000 [00:01<00:00, 1612.31it/s]warmup should be done:  90%|█████████ | 2708/3000 [00:01<00:00, 1610.25it/s]warmup should be done:  92%|█████████▏| 2765/3000 [00:01<00:00, 1609.61it/s]warmup should be done:  92%|█████████▏| 2761/3000 [00:01<00:00, 1602.39it/s]warmup should be done:  91%|█████████ | 2719/3000 [00:01<00:00, 1535.70it/s]warmup should be done:  90%|█████████ | 2705/3000 [00:01<00:00, 1585.30it/s]warmup should be done:  98%|█████████▊| 2945/3000 [00:01<00:00, 1645.83it/s]warmup should be done:  99%|█████████▉| 2970/3000 [00:01<00:00, 1659.52it/s]warmup should be done:  97%|█████████▋| 2913/3000 [00:01<00:00, 1617.78it/s]warmup should be done:  96%|█████████▌| 2871/3000 [00:01<00:00, 1614.78it/s]warmup should be done:  98%|█████████▊| 2929/3000 [00:01<00:00, 1616.71it/s]warmup should be done:  97%|█████████▋| 2923/3000 [00:01<00:00, 1605.77it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1643.95it/s]warmup should be done:  96%|█████████▌| 2873/3000 [00:01<00:00, 1535.92it/s]warmup should be done:  96%|█████████▌| 2867/3000 [00:01<00:00, 1593.43it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1631.64it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1615.29it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1611.12it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1608.92it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1589.22it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1572.04it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1562.30it/s]




warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]


warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   6%|▌         | 165/3000 [00:00<00:01, 1649.89it/s]warmup should be done:   6%|▌         | 169/3000 [00:00<00:01, 1687.11it/s]warmup should be done:   6%|▌         | 169/3000 [00:00<00:01, 1685.83it/s]warmup should be done:   6%|▌         | 167/3000 [00:00<00:01, 1665.82it/s]warmup should be done:   6%|▌         | 166/3000 [00:00<00:01, 1655.62it/s]warmup should be done:   5%|▌         | 164/3000 [00:00<00:01, 1634.14it/s]warmup should be done:   5%|▌         | 159/3000 [00:00<00:01, 1585.31it/s]warmup should be done:   6%|▌         | 165/3000 [00:00<00:01, 1641.47it/s]warmup should be done:  11%|█         | 332/3000 [00:00<00:01, 1658.24it/s]warmup should be done:  11%|█         | 334/3000 [00:00<00:01, 1666.53it/s]warmup should be done:  11%|█▏        | 340/3000 [00:00<00:01, 1697.51it/s]warmup should be done:  11%|█         | 333/3000 [00:00<00:01, 1661.35it/s]warmup should be done:  11%|█▏        | 338/3000 [00:00<00:01, 1684.61it/s]warmup should be done:  11%|█         | 329/3000 [00:00<00:01, 1640.31it/s]warmup should be done:  11%|█         | 324/3000 [00:00<00:01, 1617.85it/s]warmup should be done:  11%|█         | 330/3000 [00:00<00:01, 1642.00it/s]warmup should be done:  17%|█▋        | 499/3000 [00:00<00:01, 1663.55it/s]warmup should be done:  17%|█▋        | 507/3000 [00:00<00:01, 1686.37it/s]warmup should be done:  16%|█▋        | 494/3000 [00:00<00:01, 1642.60it/s]warmup should be done:  17%|█▋        | 502/3000 [00:00<00:01, 1668.19it/s]warmup should be done:  17%|█▋        | 512/3000 [00:00<00:01, 1703.20it/s]warmup should be done:  17%|█▋        | 501/3000 [00:00<00:01, 1666.09it/s]warmup should be done:  16%|█▋        | 489/3000 [00:00<00:01, 1631.18it/s]warmup should be done:  17%|█▋        | 497/3000 [00:00<00:01, 1652.14it/s]warmup should be done:  22%|██▏       | 666/3000 [00:00<00:01, 1663.59it/s]warmup should be done:  22%|██▏       | 670/3000 [00:00<00:01, 1672.14it/s]warmup should be done:  23%|██▎       | 677/3000 [00:00<00:01, 1688.40it/s]warmup should be done:  23%|██▎       | 684/3000 [00:00<00:01, 1706.91it/s]warmup should be done:  22%|██▏       | 654/3000 [00:00<00:01, 1634.79it/s]warmup should be done:  22%|██▏       | 659/3000 [00:00<00:01, 1639.60it/s]warmup should be done:  22%|██▏       | 664/3000 [00:00<00:01, 1655.99it/s]warmup should be done:  22%|██▏       | 668/3000 [00:00<00:01, 1648.17it/s]warmup should be done:  28%|██▊       | 833/3000 [00:00<00:01, 1664.73it/s]warmup should be done:  28%|██▊       | 838/3000 [00:00<00:01, 1674.58it/s]warmup should be done:  28%|██▊       | 847/3000 [00:00<00:01, 1689.74it/s]warmup should be done:  28%|██▊       | 855/3000 [00:00<00:01, 1706.38it/s]warmup should be done:  27%|██▋       | 818/3000 [00:00<00:01, 1634.94it/s]warmup should be done:  27%|██▋       | 823/3000 [00:00<00:01, 1638.11it/s]warmup should be done:  28%|██▊       | 830/3000 [00:00<00:01, 1655.28it/s]warmup should be done:  28%|██▊       | 833/3000 [00:00<00:01, 1627.29it/s]warmup should be done:  33%|███▎      | 1001/3000 [00:00<00:01, 1667.12it/s]warmup should be done:  34%|███▍      | 1016/3000 [00:00<00:01, 1688.73it/s]warmup should be done:  33%|███▎      | 996/3000 [00:00<00:01, 1656.22it/s]warmup should be done:  33%|███▎      | 988/3000 [00:00<00:01, 1640.81it/s]warmup should be done:  33%|███▎      | 983/3000 [00:00<00:01, 1637.32it/s]warmup should be done:  34%|███▎      | 1006/3000 [00:00<00:01, 1667.90it/s]warmup should be done:  34%|███▍      | 1026/3000 [00:00<00:01, 1696.38it/s]warmup should be done:  33%|███▎      | 998/3000 [00:00<00:01, 1632.70it/s]warmup should be done:  39%|███▉      | 1168/3000 [00:00<00:01, 1667.25it/s]warmup should be done:  40%|███▉      | 1185/3000 [00:00<00:01, 1686.94it/s]warmup should be done:  38%|███▊      | 1148/3000 [00:00<00:01, 1641.11it/s]warmup should be done:  38%|███▊      | 1153/3000 [00:00<00:01, 1642.50it/s]warmup should be done:  39%|███▉      | 1163/3000 [00:00<00:01, 1659.21it/s]warmup should be done:  39%|███▉      | 1173/3000 [00:00<00:01, 1666.69it/s]warmup should be done:  40%|███▉      | 1197/3000 [00:00<00:01, 1699.37it/s]warmup should be done:  39%|███▉      | 1164/3000 [00:00<00:01, 1638.92it/s]warmup should be done:  44%|████▍     | 1335/3000 [00:00<00:00, 1665.90it/s]warmup should be done:  45%|████▌     | 1355/3000 [00:00<00:00, 1688.71it/s]warmup should be done:  44%|████▍     | 1329/3000 [00:00<00:01, 1657.18it/s]warmup should be done:  45%|████▍     | 1341/3000 [00:00<00:00, 1669.53it/s]warmup should be done:  44%|████▍     | 1313/3000 [00:00<00:01, 1638.98it/s]warmup should be done:  44%|████▍     | 1318/3000 [00:00<00:01, 1638.83it/s]warmup should be done:  46%|████▌     | 1369/3000 [00:00<00:00, 1703.11it/s]warmup should be done:  44%|████▍     | 1333/3000 [00:00<00:01, 1653.15it/s]warmup should be done:  50%|█████     | 1502/3000 [00:00<00:00, 1665.14it/s]warmup should be done:  51%|█████     | 1525/3000 [00:00<00:00, 1689.29it/s]warmup should be done:  50%|████▉     | 1495/3000 [00:00<00:00, 1656.31it/s]warmup should be done:  50%|█████     | 1509/3000 [00:00<00:00, 1670.91it/s]warmup should be done:  49%|████▉     | 1482/3000 [00:00<00:00, 1638.59it/s]warmup should be done:  49%|████▉     | 1479/3000 [00:00<00:00, 1643.08it/s]warmup should be done:  51%|█████▏    | 1540/3000 [00:00<00:00, 1703.76it/s]warmup should be done:  50%|█████     | 1503/3000 [00:00<00:00, 1665.50it/s]warmup should be done:  56%|█████▌    | 1670/3000 [00:01<00:00, 1669.63it/s]warmup should be done:  56%|█████▋    | 1694/3000 [00:01<00:00, 1689.18it/s]warmup should be done:  55%|█████▌    | 1662/3000 [00:01<00:00, 1657.79it/s]warmup should be done:  55%|█████▍    | 1646/3000 [00:01<00:00, 1638.36it/s]warmup should be done:  56%|█████▌    | 1678/3000 [00:01<00:00, 1675.01it/s]warmup should be done:  55%|█████▍    | 1645/3000 [00:01<00:00, 1648.19it/s]warmup should be done:  57%|█████▋    | 1712/3000 [00:01<00:00, 1706.01it/s]warmup should be done:  56%|█████▌    | 1674/3000 [00:01<00:00, 1677.14it/s]warmup should be done:  61%|██████▏   | 1838/3000 [00:01<00:00, 1670.51it/s]warmup should be done:  62%|██████▏   | 1864/3000 [00:01<00:00, 1690.85it/s]warmup should be done:  62%|██████▏   | 1846/3000 [00:01<00:00, 1675.41it/s]warmup should be done:  61%|██████    | 1829/3000 [00:01<00:00, 1659.37it/s]warmup should be done:  60%|██████    | 1811/3000 [00:01<00:00, 1639.69it/s]warmup should be done:  60%|██████    | 1812/3000 [00:01<00:00, 1651.90it/s]warmup should be done:  63%|██████▎   | 1883/3000 [00:01<00:00, 1706.87it/s]warmup should be done:  61%|██████▏   | 1844/3000 [00:01<00:00, 1683.43it/s]warmup should be done:  68%|██████▊   | 2034/3000 [00:01<00:00, 1690.08it/s]warmup should be done:  67%|██████▋   | 2006/3000 [00:01<00:00, 1665.89it/s]warmup should be done:  67%|██████▋   | 1996/3000 [00:01<00:00, 1662.30it/s]warmup should be done:  66%|██████▌   | 1975/3000 [00:01<00:00, 1637.49it/s]warmup should be done:  68%|██████▊   | 2054/3000 [00:01<00:00, 1707.66it/s]warmup should be done:  67%|██████▋   | 2014/3000 [00:01<00:00, 1670.49it/s]warmup should be done:  66%|██████▌   | 1978/3000 [00:01<00:00, 1649.13it/s]warmup should be done:  67%|██████▋   | 2014/3000 [00:01<00:00, 1686.85it/s]warmup should be done:  72%|███████▏  | 2164/3000 [00:01<00:00, 1666.65it/s]warmup should be done:  72%|███████▏  | 2173/3000 [00:01<00:00, 1664.52it/s]warmup should be done:  71%|███████▏  | 2141/3000 [00:01<00:00, 1641.90it/s]warmup should be done:  73%|███████▎  | 2204/3000 [00:01<00:00, 1685.56it/s]warmup should be done:  71%|███████▏  | 2143/3000 [00:01<00:00, 1645.78it/s]warmup should be done:  73%|███████▎  | 2182/3000 [00:01<00:00, 1667.08it/s]warmup should be done:  73%|███████▎  | 2184/3000 [00:01<00:00, 1689.56it/s]warmup should be done:  74%|███████▍  | 2225/3000 [00:01<00:00, 1665.70it/s]warmup should be done:  78%|███████▊  | 2333/3000 [00:01<00:00, 1671.59it/s]warmup should be done:  78%|███████▊  | 2343/3000 [00:01<00:00, 1673.69it/s]warmup should be done:  77%|███████▋  | 2307/3000 [00:01<00:00, 1647.27it/s]warmup should be done:  79%|███████▉  | 2373/3000 [00:01<00:00, 1682.99it/s]warmup should be done:  77%|███████▋  | 2308/3000 [00:01<00:00, 1642.93it/s]warmup should be done:  78%|███████▊  | 2350/3000 [00:01<00:00, 1667.95it/s]warmup should be done:  78%|███████▊  | 2355/3000 [00:01<00:00, 1693.97it/s]warmup should be done:  80%|███████▉  | 2396/3000 [00:01<00:00, 1677.42it/s]warmup should be done:  83%|████████▎ | 2502/3000 [00:01<00:00, 1675.69it/s]warmup should be done:  84%|████████▍ | 2514/3000 [00:01<00:00, 1682.65it/s]warmup should be done:  82%|████████▏ | 2473/3000 [00:01<00:00, 1648.59it/s]warmup should be done:  84%|████████▍ | 2518/3000 [00:01<00:00, 1671.40it/s]warmup should be done:  85%|████████▍ | 2542/3000 [00:01<00:00, 1679.74it/s]warmup should be done:  82%|████████▏ | 2473/3000 [00:01<00:00, 1636.72it/s]warmup should be done:  84%|████████▍ | 2526/3000 [00:01<00:00, 1697.17it/s]warmup should be done:  86%|████████▌ | 2568/3000 [00:01<00:00, 1687.98it/s]warmup should be done:  89%|████████▉ | 2670/3000 [00:01<00:00, 1676.72it/s]warmup should be done:  90%|████████▉ | 2685/3000 [00:01<00:00, 1688.04it/s]warmup should be done:  88%|████████▊ | 2639/3000 [00:01<00:00, 1650.42it/s]warmup should be done:  90%|████████▉ | 2686/3000 [00:01<00:00, 1668.74it/s]warmup should be done:  90%|█████████ | 2710/3000 [00:01<00:00, 1672.90it/s]warmup should be done:  88%|████████▊ | 2637/3000 [00:01<00:00, 1633.11it/s]warmup should be done:  90%|████████▉ | 2697/3000 [00:01<00:00, 1698.34it/s]warmup should be done:  91%|█████████▏| 2740/3000 [00:01<00:00, 1695.57it/s]warmup should be done:  95%|█████████▍| 2839/3000 [00:01<00:00, 1678.10it/s]warmup should be done:  95%|█████████▌| 2855/3000 [00:01<00:00, 1690.79it/s]warmup should be done:  94%|█████████▎| 2805/3000 [00:01<00:00, 1649.74it/s]warmup should be done:  95%|█████████▌| 2854/3000 [00:01<00:00, 1669.65it/s]warmup should be done:  93%|█████████▎| 2801/3000 [00:01<00:00, 1631.98it/s]warmup should be done:  96%|█████████▌| 2878/3000 [00:01<00:00, 1667.21it/s]warmup should be done:  96%|█████████▌| 2867/3000 [00:01<00:00, 1697.93it/s]warmup should be done:  97%|█████████▋| 2912/3000 [00:01<00:00, 1700.20it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1697.54it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1681.27it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1675.77it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1674.45it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1670.31it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1665.86it/s]warmup should be done:  99%|█████████▉| 2971/3000 [00:01<00:00, 1652.65it/s]warmup should be done:  99%|█████████▉| 2965/3000 [00:01<00:00, 1631.70it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1643.83it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1637.29it/s]2022-12-11 23:55:37.857273: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f7a13f93a00 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 23:55:37.857340: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 23:55:39.488772: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f5e24029eb0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 23:55:39.488834: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 23:55:39.856846: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f7a17830850 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 23:55:39.856913: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 23:55:39.879048: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f5d9802a490 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 23:55:39.879115: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 23:55:40.026729: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 23:55:40.028277: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f7a1b833070 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 23:55:40.028337: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 23:55:40.031179: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f7a178312a0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 23:55:40.031227: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 23:55:40.042367: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f7a17830f10 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 23:55:40.042429: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 23:55:40.084745: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f5df802fcc0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 23:55:40.084821: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 23:55:41.794268: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 23:55:42.163805: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 23:55:42.198583: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 23:55:42.293212: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 23:55:42.297315: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 23:55:42.337761: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 23:55:42.412941: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 23:55:42.854191: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 23:55:44.711779: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 23:55:45.101139: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 23:55:45.165716: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 23:55:45.202621: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 23:55:45.229920: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 23:55:45.257907: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 23:55:45.324966: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
[HCTR][23:56:20.322][ERROR][RK0][main]: replica 2 reaches 1000, calling init pre replica
[HCTR][23:56:20.322][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][23:56:20.324][ERROR][RK0][tid #140162499606272]: replica 3 reaches 1000, calling init pre replica
[HCTR][23:56:20.324][ERROR][RK0][tid #140162499606272]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][23:56:20.328][ERROR][RK0][tid #140162499606272]: coll ps creation done
[HCTR][23:56:20.328][ERROR][RK0][tid #140162499606272]: replica 3 waits for coll ps creation barrier
[HCTR][23:56:20.331][ERROR][RK0][main]: coll ps creation done
[HCTR][23:56:20.331][ERROR][RK0][main]: replica 2 waits for coll ps creation barrier
[HCTR][23:56:20.339][ERROR][RK0][main]: replica 1 reaches 1000, calling init pre replica
[HCTR][23:56:20.339][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][23:56:20.344][ERROR][RK0][main]: coll ps creation done
[HCTR][23:56:20.345][ERROR][RK0][main]: replica 1 waits for coll ps creation barrier
[HCTR][23:56:20.358][ERROR][RK0][tid #140162499606272]: replica 7 reaches 1000, calling init pre replica
[HCTR][23:56:20.358][ERROR][RK0][tid #140162499606272]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][23:56:20.365][ERROR][RK0][tid #140162499606272]: coll ps creation done
[HCTR][23:56:20.365][ERROR][RK0][tid #140162499606272]: replica 7 waits for coll ps creation barrier
[HCTR][23:56:20.371][ERROR][RK0][main]: replica 0 reaches 1000, calling init pre replica
[HCTR][23:56:20.372][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][23:56:20.377][ERROR][RK0][main]: coll ps creation done
[HCTR][23:56:20.377][ERROR][RK0][main]: replica 0 waits for coll ps creation barrier
[HCTR][23:56:20.385][ERROR][RK0][tid #140162499606272]: replica 6 reaches 1000, calling init pre replica
[HCTR][23:56:20.385][ERROR][RK0][tid #140162499606272]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][23:56:20.393][ERROR][RK0][tid #140162499606272]: coll ps creation done
[HCTR][23:56:20.393][ERROR][RK0][tid #140162499606272]: replica 6 waits for coll ps creation barrier
[HCTR][23:56:20.475][ERROR][RK0][tid #140162491213568]: replica 4 reaches 1000, calling init pre replica
[HCTR][23:56:20.475][ERROR][RK0][tid #140162491213568]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][23:56:20.479][ERROR][RK0][tid #140162491213568]: coll ps creation done
[HCTR][23:56:20.479][ERROR][RK0][tid #140162491213568]: replica 4 waits for coll ps creation barrier
[HCTR][23:56:20.498][ERROR][RK0][main]: replica 5 reaches 1000, calling init pre replica
[HCTR][23:56:20.498][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][23:56:20.505][ERROR][RK0][main]: coll ps creation done
[HCTR][23:56:20.505][ERROR][RK0][main]: replica 5 waits for coll ps creation barrier
[HCTR][23:56:20.506][ERROR][RK0][main]: replica 0 preparing frequency
[HCTR][23:56:21.391][ERROR][RK0][main]: replica 0 preparing frequency done
[HCTR][23:56:21.423][ERROR][RK0][main]: replica 0 calling init per replica
[HCTR][23:56:21.423][ERROR][RK0][main]: replica 2 calling init per replica
[HCTR][23:56:21.423][ERROR][RK0][tid #140162499606272]: replica 3 calling init per replica
[HCTR][23:56:21.423][ERROR][RK0][tid #140162499606272]: replica 7 calling init per replica
[HCTR][23:56:21.423][ERROR][RK0][main]: replica 5 calling init per replica
[HCTR][23:56:21.423][ERROR][RK0][main]: replica 1 calling init per replica
[HCTR][23:56:21.423][ERROR][RK0][tid #140162491213568]: replica 4 calling init per replica
[HCTR][23:56:21.423][ERROR][RK0][tid #140162499606272]: replica 6 calling init per replica
[HCTR][23:56:21.423][ERROR][RK0][main]: Calling build_v2
[HCTR][23:56:21.423][ERROR][RK0][tid #140162491213568]: Calling build_v2
[HCTR][23:56:21.423][ERROR][RK0][main]: Calling build_v2
[HCTR][23:56:21.423][ERROR][RK0][tid #140162499606272]: Calling build_v2
[HCTR][23:56:21.423][ERROR][RK0][tid #140162499606272]: Calling build_v2
[HCTR][23:56:21.423][ERROR][RK0][main]: Calling build_v2
[HCTR][23:56:21.423][ERROR][RK0][main]: Calling build_v2
[HCTR][23:56:21.423][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][23:56:21.423][ERROR][RK0][tid #140162499606272]: Calling build_v2
[HCTR][23:56:21.423][ERROR][RK0][tid #140162491213568]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][23:56:21.423][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][23:56:21.423][ERROR][RK0][tid #140162499606272]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][23:56:21.423][ERROR][RK0][tid #140162499606272]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][23:56:21.423][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][23:56:21.423][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][23:56:21.423][ERROR][RK0][tid #140162499606272]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[[[[[[[[2022-12-11 23:56:212022-12-11 23:56:212022-12-11 23:56:212022-12-11 23:56:212022-12-11 23:56:21.2022-12-11 23:56:212022-12-11 23:56:21.2022-12-11 23:56:21...423681..423693.423677423681423682: 423693423672: 423678: : : E: : E: EEE EE E   /hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc  /hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc /hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc:/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc:/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc:::136::136:136136136] 136136] 136] ] ] using concurrent impl MPS] ] using concurrent impl MPS] using concurrent impl MPSusing concurrent impl MPSusing concurrent impl MPS
using concurrent impl MPSusing concurrent impl MPS
using concurrent impl MPS





[2022-12-11 23:56:21.428004: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:178] v100x8, slow pcie
[2022-12-11 23:56:21.428042: E[ 2022-12-11 23:56:21/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.:428047196: ] Eassigning 8 to cpu 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:178] v100x8, slow pcie
[2022-12-11 23:56:21[.2022-12-11 23:56:21428098.[: 4280962022-12-11 23:56:21E: . E428110/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc : :/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE196: ] 178/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccassigning 8 to cpu] [:
v100x8, slow pcie2022-12-11 23:56:21212
.] 428147build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8[: [
[2022-12-11 23:56:21E2022-12-11 23:56:212022-12-11 23:56:21. ..[428189/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc4281924282102022-12-11 23:56:21: [:: : .E2022-12-11 23:56:21178EE428229 .]   : /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[428242v100x8, slow pcie/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE:2022-12-11 23:56:21: 
[:: 196.E2022-12-11 23:56:21[178212/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] 428291 .2022-12-11 23:56:21] ] :assigning 8 to cpu: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc428340.v100x8, slow pciebuild asymm link desc with 8X Tesla V100-SXM2-32GB out of 8213
E:: 428353

]  178E: remote time is 8.68421[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc]  [E[
2022-12-11 23:56:21:v100x8, slow pcie/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-11 23:56:21 2022-12-11 23:56:21.[178
:./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.4284592022-12-11 23:56:21] 178428469[:428477: .v100x8, slow pcie] : 2022-12-11 23:56:21196: E428509
v100x8, slow pcieE.[] E : 
 4285492022-12-11 23:56:21assigning 8 to cpu /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: [.
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: :E2022-12-11 23:56:21428633:196/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc213 .: 212] :] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc428661E] assigning 8 to cpu[214remote time is 8.68421::  build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8
2022-12-11 23:56:21] 
196E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
.cpu time is 97.0588]  [:428746
[assigning 8 to cpu/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[2022-12-11 23:56:21196: 2022-12-11 23:56:21
:2022-12-11 23:56:21.] E.196.428807assigning 8 to cpu 428820] 428825: 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: assigning 8 to cpu: E:[E
E 2122022-12-11 23:56:21  /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] [.[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:build asymm link desc with 8X Tesla V100-SXM2-32GB out of 82022-12-11 23:56:214289122022-12-11 23:56:21::214
.: .212213] 428979E428955] ] [cpu time is 97.0588:  : build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8remote time is 8.684212022-12-11 23:56:21
E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE

. : 429043/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[[212/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: :2022-12-11 23:56:212022-12-11 23:56:21] :E212..build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8212 ] 429115429117
] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccbuild asymm link desc with 8X Tesla V100-SXM2-32GB out of 8: : build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8:
[EE
2132022-12-11 23:56:21  [] ./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-11 23:56:21remote time is 8.68421429210:2022-12-11 23:56:21:.
: 214.213429232E] [429244] :  cpu time is 97.05882022-12-11 23:56:21: remote time is 8.68421E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
.E
 :429303 /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[213: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:2022-12-11 23:56:21] E:213.remote time is 8.68421 213] 429366
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] remote time is 8.68421: :remote time is 8.68421[
E214
2022-12-11 23:56:21 ] [./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[cpu time is 97.05882022-12-11 23:56:21429445:2022-12-11 23:56:21
.: 214.429470E] 429479:  cpu time is 97.0588: E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
E : /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc214/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:] :214cpu time is 97.0588214] 
] cpu time is 97.0588cpu time is 97.0588

[2022-12-11 23:57:38.424377: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:118] solver created. now build & solve
[2022-12-11 23:57:38.464517: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:123] solver built. now solve
block 0 storage is 00010001
	access is	0	0	0	0	4	4	4	4	
block 1 storage is 00100010
	access is	1	1	1	1	5	5	5	5	
block 2 storage is 01000100
	access is	2	2	2	2	6	6	6	6	
block 3 storage is 10001000
	access is	3	3	3	3	7	7	7	7	
block 4 storage is 00000000
	access is	8	8	8	8	8	8	8	8	
[2022-12-11 23:57:38.570115: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:127] solver solved
[2022-12-11 23:57:38.570180: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:195] 0 solved master
[2022-12-11 23:57:38.734159: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 0 solved
[2022-12-11 23:57:38.734199: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 0 initing device 0
[2022-12-11 23:57:38.734695: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-11 23:57:38.734746: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 23:57:38.735689: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 23:57:38.736491: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 23:57:38.749378: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 5 solved
[2022-12-11 23:57:38.749446: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 5 initing device 5
[2022-12-11 23:57:38.749685: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 3 solved
[2022-12-11 23:57:38.749739: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] [worker 0 thread 3 initing device 32022-12-11 23:57:38
.749746: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 6 solved
[2022-12-11 23:57:38.749803: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 6 initing device 6
[2022-12-11 23:57:38.749872: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-11 23:57:38.749915: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 23:57:38.750169: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-11 23:57:38.750211: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] [eager alloc mem 381.47 MB2022-12-11 23:57:38
.750228: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-11 23:57:38.750275: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 23:57:38.752056: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 23:57:38.752154: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 23:57:38.752200: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 23:57:38.754007: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 23:57:38.754104: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 23:57:38.754153: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 23:57:38.757569: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 7 solved
[2022-12-11 23:57:38.757623: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 7 initing device 7
[2022-12-11 23:57:38.757877: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 2 solved
[2022-12-11 23:57:38.757932: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 2 initing device 2
[2022-12-11 23:57:38.758048: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-11 23:57:38.758090: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 23:57:38.758355: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-11 23:57:38.758397: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 23:57:38.759357: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 4 solved
[2022-12-11 23:57:38.759427: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205[] 2022-12-11 23:57:38worker 0 thread 4 initing device 4.
759440: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 1 solved
[2022-12-11 23:57:38.759498: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 1 initing device 1
[2022-12-11 23:57:38.759623: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 23:57:38.759723: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 23:57:38.760012[: 2022-12-11 23:57:38E. 760025/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu: :E1815 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuBuilding Coll Cache with ... num gpu device is 8:
1815] Building Coll Cache with ... num gpu device is 8
[2022-12-11 23:57:38[.2022-12-11 23:57:38760082.: 760085E:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu :/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu1980:] 1980eager alloc mem 381.47 MB] 
eager alloc mem 381.47 MB
[2022-12-11 23:57:38.762086: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 23:57:38.762323: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 23:57:38.762862: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 23:57:38.762917: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 23:57:38.765076: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 23:57:38.765125: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 23:57:38.814872: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.00 Bytes
[2022-12-11 23:57:38.820172: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5
[2022-12-11 23:57:38.820301: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-11 23:57:38.821133: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 15.64 MB
[2022-12-11 23:57:38.821712: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:57:38.822713: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:57:38.822762: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 15.26 MB
[2022-12-11 23:57:38.831623: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 23:57:38.832450: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 23:57:38.832499: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1.91 GB
[[[2022-12-11 23:57:382022-12-11 23:57:382022-12-11 23:57:38...834796834790834796: : : EEE   /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:::198019801980] ] ] eager alloc mem 5.00 Byteseager alloc mem 5.00 Byteseager alloc mem 5.00 Bytes


[2022-12-11 23:57:38.840390: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5
[2022-12-11 23:57:38.[8404732022-12-11 23:57:38: .E840500 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccE: 638/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc] :eager release cuda mem 5638
] eager release cuda mem 400000000
[2022-12-11 23:57:38.840533: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5
[2022-12-11 23:57:38.840585: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-11 23:57:38.840620: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-11 23:57:38.841482: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 15.64 MB
[2022-12-11 23:57:38.841631: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu[:2022-12-11 23:57:381980.] 841652eager alloc mem 5.00 Bytes: 
E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.00 Bytes
[2022-12-11 23:57:38.842080: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 15.64 MB
[2022-12-11 23:57:38.842654: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 15.64 MB
[2022-12-11 23:57:38.843308: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:57:38.843340: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:57:38.843446: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:57:38.844293: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc[:2022-12-11 23:57:38638.] 844308eager release cuda mem 625663: 
E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:57:38.844347: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc[:2022-12-11 23:57:3843.] 844362WORKER[0] alloc host memory 15.26 MB: 
W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 15.26 MB
[2022-12-11 23:57:38.844416: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:57:38.844459: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 15.26 MB
[2022-12-11 23:57:38.846631: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5
[2022-12-11 23:57:38.846709: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000[
2022-12-11 23:57:38.846717: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5
[2022-12-11 23:57:38.846798: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-11 23:57:38.847823: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 15.64 MB
[2022-12-11 23:57:38.848347: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 15.64 MB
[2022-12-11 23:57:38.848952: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:57:38.849092: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:57:38.849922: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:57:38.849976: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 15.26 MB
[2022-12-11 23:57:38.850050: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:57:38.850098: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 15.26 MB
[2022-12-11 23:57:38.855546: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 23:57:38.855885: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 23:57:38.856176: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 23:57:38.856221: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1.91 GB
[2022-12-11 23:57:38.856473: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] [eager alloc mem 25.25 KB2022-12-11 23:57:38
.856510: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 23:57:38.856556: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1.91 GB
[2022-12-11 23:57:38.857089: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 23:57:38.857133: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1.91 GB
[2022-12-11 23:57:38.860820: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 23:57:38.861277: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 23:57:38.861442: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 23:57:38.861486: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1.91 GB
[2022-12-11 23:57:38.861886: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 23:57:38.861927: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1.91 GB
[[2022-12-11 23:57:382022-12-11 23:57:38..877071877083: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::19801980] ] eager alloc mem 5.00 Byteseager alloc mem 5.00 Bytes

[2022-12-11 23:57:38.883334: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5
[2022-12-11 23:57:38.883435: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000[
2022-12-11 23:57:38.883455: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5
[2022-12-11 23:57:38.883536: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-11 23:57:38.884445: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 15.64 MB
[2022-12-11 23:57:38.885224: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 15.64 MB
[2022-12-11 23:57:38.891530: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] [eager alloc mem 611.00 KB2022-12-11 23:57:38
.891568: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[[2022-12-11 23:57:382022-12-11 23:57:38..892570892573: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::638638] ] eager release cuda mem 625663eager release cuda mem 625663

[[2022-12-11 23:57:382022-12-11 23:57:38..892644892644: : WW  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc::4343] ] WORKER[0] alloc host memory 15.26 MBWORKER[0] alloc host memory 15.26 MB

[2022-12-11 23:57:38.910064: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 23:57:38.910253: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 23:57:38.910714: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 23:57:38.910773: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1.91 GB
[2022-12-11 23:57:38.910873: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 23:57:38.910940: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1.91 GB
[[[[[[[[2022-12-11 23:57:392022-12-11 23:57:392022-12-11 23:57:392022-12-11 23:57:392022-12-11 23:57:392022-12-11 23:57:392022-12-11 23:57:392022-12-11 23:57:39........561500561501561500561501561501561500561500561501: : : : : : : : EEEEEEEE        /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::::::::19261926192619261926192619261926] ] ] ] ] ] Device 1 init p2p of link 7] ] Device 7 init p2p of link 4Device 4 init p2p of link 5Device 0 init p2p of link 3Device 5 init p2p of link 6Device 3 init p2p of link 2
Device 2 init p2p of link 1Device 6 init p2p of link 0






[[[2022-12-11 23:57:39[[2022-12-11 23:57:39[2022-12-11 23:57:39.[2022-12-11 23:57:39[2022-12-11 23:57:39.2022-12-11 23:57:39.5619662022-12-11 23:57:39.2022-12-11 23:57:39.561966.561966: .561968.561966: 561974: E561979: 561981: E: E : E: E E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu: /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980:1980] :1980:1980] 1980] eager alloc mem 611.00 KB1980] 1980] eager alloc mem 611.00 KB] eager alloc mem 611.00 KB
] eager alloc mem 611.00 KB] eager alloc mem 611.00 KB
eager alloc mem 611.00 KB
eager alloc mem 611.00 KB
eager alloc mem 611.00 KB



[2022-12-11 23:57:39.[5629352022-12-11 23:57:39: .[E[5629412022-12-11 23:57:39 [2022-12-11 23:57:39[: .[/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc2022-12-11 23:57:39.2022-12-11 23:57:39E562949[2022-12-11 23:57:39:.562950. : 2022-12-11 23:57:39.638562963: 562960/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccE.562963] : E: : 562975: eager release cuda mem 625663E E638/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc: E
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc ] :E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cceager release cuda mem 625663638 /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638:
] /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] 638eager release cuda mem 625663:638] eager release cuda mem 625663] 
638] eager release cuda mem 625663
eager release cuda mem 625663] eager release cuda mem 625663

eager release cuda mem 625663

[2022-12-11 23:57:39.575165: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 6 init p2p of link 5
[2022-12-11 23:57:39.575322: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:57:39.575394: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 2
[2022-12-11 23:57:39.575538: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:57:39.576141: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:57:39.576358: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:57:39.577274: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 3 init p2p of link 0
[2022-12-11 23:57:39.577426: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:57:39.577509: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 7 init p2p of link 1
[2022-12-11 23:57:39.577641: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:57:39.577738: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 2 init p2p of link 3
[2022-12-11 23:57:39.577866: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:57:39.577918: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 0 init p2p of link 6
[2022-12-11 23:57:39.578060: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:57:39.578098: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 5 init p2p of link 4
[[2022-12-11 23:57:392022-12-11 23:57:39..578242578246: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::6381980] ] [eager release cuda mem 625663eager alloc mem 611.00 KB2022-12-11 23:57:39

.578274: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 4 init p2p of link 7
[2022-12-11 23:57:39.578415: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638[] 2022-12-11 23:57:39eager release cuda mem 625663.
578434: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:57:39.578668: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:57:39.578830: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:57:39.579079: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:57:39.579246: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:57:39.588045: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 6 init p2p of link 4
[2022-12-11 23:57:39.588167: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:57:39.588280: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 3
[2022-12-11 23:57:39.588397: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:57:39.588970: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:57:39.589195: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:57:39.589277: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 7 init p2p of link 6
[2022-12-11 23:57:39.589393: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:57:39.589741: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 0 init p2p of link 1
[2022-12-11 23:57:39.589863: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:57:39.590199: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:57:39.590696: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:57:39.590753: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 3 init p2p of link 5
[2022-12-11 23:57:39.590875: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:57:39.591151: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 2 init p2p of link 0
[2022-12-11 23:57:39.591267: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:57:39.591631[: 2022-12-11 23:57:39E. 591653/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu: :E1926 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccDevice 5 init p2p of link 7:
638] eager release cuda mem 625663
[2022-12-11 23:57:39.591771: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:57:39.591860: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 4 init p2p of link 2
[2022-12-11 23:57:39.591982: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:57:39.592063: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:57:39.592577: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:57:39.592748: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:57:39.605565: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 3 init p2p of link 1
[2022-12-11 23:57:39.605682: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:57:39.606500: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:57:39.606617: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 4 init p2p of link 6
[2022-12-11 23:57:39.606732: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:57:39.607351: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 6 init p2p of link 7
[2022-12-11 23:57:39.607469: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:57:39.607554: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:57:39.607663: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 0
[2022-12-11 23:57:39.607778: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:57:39.608123: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 7 init p2p of link 5
[2022-12-11 23:57:39.608238: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:57:39.608275: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:57:39.608511: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 0 init p2p of link 2
[2022-12-11 23:57:39.608588: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:57:39.608632: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:57:39.609052: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:57:39.609170: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 2 init p2p of link 4
[2022-12-11 23:57:39.609282: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:57:39.609446: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:57:39.609569: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 5 init p2p of link 3
[2022-12-11 23:57:39.609684: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:57:39.610043: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:57:39.610451: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:57:39.622412: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 16399996
[2022-12-11 23:57:39.623078: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 16399996
[2022-12-11 23:57:39.623422: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 16399996
[2022-12-11 23:57:39.623924: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 3999999 / 100000000 nodes ( 4.00 %~4.00 %) | remote 11999997 / 100000000 nodes ( 12.00 %) | cpu 84000004 / 100000000 nodes ( 84.00 %) | 1.91 GB | 0.873717 secs 
[2022-12-11 23:57:39.624126: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 3999999 / 100000000 nodes ( 4.00 %~4.00 %) | remote 11999997 / 100000000 nodes ( 12.00 %) | cpu 84000004 / 100000000 nodes ( 84.00 %) | 1.91 GB | 0.865735 secs 
[2022-12-11 23:57:39.624207: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 3999999 / 100000000 nodes ( 4.00 %~4.00 %) | remote 11999997 / 100000000 nodes ( 12.00 %) | cpu 84000004 / 100000000 nodes ( 84.00 %) | 1.91 GB | 0.874301 secs 
[2022-12-11 23:57:39.624359: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 16399996
[2022-12-11 23:57:39.624964: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 16399996
[2022-12-11 23:57:39.625403: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 16399996
[2022-12-11 23:57:39.625736: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 16399996
[2022-12-11 23:57:39.625976: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 3999999 / 100000000 nodes ( 4.00 %~4.00 %) | remote 11999997 / 100000000 nodes ( 12.00 %) | cpu 84000004 / 100000000 nodes ( 84.00 %) | 1.91 GB | 0.865905 secs 
[2022-12-11 23:57:39.626064: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 16399996
[2022-12-11 23:57:39.627795: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 3999999 / 100000000 nodes ( 4.00 %~4.00 %) | remote 11999997 / 100000000 nodes ( 12.00 %) | cpu 84000004 / 100000000 nodes ( 84.00 %) | 1.91 GB | 0.877527 secs 
[2022-12-11 23:57:39.628005: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 3999999 / 100000000 nodes ( 4.00 %~4.00 %) | remote 11999997 / 100000000 nodes ( 12.00 %) | cpu 84000004 / 100000000 nodes ( 84.00 %) | 1.91 GB | 0.867923 secs 
[2022-12-11 23:57:39.628214: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 3999999 / 100000000 nodes ( 4.00 %~4.00 %) | remote 11999997 / 100000000 nodes ( 12.00 %) | cpu 84000004 / 100000000 nodes ( 84.00 %) | 1.91 GB | 0.870132 secs 
[2022-12-11 23:57:39.629018: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 3999999 / 100000000 nodes ( 4.00 %~4.00 %) | remote 11999997 / 100000000 nodes ( 12.00 %) | cpu 84000004 / 100000000 nodes ( 84.00 %) | 1.91 GB | 0.894284 secs 
[2022-12-11 23:57:39.632003: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 8.55 GB
[2022-12-11 23:57:40.968775: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 8.81 GB
[2022-12-11 23:57:40.968929: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 8.81 GB
[2022-12-11 23:57:40.969192: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 8.81 GB
[2022-12-11 23:57:42.340308: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 9.08 GB
[2022-12-11 23:57:42.340445: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 9.08 GB
[2022-12-11 23:57:42.340773: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 9.08 GB
[2022-12-11 23:57:43.492598: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 9.29 GB
[2022-12-11 23:57:43.492853: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 9.29 GB
[2022-12-11 23:57:43.493288: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 9.29 GB
[2022-12-11 23:57:44.473412: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 9.51 GB
[2022-12-11 23:57:44.473967: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 9.51 GB
[2022-12-11 23:57:44.475159: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 9.51 GB
[2022-12-11 23:57:46. 40066: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 9.96 GB
[2022-12-11 23:57:46. 40557: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 9.96 GB
[2022-12-11 23:57:46. 41405: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 9.96 GB
[2022-12-11 23:57:47.429961: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 10.16 GB
[2022-12-11 23:57:47.441161: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 10.16 GB
[HCTR][23:57:48.732][ERROR][RK0][tid #140162491213568]: replica 4 calling init per replica done, doing barrier
[HCTR][23:57:48.732][ERROR][RK0][main]: replica 1 calling init per replica done, doing barrier
[HCTR][23:57:48.732][ERROR][RK0][tid #140162499606272]: replica 3 calling init per replica done, doing barrier
[HCTR][23:57:48.732][ERROR][RK0][main]: replica 5 calling init per replica done, doing barrier
[HCTR][23:57:48.732][ERROR][RK0][main]: replica 2 calling init per replica done, doing barrier
[HCTR][23:57:48.732][ERROR][RK0][main]: replica 0 calling init per replica done, doing barrier
[HCTR][23:57:48.732][ERROR][RK0][tid #140162499606272]: replica 6 calling init per replica done, doing barrier
[HCTR][23:57:48.732][ERROR][RK0][tid #140162499606272]: replica 7 calling init per replica done, doing barrier
[HCTR][23:57:48.732][ERROR][RK0][tid #140162499606272]: replica 6 calling init per replica done, doing barrier done
[HCTR][23:57:48.732][ERROR][RK0][tid #140162491213568]: replica 4 calling init per replica done, doing barrier done
[HCTR][23:57:48.732][ERROR][RK0][tid #140162499606272]: replica 7 calling init per replica done, doing barrier done
[HCTR][23:57:48.732][ERROR][RK0][main]: replica 0 calling init per replica done, doing barrier done
[HCTR][23:57:48.732][ERROR][RK0][main]: replica 1 calling init per replica done, doing barrier done
[HCTR][23:57:48.732][ERROR][RK0][main]: replica 5 calling init per replica done, doing barrier done
[HCTR][23:57:48.732][ERROR][RK0][main]: replica 2 calling init per replica done, doing barrier done
[HCTR][23:57:48.732][ERROR][RK0][tid #140162499606272]: replica 3 calling init per replica done, doing barrier done
[HCTR][23:57:48.732][ERROR][RK0][main]: init per replica done
[HCTR][23:57:48.732][ERROR][RK0][tid #140162499606272]: init per replica done
[HCTR][23:57:48.732][ERROR][RK0][tid #140162491213568]: init per replica done
[HCTR][23:57:48.732][ERROR][RK0][tid #140162499606272]: init per replica done
[HCTR][23:57:48.732][ERROR][RK0][main]: init per replica done
[HCTR][23:57:48.732][ERROR][RK0][main]: init per replica done
[HCTR][23:57:48.732][ERROR][RK0][tid #140162499606272]: init per replica done
[HCTR][23:57:48.735][ERROR][RK0][main]: init per replica done
[HCTR][23:57:48.738][ERROR][RK0][tid #140162499606272]: 7 allocated 3276800 at 0x7f6733520000
[HCTR][23:57:48.738][ERROR][RK0][tid #140162499606272]: 7 allocated 6553600 at 0x7f7c05200000
[HCTR][23:57:48.738][ERROR][RK0][tid #140162499606272]: 7 allocated 3276800 at 0x7f7c05840000
[HCTR][23:57:48.738][ERROR][RK0][tid #140162499606272]: 7 allocated 6553600 at 0x7f7c05b60000
[HCTR][23:57:48.739][ERROR][RK0][tid #140163497846528]: 2 allocated 3276800 at 0x7f672b520000
[HCTR][23:57:48.739][ERROR][RK0][tid #140163497846528]: 2 allocated 6553600 at 0x7f7c05200000
[HCTR][23:57:48.739][ERROR][RK0][tid #140163497846528]: 2 allocated 3276800 at 0x7f7c05840000
[HCTR][23:57:48.739][ERROR][RK0][tid #140163497846528]: 2 allocated 6553600 at 0x7f7c05b60000
[HCTR][23:57:48.739][ERROR][RK0][tid #140162625431296]: 1 allocated 3276800 at 0x7f672f520000
[HCTR][23:57:48.739][ERROR][RK0][tid #140162625431296]: 1 allocated 6553600 at 0x7f7c05200000
[HCTR][23:57:48.739][ERROR][RK0][tid #140162625431296]: 1 allocated 3276800 at 0x7f7c05840000
[HCTR][23:57:48.739][ERROR][RK0][main]: 3 allocated 3276800 at 0x7f672f520000
[HCTR][23:57:48.739][ERROR][RK0][tid #140162625431296]: 1 allocated 6553600 at 0x7f7c05b60000
[HCTR][23:57:48.739][ERROR][RK0][main]: 3 allocated 6553600 at 0x7f7c05200000
[HCTR][23:57:48.739][ERROR][RK0][main]: 3 allocated 3276800 at 0x7f7c05840000
[HCTR][23:57:48.739][ERROR][RK0][main]: 3 allocated 6553600 at 0x7f7c05b60000
[HCTR][23:57:48.739][ERROR][RK0][main]: 5 allocated 3276800 at 0x7f672f520000
[HCTR][23:57:48.739][ERROR][RK0][main]: 5 allocated 6553600 at 0x7f7c05200000
[HCTR][23:57:48.739][ERROR][RK0][main]: 5 allocated 3276800 at 0x7f7c05840000
[HCTR][23:57:48.739][ERROR][RK0][main]: 5 allocated 6553600 at 0x7f7c05b60000
[HCTR][23:57:48.739][ERROR][RK0][main]: 6 allocated 3276800 at 0x7f672f520000
[HCTR][23:57:48.739][ERROR][RK0][main]: 6 allocated 6553600 at 0x7f7c05200000
[HCTR][23:57:48.739][ERROR][RK0][main]: 6 allocated 3276800 at 0x7f7c05840000
[HCTR][23:57:48.739][ERROR][RK0][main]: 6 allocated 6553600 at 0x7f7c05b60000
[HCTR][23:57:48.739][ERROR][RK0][main]: 4 allocated 3276800 at 0x7f6733520000
[HCTR][23:57:48.739][ERROR][RK0][main]: 4 allocated 6553600 at 0x7f7c05200000
[HCTR][23:57:48.739][ERROR][RK0][main]: 4 allocated 3276800 at 0x7f7c05840000
[HCTR][23:57:48.739][ERROR][RK0][main]: 4 allocated 6553600 at 0x7f7c05b60000
[HCTR][23:57:48.742][ERROR][RK0][tid #140162432497408]: 0 allocated 3276800 at 0x7f7c07320000
[HCTR][23:57:48.742][ERROR][RK0][tid #140162432497408]: 0 allocated 6553600 at 0x7f7c07800000
[HCTR][23:57:48.742][ERROR][RK0][tid #140162432497408]: 0 allocated 3276800 at 0x7f7c0850e800
[HCTR][23:57:48.742][ERROR][RK0][tid #140162432497408]: 0 allocated 6553600 at 0x7f7c0882e800








