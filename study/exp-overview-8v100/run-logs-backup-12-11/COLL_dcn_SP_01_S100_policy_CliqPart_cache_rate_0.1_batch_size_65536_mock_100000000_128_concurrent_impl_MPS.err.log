2022-12-12 02:51:45.480227: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.487982: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.493718: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.497851: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.509734: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.522549: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.530862: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.534964: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.585831: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.588751: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.589711: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.590900: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.591982: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.593676: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.595004: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.596844: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.597771: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.597934: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.599450: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.599460: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.600958: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.601062: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.602386: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.602725: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.604043: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.604391: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.605623: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.606054: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.607119: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.607870: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.608806: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.609680: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.611036: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.612074: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.612966: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.613909: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.614832: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.615920: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.616940: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.617959: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.623260: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 02:51:45.623998: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.625189: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.626226: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.627264: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.628491: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.629590: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.630867: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.632181: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.633037: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.635024: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.635021: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.637248: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.637420: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.637448: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.640223: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.640228: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.640641: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.642860: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.642893: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.643293: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.645771: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.645810: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.646414: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.648927: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.648970: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.649592: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.650600: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.652028: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.652206: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.652789: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.654130: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.654529: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.655283: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.655367: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.655621: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.655939: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.657887: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.658569: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.659273: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.660095: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.660579: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.662186: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.662514: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.663296: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.663943: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.665277: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.665475: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.665888: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.676716: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.676982: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.677116: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.678635: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.680206: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.680428: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.680516: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.681248: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.682931: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.683073: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.683248: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.684044: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.704422: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.707002: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.710983: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.719796: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.720371: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.720915: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.721352: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.721537: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.721584: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.723626: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.725404: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.725671: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.725713: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.725768: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.728036: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.730101: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.730283: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.730369: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.730423: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.731899: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.733371: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.733551: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.733594: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.733724: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.735190: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.737181: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.737275: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.737415: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.737470: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.739149: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.740931: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.741076: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.741119: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.741174: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.742751: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.744316: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.744442: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.744485: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.744582: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.745945: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.747617: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.747661: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.747703: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.747822: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.749345: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.750824: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.750877: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.750920: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.750985: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.753396: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.754627: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.755170: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.755245: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.755388: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.757563: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.758488: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.758724: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.758770: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.758922: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.761081: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.762201: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.762343: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.762439: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.762587: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.764135: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.764681: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.765740: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.765827: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.766110: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.766336: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.768129: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.768841: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.768946: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.770046: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.770333: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.770622: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.770691: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.773112: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.773873: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.774028: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.774841: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.775446: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.775656: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.775683: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.778109: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.779060: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.779392: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.781114: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.781598: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.782029: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.782304: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.783824: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.784332: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.784603: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.785755: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.786184: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.786445: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.786634: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.788712: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.789251: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.790355: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.790803: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.791219: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.791491: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.792659: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.793103: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.794181: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 02:51:45.794302: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.794637: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.795224: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.795504: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.796446: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.796927: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.798160: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.798501: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.799553: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.799819: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.801614: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.801859: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.803506: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.803782: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.804201: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.804294: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.804556: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.805985: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.806010: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.808714: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.808759: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.808814: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.809039: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.810335: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.810415: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.812024: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 02:51:45.812412: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.812616: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.812644: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.812813: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.814130: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.814150: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.817617: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.817648: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.820387: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 02:51:45.820450: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 02:51:45.820860: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 02:51:45.821218: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.821274: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.821434: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.824253: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.824397: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.824535: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.827036: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.827227: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.827355: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.830031: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.830223: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.830448: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.830541: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.831215: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.835793: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.836503: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.836634: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.837292: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.837351: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.839869: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.840742: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.840867: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.841875: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.841944: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.846661: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.846796: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.879205: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.879351: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.890928: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.891062: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.897654: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.897859: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.934023: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.934319: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.940954: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.943056: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 02:51:45.948755: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 02:51:45.952693: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.958290: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.958346: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.963170: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:45.963319: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:46.016604: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:46.987378: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:46.988216: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:46.988751: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:46.989214: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 02:51:46.989272: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-12 02:51:47.006427: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:47.007314: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:47.008112: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:47.008738: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:47.009659: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:47.010132: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-12 02:51:47.057343: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 02:51:47.057556: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 02:51:47.084402: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12340
2022-12-12 02:51:47.231219: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:47.232020: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:47.232552: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:47.233029: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 02:51:47.233089: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-12 02:51:47.250249: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:47.250882: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:47.251614: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:47.252206: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:47.252750: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:47.253216: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:1/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-12 02:51:47.282522: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:47.283376: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:47.283918: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:47.284389: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 02:51:47.284449: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-12 02:51:47.298183: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:47.298809: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:47.299348: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:47.299826: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 02:51:47.299885: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-12 02:51:47.301390: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:47.301726: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:47.302245: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:47.302727: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:47.303094: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:47.303916: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:47.304575: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:47.305134: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 02:51:47.305190: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-12 02:51:47.305471: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:47.305963: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:2/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-12 02:51:47.316112: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:47.316739: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:47.317230: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:47.317261: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:47.318344: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 02:51:47.318405: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-12 02:51:47.318494: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:47.319315: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:47.319925: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:47.320441: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:47.320918: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:6/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-12 02:51:47.322344: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:47.323061: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:47.324126: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:47.324708: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:47.325392: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:47.325865: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:7/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-12 02:51:47.336941: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:47.337487: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 02:51:47.337566: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:47.337680: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 02:51:47.338083: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:47.338692: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:47.339232: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:47.339606: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12341
2022-12-12 02:51:47.339712: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:3/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-12 02:51:47.356338: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:47.357464: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:47.358046: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:47.358523: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 02:51:47.358594: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-12 02:51:47.363729: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:47.364359: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:47.364900: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:47.365374: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 02:51:47.365431: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-12 02:51:47.367913: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 02:51:47.368098: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 02:51:47.369879: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12346
2022-12-12 02:51:47.371697: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 02:51:47.371830: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 02:51:47.373594: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12347
2022-12-12 02:51:47.376387: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:47.377047: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:47.377551: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:47.378546: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:47.379087: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:47.379572: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:5/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-12 02:51:47.382434: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:47.383026: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:47.383546: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:47.384117: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:47.384632: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:51:47.385095: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:4/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-12 02:51:47.386834: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 02:51:47.386982: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 02:51:47.390021: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12343
2022-12-12 02:51:47.391886: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 02:51:47.392062: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 02:51:47.393897: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12342
2022-12-12 02:51:47.426002: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 02:51:47.426199: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 02:51:47.428086: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12345
2022-12-12 02:51:47.431477: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 02:51:47.431667: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 02:51:47.433523: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12344
[HCTR][02:51:48.693][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][02:51:48.694][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][02:51:48.705][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][02:51:48.705][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][02:51:48.705][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][02:51:48.705][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][02:51:48.705][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][02:51:48.705][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 1it [00:01,  1.52s/it]warmup run: 102it [00:01, 87.18it/s]warmup run: 1it [00:01,  1.51s/it]warmup run: 1it [00:01,  1.50s/it]warmup run: 1it [00:01,  1.55s/it]warmup run: 1it [00:01,  1.51s/it]warmup run: 1it [00:01,  1.56s/it]warmup run: 203it [00:01, 187.77it/s]warmup run: 98it [00:01, 84.51it/s]warmup run: 98it [00:01, 84.73it/s]warmup run: 1it [00:01,  1.49s/it]warmup run: 102it [00:01, 85.65it/s]warmup run: 99it [00:01, 85.32it/s]warmup run: 98it [00:01, 81.79it/s]warmup run: 1it [00:01,  1.47s/it]warmup run: 304it [00:01, 298.39it/s]warmup run: 197it [00:01, 183.99it/s]warmup run: 195it [00:01, 182.30it/s]warmup run: 94it [00:01, 81.97it/s]warmup run: 202it [00:01, 183.72it/s]warmup run: 192it [00:01, 178.02it/s]warmup run: 197it [00:01, 178.77it/s]warmup run: 96it [00:01, 84.62it/s]warmup run: 406it [00:01, 414.47it/s]warmup run: 294it [00:01, 290.58it/s]warmup run: 295it [00:01, 293.28it/s]warmup run: 188it [00:01, 177.27it/s]warmup run: 301it [00:01, 290.58it/s]warmup run: 286it [00:01, 281.16it/s]warmup run: 296it [00:01, 285.86it/s]warmup run: 192it [00:01, 182.79it/s]warmup run: 506it [00:02, 523.60it/s]warmup run: 395it [00:01, 406.75it/s]warmup run: 392it [00:01, 402.73it/s]warmup run: 282it [00:01, 281.40it/s]warmup run: 397it [00:01, 397.10it/s]warmup run: 381it [00:01, 389.54it/s]warmup run: 394it [00:01, 395.77it/s]warmup run: 287it [00:01, 288.49it/s]warmup run: 608it [00:02, 627.94it/s]warmup run: 495it [00:02, 518.10it/s]warmup run: 380it [00:01, 395.03it/s]warmup run: 481it [00:02, 481.60it/s]warmup run: 477it [00:02, 496.78it/s]warmup run: 487it [00:02, 482.02it/s]warmup run: 493it [00:02, 504.80it/s]warmup run: 383it [00:01, 398.89it/s]warmup run: 709it [00:02, 715.26it/s]warmup run: 596it [00:02, 622.08it/s]warmup run: 475it [00:01, 499.77it/s]warmup run: 582it [00:02, 590.51it/s]warmup run: 568it [00:02, 584.05it/s]warmup run: 588it [00:02, 591.03it/s]warmup run: 594it [00:02, 609.73it/s]warmup run: 479it [00:01, 504.89it/s]warmup run: 808it [00:02, 780.94it/s]warmup run: 692it [00:02, 697.39it/s]warmup run: 571it [00:02, 597.39it/s]warmup run: 683it [00:02, 685.25it/s]warmup run: 667it [00:02, 677.86it/s]warmup run: 691it [00:02, 689.75it/s]warmup run: 697it [00:02, 704.66it/s]warmup run: 577it [00:02, 605.47it/s]warmup run: 909it [00:02, 840.70it/s]warmup run: 791it [00:02, 769.20it/s]warmup run: 667it [00:02, 680.80it/s]warmup run: 783it [00:02, 762.05it/s]warmup run: 766it [00:02, 754.37it/s]warmup run: 792it [00:02, 767.72it/s]warmup run: 799it [00:02, 781.44it/s]warmup run: 675it [00:02, 691.53it/s]warmup run: 1011it [00:02, 889.17it/s]warmup run: 893it [00:02, 832.69it/s]warmup run: 763it [00:02, 750.23it/s]warmup run: 883it [00:02, 823.09it/s]warmup run: 864it [00:02, 811.72it/s]warmup run: 891it [00:02, 825.31it/s]warmup run: 899it [00:02, 838.02it/s]warmup run: 772it [00:02, 759.73it/s]warmup run: 1112it [00:02, 920.64it/s]warmup run: 994it [00:02, 874.83it/s]warmup run: 858it [00:02, 802.51it/s]warmup run: 983it [00:02, 869.84it/s]warmup run: 964it [00:02, 860.81it/s]warmup run: 990it [00:02, 868.72it/s]warmup run: 998it [00:02, 874.25it/s]warmup run: 868it [00:02, 811.26it/s]warmup run: 1215it [00:02, 950.85it/s]warmup run: 1098it [00:02, 920.60it/s]warmup run: 954it [00:02, 845.08it/s]warmup run: 1083it [00:02, 905.03it/s]warmup run: 1065it [00:02, 900.99it/s]warmup run: 1091it [00:02, 907.62it/s]warmup run: 1100it [00:02, 913.42it/s]warmup run: 963it [00:02, 847.34it/s]warmup run: 1316it [00:02, 938.93it/s]warmup run: 1202it [00:02, 954.20it/s]warmup run: 1050it [00:02, 874.90it/s]warmup run: 1185it [00:02, 936.69it/s]warmup run: 1193it [00:02, 937.62it/s]warmup run: 1163it [00:02, 867.01it/s]warmup run: 1202it [00:02, 942.74it/s]warmup run: 1059it [00:02, 878.45it/s]warmup run: 1306it [00:02, 976.83it/s]warmup run: 1414it [00:02, 918.52it/s]warmup run: 1147it [00:02, 900.27it/s]warmup run: 1288it [00:02, 961.16it/s]warmup run: 1293it [00:02, 955.41it/s]warmup run: 1261it [00:02, 896.10it/s]warmup run: 1156it [00:02, 902.41it/s]warmup run: 1303it [00:02, 957.03it/s]warmup run: 1411it [00:02, 996.45it/s]warmup run: 1514it [00:03, 941.38it/s]warmup run: 1243it [00:02, 913.12it/s]warmup run: 1392it [00:02, 981.93it/s]warmup run: 1393it [00:02, 968.14it/s]warmup run: 1357it [00:02, 912.63it/s]warmup run: 1405it [00:02, 973.76it/s]warmup run: 1252it [00:02, 914.52it/s]warmup run: 1515it [00:03, 1007.54it/s]warmup run: 1617it [00:03, 966.74it/s]warmup run: 1339it [00:02, 926.33it/s]warmup run: 1496it [00:03, 996.65it/s]warmup run: 1494it [00:03, 978.02it/s]warmup run: 1456it [00:03, 934.24it/s]warmup run: 1507it [00:03, 986.95it/s]warmup run: 1348it [00:02, 924.83it/s]warmup run: 1619it [00:03, 1016.49it/s]warmup run: 1716it [00:03, 971.33it/s]warmup run: 1436it [00:02, 937.22it/s]warmup run: 1601it [00:03, 1009.67it/s]warmup run: 1596it [00:03, 990.08it/s]warmup run: 1555it [00:03, 950.03it/s]warmup run: 1610it [00:03, 999.28it/s]warmup run: 1445it [00:02, 936.13it/s]warmup run: 1723it [00:03, 1013.39it/s]warmup run: 1818it [00:03, 985.50it/s]warmup run: 1532it [00:03, 941.70it/s]warmup run: 1705it [00:03, 1016.04it/s]warmup run: 1697it [00:03, 989.20it/s]warmup run: 1654it [00:03, 960.93it/s]warmup run: 1712it [00:03, 1004.15it/s]warmup run: 1541it [00:03, 943.03it/s]warmup run: 1921it [00:03, 998.17it/s]warmup run: 1826it [00:03, 1004.21it/s]warmup run: 1628it [00:03, 940.04it/s]warmup run: 1808it [00:03, 1019.19it/s]warmup run: 1797it [00:03, 986.91it/s]warmup run: 1752it [00:03, 964.91it/s]warmup run: 1814it [00:03, 1006.40it/s]warmup run: 1637it [00:03, 947.91it/s]warmup run: 2024it [00:03, 1005.56it/s]warmup run: 1928it [00:03, 997.29it/s] warmup run: 1723it [00:03, 940.13it/s]warmup run: 1911it [00:03, 1014.80it/s]warmup run: 1897it [00:03, 990.39it/s]warmup run: 1850it [00:03, 966.25it/s]warmup run: 1734it [00:03, 952.17it/s]warmup run: 1916it [00:03, 1000.38it/s]warmup run: 2146it [00:03, 1067.64it/s]warmup run: 2032it [00:03, 1008.55it/s]warmup run: 1818it [00:03, 941.50it/s]warmup run: 2017it [00:03, 1025.73it/s]warmup run: 1998it [00:03, 995.82it/s]warmup run: 1950it [00:03, 975.77it/s]warmup run: 1832it [00:03, 958.82it/s]warmup run: 2017it [00:03, 994.83it/s] warmup run: 2268it [00:03, 1111.50it/s]warmup run: 2155it [00:03, 1071.74it/s]warmup run: 1913it [00:03, 940.23it/s]warmup run: 2140it [00:03, 1084.21it/s]warmup run: 2115it [00:03, 1045.85it/s]warmup run: 2058it [00:03, 1006.43it/s]warmup run: 1930it [00:03, 963.17it/s]warmup run: 2128it [00:03, 1026.39it/s]warmup run: 2391it [00:03, 1146.07it/s]warmup run: 2272it [00:03, 1099.92it/s]warmup run: 2009it [00:03, 944.78it/s]warmup run: 2263it [00:03, 1125.69it/s]warmup run: 2233it [00:03, 1083.92it/s]warmup run: 2175it [00:03, 1054.66it/s]warmup run: 2033it [00:03, 981.61it/s]warmup run: 2239it [00:03, 1050.75it/s]warmup run: 2515it [00:03, 1171.92it/s]warmup run: 2392it [00:03, 1129.59it/s]warmup run: 2127it [00:03, 1012.61it/s]warmup run: 2386it [00:03, 1154.97it/s]warmup run: 2350it [00:03, 1109.42it/s]warmup run: 2291it [00:03, 1085.23it/s]warmup run: 2151it [00:03, 1040.20it/s]warmup run: 2351it [00:03, 1069.04it/s]warmup run: 2637it [00:04, 1184.18it/s]warmup run: 2513it [00:03, 1152.90it/s]warmup run: 2245it [00:03, 1061.99it/s]warmup run: 2509it [00:03, 1176.32it/s]warmup run: 2467it [00:03, 1127.12it/s]warmup run: 2407it [00:03, 1106.32it/s]warmup run: 2269it [00:03, 1081.90it/s]warmup run: 2463it [00:03, 1081.75it/s]warmup run: 2756it [00:04, 1185.85it/s]warmup run: 2635it [00:04, 1172.18it/s]warmup run: 2363it [00:03, 1096.36it/s]warmup run: 2632it [00:04, 1191.71it/s]warmup run: 2584it [00:04, 1138.97it/s]warmup run: 2523it [00:04, 1120.99it/s]warmup run: 2387it [00:03, 1110.35it/s]warmup run: 2575it [00:04, 1090.59it/s]warmup run: 2877it [00:04, 1190.59it/s]warmup run: 2757it [00:04, 1185.30it/s]warmup run: 2481it [00:04, 1121.18it/s]warmup run: 2754it [00:04, 1199.07it/s]warmup run: 2702it [00:04, 1148.90it/s]warmup run: 2643it [00:04, 1143.95it/s]warmup run: 2506it [00:03, 1131.35it/s]warmup run: 2687it [00:04, 1098.36it/s]warmup run: 2998it [00:04, 1193.72it/s]warmup run: 2878it [00:04, 1190.19it/s]warmup run: 3000it [00:04, 687.19it/s] warmup run: 2599it [00:04, 1137.68it/s]warmup run: 2874it [00:04, 1196.46it/s]warmup run: 2818it [00:04, 1149.20it/s]warmup run: 2760it [00:04, 1150.72it/s]warmup run: 2624it [00:04, 1145.34it/s]warmup run: 2797it [00:04, 1098.32it/s]warmup run: 3000it [00:04, 1196.82it/s]warmup run: 3000it [00:04, 691.02it/s] warmup run: 2717it [00:04, 1150.18it/s]warmup run: 2996it [00:04, 1202.24it/s]warmup run: 2934it [00:04, 1149.96it/s]warmup run: 3000it [00:04, 690.54it/s] warmup run: 2878it [00:04, 1155.74it/s]warmup run: 2743it [00:04, 1155.95it/s]warmup run: 2913it [00:04, 1115.16it/s]warmup run: 3000it [00:04, 675.61it/s] warmup run: 2835it [00:04, 1157.55it/s]warmup run: 2994it [00:04, 1147.61it/s]warmup run: 3000it [00:04, 672.03it/s] warmup run: 3000it [00:04, 672.75it/s] warmup run: 2861it [00:04, 1163.10it/s]warmup run: 2957it [00:04, 1174.32it/s]warmup run: 3000it [00:04, 674.27it/s] warmup run: 2983it [00:04, 1177.76it/s]warmup run: 3000it [00:04, 680.56it/s] 
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]






warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   5%|▌         | 162/3000 [00:00<00:01, 1618.45it/s]warmup should be done:   5%|▌         | 164/3000 [00:00<00:01, 1639.60it/s]warmup should be done:   5%|▌         | 158/3000 [00:00<00:01, 1579.56it/s]warmup should be done:   6%|▌         | 166/3000 [00:00<00:01, 1659.63it/s]warmup should be done:   5%|▌         | 164/3000 [00:00<00:01, 1637.19it/s]warmup should be done:   5%|▌         | 159/3000 [00:00<00:01, 1586.28it/s]warmup should be done:   5%|▌         | 164/3000 [00:00<00:01, 1632.81it/s]warmup should be done:   6%|▌         | 166/3000 [00:00<00:01, 1650.57it/s]warmup should be done:  11%|█         | 325/3000 [00:00<00:01, 1623.25it/s]warmup should be done:  11%|█         | 334/3000 [00:00<00:01, 1669.91it/s]warmup should be done:  11%|█         | 330/3000 [00:00<00:01, 1648.70it/s]warmup should be done:  11%|█         | 318/3000 [00:00<00:01, 1586.08it/s]warmup should be done:  11%|█         | 329/3000 [00:00<00:01, 1639.58it/s]warmup should be done:  11%|█         | 322/3000 [00:00<00:01, 1607.22it/s]warmup should be done:  11%|█         | 333/3000 [00:00<00:01, 1658.83it/s]warmup should be done:  11%|█         | 331/3000 [00:00<00:01, 1648.74it/s]warmup should be done:  17%|█▋        | 502/3000 [00:00<00:01, 1670.55it/s]warmup should be done:  16%|█▌        | 478/3000 [00:00<00:01, 1590.65it/s]warmup should be done:  16%|█▋        | 493/3000 [00:00<00:01, 1636.89it/s]warmup should be done:  16%|█▋        | 495/3000 [00:00<00:01, 1642.10it/s]warmup should be done:  16%|█▋        | 488/3000 [00:00<00:01, 1615.60it/s]warmup should be done:  17%|█▋        | 499/3000 [00:00<00:01, 1652.02it/s]warmup should be done:  17%|█▋        | 496/3000 [00:00<00:01, 1637.19it/s]warmup should be done:  16%|█▌        | 483/3000 [00:00<00:01, 1575.43it/s]warmup should be done:  22%|██▏       | 670/3000 [00:00<00:01, 1670.11it/s]warmup should be done:  22%|██▏       | 657/3000 [00:00<00:01, 1634.84it/s]warmup should be done:  22%|██▏       | 660/3000 [00:00<00:01, 1640.21it/s]warmup should be done:  21%|██▏       | 638/3000 [00:00<00:01, 1586.66it/s]warmup should be done:  22%|██▏       | 665/3000 [00:00<00:01, 1631.78it/s]warmup should be done:  22%|██▏       | 650/3000 [00:00<00:01, 1592.68it/s]warmup should be done:  22%|██▏       | 660/3000 [00:00<00:01, 1603.33it/s]warmup should be done:  21%|██▏       | 641/3000 [00:00<00:01, 1552.51it/s]warmup should be done:  27%|██▋       | 821/3000 [00:00<00:01, 1631.39it/s]warmup should be done:  28%|██▊       | 838/3000 [00:00<00:01, 1665.38it/s]warmup should be done:  28%|██▊       | 825/3000 [00:00<00:01, 1625.00it/s]warmup should be done:  28%|██▊       | 831/3000 [00:00<00:01, 1639.03it/s]warmup should be done:  27%|██▋       | 797/3000 [00:00<00:01, 1562.49it/s]warmup should be done:  27%|██▋       | 810/3000 [00:00<00:01, 1576.01it/s]warmup should be done:  27%|██▋       | 801/3000 [00:00<00:01, 1567.91it/s]warmup should be done:  27%|██▋       | 821/3000 [00:00<00:01, 1582.67it/s]warmup should be done:  33%|███▎      | 985/3000 [00:00<00:01, 1627.10it/s]warmup should be done:  34%|███▎      | 1005/3000 [00:00<00:01, 1659.41it/s]warmup should be done:  33%|███▎      | 989/3000 [00:00<00:01, 1629.67it/s]warmup should be done:  33%|███▎      | 997/3000 [00:00<00:01, 1642.89it/s]warmup should be done:  32%|███▏      | 954/3000 [00:00<00:01, 1560.48it/s]warmup should be done:  32%|███▏      | 958/3000 [00:00<00:01, 1565.00it/s]warmup should be done:  32%|███▏      | 968/3000 [00:00<00:01, 1564.72it/s]warmup should be done:  33%|███▎      | 980/3000 [00:00<00:01, 1569.30it/s]warmup should be done:  38%|███▊      | 1148/3000 [00:00<00:01, 1621.74it/s]warmup should be done:  39%|███▉      | 1171/3000 [00:00<00:01, 1651.96it/s]warmup should be done:  38%|███▊      | 1155/3000 [00:00<00:01, 1637.72it/s]warmup should be done:  39%|███▊      | 1162/3000 [00:00<00:01, 1640.31it/s]warmup should be done:  37%|███▋      | 1114/3000 [00:00<00:01, 1571.05it/s]warmup should be done:  37%|███▋      | 1115/3000 [00:00<00:01, 1556.20it/s]warmup should be done:  38%|███▊      | 1125/3000 [00:00<00:01, 1555.75it/s]warmup should be done:  38%|███▊      | 1137/3000 [00:00<00:01, 1558.68it/s]warmup should be done:  44%|████▍     | 1313/3000 [00:00<00:01, 1629.07it/s]warmup should be done:  45%|████▍     | 1337/3000 [00:00<00:01, 1654.13it/s]warmup should be done:  44%|████▍     | 1322/3000 [00:00<00:01, 1647.79it/s]warmup should be done:  42%|████▏     | 1274/3000 [00:00<00:01, 1577.31it/s]warmup should be done:  44%|████▍     | 1327/3000 [00:00<00:01, 1626.31it/s]warmup should be done:  43%|████▎     | 1276/3000 [00:00<00:01, 1570.56it/s]warmup should be done:  43%|████▎     | 1282/3000 [00:00<00:01, 1558.45it/s]warmup should be done:  43%|████▎     | 1294/3000 [00:00<00:01, 1559.45it/s]warmup should be done:  50%|████▉     | 1489/3000 [00:00<00:00, 1654.61it/s]warmup should be done:  49%|████▉     | 1478/3000 [00:00<00:00, 1633.66it/s]warmup should be done:  50%|█████     | 1503/3000 [00:00<00:00, 1654.97it/s]warmup should be done:  48%|████▊     | 1432/3000 [00:00<00:00, 1577.14it/s]warmup should be done:  50%|████▉     | 1492/3000 [00:00<00:00, 1632.52it/s]warmup should be done:  48%|████▊     | 1438/3000 [00:00<00:00, 1585.49it/s]warmup should be done:  48%|████▊     | 1448/3000 [00:00<00:00, 1589.52it/s]warmup should be done:  49%|████▊     | 1458/3000 [00:00<00:00, 1583.29it/s]warmup should be done:  55%|█████▌    | 1657/3000 [00:01<00:00, 1660.54it/s]warmup should be done:  55%|█████▍    | 1643/3000 [00:01<00:00, 1636.93it/s]warmup should be done:  56%|█████▌    | 1670/3000 [00:01<00:00, 1656.59it/s]warmup should be done:  53%|█████▎    | 1593/3000 [00:01<00:00, 1585.15it/s]warmup should be done:  55%|█████▌    | 1656/3000 [00:01<00:00, 1633.81it/s]warmup should be done:  53%|█████▎    | 1600/3000 [00:01<00:00, 1595.50it/s]warmup should be done:  54%|█████▍    | 1614/3000 [00:01<00:00, 1610.55it/s]warmup should be done:  54%|█████▍    | 1623/3000 [00:01<00:00, 1601.45it/s]warmup should be done:  61%|██████    | 1824/3000 [00:01<00:00, 1663.27it/s]warmup should be done:  60%|██████    | 1808/3000 [00:01<00:00, 1638.65it/s]warmup should be done:  61%|██████    | 1836/3000 [00:01<00:00, 1656.23it/s]warmup should be done:  61%|██████    | 1820/3000 [00:01<00:00, 1635.63it/s]warmup should be done:  58%|█████▊    | 1752/3000 [00:01<00:00, 1566.82it/s]warmup should be done:  59%|█████▊    | 1761/3000 [00:01<00:00, 1597.87it/s]warmup should be done:  59%|█████▉    | 1780/3000 [00:01<00:00, 1625.33it/s]warmup should be done:  60%|█████▉    | 1788/3000 [00:01<00:00, 1614.47it/s]warmup should be done:  66%|██████▋   | 1991/3000 [00:01<00:00, 1664.73it/s]warmup should be done:  66%|██████▌   | 1973/3000 [00:01<00:00, 1639.94it/s]warmup should be done:  67%|██████▋   | 2003/3000 [00:01<00:00, 1658.00it/s]warmup should be done:  66%|██████▌   | 1985/3000 [00:01<00:00, 1637.39it/s]warmup should be done:  64%|██████▍   | 1913/3000 [00:01<00:00, 1577.12it/s]warmup should be done:  65%|██████▍   | 1946/3000 [00:01<00:00, 1634.26it/s]warmup should be done:  64%|██████▍   | 1921/3000 [00:01<00:00, 1590.59it/s]warmup should be done:  65%|██████▌   | 1953/3000 [00:01<00:00, 1622.22it/s]warmup should be done:  72%|███████▏  | 2158/3000 [00:01<00:00, 1664.99it/s]warmup should be done:  72%|███████▏  | 2170/3000 [00:01<00:00, 1659.81it/s]warmup should be done:  71%|███████▏  | 2138/3000 [00:01<00:00, 1640.23it/s]warmup should be done:  72%|███████▏  | 2149/3000 [00:01<00:00, 1634.71it/s]warmup should be done:  69%|██████▉   | 2071/3000 [00:01<00:00, 1575.54it/s]warmup should be done:  70%|███████   | 2112/3000 [00:01<00:00, 1641.18it/s]warmup should be done:  69%|██████▉   | 2081/3000 [00:01<00:00, 1588.69it/s]warmup should be done:  71%|███████   | 2116/3000 [00:01<00:00, 1624.22it/s]warmup should be done:  78%|███████▊  | 2337/3000 [00:01<00:00, 1661.77it/s]warmup should be done:  77%|███████▋  | 2303/3000 [00:01<00:00, 1641.83it/s]warmup should be done:  78%|███████▊  | 2325/3000 [00:01<00:00, 1651.73it/s]warmup should be done:  77%|███████▋  | 2313/3000 [00:01<00:00, 1634.99it/s]warmup should be done:  76%|███████▌  | 2278/3000 [00:01<00:00, 1645.83it/s]warmup should be done:  74%|███████▍  | 2229/3000 [00:01<00:00, 1575.01it/s]warmup should be done:  75%|███████▍  | 2242/3000 [00:01<00:00, 1594.70it/s]warmup should be done:  76%|███████▌  | 2279/3000 [00:01<00:00, 1623.38it/s]warmup should be done:  83%|████████▎ | 2504/3000 [00:01<00:00, 1660.54it/s]warmup should be done:  82%|████████▏ | 2468/3000 [00:01<00:00, 1641.05it/s]warmup should be done:  83%|████████▎ | 2491/3000 [00:01<00:00, 1653.09it/s]warmup should be done:  83%|████████▎ | 2478/3000 [00:01<00:00, 1636.53it/s]warmup should be done:  81%|████████▏ | 2443/3000 [00:01<00:00, 1646.91it/s]warmup should be done:  80%|████████  | 2402/3000 [00:01<00:00, 1594.48it/s]warmup should be done:  80%|███████▉  | 2387/3000 [00:01<00:00, 1563.67it/s]warmup should be done:  81%|████████▏ | 2442/3000 [00:01<00:00, 1620.84it/s]warmup should be done:  88%|████████▊ | 2633/3000 [00:01<00:00, 1641.37it/s]warmup should be done:  89%|████████▉ | 2671/3000 [00:01<00:00, 1658.14it/s]warmup should be done:  89%|████████▊ | 2658/3000 [00:01<00:00, 1657.84it/s]warmup should be done:  88%|████████▊ | 2643/3000 [00:01<00:00, 1637.70it/s]warmup should be done:  87%|████████▋ | 2608/3000 [00:01<00:00, 1647.68it/s]warmup should be done:  85%|████████▌ | 2562/3000 [00:01<00:00, 1587.01it/s]warmup should be done:  85%|████████▍ | 2545/3000 [00:01<00:00, 1556.26it/s]warmup should be done:  87%|████████▋ | 2605/3000 [00:01<00:00, 1620.97it/s]warmup should be done:  93%|█████████▎| 2798/3000 [00:01<00:00, 1641.83it/s]warmup should be done:  95%|█████████▍| 2838/3000 [00:01<00:00, 1661.38it/s]warmup should be done:  94%|█████████▍| 2826/3000 [00:01<00:00, 1664.08it/s]warmup should be done:  92%|█████████▏| 2774/3000 [00:01<00:00, 1649.89it/s]warmup should be done:  91%|█████████ | 2722/3000 [00:01<00:00, 1589.06it/s]warmup should be done:  94%|█████████▎| 2807/3000 [00:01<00:00, 1610.05it/s]warmup should be done:  90%|█████████ | 2702/3000 [00:01<00:00, 1559.76it/s]warmup should be done:  92%|█████████▏| 2770/3000 [00:01<00:00, 1628.07it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1660.66it/s]warmup should be done:  99%|█████████▉| 2964/3000 [00:01<00:00, 1646.68it/s]warmup should be done: 100%|█████████▉| 2995/3000 [00:01<00:00, 1669.19it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1654.14it/s]warmup should be done:  98%|█████████▊| 2942/3000 [00:01<00:00, 1657.12it/s]warmup should be done:  96%|█████████▌| 2881/3000 [00:01<00:00, 1587.78it/s]warmup should be done:  99%|█████████▉| 2973/3000 [00:01<00:00, 1624.04it/s]warmup should be done:  96%|█████████▌| 2866/3000 [00:01<00:00, 1581.92it/s]warmup should be done:  98%|█████████▊| 2938/3000 [00:01<00:00, 1641.04it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1637.82it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1633.41it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1620.74it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1612.66it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1582.25it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1574.20it/s]

warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]




warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   5%|▌         | 161/3000 [00:00<00:01, 1609.71it/s]warmup should be done:   6%|▌         | 169/3000 [00:00<00:01, 1688.33it/s]warmup should be done:   6%|▌         | 171/3000 [00:00<00:01, 1709.10it/s]warmup should be done:   5%|▌         | 162/3000 [00:00<00:01, 1615.95it/s]warmup should be done:   6%|▌         | 171/3000 [00:00<00:01, 1704.69it/s]warmup should be done:   6%|▌         | 171/3000 [00:00<00:01, 1702.28it/s]warmup should be done:   6%|▌         | 169/3000 [00:00<00:01, 1682.31it/s]warmup should be done:   6%|▌         | 170/3000 [00:00<00:01, 1691.10it/s]warmup should be done:  11%|█         | 323/3000 [00:00<00:01, 1614.59it/s]warmup should be done:  11%|█▏        | 342/3000 [00:00<00:01, 1705.90it/s]warmup should be done:  11%|█▏        | 342/3000 [00:00<00:01, 1705.82it/s]warmup should be done:  11%|█▏        | 343/3000 [00:00<00:01, 1709.84it/s]warmup should be done:  11%|█▏        | 340/3000 [00:00<00:01, 1693.31it/s]warmup should be done:  11%|█▏        | 338/3000 [00:00<00:01, 1680.77it/s]warmup should be done:  11%|█         | 324/3000 [00:00<00:01, 1587.96it/s]warmup should be done:  11%|█▏        | 338/3000 [00:00<00:01, 1639.46it/s]warmup should be done:  16%|█▌        | 486/3000 [00:00<00:01, 1617.66it/s]warmup should be done:  17%|█▋        | 513/3000 [00:00<00:01, 1706.82it/s]warmup should be done:  17%|█▋        | 508/3000 [00:00<00:01, 1688.39it/s]warmup should be done:  17%|█▋        | 515/3000 [00:00<00:01, 1711.44it/s]warmup should be done:  17%|█▋        | 510/3000 [00:00<00:01, 1693.72it/s]warmup should be done:  16%|█▌        | 486/3000 [00:00<00:01, 1601.56it/s]warmup should be done:  17%|█▋        | 513/3000 [00:00<00:01, 1680.96it/s]warmup should be done:  17%|█▋        | 507/3000 [00:00<00:01, 1661.98it/s]warmup should be done:  23%|██▎       | 685/3000 [00:00<00:01, 1708.67it/s]warmup should be done:  23%|██▎       | 680/3000 [00:00<00:01, 1695.49it/s]warmup should be done:  23%|██▎       | 679/3000 [00:00<00:01, 1695.17it/s]warmup should be done:  23%|██▎       | 687/3000 [00:00<00:01, 1713.26it/s]warmup should be done:  22%|██▏       | 648/3000 [00:00<00:01, 1610.60it/s]warmup should be done:  23%|██▎       | 686/3000 [00:00<00:01, 1699.59it/s]warmup should be done:  22%|██▏       | 658/3000 [00:00<00:01, 1644.81it/s]warmup should be done:  23%|██▎       | 678/3000 [00:00<00:01, 1677.11it/s]warmup should be done:  28%|██▊       | 850/3000 [00:00<00:01, 1696.95it/s]warmup should be done:  29%|██▊       | 857/3000 [00:00<00:01, 1709.61it/s]warmup should be done:  29%|██▊       | 859/3000 [00:00<00:01, 1713.31it/s]warmup should be done:  28%|██▊       | 850/3000 [00:00<00:01, 1697.59it/s]warmup should be done:  29%|██▊       | 857/3000 [00:00<00:01, 1702.54it/s]warmup should be done:  27%|██▋       | 810/3000 [00:00<00:01, 1599.15it/s]warmup should be done:  28%|██▊       | 830/3000 [00:00<00:01, 1668.84it/s]warmup should be done:  28%|██▊       | 849/3000 [00:00<00:01, 1687.50it/s]warmup should be done:  34%|███▍      | 1020/3000 [00:00<00:01, 1695.80it/s]warmup should be done:  34%|███▍      | 1028/3000 [00:00<00:01, 1708.69it/s]warmup should be done:  34%|███▍      | 1020/3000 [00:00<00:01, 1697.04it/s]warmup should be done:  34%|███▍      | 1031/3000 [00:00<00:01, 1712.34it/s]warmup should be done:  34%|███▍      | 1030/3000 [00:00<00:01, 1708.46it/s]warmup should be done:  33%|███▎      | 1002/3000 [00:00<00:01, 1683.46it/s]warmup should be done:  34%|███▍      | 1018/3000 [00:00<00:01, 1686.57it/s]warmup should be done:  32%|███▏      | 970/3000 [00:00<00:01, 1584.03it/s]warmup should be done:  40%|███▉      | 1199/3000 [00:00<00:01, 1705.02it/s]warmup should be done:  40%|███▉      | 1190/3000 [00:00<00:01, 1691.69it/s]warmup should be done:  40%|███▉      | 1190/3000 [00:00<00:01, 1693.16it/s]warmup should be done:  40%|████      | 1203/3000 [00:00<00:01, 1703.15it/s]warmup should be done:  40%|████      | 1201/3000 [00:00<00:01, 1707.55it/s]warmup should be done:  39%|███▉      | 1172/3000 [00:00<00:01, 1687.75it/s]warmup should be done:  40%|███▉      | 1187/3000 [00:00<00:01, 1685.41it/s]warmup should be done:  38%|███▊      | 1129/3000 [00:00<00:01, 1544.08it/s]warmup should be done:  45%|████▌     | 1360/3000 [00:00<00:00, 1694.10it/s]warmup should be done:  46%|████▌     | 1371/3000 [00:00<00:00, 1707.09it/s]warmup should be done:  45%|████▌     | 1361/3000 [00:00<00:00, 1695.61it/s]warmup should be done:  46%|████▌     | 1376/3000 [00:00<00:00, 1708.53it/s]warmup should be done:  46%|████▌     | 1374/3000 [00:00<00:00, 1714.47it/s]warmup should be done:  45%|████▍     | 1341/3000 [00:00<00:00, 1685.57it/s]warmup should be done:  45%|████▌     | 1358/3000 [00:00<00:00, 1691.69it/s]warmup should be done:  43%|████▎     | 1284/3000 [00:00<00:01, 1541.50it/s]warmup should be done:  51%|█████     | 1530/3000 [00:00<00:00, 1693.08it/s]warmup should be done:  51%|█████▏    | 1542/3000 [00:00<00:00, 1706.38it/s]warmup should be done:  51%|█████     | 1531/3000 [00:00<00:00, 1695.12it/s]warmup should be done:  52%|█████▏    | 1548/3000 [00:00<00:00, 1709.98it/s]warmup should be done:  52%|█████▏    | 1547/3000 [00:00<00:00, 1716.36it/s]warmup should be done:  50%|█████     | 1510/3000 [00:00<00:00, 1683.56it/s]warmup should be done:  51%|█████     | 1528/3000 [00:00<00:00, 1689.67it/s]warmup should be done:  48%|████▊     | 1444/3000 [00:00<00:00, 1557.33it/s]warmup should be done:  57%|█████▋    | 1700/3000 [00:01<00:00, 1692.96it/s]warmup should be done:  57%|█████▋    | 1701/3000 [00:01<00:00, 1694.67it/s]warmup should be done:  57%|█████▋    | 1713/3000 [00:01<00:00, 1702.86it/s]warmup should be done:  57%|█████▋    | 1719/3000 [00:01<00:00, 1714.54it/s]warmup should be done:  56%|█████▌    | 1679/3000 [00:01<00:00, 1680.30it/s]warmup should be done:  57%|█████▋    | 1720/3000 [00:01<00:00, 1696.34it/s]warmup should be done:  57%|█████▋    | 1699/3000 [00:01<00:00, 1692.75it/s]warmup should be done:  54%|█████▎    | 1605/3000 [00:01<00:00, 1570.74it/s]warmup should be done:  62%|██████▏   | 1870/3000 [00:01<00:00, 1693.31it/s]warmup should be done:  63%|██████▎   | 1884/3000 [00:01<00:00, 1703.37it/s]warmup should be done:  62%|██████▏   | 1871/3000 [00:01<00:00, 1693.58it/s]warmup should be done:  63%|██████▎   | 1892/3000 [00:01<00:00, 1718.66it/s]warmup should be done:  62%|██████▏   | 1848/3000 [00:01<00:00, 1682.72it/s]warmup should be done:  63%|██████▎   | 1891/3000 [00:01<00:00, 1700.40it/s]warmup should be done:  62%|██████▏   | 1870/3000 [00:01<00:00, 1696.73it/s]warmup should be done:  59%|█████▉    | 1769/3000 [00:01<00:00, 1591.40it/s]warmup should be done:  68%|██████▊   | 2040/3000 [00:01<00:00, 1693.20it/s]warmup should be done:  68%|██████▊   | 2041/3000 [00:01<00:00, 1695.30it/s]warmup should be done:  68%|██████▊   | 2055/3000 [00:01<00:00, 1703.65it/s]warmup should be done:  69%|██████▉   | 2065/3000 [00:01<00:00, 1720.68it/s]warmup should be done:  67%|██████▋   | 2017/3000 [00:01<00:00, 1683.05it/s]warmup should be done:  69%|██████▉   | 2063/3000 [00:01<00:00, 1704.36it/s]warmup should be done:  68%|██████▊   | 2041/3000 [00:01<00:00, 1699.46it/s]warmup should be done:  64%|██████▍   | 1932/3000 [00:01<00:00, 1601.58it/s]warmup should be done:  74%|███████▎  | 2210/3000 [00:01<00:00, 1692.34it/s]warmup should be done:  74%|███████▍  | 2226/3000 [00:01<00:00, 1704.03it/s]warmup should be done:  74%|███████▎  | 2211/3000 [00:01<00:00, 1689.86it/s]warmup should be done:  75%|███████▍  | 2238/3000 [00:01<00:00, 1720.99it/s]warmup should be done:  73%|███████▎  | 2188/3000 [00:01<00:00, 1690.99it/s]warmup should be done:  74%|███████▍  | 2235/3000 [00:01<00:00, 1706.86it/s]warmup should be done:  74%|███████▎  | 2211/3000 [00:01<00:00, 1688.71it/s]warmup should be done:  70%|██████▉   | 2095/3000 [00:01<00:00, 1609.21it/s]warmup should be done:  80%|███████▉  | 2397/3000 [00:01<00:00, 1704.99it/s]warmup should be done:  79%|███████▉  | 2380/3000 [00:01<00:00, 1692.21it/s]warmup should be done:  79%|███████▉  | 2380/3000 [00:01<00:00, 1689.18it/s]warmup should be done:  79%|███████▊  | 2359/3000 [00:01<00:00, 1696.15it/s]warmup should be done:  80%|████████  | 2411/3000 [00:01<00:00, 1716.07it/s]warmup should be done:  80%|████████  | 2407/3000 [00:01<00:00, 1708.70it/s]warmup should be done:  79%|███████▉  | 2380/3000 [00:01<00:00, 1685.28it/s]warmup should be done:  75%|███████▌  | 2256/3000 [00:01<00:00, 1608.17it/s]warmup should be done:  85%|████████▌ | 2550/3000 [00:01<00:00, 1694.46it/s]warmup should be done:  86%|████████▌ | 2569/3000 [00:01<00:00, 1707.73it/s]warmup should be done:  85%|████████▌ | 2550/3000 [00:01<00:00, 1691.65it/s]warmup should be done:  84%|████████▍ | 2531/3000 [00:01<00:00, 1703.13it/s]warmup should be done:  86%|████████▌ | 2584/3000 [00:01<00:00, 1717.85it/s]warmup should be done:  86%|████████▌ | 2579/3000 [00:01<00:00, 1711.39it/s]warmup should be done:  85%|████████▌ | 2550/3000 [00:01<00:00, 1687.19it/s]warmup should be done:  81%|████████  | 2421/3000 [00:01<00:00, 1618.10it/s]warmup should be done:  91%|█████████ | 2720/3000 [00:01<00:00, 1695.71it/s]warmup should be done:  91%|█████████▏| 2741/3000 [00:01<00:00, 1708.98it/s]warmup should be done:  91%|█████████ | 2721/3000 [00:01<00:00, 1694.99it/s]warmup should be done:  90%|█████████ | 2704/3000 [00:01<00:00, 1708.58it/s]warmup should be done:  92%|█████████▏| 2757/3000 [00:01<00:00, 1720.99it/s]warmup should be done:  92%|█████████▏| 2751/3000 [00:01<00:00, 1712.93it/s]warmup should be done:  91%|█████████ | 2719/3000 [00:01<00:00, 1687.23it/s]warmup should be done:  86%|████████▌ | 2583/3000 [00:01<00:00, 1617.37it/s]warmup should be done:  96%|█████████▋| 2890/3000 [00:01<00:00, 1694.17it/s]warmup should be done:  97%|█████████▋| 2912/3000 [00:01<00:00, 1708.58it/s]warmup should be done:  96%|█████████▋| 2891/3000 [00:01<00:00, 1695.14it/s]warmup should be done:  96%|█████████▌| 2876/3000 [00:01<00:00, 1711.58it/s]warmup should be done:  98%|█████████▊| 2930/3000 [00:01<00:00, 1722.74it/s]warmup should be done:  97%|█████████▋| 2923/3000 [00:01<00:00, 1714.56it/s]warmup should be done:  96%|█████████▋| 2888/3000 [00:01<00:00, 1686.45it/s]warmup should be done:  92%|█████████▏| 2745/3000 [00:01<00:00, 1614.42it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1714.18it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1708.96it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1706.33it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1693.64it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1693.43it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1685.95it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1684.48it/s]warmup should be done:  97%|█████████▋| 2907/3000 [00:01<00:00, 1602.15it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1592.35it/s]
WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7fde466cd2b0>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7fde466bf190>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7fde47a03e80>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7fde466c20d0>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7fde466bf1f0>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7fde47a04730>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7fde47a06d30>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7fde466c01c0>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
2022-12-12 02:53:18.044220: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fd972830830 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 02:53:18.044281: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 02:53:18.046183: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fd9630291b0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 02:53:18.046235: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 02:53:18.053836: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 02:53:18.054943: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 02:53:19.169624: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fd976834700 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 02:53:19.169689: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 02:53:19.179313: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 02:53:19.187015: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fd96b02a170 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 02:53:19.187062: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 02:53:19.196463: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 02:53:19.236909: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fd97a834820 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 02:53:19.236969: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 02:53:19.246595: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 02:53:19.248637: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fd972f96ec0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 02:53:19.248698: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 02:53:19.257422: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 02:53:19.314863: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fd97302cb10 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 02:53:19.314925: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 02:53:19.322856: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 02:53:19.346915: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fd96a8347b0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 02:53:19.346984: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 02:53:19.356651: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 02:53:25.355911: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 02:53:25.637884: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 02:53:26.031039: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 02:53:26.102738: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 02:53:26.124454: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 02:53:26.128885: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 02:53:26.137589: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 02:53:26.263077: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
[HCTR][02:54:19.353][ERROR][RK0][main]: replica 5 reaches 1000, calling init pre replica
[HCTR][02:54:19.353][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][02:54:19.362][ERROR][RK0][main]: coll ps creation done
[HCTR][02:54:19.362][ERROR][RK0][main]: replica 5 waits for coll ps creation barrier
[HCTR][02:54:19.560][ERROR][RK0][main]: replica 1 reaches 1000, calling init pre replica
[HCTR][02:54:19.560][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][02:54:19.564][ERROR][RK0][main]: replica 4 reaches 1000, calling init pre replica
[HCTR][02:54:19.564][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][02:54:19.569][ERROR][RK0][main]: coll ps creation done
[HCTR][02:54:19.569][ERROR][RK0][main]: replica 1 waits for coll ps creation barrier
[HCTR][02:54:19.569][ERROR][RK0][main]: coll ps creation done
[HCTR][02:54:19.569][ERROR][RK0][main]: replica 4 waits for coll ps creation barrier
[HCTR][02:54:19.572][ERROR][RK0][tid #140572056610560]: replica 0 reaches 1000, calling init pre replica
[HCTR][02:54:19.572][ERROR][RK0][tid #140572056610560]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][02:54:19.579][ERROR][RK0][tid #140572056610560]: coll ps creation done
[HCTR][02:54:19.579][ERROR][RK0][tid #140572056610560]: replica 0 waits for coll ps creation barrier
[HCTR][02:54:19.596][ERROR][RK0][main]: replica 7 reaches 1000, calling init pre replica
[HCTR][02:54:19.597][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][02:54:19.602][ERROR][RK0][main]: coll ps creation done
[HCTR][02:54:19.602][ERROR][RK0][main]: replica 7 waits for coll ps creation barrier
[HCTR][02:54:19.648][ERROR][RK0][main]: replica 3 reaches 1000, calling init pre replica
[HCTR][02:54:19.649][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][02:54:19.657][ERROR][RK0][main]: coll ps creation done
[HCTR][02:54:19.657][ERROR][RK0][main]: replica 3 waits for coll ps creation barrier
[HCTR][02:54:19.734][ERROR][RK0][tid #140572618626816]: replica 6 reaches 1000, calling init pre replica
[HCTR][02:54:19.734][ERROR][RK0][tid #140572618626816]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][02:54:19.743][ERROR][RK0][tid #140572618626816]: coll ps creation done
[HCTR][02:54:19.743][ERROR][RK0][tid #140572618626816]: replica 6 waits for coll ps creation barrier
[HCTR][02:54:19.826][ERROR][RK0][tid #140573398787840]: replica 2 reaches 1000, calling init pre replica
[HCTR][02:54:19.826][ERROR][RK0][tid #140573398787840]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][02:54:19.831][ERROR][RK0][tid #140573398787840]: coll ps creation done
[HCTR][02:54:19.831][ERROR][RK0][tid #140573398787840]: replica 2 waits for coll ps creation barrier
[HCTR][02:54:19.831][ERROR][RK0][tid #140572056610560]: replica 0 preparing frequency
[HCTR][02:54:20.777][ERROR][RK0][tid #140572056610560]: replica 0 preparing frequency done
[HCTR][02:54:20.816][ERROR][RK0][tid #140572056610560]: replica 0 calling init per replica
[HCTR][02:54:20.816][ERROR][RK0][main]: replica 4 calling init per replica
[HCTR][02:54:20.816][ERROR][RK0][main]: replica 7 calling init per replica
[HCTR][02:54:20.816][ERROR][RK0][main]: replica 3 calling init per replica
[HCTR][02:54:20.816][ERROR][RK0][tid #140573398787840]: replica 2 calling init per replica
[HCTR][02:54:20.816][ERROR][RK0][tid #140572618626816]: replica 6 calling init per replica
[HCTR][02:54:20.816][ERROR][RK0][main]: replica 1 calling init per replica
[HCTR][02:54:20.816][ERROR][RK0][main]: replica 5 calling init per replica
[HCTR][02:54:20.816][ERROR][RK0][tid #140572056610560]: Calling build_v2
[HCTR][02:54:20.816][ERROR][RK0][main]: Calling build_v2
[HCTR][02:54:20.816][ERROR][RK0][main]: Calling build_v2
[HCTR][02:54:20.816][ERROR][RK0][main]: Calling build_v2
[HCTR][02:54:20.816][ERROR][RK0][main]: Calling build_v2
[HCTR][02:54:20.816][ERROR][RK0][tid #140573398787840]: Calling build_v2
[HCTR][02:54:20.816][ERROR][RK0][tid #140572618626816]: Calling build_v2
[HCTR][02:54:20.816][ERROR][RK0][main]: Calling build_v2
[HCTR][02:54:20.816][ERROR][RK0][tid #140572056610560]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][02:54:20.816][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][02:54:20.816][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][02:54:20.816][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][02:54:20.816][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][02:54:20.816][ERROR][RK0][tid #140573398787840]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][02:54:20.816][ERROR][RK0][tid #140572618626816]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][02:54:20.816][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[[[[[[2022-12-12 02:54:202022-12-12 02:54:202022-12-12 02:54:202022-12-12 02:54:20[.[2022-12-12 02:54:20.2022-12-12 02:54:20..816814.816814.2022-12-12 02:54:20816819816814: 816815: 2022-12-12 02:54:20816807.: : E: E.: 816842EE E 816844E:   /hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc /hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc:/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc:E/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc ::136:136 :/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc136136] 136] /hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc136:] ] using concurrent impl MPS] using concurrent impl MPS:] 136using concurrent impl MPSusing concurrent impl MPS
using concurrent impl MPS
136using concurrent impl MPS] 


] 
using concurrent impl MPSusing concurrent impl MPS

[2022-12-12 02:54:20.821146: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:178] v100x8, slow pcie
[2022-12-12 02:54:20.821183: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:[1962022-12-12 02:54:20] .assigning 8 to cpu821192
: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:178] v100x8, slow pcie
[2022-12-12 02:54:20.821237[: 2022-12-12 02:54:20E. 821238/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[: :2022-12-12 02:54:20E196. ] 821258/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccassigning 8 to cpu: :
E178[ ] 2022-12-12 02:54:20/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccv100x8, slow pcie.:
821285212: ] E[build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8 2022-12-12 02:54:20
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[.:[2022-12-12 02:54:208213171782022-12-12 02:54:20[.: ] .2022-12-12 02:54:20821327Ev100x8, slow pcie821329.:  
: 821347E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[E: [ :2022-12-12 02:54:20 E2022-12-12 02:54:20/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc196./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc .:] 821381[:/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc821391212assigning 8 to cpu: 2022-12-12 02:54:20178:: ] 
E.] 213Ebuild asymm link desc with 8X Tesla V100-SXM2-32GB out of 8 821426v100x8, slow pcie]  [
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: 
remote time is 8.68421/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 02:54:20:E
[:[[.178 2022-12-12 02:54:201962022-12-12 02:54:20[2022-12-12 02:54:20821486] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.] .2022-12-12 02:54:20.: v100x8, slow pcie:821531assigning 8 to cpu821532.821533E
178: 
: 821554:  ] E[2022-12-12 02:54:20E: E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccv100x8, slow pcie . [E :
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc821630/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 02:54:20 /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc178:[: :./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:] 2132022-12-12 02:54:20E212821667:196v100x8, slow pcie] . ] : 214] 
remote time is 8.68421821710/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccbuild asymm link desc with 8X Tesla V100-SXM2-32GB out of 8E] assigning 8 to cpu
: :[
 cpu time is 97.0588
E196[2022-12-12 02:54:20/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
 [] 2022-12-12 02:54:20:./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 02:54:20assigning 8 to cpu.212[821795:.
821816] 2022-12-12 02:54:20: 196821834: build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8.E] : E
821865 assigning 8 to cpuE : /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
[ /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE[:2022-12-12 02:54:20/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: 2022-12-12 02:54:20196.:214[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.] 821952213] 2022-12-12 02:54:20:821958assigning 8 to cpu: ] cpu time is 97.0588.212: 
Eremote time is 8.68421
821992] E 
: build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8 /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE[
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: 2022-12-12 02:54:20[:213[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.2022-12-12 02:54:20212] 2022-12-12 02:54:20:822102.] remote time is 8.68421.212: 822116build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8
822131] E: 
: [build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8 EE2022-12-12 02:54:20
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[  .:2022-12-12 02:54:20/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc822220214.:2022-12-12 02:54:20:: ] 822244212.213Ecpu time is 97.0588: ] 822268]  
Ebuild asymm link desc with 8X Tesla V100-SXM2-32GB out of 8: remote time is 8.68421/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc 
E
:/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc [214:[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 02:54:20] 2132022-12-12 02:54:20:.cpu time is 97.0588] .213822400
remote time is 8.68421822400] : 
: remote time is 8.68421EE[
  2022-12-12 02:54:20/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[.::2022-12-12 02:54:20822504214213.: ] ] 822527Ecpu time is 97.0588remote time is 8.68421:  

E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc :/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc214:] [214cpu time is 97.05882022-12-12 02:54:20] 
.cpu time is 97.0588822624
: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:214] cpu time is 97.0588
[2022-12-12 02:55:39.770100: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:118] solver created. now build & solve
[2022-12-12 02:55:39.810419: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:123] solver built. now solve
block 0 storage is 00010001
	access is	0	0	0	0	4	4	4	4	
block 1 storage is 00100010
	access is	1	1	1	1	5	5	5	5	
block 2 storage is 01000100
	access is	2	2	2	2	6	6	6	6	
block 3 storage is 10001000
	access is	3	3	3	3	7	7	7	7	
block 4 storage is 00000000
	access is	8	8	8	8	8	8	8	8	
[2022-12-12 02:55:39.925248: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:127] solver solved
[2022-12-12 02:55:39.925309: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:195] 0 solved master
[2022-12-12 02:55:40. 73816: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 0 solved
[2022-12-12 02:55:40. 73856: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 0 initing device 0
[2022-12-12 02:55:40. 74374: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-12 02:55:40. 74423: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 02:55:40. 75426: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 02:55:40. 76223: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 02:55:40. 89930: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 2 solved
[2022-12-12 02:55:40. 90005: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:[2052022-12-12 02:55:40] .worker 0 thread 2 initing device 2 90005
: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 1 solved
[2022-12-12 02:55:40. 90077: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 1 initing device 1
[2022-12-12 02:55:40. 90310: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 7 solved
[2022-12-12 02:55:40. 90370: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 7 initing device 7
[2022-12-12 02:55:40. 90453: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-12 02:55:40. 90493[: 2022-12-12 02:55:40E.  90503/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu: :E1815 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuBuilding Coll Cache with ... num gpu device is 8:
1980] eager alloc mem 381.47 MB
[2022-12-12 02:55:40. 90562: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 02:55:40. 90806: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-12 02:55:40. 90854: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 02:55:40. 91159: E [/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc2022-12-12 02:55:40:.202 91182] : 5 solvedE
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc[:2022-12-12 02:55:40202.]  912524 solved: 
E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205[] 2022-12-12 02:55:40worker 0 thread 5 initing device 5.
 91303: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 4 initing device 4
[2022-12-12 02:55:40. 91713: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-12 02:55:40. 91757: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu[:2022-12-12 02:55:401980.]  91768eager alloc mem 381.47 MB: 
E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-12 02:55:40. 91839: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 02:55:40. 93891: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 02:55:40. 94007: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[[2022-12-12 02:55:402022-12-12 02:55:40.. 94073 94084: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc::202202] ] 6 solved3 solved

[2022-12-12 02:55:40[.2022-12-12 02:55:40 94185.:  94188E:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc [:/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc2022-12-12 02:55:40205:.] 205 94217worker 0 thread 6 initing device 6] : 
worker 0 thread 3 initing device 3E
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 02:55:40[.2022-12-12 02:55:40 94685.:  94688E:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu :/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu1815:] 1815Building Coll Cache with ... num gpu device is 8] 
Building Coll Cache with ... num gpu device is 8
[2022-12-12 02:55:40.[ 947582022-12-12 02:55:40: [.E2022-12-12 02:55:40 94775 .: /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu 94780E::  1980E/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu]  :eager alloc mem 381.47 MB/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu1980
:] 1980eager alloc mem 381.47 MB] 
eager alloc mem 381.47 MB
[2022-12-12 02:55:40. 95276: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 02:55:40. 98333: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 02:55:40. 98623: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 02:55:40. 98797: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 02:55:40. 98843: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 02:55:40. 99369: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 02:55:40. 99429: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 02:55:40. 99967: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 02:55:40.103378: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 02:55:40.103486: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 02:55:40.154111: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.00 Bytes
[2022-12-12 02:55:40.159426: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5
[2022-12-12 02:55:40.159538: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 02:55:40.160359: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 38.53 MB
[2022-12-12 02:55:40.160934: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 02:55:40.161974: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:55:40.162021: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 38.15 MB
[[[2022-12-12 02:55:402022-12-12 02:55:402022-12-12 02:55:40...188181188174188181: : : EEE   /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:::198019801980] ] eager alloc mem 5.00 Bytes] eager alloc mem 5.00 Bytes
eager alloc mem 5.00 Bytes

[[2022-12-12 02:55:402022-12-12 02:55:40..188656188656: : E[E [2022-12-12 02:55:40 /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu2022-12-12 02:55:40./hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:.188716:1980188719: 1980] : E] eager alloc mem 5.00 BytesE eager alloc mem 5.00 Bytes
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu
/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::19801980] ] eager alloc mem 5.00 Byteseager alloc mem 5.00 Bytes

[2022-12-12 02:55:40.190356: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 02:55:40.191017: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 02:55:40.191064: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 4.77 GB
[2022-12-12 02:55:40.201610: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5
[2022-12-12 02:55:40.201692: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 02:55:40.201718: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5
[[2022-12-12 02:55:402022-12-12 02:55:40..201789201803: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::638638] ] eager release cuda mem 5eager release cuda mem 400000000

[2022-12-12 02:55:40.201891: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 02:55:40.202027: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5
[2022-12-12 02:55:40.202116: E [/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc2022-12-12 02:55:40:.638202113] : eager release cuda mem 400000000E
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5
[2022-12-12 02:55:40.202187: E [/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc2022-12-12 02:55:40:.638202216] : eager release cuda mem 5E
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 02:55:40.202280[: 2022-12-12 02:55:40E. 202274/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc: :E638 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cceager release cuda mem 400000000:
638] eager release cuda mem 5
[2022-12-12 02:55:40.202374: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 02:55:40.202630: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 38.53 MB
[2022-12-12 02:55:40.203755: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 38.53 MB
[2022-12-12 02:55:40.204257: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 38.53 MB
[2022-12-12 02:55:40.205426: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 38.53 MB
[2022-12-12 02:55:40.205965: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 38.53 MB
[2022-12-12 02:55:40.206490: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 38.53 MB
[2022-12-12 02:55:40.207009: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 38.53 MB
[2022-12-12 02:55:40.207858: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 02:55:40.208301: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 02:55:40.208334: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 02:55:40.208691: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 02:55:40.208748: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 02:55:40.208791: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 02:55:40.208872: E [/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc2022-12-12 02:55:40:.638208879] : eager release cuda mem 625663E
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 02:55:40.208947: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 38.15 MB
[2022-12-12 02:55:40.209320: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:[6382022-12-12 02:55:40] .eager release cuda mem 625663209337
: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:55:40.209375: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:[432022-12-12 02:55:40] .WORKER[0] alloc host memory 38.15 MB209390
: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 38.15 MB
[2022-12-12 02:55:40.209711: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:55:40.209759: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:[432022-12-12 02:55:40] .WORKER[0] alloc host memory 38.15 MB209769
: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:55:40.209808: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638[] 2022-12-12 02:55:40eager release cuda mem 625663.
209828: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 38.15 MB
[2022-12-12 02:55:40.209859: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 38.15 MB
[2022-12-12 02:55:40.209921: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:55:40.209965: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 38.15 MB
[2022-12-12 02:55:40.235425: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 02:55:40.235523: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 02:55:40.[2359382022-12-12 02:55:40: .E235950 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE: 1980/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu] :eager alloc mem 25.25 KB1980
] eager alloc mem 25.25 KB
[2022-12-12 02:55:40.236026: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 02:55:40.236068: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 4.77 GB
[2022-12-12 02:55:40.236133: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 02:55:40.236178: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 4.77 GB
[2022-12-12 02:55:40.[2362462022-12-12 02:55:40: .E236254 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE: 1980/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu] :eager alloc mem 25.25 KB1980
] eager alloc mem 25.25 KB
[2022-12-12 02:55:40.236557: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638[] 2022-12-12 02:55:40eager release cuda mem 25855.
236582: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 02:55:40.236616: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] [eager alloc mem 4.77 GB2022-12-12 02:55:40
.236636[: 2022-12-12 02:55:40E. 236634/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu: :E1980 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cueager alloc mem 4.77 GB:
1980] eager alloc mem 25.25 KB
[2022-12-12 02:55:40.[2368972022-12-12 02:55:40: .E236903 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccE: 638/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc] :eager release cuda mem 25855638
] eager release cuda mem 25855
[2022-12-12 02:55:40.236997[: 2022-12-12 02:55:40E. 237003/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu: :E1980 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cueager alloc mem 4.77 GB:
1980] eager alloc mem 4.77 GB
[2022-12-12 02:55:40.237280: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 02:55:40.237322: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 4.77 GB
[[[[[[[[2022-12-12 02:55:422022-12-12 02:55:422022-12-12 02:55:422022-12-12 02:55:422022-12-12 02:55:422022-12-12 02:55:422022-12-12 02:55:422022-12-12 02:55:42.. 64001...... 64001:  64002 64001 64001 64009 64001 64008: E: : : : : : E EEEEEE /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu      /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926::::::1926] 192619261926192619261926] Device 3 init p2p of link 2] ] Device 5 init p2p of link 6] ] ] ] Device 2 init p2p of link 1
Device 4 init p2p of link 5
Device 7 init p2p of link 4Device 1 init p2p of link 7Device 6 init p2p of link 0Device 0 init p2p of link 3





[2022-12-12 02:55:42. 64537: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu[:2022-12-12 02:55:421980.] [ 64553eager alloc mem 611.00 KB[2022-12-12 02:55:42: [[[
2022-12-12 02:55:42.E[2022-12-12 02:55:422022-12-12 02:55:422022-12-12 02:55:42. 64562 2022-12-12 02:55:42... 64568: /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu. 64568 64585 64572: E: 64585: : : E 1980: EEE /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu] E   /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:eager alloc mem 611.00 KB /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980
/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:::1980] :198019801980] eager alloc mem 611.00 KB1980] ] ] eager alloc mem 611.00 KB
] eager alloc mem 611.00 KBeager alloc mem 611.00 KBeager alloc mem 611.00 KB
eager alloc mem 611.00 KB



[2022-12-12 02:55:42. 65450: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:55:42[.2022-12-12 02:55:42 65586.:  65590E: [ E2022-12-12 02:55:42/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc .:/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc 65618[638:: 2022-12-12 02:55:42] 638E[[.eager release cuda mem 625663] [ 2022-12-12 02:55:422022-12-12 02:55:42 65649
eager release cuda mem 6256632022-12-12 02:55:42/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc..: 
.: 65683 65672E 65681638: :  : ] EE/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccEeager release cuda mem 625663  : 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc638/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::] :638638eager release cuda mem 625663638] ] 
] eager release cuda mem 625663eager release cuda mem 625663eager release cuda mem 625663


[2022-12-12 02:55:42. 79211: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 6 init p2p of link 5
[2022-12-12 02:55:42. 79386: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 02:55:42. 79439: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 2
[2022-12-12 02:55:42. 79638: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 02:55:42. 80274: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:55:42. 80540: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:55:42. 83338: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 3 init p2p of link 0
[2022-12-12 02:55:42. 83494: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 02:55:42. 83596: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 5 init p2p of link 4
[2022-12-12 02:55:42. 83742: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 02:55:42. 84333: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 2 init p2p of link 3
[2022-12-12 02:55:42. 84371: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[[2022-12-12 02:55:422022-12-12 02:55:42.. 84474 84486: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::19261980] ] Device 7 init p2p of link 1eager alloc mem 611.00 KB

[2022-12-12 02:55:42. 84655: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638[] 2022-12-12 02:55:42eager release cuda mem 625663.
 84662[: 2022-12-12 02:55:42E.  84699/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu: :E1926 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuDevice 4 init p2p of link 7:
1980] eager alloc mem 611.00 KB
[2022-12-12 02:55:42. 84826: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:[19262022-12-12 02:55:42] .Device 0 init p2p of link 6 84860
: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 02:55:42. 85012: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 02:55:42. 85431: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:55:42. 85502: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:55:42. 85720: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:55:42. 85788: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:55:42. 88090: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 6 init p2p of link 4
[2022-12-12 02:55:42. 88212: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 02:55:42. 88384: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 3
[2022-12-12 02:55:42. 88524: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 02:55:42. 89062: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:55:42. 89372: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:55:42. 97569: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 7 init p2p of link 6
[2022-12-12 02:55:42. 97687: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 02:55:42. 98314: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 0 init p2p of link 1
[2022-12-12 02:55:42. 98434: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 02:55:42. 98553: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:55:42. 99322: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:55:42.100806: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 3 init p2p of link 5
[2022-12-12 02:55:42.100932: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 02:55:42.101245: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 5 init p2p of link 7
[2022-12-12 02:55:42.101370: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 02:55:42.101719: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:55:42.101938: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 2 init p2p of link 0
[2022-12-12 02:55:42.102056: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 02:55:42.102224: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:55:42.102642: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 4 init p2p of link 2
[2022-12-12 02:55:42.102763: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 02:55:42.102904: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:55:42.103538: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:55:42.107937: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 6 init p2p of link 7
[2022-12-12 02:55:42.108052: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 02:55:42.108219: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 0
[2022-12-12 02:55:42.108351: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 02:55:42.108913: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:55:42.109215: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:55:42.111748: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 3 init p2p of link 1
[2022-12-12 02:55:42.111863: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 02:55:42.112716: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:55:42.113033: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 4 init p2p of link 6
[2022-12-12 02:55:42.113149: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 02:55:42.114005: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:55:42.114933: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 7 init p2p of link 5
[2022-12-12 02:55:42.115044: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 02:55:42.115361: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 0 init p2p of link 2
[2022-12-12 02:55:42.115481: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 02:55:42.115908: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:55:42.116343: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:55:42.121373: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 5 init p2p of link 3
[2022-12-12 02:55:42.121499: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 02:55:42.121775: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 2 init p2p of link 4
[2022-12-12 02:55:42.121886: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 02:55:42.122269: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:55:42.122658: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:55:42.127349: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 40400000
[2022-12-12 02:55:42.127687: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 40400000
[2022-12-12 02:55:42.128020: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 10000000 / 100000000 nodes ( 10.00 %~10.00 %) | remote 30000000 / 100000000 nodes ( 30.00 %) | cpu 60000000 / 100000000 nodes ( 60.00 %) | 4.77 GB | 2.03627 secs 
[2022-12-12 02:55:42.128154: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 40400000
[2022-12-12 02:55:42.128231: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 10000000 / 100000000 nodes ( 10.00 %~10.00 %) | remote 30000000 / 100000000 nodes ( 30.00 %) | cpu 60000000 / 100000000 nodes ( 60.00 %) | 4.77 GB | 2.03774 secs 
[2022-12-12 02:55:42.128540: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 10000000 / 100000000 nodes ( 10.00 %~10.00 %) | remote 30000000 / 100000000 nodes ( 30.00 %) | cpu 60000000 / 100000000 nodes ( 60.00 %) | 4.77 GB | 2.03377 secs 
[2022-12-12 02:55:42.128622: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 40400000
[2022-12-12 02:55:42.128993: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 10000000 / 100000000 nodes ( 10.00 %~10.00 %) | remote 30000000 / 100000000 nodes ( 30.00 %) | cpu 60000000 / 100000000 nodes ( 60.00 %) | 4.77 GB | 2.03844 secs 
[2022-12-12 02:55:42.130941: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 40400000
[2022-12-12 02:55:42.131505: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 40400000
[2022-12-12 02:55:42.133746: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 10000000 / 100000000 nodes ( 10.00 %~10.00 %) | remote 30000000 / 100000000 nodes ( 30.00 %) | cpu 60000000 / 100000000 nodes ( 60.00 %) | 4.77 GB | 2.03897 secs 
[2022-12-12 02:55:42.133842: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 40400000
[2022-12-12 02:55:42.134312: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 40400000
[2022-12-12 02:55:42.134876: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 10000000 / 100000000 nodes ( 10.00 %~10.00 %) | remote 30000000 / 100000000 nodes ( 30.00 %) | cpu 60000000 / 100000000 nodes ( 60.00 %) | 4.77 GB | 2.04305 secs 
[2022-12-12 02:55:42.136380: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 10000000 / 100000000 nodes ( 10.00 %~10.00 %) | remote 30000000 / 100000000 nodes ( 30.00 %) | cpu 60000000 / 100000000 nodes ( 60.00 %) | 4.77 GB | 2.04553 secs 
[2022-12-12 02:55:42.136891: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 10000000 / 100000000 nodes ( 10.00 %~10.00 %) | remote 30000000 / 100000000 nodes ( 30.00 %) | cpu 60000000 / 100000000 nodes ( 60.00 %) | 4.77 GB | 2.06248 secs 
[2022-12-12 02:55:42.137907: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 13.09 GB
[2022-12-12 02:55:43.552422: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 13.36 GB
[2022-12-12 02:55:43.553132: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 13.36 GB
[2022-12-12 02:55:43.553740: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 13.36 GB
[2022-12-12 02:55:45.105000: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 13.62 GB
[2022-12-12 02:55:45.105684: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 13.62 GB
[2022-12-12 02:55:45.106262: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 13.62 GB
[2022-12-12 02:55:46.351727: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 13.83 GB
[2022-12-12 02:55:46.351857: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 13.83 GB
[2022-12-12 02:55:46.352914: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 13.83 GB
[2022-12-12 02:55:47.599876: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 14.05 GB
[2022-12-12 02:55:47.600129: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 14.05 GB
[2022-12-12 02:55:47.600531: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 14.05 GB
[2022-12-12 02:55:48.629522: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 14.51 GB
[2022-12-12 02:55:48.630523: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 14.51 GB
[2022-12-12 02:55:48.632131: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 14.51 GB
[2022-12-12 02:55:50.118633: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 14.70 GB
[2022-12-12 02:55:50.119784: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 14.70 GB
[HCTR][02:55:51.411][ERROR][RK0][main]: replica 3 calling init per replica done, doing barrier
[HCTR][02:55:51.411][ERROR][RK0][tid #140573398787840]: replica 2 calling init per replica done, doing barrier
[HCTR][02:55:51.411][ERROR][RK0][main]: replica 5 calling init per replica done, doing barrier
[HCTR][02:55:51.411][ERROR][RK0][main]: replica 7 calling init per replica done, doing barrier
[HCTR][02:55:51.411][ERROR][RK0][tid #140572056610560]: replica 0 calling init per replica done, doing barrier
[HCTR][02:55:51.411][ERROR][RK0][main]: replica 4 calling init per replica done, doing barrier
[HCTR][02:55:51.411][ERROR][RK0][tid #140572618626816]: replica 6 calling init per replica done, doing barrier
[HCTR][02:55:51.411][ERROR][RK0][main]: replica 1 calling init per replica done, doing barrier
[HCTR][02:55:51.411][ERROR][RK0][main]: replica 3 calling init per replica done, doing barrier done
[HCTR][02:55:51.411][ERROR][RK0][main]: replica 5 calling init per replica done, doing barrier done
[HCTR][02:55:51.411][ERROR][RK0][main]: replica 1 calling init per replica done, doing barrier done
[HCTR][02:55:51.411][ERROR][RK0][tid #140573398787840]: replica 2 calling init per replica done, doing barrier done
[HCTR][02:55:51.411][ERROR][RK0][tid #140572056610560]: replica 0 calling init per replica done, doing barrier done
[HCTR][02:55:51.411][ERROR][RK0][main]: replica 7 calling init per replica done, doing barrier done
[HCTR][02:55:51.411][ERROR][RK0][main]: replica 4 calling init per replica done, doing barrier done
[HCTR][02:55:51.411][ERROR][RK0][tid #140572618626816]: replica 6 calling init per replica done, doing barrier done
[HCTR][02:55:51.411][ERROR][RK0][main]: init per replica done
[HCTR][02:55:51.411][ERROR][RK0][main]: init per replica done
[HCTR][02:55:51.411][ERROR][RK0][main]: init per replica done
[HCTR][02:55:51.411][ERROR][RK0][tid #140573398787840]: init per replica done
[HCTR][02:55:51.411][ERROR][RK0][main]: init per replica done
[HCTR][02:55:51.411][ERROR][RK0][main]: init per replica done
[HCTR][02:55:51.411][ERROR][RK0][tid #140572618626816]: init per replica done
[HCTR][02:55:51.414][ERROR][RK0][tid #140572056610560]: init per replica done
[HCTR][02:55:51.449][ERROR][RK0][tid #140572534765312]: 7 allocated 3276800 at 0x7fbc64238400
[HCTR][02:55:51.449][ERROR][RK0][tid #140572534765312]: 7 allocated 6553600 at 0x7fbc64558400
[HCTR][02:55:51.449][ERROR][RK0][tid #140572534765312]: 7 allocated 3276800 at 0x7fbc64b98400
[HCTR][02:55:51.449][ERROR][RK0][tid #140572534765312]: 7 allocated 6553600 at 0x7fbc64eb8400
[HCTR][02:55:51.450][ERROR][RK0][tid #140573398787840]: 2 allocated 3276800 at 0x7fbc78238400
[HCTR][02:55:51.450][ERROR][RK0][tid #140573398787840]: 2 allocated 6553600 at 0x7fbc78558400
[HCTR][02:55:51.450][ERROR][RK0][tid #140573398787840]: 2 allocated 3276800 at 0x7fbc78b98400
[HCTR][02:55:51.450][ERROR][RK0][tid #140573398787840]: 2 allocated 6553600 at 0x7fbc78eb8400
[HCTR][02:55:51.450][ERROR][RK0][main]: 5 allocated 3276800 at 0x7fbccc238400
[HCTR][02:55:51.450][ERROR][RK0][main]: 5 allocated 6553600 at 0x7fbccc558400
[HCTR][02:55:51.450][ERROR][RK0][main]: 5 allocated 3276800 at 0x7fbcccb98400
[HCTR][02:55:51.450][ERROR][RK0][main]: 5 allocated 6553600 at 0x7fbccceb8400
[HCTR][02:55:51.450][ERROR][RK0][main]: 4 allocated 3276800 at 0x7fbd0c238400
[HCTR][02:55:51.450][ERROR][RK0][main]: 4 allocated 6553600 at 0x7fbd0c558400
[HCTR][02:55:51.450][ERROR][RK0][main]: 4 allocated 3276800 at 0x7fbd0cb98400
[HCTR][02:55:51.450][ERROR][RK0][main]: 4 allocated 6553600 at 0x7fbd0ceb8400
[HCTR][02:55:51.450][ERROR][RK0][main]: 6 allocated 3276800 at 0x7fbbd8238400
[HCTR][02:55:51.450][ERROR][RK0][main]: 3 allocated 3276800 at 0x7fbc10238400
[HCTR][02:55:51.450][ERROR][RK0][main]: 6 allocated 6553600 at 0x7fbbd8558400
[HCTR][02:55:51.450][ERROR][RK0][main]: 3 allocated 6553600 at 0x7fbc10558400
[HCTR][02:55:51.450][ERROR][RK0][main]: 6 allocated 3276800 at 0x7fbbd8b98400
[HCTR][02:55:51.450][ERROR][RK0][main]: 3 allocated 3276800 at 0x7fbc10b98400
[HCTR][02:55:51.450][ERROR][RK0][main]: 6 allocated 6553600 at 0x7fbbd8eb8400
[HCTR][02:55:51.450][ERROR][RK0][main]: 3 allocated 6553600 at 0x7fbc10eb8400
[HCTR][02:55:51.450][ERROR][RK0][main]: 1 allocated 3276800 at 0x7fbcbc238400
[HCTR][02:55:51.450][ERROR][RK0][main]: 1 allocated 6553600 at 0x7fbcbc558400
[HCTR][02:55:51.450][ERROR][RK0][main]: 1 allocated 3276800 at 0x7fbcbcb98400
[HCTR][02:55:51.450][ERROR][RK0][main]: 1 allocated 6553600 at 0x7fbcbceb8400
[HCTR][02:55:51.453][ERROR][RK0][tid #140572056610560]: 0 allocated 3276800 at 0x7fbcb4320000
[HCTR][02:55:51.453][ERROR][RK0][tid #140572056610560]: 0 allocated 6553600 at 0x7fbcb4640000
[HCTR][02:55:51.453][ERROR][RK0][tid #140572056610560]: 0 allocated 3276800 at 0x7fbcb4c80000
[HCTR][02:55:51.453][ERROR][RK0][tid #140572056610560]: 0 allocated 6553600 at 0x7fbcb4fa0000
