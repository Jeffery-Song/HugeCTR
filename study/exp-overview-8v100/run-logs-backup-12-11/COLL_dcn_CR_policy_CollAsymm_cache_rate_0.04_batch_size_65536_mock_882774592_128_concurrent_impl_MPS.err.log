2022-12-11 19:56:36.044430: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.052764: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.060766: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.065317: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.070558: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.074578: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.087286: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.096275: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.143927: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.153141: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.156104: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.158753: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.167913: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.169478: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.171976: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.173797: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.174777: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.175596: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.176801: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.177297: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.178620: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.178859: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.180293: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.180383: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.181865: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.182859: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.183918: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.184980: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.186041: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.187069: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.188074: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.189032: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.190764: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.191999: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.193007: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.194029: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.195039: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.196332: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.198051: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.198691: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.199350: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.200319: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.201527: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.202584: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.203948: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.204575: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 19:56:36.205749: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.206572: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.206909: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.208599: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.208815: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.210417: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.211912: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.213237: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.214094: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.214640: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.215554: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.216235: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.216897: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.217900: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.227043: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.229077: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.230477: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.231532: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.232460: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.232959: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.248173: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.249997: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.250063: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.250413: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.250929: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.254188: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.254263: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.254599: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.254648: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.254730: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.255561: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.263884: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.290708: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.290861: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.291052: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.291235: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.291416: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.292901: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.292972: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.296285: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.296464: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.296553: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.296931: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.297245: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.297691: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.297929: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.301622: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.301916: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.303023: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.303323: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.303627: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.303687: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.303869: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.306674: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.307619: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.307987: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.308223: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.308276: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.308467: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.308531: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.311892: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.313057: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.313330: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.313377: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.313524: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.314022: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.316862: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.318033: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.318249: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.318467: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.319055: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.320522: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.321609: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.321750: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.322530: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.323637: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.324543: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.324656: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.326343: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.327046: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.327090: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.328674: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.329638: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.329734: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.331753: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.331818: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.331890: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.334067: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.334173: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.334297: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.336525: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.336621: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.336882: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.338761: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.338854: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.339156: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.340981: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.341119: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.341673: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.344124: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.344442: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.344444: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.346160: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.346732: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.346819: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.348427: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.348998: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.349329: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.351296: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.351456: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.353425: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.353514: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.355123: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 19:56:36.355385: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.355485: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.357211: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.357253: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.359186: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.359293: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.359408: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.361875: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.362937: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.363060: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.364882: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.365036: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.365222: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.366291: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 19:56:36.366299: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 19:56:36.367039: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.367047: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.367326: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.367675: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.369758: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.369924: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.370133: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.370261: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.370568: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.373262: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.373334: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.373593: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.373780: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.375669: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.375670: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.376921: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.376956: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.377644: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.377991: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.380280: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.380418: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.381403: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.381418: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.382124: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.382311: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.384584: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.384816: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.416248: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.416322: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.416919: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.417041: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.420739: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.420942: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.421756: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.422067: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.425750: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.425921: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.426578: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.426868: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.431721: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.431854: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.432511: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.432730: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.436750: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.436859: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.437400: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.437662: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.442313: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.442364: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.443033: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.443354: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.447278: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.447366: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.448031: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.448248: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.451786: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.451989: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.454053: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.455328: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.457380: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.457424: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.458418: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.460108: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.462753: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.463367: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.463650: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.495000: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.497128: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.497738: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.498346: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.499732: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.503278: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.504909: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.505056: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.507189: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.508547: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.509994: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.510836: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.511690: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.513447: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.514190: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.567303: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.567759: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.570598: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.572097: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.573689: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.575454: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.577804: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.579501: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.581503: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.585114: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.585967: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.586729: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 19:56:36.586799: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.588962: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.591930: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.593871: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 19:56:36.594016: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.596463: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.597399: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.601149: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.601429: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.603445: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.603534: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 19:56:36.605842: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.607362: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.609941: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 19:56:36.610517: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.612985: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.617915: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.618982: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.623866: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.624832: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:36.629675: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:37.471504: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:37.472121: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:37.472848: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:37.473441: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 19:56:37.473502: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-11 19:56:37.492529: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:37.493348: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:37.493954: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:37.494542: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:37.495070: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:37.495602: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-11 19:56:37.542173: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 19:56:37.542385: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 19:56:37.587879: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12340
2022-12-11 19:56:37.722763: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:37.723508: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:37.724284: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:37.724831: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:37.724943: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 19:56:37.725001: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-11 19:56:37.725684: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:37.726403: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:37.726872: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 19:56:37.726937: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-11 19:56:37.743043: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:37.743674: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:37.744189: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:37.744948: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:37.745458: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:37.745934: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:2/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-11 19:56:37.746394: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:37.747035: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:37.747567: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:37.748373: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:37.748906: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:37.749375: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:1/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-11 19:56:37.772503: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:37.773125: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:37.773643: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:37.774107: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 19:56:37.774159: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-11 19:56:37.793650: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:37.794282: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:37.794789: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:37.795732: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:37.796299: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:37.796791: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:3/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-11 19:56:37.821647: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 19:56:37.821852: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 19:56:37.823698: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12342
2022-12-11 19:56:37.841717: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 19:56:37.841920: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 19:56:37.843669: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12343
2022-12-11 19:56:37.845948: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 19:56:37.846127: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 19:56:37.846431: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:37.847032: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:37.847309: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12341
2022-12-11 19:56:37.847858: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:37.848327: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 19:56:37.848375: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-11 19:56:37.854788: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:37.855553: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:37.856090: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:37.856550: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 19:56:37.856600: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-11 19:56:37.856985: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:37.857551: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:37.858079: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:37.858533: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 19:56:37.858576: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-11 19:56:37.860608: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:37.861199: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:37.861726: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:37.862191: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 19:56:37.862236: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-11 19:56:37.867299: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:37.867948: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:37.868477: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:37.869058: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:37.869562: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:37.870034: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:7/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-11 19:56:37.874904: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:37.875565: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:37.876089: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:37.876702: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:37.876750: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:37.878185: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:37.878189: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:37.879325: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:37.879334: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:4/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-11 19:56:37.879938: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:37.880456: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:37.880930: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:5/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-11 19:56:37.881691: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:37.882296: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:37.882807: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:37.884276: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:37.884802: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:56:37.885263: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:6/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-11 19:56:37.913881: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 19:56:37.914074: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 19:56:37.915010: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12347
2022-12-11 19:56:37.924700: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 19:56:37.924896: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 19:56:37.926083: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 19:56:37.926243: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 19:56:37.926706: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12344
2022-12-11 19:56:37.928079: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12345
2022-12-11 19:56:37.930338: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 19:56:37.930521: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 19:56:37.931444: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12346
[HCTR][19:56:39.188][ERROR][RK0][main]: using mock embedding with 882774585 * 128 elements
[HCTR][19:56:39.188][ERROR][RK0][main]: using mock embedding with 882774585 * 128 elements
[HCTR][19:56:39.199][ERROR][RK0][main]: using mock embedding with 882774585 * 128 elements
[HCTR][19:56:39.199][ERROR][RK0][main]: using mock embedding with 882774585 * 128 elements
[HCTR][19:56:39.199][ERROR][RK0][main]: using mock embedding with 882774585 * 128 elements
[HCTR][19:56:39.199][ERROR][RK0][main]: using mock embedding with 882774585 * 128 elements
[HCTR][19:56:39.199][ERROR][RK0][main]: using mock embedding with 882774585 * 128 elements
[HCTR][19:56:39.199][ERROR][RK0][main]: using mock embedding with 882774585 * 128 elements
warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 1it [00:00,  2.54it/s]warmup run: 1it [00:00,  2.54it/s]warmup run: 67it [00:00, 177.45it/s]warmup run: 64it [00:00, 169.13it/s]warmup run: 124it [00:00, 284.91it/s]warmup run: 116it [00:00, 265.22it/s]warmup run: 180it [00:00, 360.83it/s]warmup run: 168it [00:00, 336.12it/s]warmup run: 234it [00:00, 411.37it/s]warmup run: 221it [00:00, 391.56it/s]warmup run: 290it [00:00, 454.14it/s]warmup run: 275it [00:00, 432.58it/s]warmup run: 346it [00:00, 485.09it/s]warmup run: 328it [00:00, 460.59it/s]warmup run: 403it [00:01, 509.68it/s]warmup run: 381it [00:01, 480.77it/s]warmup run: 460it [00:01, 526.93it/s]warmup run: 434it [00:01, 495.25it/s]warmup run: 517it [00:01, 538.34it/s]warmup run: 487it [00:01, 504.58it/s]warmup run: 1it [00:01,  1.38s/it]warmup run: 1it [00:01,  1.38s/it]warmup run: 574it [00:01, 547.36it/s]warmup run: 1it [00:01,  1.40s/it]warmup run: 1it [00:01,  1.40s/it]warmup run: 1it [00:01,  1.40s/it]warmup run: 1it [00:01,  1.40s/it]warmup run: 540it [00:01, 501.01it/s]warmup run: 99it [00:01, 92.41it/s]warmup run: 97it [00:01, 90.44it/s]warmup run: 94it [00:01, 86.74it/s]warmup run: 98it [00:01, 90.40it/s]warmup run: 631it [00:01, 551.98it/s]warmup run: 96it [00:01, 88.57it/s]warmup run: 89it [00:01, 82.03it/s]warmup run: 592it [00:01, 491.19it/s]warmup run: 198it [00:01, 198.05it/s]warmup run: 194it [00:01, 193.90it/s]warmup run: 189it [00:01, 187.53it/s]warmup run: 196it [00:01, 194.17it/s]warmup run: 190it [00:01, 187.86it/s]warmup run: 688it [00:01, 556.21it/s]warmup run: 180it [00:01, 178.42it/s]warmup run: 643it [00:01, 484.88it/s]warmup run: 296it [00:01, 310.61it/s]warmup run: 286it [00:01, 298.17it/s]warmup run: 286it [00:01, 298.95it/s]warmup run: 295it [00:01, 307.62it/s]warmup run: 279it [00:01, 287.80it/s]warmup run: 746it [00:01, 560.70it/s]warmup run: 271it [00:01, 282.51it/s]warmup run: 693it [00:01, 484.67it/s]warmup run: 390it [00:01, 417.94it/s]warmup run: 383it [00:01, 412.23it/s]warmup run: 383it [00:01, 412.33it/s]warmup run: 388it [00:01, 412.35it/s]warmup run: 374it [00:01, 399.41it/s]warmup run: 359it [00:01, 382.89it/s]warmup run: 803it [00:01, 557.96it/s]warmup run: 743it [00:01, 488.45it/s]warmup run: 487it [00:01, 525.91it/s]warmup run: 480it [00:01, 521.16it/s]warmup run: 480it [00:01, 519.98it/s]warmup run: 479it [00:01, 508.34it/s]warmup run: 473it [00:01, 513.61it/s]warmup run: 446it [00:01, 477.50it/s]warmup run: 860it [00:01, 542.68it/s]warmup run: 793it [00:01, 491.61it/s]warmup run: 586it [00:01, 626.34it/s]warmup run: 578it [00:01, 619.88it/s]warmup run: 575it [00:02, 613.23it/s]warmup run: 570it [00:02, 594.95it/s]warmup run: 566it [00:02, 603.95it/s]warmup run: 537it [00:02, 570.42it/s]warmup run: 915it [00:02, 531.15it/s]warmup run: 846it [00:02, 501.01it/s]warmup run: 684it [00:02, 708.93it/s]warmup run: 674it [00:02, 700.43it/s]warmup run: 672it [00:02, 697.50it/s]warmup run: 662it [00:02, 670.76it/s]warmup run: 663it [00:02, 688.56it/s]warmup run: 636it [00:02, 668.12it/s]warmup run: 969it [00:02, 525.13it/s]warmup run: 904it [00:02, 523.47it/s]warmup run: 782it [00:02, 777.38it/s]warmup run: 770it [00:02, 764.87it/s]warmup run: 770it [00:02, 768.08it/s]warmup run: 755it [00:02, 735.97it/s]warmup run: 764it [00:02, 768.06it/s]warmup run: 732it [00:02, 741.25it/s]warmup run: 961it [00:02, 537.21it/s]warmup run: 1022it [00:02, 520.85it/s]warmup run: 880it [00:02, 830.25it/s]warmup run: 867it [00:02, 819.13it/s]warmup run: 866it [00:02, 818.34it/s]warmup run: 851it [00:02, 794.81it/s]warmup run: 865it [00:02, 829.75it/s]warmup run: 827it [00:02, 796.42it/s]warmup run: 1018it [00:02, 546.38it/s]warmup run: 1077it [00:02, 527.56it/s]warmup run: 978it [00:02, 870.77it/s]warmup run: 962it [00:02, 854.18it/s]warmup run: 944it [00:02, 831.90it/s]warmup run: 962it [00:02, 852.87it/s]warmup run: 963it [00:02, 868.91it/s]warmup run: 920it [00:02, 784.67it/s]warmup run: 1074it [00:02, 548.99it/s]warmup run: 1132it [00:02, 532.45it/s]warmup run: 1075it [00:02, 896.46it/s]warmup run: 1057it [00:02, 878.33it/s]warmup run: 1038it [00:02, 859.85it/s]warmup run: 1059it [00:02, 884.27it/s]warmup run: 1062it [00:02, 902.74it/s]warmup run: 1017it [00:02, 832.76it/s]warmup run: 1131it [00:02, 553.99it/s]warmup run: 1188it [00:02, 539.19it/s]warmup run: 1175it [00:02, 924.40it/s]warmup run: 1152it [00:02, 898.16it/s]warmup run: 1132it [00:02, 881.75it/s]warmup run: 1161it [00:02, 925.21it/s]warmup run: 1155it [00:02, 891.39it/s]warmup run: 1116it [00:02, 874.75it/s]warmup run: 1244it [00:02, 545.17it/s]warmup run: 1187it [00:02, 541.55it/s]warmup run: 1247it [00:02, 911.86it/s]warmup run: 1273it [00:02, 913.91it/s]warmup run: 1225it [00:02, 890.12it/s]warmup run: 1260it [00:02, 941.93it/s]warmup run: 1252it [00:02, 913.72it/s]warmup run: 1214it [00:02, 903.27it/s]warmup run: 1301it [00:02, 549.81it/s]warmup run: 1242it [00:02, 535.73it/s]warmup run: 1318it [00:02, 899.94it/s]warmup run: 1360it [00:02, 957.02it/s]warmup run: 1351it [00:02, 934.61it/s]warmup run: 1312it [00:02, 924.36it/s]warmup run: 1358it [00:02, 553.64it/s]warmup run: 1342it [00:02, 780.56it/s]warmup run: 1296it [00:02, 531.15it/s]warmup run: 1368it [00:02, 766.89it/s]warmup run: 1415it [00:02, 555.93it/s]warmup run: 1350it [00:02, 529.19it/s]warmup run: 1447it [00:02, 825.76it/s]warmup run: 1411it [00:02, 779.15it/s]warmup run: 1459it [00:02, 780.47it/s]warmup run: 1408it [00:03, 755.91it/s]warmup run: 1451it [00:03, 696.62it/s]warmup run: 1426it [00:03, 664.08it/s]warmup run: 1472it [00:03, 559.89it/s]warmup run: 1405it [00:03, 535.12it/s]warmup run: 1494it [00:03, 710.69it/s]warmup run: 1534it [00:03, 735.34it/s]warmup run: 1545it [00:03, 699.33it/s]warmup run: 1529it [00:03, 545.30it/s]warmup run: 1461it [00:03, 540.18it/s]warmup run: 1526it [00:03, 639.40it/s]warmup run: 1499it [00:03, 623.33it/s]warmup run: 1491it [00:03, 673.02it/s]warmup run: 1570it [00:03, 666.94it/s]warmup run: 1612it [00:03, 678.99it/s]warmup run: 1584it [00:03, 534.64it/s]warmup run: 1516it [00:03, 524.09it/s]warmup run: 1621it [00:03, 662.08it/s]warmup run: 1594it [00:03, 613.88it/s]warmup run: 1566it [00:03, 600.69it/s]warmup run: 1565it [00:03, 619.83it/s]warmup run: 1640it [00:03, 633.95it/s]warmup run: 1638it [00:03, 517.09it/s]warmup run: 1684it [00:03, 644.04it/s]warmup run: 1569it [00:03, 508.31it/s]warmup run: 1658it [00:03, 600.02it/s]warmup run: 1692it [00:03, 635.01it/s]warmup run: 1630it [00:03, 583.78it/s]warmup run: 1632it [00:03, 586.06it/s]warmup run: 1690it [00:03, 512.38it/s]warmup run: 1706it [00:03, 615.59it/s]warmup run: 1620it [00:03, 502.49it/s]warmup run: 1751it [00:03, 618.49it/s]warmup run: 1720it [00:03, 586.05it/s]warmup run: 1691it [00:03, 572.58it/s]warmup run: 1759it [00:03, 612.77it/s]warmup run: 1694it [00:03, 564.12it/s]warmup run: 1676it [00:03, 517.58it/s]warmup run: 1769it [00:03, 603.60it/s]warmup run: 1742it [00:03, 485.38it/s]warmup run: 1815it [00:03, 597.31it/s]warmup run: 1780it [00:03, 580.71it/s]warmup run: 1750it [00:03, 563.28it/s]warmup run: 1823it [00:03, 601.68it/s]warmup run: 1734it [00:03, 534.36it/s]warmup run: 1831it [00:03, 594.81it/s]warmup run: 1753it [00:03, 528.90it/s]warmup run: 1791it [00:03, 467.26it/s]warmup run: 1839it [00:03, 576.79it/s]warmup run: 1876it [00:03, 581.27it/s]warmup run: 1808it [00:03, 554.97it/s]warmup run: 1885it [00:03, 592.10it/s]warmup run: 1791it [00:03, 542.93it/s]warmup run: 1891it [00:03, 587.74it/s]warmup run: 1838it [00:03, 460.21it/s]warmup run: 1808it [00:03, 509.61it/s]warmup run: 1898it [00:03, 572.15it/s]warmup run: 1865it [00:03, 556.54it/s]warmup run: 1935it [00:03, 558.95it/s]warmup run: 1945it [00:03, 585.31it/s]warmup run: 1848it [00:03, 548.32it/s]warmup run: 1951it [00:03, 582.20it/s]warmup run: 1887it [00:03, 467.44it/s]warmup run: 1860it [00:03, 507.73it/s]warmup run: 1956it [00:03, 569.78it/s]warmup run: 1922it [00:03, 552.41it/s]warmup run: 2004it [00:03, 582.51it/s]warmup run: 1992it [00:03, 535.23it/s]warmup run: 1903it [00:03, 541.01it/s]warmup run: 2010it [00:04, 579.14it/s]warmup run: 1938it [00:04, 479.15it/s]warmup run: 2014it [00:04, 569.44it/s]warmup run: 1912it [00:04, 503.43it/s]warmup run: 1978it [00:04, 550.87it/s]warmup run: 2063it [00:04, 579.90it/s]warmup run: 2046it [00:04, 519.79it/s]warmup run: 1958it [00:04, 533.27it/s]warmup run: 2069it [00:04, 576.87it/s]warmup run: 1990it [00:04, 489.05it/s]warmup run: 2072it [00:04, 569.07it/s]warmup run: 1967it [00:04, 514.60it/s]warmup run: 2034it [00:04, 547.66it/s]warmup run: 2122it [00:04, 577.56it/s]warmup run: 2099it [00:04, 520.09it/s]warmup run: 2012it [00:04, 520.54it/s]warmup run: 2127it [00:04, 574.30it/s]warmup run: 2042it [00:04, 496.93it/s]warmup run: 2024it [00:04, 527.96it/s]warmup run: 2129it [00:04, 563.12it/s]warmup run: 2089it [00:04, 547.40it/s]warmup run: 2180it [00:04, 575.30it/s]warmup run: 2155it [00:04, 529.33it/s]warmup run: 2065it [00:04, 516.67it/s]warmup run: 2093it [00:04, 498.85it/s]warmup run: 2185it [00:04, 555.90it/s]warmup run: 2078it [00:04, 526.75it/s]warmup run: 2186it [00:04, 555.87it/s]warmup run: 2144it [00:04, 547.13it/s]warmup run: 2238it [00:04, 573.14it/s]warmup run: 2209it [00:04, 532.03it/s]warmup run: 2117it [00:04, 510.20it/s]warmup run: 2145it [00:04, 503.38it/s]warmup run: 2241it [00:04, 550.50it/s]warmup run: 2132it [00:04, 528.52it/s]warmup run: 2242it [00:04, 549.06it/s]warmup run: 2199it [00:04, 546.75it/s]warmup run: 2296it [00:04, 571.48it/s]warmup run: 2263it [00:04, 532.98it/s]warmup run: 2169it [00:04, 504.24it/s]warmup run: 2196it [00:04, 504.57it/s]warmup run: 2297it [00:04, 547.23it/s]warmup run: 2186it [00:04, 521.75it/s]warmup run: 2297it [00:04, 547.06it/s]warmup run: 2254it [00:04, 547.67it/s]warmup run: 2354it [00:04, 568.06it/s]warmup run: 2317it [00:04, 524.18it/s]warmup run: 2222it [00:04, 511.36it/s]warmup run: 2247it [00:04, 491.71it/s]warmup run: 2353it [00:04, 549.12it/s]warmup run: 2352it [00:04, 546.04it/s]warmup run: 2309it [00:04, 547.92it/s]warmup run: 2239it [00:04, 495.58it/s]warmup run: 2412it [00:04, 568.80it/s]warmup run: 2370it [00:04, 515.08it/s]warmup run: 2276it [00:04, 519.65it/s]warmup run: 2297it [00:04, 484.53it/s]warmup run: 2409it [00:04, 550.26it/s]warmup run: 2365it [00:04, 551.39it/s]warmup run: 2407it [00:04, 542.45it/s]warmup run: 2469it [00:04, 568.29it/s]warmup run: 2289it [00:04, 487.79it/s]warmup run: 2426it [00:04, 526.78it/s]warmup run: 2331it [00:04, 527.01it/s]warmup run: 2346it [00:04, 484.39it/s]warmup run: 2465it [00:04, 549.32it/s]warmup run: 2423it [00:04, 558.32it/s]warmup run: 2464it [00:04, 550.40it/s]warmup run: 2526it [00:04, 566.70it/s]warmup run: 2338it [00:04, 488.25it/s]warmup run: 2483it [00:04, 538.95it/s]warmup run: 2386it [00:04, 531.61it/s]warmup run: 2395it [00:04, 479.06it/s]warmup run: 2520it [00:04, 543.98it/s]warmup run: 2479it [00:04, 557.60it/s]warmup run: 2521it [00:04, 554.02it/s]warmup run: 2584it [00:04, 567.61it/s]warmup run: 2388it [00:04, 490.42it/s]warmup run: 2537it [00:05, 536.25it/s]warmup run: 2441it [00:05, 536.34it/s]warmup run: 2449it [00:05, 495.25it/s]warmup run: 2535it [00:05, 556.80it/s]warmup run: 2578it [00:05, 557.57it/s]warmup run: 2575it [00:05, 529.68it/s]warmup run: 2641it [00:05, 566.53it/s]warmup run: 2445it [00:05, 513.07it/s]warmup run: 2591it [00:05, 530.80it/s]warmup run: 2498it [00:05, 544.39it/s]warmup run: 2505it [00:05, 512.41it/s]warmup run: 2593it [00:05, 560.79it/s]warmup run: 2635it [00:05, 558.84it/s]warmup run: 2632it [00:05, 539.39it/s]warmup run: 2502it [00:05, 527.44it/s]warmup run: 2648it [00:05, 539.76it/s]warmup run: 2553it [00:05, 543.42it/s]warmup run: 2560it [00:05, 522.67it/s]warmup run: 2650it [00:05, 557.93it/s]warmup run: 2558it [00:05, 536.45it/s]warmup run: 2609it [00:05, 547.74it/s]warmup run: 2617it [00:05, 534.63it/s]warmup run: 2698it [00:05, 355.83it/s]warmup run: 2614it [00:05, 543.08it/s]warmup run: 2691it [00:05, 356.36it/s]warmup run: 2687it [00:05, 348.03it/s]warmup run: 2752it [00:05, 393.88it/s]warmup run: 2703it [00:05, 341.74it/s]warmup run: 2748it [00:05, 400.37it/s]warmup run: 2739it [00:05, 383.05it/s]warmup run: 2706it [00:05, 348.38it/s]warmup run: 2809it [00:05, 434.24it/s]warmup run: 2759it [00:05, 386.17it/s]warmup run: 2664it [00:05, 358.12it/s]warmup run: 2671it [00:05, 344.90it/s]warmup run: 2804it [00:05, 437.37it/s]warmup run: 2793it [00:05, 418.01it/s]warmup run: 2757it [00:05, 380.99it/s]warmup run: 2866it [00:05, 467.62it/s]warmup run: 2815it [00:05, 424.95it/s]warmup run: 2721it [00:05, 403.66it/s]warmup run: 2669it [00:05, 310.37it/s]warmup run: 2720it [00:05, 374.53it/s]warmup run: 2859it [00:05, 463.90it/s]warmup run: 2847it [00:05, 447.08it/s]warmup run: 2809it [00:05, 412.41it/s]warmup run: 2923it [00:05, 494.08it/s]warmup run: 2872it [00:05, 460.17it/s]warmup run: 2779it [00:05, 443.99it/s]warmup run: 2723it [00:05, 354.52it/s]warmup run: 2769it [00:05, 401.06it/s]warmup run: 2911it [00:05, 478.45it/s]warmup run: 2902it [00:05, 472.61it/s]warmup run: 2864it [00:05, 444.48it/s]warmup run: 2980it [00:05, 514.47it/s]warmup run: 2930it [00:05, 489.36it/s]warmup run: 2836it [00:05, 474.46it/s]warmup run: 3000it [00:05, 506.51it/s]warmup run: 2775it [00:05, 389.88it/s]warmup run: 2825it [00:05, 440.74it/s]warmup run: 2964it [00:05, 491.33it/s]warmup run: 2954it [00:05, 484.75it/s]warmup run: 2917it [00:05, 464.59it/s]warmup run: 2987it [00:06, 509.61it/s]warmup run: 2893it [00:06, 499.24it/s]warmup run: 3000it [00:06, 497.31it/s]warmup run: 2825it [00:06, 415.86it/s]warmup run: 3000it [00:06, 496.37it/s]warmup run: 2880it [00:06, 467.23it/s]warmup run: 3000it [00:06, 495.50it/s]warmup run: 2974it [00:06, 491.85it/s]warmup run: 2949it [00:06, 514.61it/s]warmup run: 3000it [00:06, 489.77it/s]warmup run: 2880it [00:06, 449.05it/s]warmup run: 2933it [00:06, 483.59it/s]warmup run: 3000it [00:06, 482.82it/s]warmup run: 2937it [00:06, 481.02it/s]warmup run: 2985it [00:06, 493.19it/s]warmup run: 3000it [00:06, 477.44it/s]warmup run: 2995it [00:06, 506.44it/s]warmup run: 3000it [00:06, 472.08it/s]

warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]



warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   6%|▌         | 167/3000 [00:00<00:01, 1668.00it/s]warmup should be done:   6%|▌         | 168/3000 [00:00<00:01, 1677.19it/s]warmup should be done:   6%|▌         | 168/3000 [00:00<00:01, 1673.49it/s]warmup should be done:   5%|▌         | 161/3000 [00:00<00:01, 1604.74it/s]warmup should be done:   5%|▌         | 158/3000 [00:00<00:01, 1573.69it/s]warmup should be done:   6%|▌         | 169/3000 [00:00<00:01, 1684.15it/s]warmup should be done:   6%|▌         | 167/3000 [00:00<00:01, 1662.42it/s]warmup should be done:   6%|▌         | 167/3000 [00:00<00:01, 1661.10it/s]warmup should be done:  11%|█         | 326/3000 [00:00<00:01, 1630.95it/s]warmup should be done:  11%|█         | 317/3000 [00:00<00:01, 1582.85it/s]warmup should be done:  11%|█▏        | 338/3000 [00:00<00:01, 1686.83it/s]warmup should be done:  11%|█         | 336/3000 [00:00<00:01, 1675.87it/s]warmup should be done:  11%|█▏        | 338/3000 [00:00<00:01, 1684.73it/s]warmup should be done:  11%|█         | 335/3000 [00:00<00:01, 1668.52it/s]warmup should be done:  11%|█         | 334/3000 [00:00<00:01, 1654.48it/s]warmup should be done:  11%|█▏        | 338/3000 [00:00<00:01, 1676.05it/s]warmup should be done:  16%|█▋        | 490/3000 [00:00<00:01, 1632.68it/s]warmup should be done:  17%|█▋        | 508/3000 [00:00<00:01, 1691.23it/s]warmup should be done:  17%|█▋        | 506/3000 [00:00<00:01, 1683.86it/s]warmup should be done:  16%|█▌        | 476/3000 [00:00<00:01, 1581.00it/s]warmup should be done:  17%|█▋        | 502/3000 [00:00<00:01, 1666.86it/s]warmup should be done:  17%|█▋        | 507/3000 [00:00<00:01, 1683.04it/s]warmup should be done:  17%|█▋        | 500/3000 [00:00<00:01, 1637.49it/s]warmup should be done:  17%|█▋        | 506/3000 [00:00<00:01, 1658.23it/s]warmup should be done:  22%|██▏       | 657/3000 [00:00<00:01, 1646.98it/s]warmup should be done:  23%|██▎       | 678/3000 [00:00<00:01, 1692.96it/s]warmup should be done:  23%|██▎       | 676/3000 [00:00<00:01, 1684.64it/s]warmup should be done:  22%|██▏       | 669/3000 [00:00<00:01, 1665.71it/s]warmup should be done:  23%|██▎       | 676/3000 [00:00<00:01, 1686.90it/s]warmup should be done:  21%|██        | 635/3000 [00:00<00:01, 1581.16it/s]warmup should be done:  22%|██▏       | 668/3000 [00:00<00:01, 1653.49it/s]warmup should be done:  22%|██▏       | 672/3000 [00:00<00:01, 1648.48it/s]warmup should be done:  27%|██▋       | 824/3000 [00:00<00:01, 1652.13it/s]warmup should be done:  28%|██▊       | 845/3000 [00:00<00:01, 1686.35it/s]warmup should be done:  27%|██▋       | 796/3000 [00:00<00:01, 1590.51it/s]warmup should be done:  28%|██▊       | 836/3000 [00:00<00:01, 1659.68it/s]warmup should be done:  28%|██▊       | 845/3000 [00:00<00:01, 1677.25it/s]warmup should be done:  28%|██▊       | 836/3000 [00:00<00:01, 1660.14it/s]warmup should be done:  28%|██▊       | 848/3000 [00:00<00:01, 1672.20it/s]warmup should be done:  28%|██▊       | 837/3000 [00:00<00:01, 1646.15it/s]warmup should be done:  33%|███▎      | 990/3000 [00:00<00:01, 1653.98it/s]warmup should be done:  34%|███▍      | 1015/3000 [00:00<00:01, 1688.79it/s]warmup should be done:  32%|███▏      | 963/3000 [00:00<00:01, 1616.90it/s]warmup should be done:  33%|███▎      | 1002/3000 [00:00<00:01, 1654.98it/s]warmup should be done:  33%|███▎      | 1003/3000 [00:00<00:01, 1647.55it/s]warmup should be done:  34%|███▍      | 1016/3000 [00:00<00:01, 1663.31it/s]warmup should be done:  34%|███▍      | 1013/3000 [00:00<00:01, 1542.98it/s]warmup should be done:  33%|███▎      | 1003/3000 [00:00<00:01, 1533.91it/s]warmup should be done:  39%|███▊      | 1157/3000 [00:00<00:01, 1657.85it/s]warmup should be done:  38%|███▊      | 1130/3000 [00:00<00:01, 1632.85it/s]warmup should be done:  40%|███▉      | 1185/3000 [00:00<00:01, 1690.38it/s]warmup should be done:  39%|███▉      | 1168/3000 [00:00<00:01, 1651.51it/s]warmup should be done:  39%|███▉      | 1169/3000 [00:00<00:01, 1648.35it/s]warmup should be done:  39%|███▉      | 1183/3000 [00:00<00:01, 1658.26it/s]warmup should be done:  39%|███▉      | 1171/3000 [00:00<00:01, 1554.25it/s]warmup should be done:  39%|███▉      | 1172/3000 [00:00<00:01, 1579.11it/s]warmup should be done:  44%|████▍     | 1326/3000 [00:00<00:01, 1667.61it/s]warmup should be done:  43%|████▎     | 1297/3000 [00:00<00:01, 1643.60it/s]warmup should be done:  45%|████▌     | 1355/3000 [00:00<00:00, 1691.46it/s]warmup should be done:  44%|████▍     | 1334/3000 [00:00<00:01, 1647.49it/s]warmup should be done:  44%|████▍     | 1334/3000 [00:00<00:01, 1648.24it/s]warmup should be done:  45%|████▌     | 1350/3000 [00:00<00:00, 1659.44it/s]warmup should be done:  44%|████▍     | 1331/3000 [00:00<00:01, 1565.29it/s]warmup should be done:  45%|████▍     | 1339/3000 [00:00<00:01, 1605.59it/s]warmup should be done:  49%|████▉     | 1464/3000 [00:00<00:00, 1649.60it/s]warmup should be done:  50%|████▉     | 1493/3000 [00:00<00:00, 1653.66it/s]warmup should be done:  51%|█████     | 1525/3000 [00:00<00:00, 1682.01it/s]warmup should be done:  50%|█████     | 1500/3000 [00:00<00:00, 1648.45it/s]warmup should be done:  50%|█████     | 1501/3000 [00:00<00:00, 1654.03it/s]warmup should be done:  51%|█████     | 1516/3000 [00:00<00:00, 1659.54it/s]warmup should be done:  50%|████▉     | 1492/3000 [00:00<00:00, 1577.02it/s]warmup should be done:  50%|█████     | 1506/3000 [00:00<00:00, 1623.21it/s]warmup should be done:  54%|█████▍    | 1630/3000 [00:01<00:00, 1652.36it/s]warmup should be done:  55%|█████▌    | 1661/3000 [00:01<00:00, 1660.47it/s]warmup should be done:  56%|█████▋    | 1694/3000 [00:01<00:00, 1673.91it/s]warmup should be done:  56%|█████▌    | 1665/3000 [00:01<00:00, 1645.76it/s]warmup should be done:  56%|█████▌    | 1668/3000 [00:01<00:00, 1656.83it/s]warmup should be done:  56%|█████▌    | 1684/3000 [00:01<00:00, 1663.11it/s]warmup should be done:  55%|█████▌    | 1653/3000 [00:01<00:00, 1584.99it/s]warmup should be done:  56%|█████▌    | 1673/3000 [00:01<00:00, 1636.12it/s]warmup should be done:  60%|█████▉    | 1797/3000 [00:01<00:00, 1655.47it/s]warmup should be done:  61%|██████    | 1828/3000 [00:01<00:00, 1658.48it/s]warmup should be done:  61%|██████    | 1830/3000 [00:01<00:00, 1643.68it/s]warmup should be done:  62%|██████▏   | 1862/3000 [00:01<00:00, 1669.84it/s]warmup should be done:  61%|██████    | 1835/3000 [00:01<00:00, 1659.46it/s]warmup should be done:  62%|██████▏   | 1851/3000 [00:01<00:00, 1664.74it/s]warmup should be done:  60%|██████    | 1814/3000 [00:01<00:00, 1592.28it/s]warmup should be done:  61%|██████▏   | 1841/3000 [00:01<00:00, 1647.26it/s]warmup should be done:  65%|██████▌   | 1964/3000 [00:01<00:00, 1658.74it/s]warmup should be done:  67%|██████▋   | 1997/3000 [00:01<00:00, 1649.66it/s]warmup should be done:  67%|██████▋   | 2002/3000 [00:01<00:00, 1661.33it/s]warmup should be done:  68%|██████▊   | 2029/3000 [00:01<00:00, 1667.11it/s]warmup should be done:  67%|██████▋   | 2020/3000 [00:01<00:00, 1671.82it/s]warmup should be done:  66%|██████▋   | 1994/3000 [00:01<00:00, 1610.79it/s]warmup should be done:  66%|██████▌   | 1979/3000 [00:01<00:00, 1606.82it/s]warmup should be done:  67%|██████▋   | 2009/3000 [00:01<00:00, 1656.75it/s]warmup should be done:  71%|███████   | 2131/3000 [00:01<00:00, 1661.69it/s]warmup should be done:  72%|███████▏  | 2164/3000 [00:01<00:00, 1653.65it/s]warmup should be done:  73%|███████▎  | 2196/3000 [00:01<00:00, 1665.73it/s]warmup should be done:  73%|███████▎  | 2188/3000 [00:01<00:00, 1673.34it/s]warmup should be done:  72%|███████▏  | 2169/3000 [00:01<00:00, 1658.59it/s]warmup should be done:  72%|███████▏  | 2156/3000 [00:01<00:00, 1589.06it/s]warmup should be done:  71%|███████▏  | 2144/3000 [00:01<00:00, 1618.69it/s]warmup should be done:  72%|███████▎  | 2175/3000 [00:01<00:00, 1651.89it/s]warmup should be done:  77%|███████▋  | 2298/3000 [00:01<00:00, 1662.76it/s]warmup should be done:  78%|███████▊  | 2331/3000 [00:01<00:00, 1656.24it/s]warmup should be done:  79%|███████▊  | 2356/3000 [00:01<00:00, 1670.86it/s]warmup should be done:  79%|███████▉  | 2363/3000 [00:01<00:00, 1658.44it/s]warmup should be done:  78%|███████▊  | 2335/3000 [00:01<00:00, 1644.26it/s]warmup should be done:  77%|███████▋  | 2316/3000 [00:01<00:00, 1576.44it/s]warmup should be done:  77%|███████▋  | 2309/3000 [00:01<00:00, 1627.38it/s]warmup should be done:  78%|███████▊  | 2343/3000 [00:01<00:00, 1659.26it/s]warmup should be done:  82%|████████▏ | 2465/3000 [00:01<00:00, 1663.16it/s]warmup should be done:  83%|████████▎ | 2500/3000 [00:01<00:00, 1666.12it/s]warmup should be done:  84%|████████▍ | 2526/3000 [00:01<00:00, 1677.11it/s]warmup should be done:  84%|████████▍ | 2529/3000 [00:01<00:00, 1656.36it/s]warmup should be done:  83%|████████▎ | 2500/3000 [00:01<00:00, 1639.36it/s]warmup should be done:  82%|████████▏ | 2474/3000 [00:01<00:00, 1568.00it/s]warmup should be done:  82%|████████▏ | 2474/3000 [00:01<00:00, 1633.42it/s]warmup should be done:  84%|████████▎ | 2510/3000 [00:01<00:00, 1639.25it/s]warmup should be done:  88%|████████▊ | 2632/3000 [00:01<00:00, 1663.94it/s]warmup should be done:  89%|████████▉ | 2669/3000 [00:01<00:00, 1672.55it/s]warmup should be done:  90%|████████▉ | 2695/3000 [00:01<00:00, 1680.40it/s]warmup should be done:  89%|████████▉ | 2668/3000 [00:01<00:00, 1649.99it/s]warmup should be done:  88%|████████▊ | 2631/3000 [00:01<00:00, 1563.50it/s]warmup should be done:  88%|████████▊ | 2639/3000 [00:01<00:00, 1637.47it/s]warmup should be done:  89%|████████▉ | 2678/3000 [00:01<00:00, 1649.21it/s]warmup should be done:  90%|████████▉ | 2695/3000 [00:01<00:00, 1531.14it/s]warmup should be done:  93%|█████████▎| 2799/3000 [00:01<00:00, 1664.83it/s]warmup should be done:  95%|█████████▍| 2838/3000 [00:01<00:00, 1676.13it/s]warmup should be done:  95%|█████████▌| 2864/3000 [00:01<00:00, 1677.98it/s]warmup should be done:  95%|█████████▍| 2837/3000 [00:01<00:00, 1659.55it/s]warmup should be done:  93%|█████████▎| 2789/3000 [00:01<00:00, 1566.15it/s]warmup should be done:  93%|█████████▎| 2804/3000 [00:01<00:00, 1640.47it/s]warmup should be done:  95%|█████████▍| 2847/3000 [00:01<00:00, 1658.54it/s]warmup should be done:  95%|█████████▌| 2856/3000 [00:01<00:00, 1551.10it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1673.23it/s]warmup should be done:  99%|█████████▉| 2967/3000 [00:01<00:00, 1669.32it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1661.57it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1657.02it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1643.86it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1642.15it/s]warmup should be done:  98%|█████████▊| 2947/3000 [00:01<00:00, 1567.37it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1638.98it/s]warmup should be done:  99%|█████████▉| 2970/3000 [00:01<00:00, 1645.22it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1620.74it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1610.69it/s]




warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]

warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   6%|▌         | 174/3000 [00:00<00:01, 1739.47it/s]warmup should be done:   5%|▌         | 162/3000 [00:00<00:01, 1617.05it/s]warmup should be done:   6%|▌         | 170/3000 [00:00<00:01, 1697.27it/s]warmup should be done:   6%|▌         | 172/3000 [00:00<00:01, 1715.36it/s]warmup should be done:   6%|▌         | 174/3000 [00:00<00:01, 1734.13it/s]warmup should be done:   6%|▌         | 169/3000 [00:00<00:01, 1681.52it/s]warmup should be done:   5%|▌         | 164/3000 [00:00<00:01, 1633.24it/s]warmup should be done:   6%|▌         | 171/3000 [00:00<00:01, 1701.04it/s]warmup should be done:  11%|█▏        | 344/3000 [00:00<00:01, 1716.64it/s]warmup should be done:  12%|█▏        | 348/3000 [00:00<00:01, 1734.48it/s]warmup should be done:  11%|█▏        | 342/3000 [00:00<00:01, 1708.29it/s]warmup should be done:  11%|█         | 325/3000 [00:00<00:01, 1619.23it/s]warmup should be done:  11%|█         | 335/3000 [00:00<00:01, 1675.77it/s]warmup should be done:  11%|█▏        | 343/3000 [00:00<00:01, 1707.26it/s]warmup should be done:  11%|█▏        | 340/3000 [00:00<00:01, 1687.92it/s]warmup should be done:  12%|█▏        | 348/3000 [00:00<00:01, 1726.27it/s]warmup should be done:  16%|█▌        | 487/3000 [00:00<00:01, 1619.34it/s]warmup should be done:  17%|█▋        | 517/3000 [00:00<00:01, 1720.68it/s]warmup should be done:  17%|█▋        | 514/3000 [00:00<00:01, 1711.72it/s]warmup should be done:  17%|█▋        | 505/3000 [00:00<00:01, 1686.09it/s]warmup should be done:  17%|█▋        | 515/3000 [00:00<00:01, 1709.35it/s]warmup should be done:  17%|█▋        | 511/3000 [00:00<00:01, 1693.85it/s]warmup should be done:  17%|█▋        | 522/3000 [00:00<00:01, 1726.51it/s]warmup should be done:  17%|█▋        | 521/3000 [00:00<00:01, 1718.70it/s]warmup should be done:  22%|██▏       | 649/3000 [00:00<00:01, 1619.31it/s]warmup should be done:  23%|██▎       | 689/3000 [00:00<00:01, 1726.31it/s]warmup should be done:  23%|██▎       | 677/3000 [00:00<00:01, 1698.41it/s]warmup should be done:  23%|██▎       | 691/3000 [00:00<00:01, 1725.64it/s]warmup should be done:  23%|██▎       | 696/3000 [00:00<00:01, 1729.96it/s]warmup should be done:  23%|██▎       | 684/3000 [00:00<00:01, 1704.29it/s]warmup should be done:  23%|██▎       | 686/3000 [00:00<00:01, 1703.00it/s]warmup should be done:  23%|██▎       | 693/3000 [00:00<00:01, 1715.18it/s]warmup should be done:  27%|██▋       | 812/3000 [00:00<00:01, 1620.88it/s]warmup should be done:  29%|██▉       | 865/3000 [00:00<00:01, 1728.59it/s]warmup should be done:  29%|██▉       | 865/3000 [00:00<00:01, 1734.80it/s]warmup should be done:  28%|██▊       | 850/3000 [00:00<00:01, 1706.32it/s]warmup should be done:  29%|██▉       | 871/3000 [00:00<00:01, 1736.52it/s]warmup should be done:  29%|██▊       | 857/3000 [00:00<00:01, 1710.95it/s]warmup should be done:  29%|██▊       | 857/3000 [00:00<00:01, 1701.95it/s]warmup should be done:  29%|██▉       | 868/3000 [00:00<00:01, 1724.15it/s]warmup should be done:  32%|███▎      | 975/3000 [00:00<00:01, 1621.48it/s]warmup should be done:  34%|███▍      | 1022/3000 [00:00<00:01, 1710.44it/s]warmup should be done:  35%|███▍      | 1038/3000 [00:00<00:01, 1727.48it/s]warmup should be done:  35%|███▍      | 1039/3000 [00:00<00:01, 1734.18it/s]warmup should be done:  35%|███▍      | 1046/3000 [00:00<00:01, 1739.29it/s]warmup should be done:  34%|███▍      | 1029/3000 [00:00<00:01, 1712.65it/s]warmup should be done:  34%|███▍      | 1029/3000 [00:00<00:01, 1705.65it/s]warmup should be done:  35%|███▍      | 1041/3000 [00:00<00:01, 1724.60it/s]warmup should be done:  40%|███▉      | 1194/3000 [00:00<00:01, 1710.70it/s]warmup should be done:  40%|████      | 1211/3000 [00:00<00:01, 1722.42it/s]warmup should be done:  41%|████      | 1220/3000 [00:00<00:01, 1737.33it/s]warmup should be done:  40%|████      | 1201/3000 [00:00<00:01, 1711.00it/s]warmup should be done:  40%|████      | 1200/3000 [00:00<00:01, 1705.17it/s]warmup should be done:  40%|████      | 1213/3000 [00:00<00:01, 1723.71it/s]warmup should be done:  40%|████      | 1214/3000 [00:00<00:01, 1725.69it/s]warmup should be done:  38%|███▊      | 1138/3000 [00:00<00:01, 1595.73it/s]warmup should be done:  46%|████▌     | 1367/3000 [00:00<00:00, 1716.38it/s]warmup should be done:  46%|████▋     | 1395/3000 [00:00<00:00, 1740.97it/s]warmup should be done:  46%|████▌     | 1384/3000 [00:00<00:00, 1720.06it/s]warmup should be done:  46%|████▌     | 1374/3000 [00:00<00:00, 1715.34it/s]warmup should be done:  46%|████▋     | 1389/3000 [00:00<00:00, 1731.85it/s]warmup should be done:  46%|████▌     | 1386/3000 [00:00<00:00, 1722.36it/s]warmup should be done:  46%|████▌     | 1371/3000 [00:00<00:00, 1700.87it/s]warmup should be done:  43%|████▎     | 1300/3000 [00:00<00:01, 1601.00it/s]warmup should be done:  52%|█████▏    | 1570/3000 [00:00<00:00, 1739.35it/s]warmup should be done:  52%|█████▏    | 1548/3000 [00:00<00:00, 1720.43it/s]warmup should be done:  52%|█████▏    | 1557/3000 [00:00<00:00, 1715.33it/s]warmup should be done:  52%|█████▏    | 1563/3000 [00:00<00:00, 1729.30it/s]warmup should be done:  51%|█████▏    | 1542/3000 [00:00<00:00, 1699.88it/s]warmup should be done:  51%|█████▏    | 1539/3000 [00:00<00:00, 1698.88it/s]warmup should be done:  52%|█████▏    | 1559/3000 [00:00<00:00, 1717.74it/s]warmup should be done:  49%|████▉     | 1463/3000 [00:00<00:00, 1608.13it/s]warmup should be done:  58%|█████▊    | 1744/3000 [00:01<00:00, 1737.36it/s]warmup should be done:  57%|█████▋    | 1721/3000 [00:01<00:00, 1723.22it/s]warmup should be done:  58%|█████▊    | 1729/3000 [00:01<00:00, 1716.59it/s]warmup should be done:  58%|█████▊    | 1737/3000 [00:01<00:00, 1731.85it/s]warmup should be done:  57%|█████▋    | 1713/3000 [00:01<00:00, 1701.56it/s]warmup should be done:  58%|█████▊    | 1731/3000 [00:01<00:00, 1712.28it/s]warmup should be done:  54%|█████▍    | 1626/3000 [00:01<00:00, 1613.63it/s]warmup should be done:  57%|█████▋    | 1709/3000 [00:01<00:00, 1549.68it/s]warmup should be done:  64%|██████▍   | 1918/3000 [00:01<00:00, 1737.31it/s]warmup should be done:  63%|██████▎   | 1895/3000 [00:01<00:00, 1727.92it/s]warmup should be done:  63%|██████▎   | 1902/3000 [00:01<00:00, 1720.42it/s]warmup should be done:  64%|██████▎   | 1911/3000 [00:01<00:00, 1733.27it/s]warmup should be done:  63%|██████▎   | 1884/3000 [00:01<00:00, 1698.52it/s]warmup should be done:  63%|██████▎   | 1903/3000 [00:01<00:00, 1712.70it/s]warmup should be done:  60%|█████▉    | 1788/3000 [00:01<00:00, 1615.40it/s]warmup should be done:  63%|██████▎   | 1882/3000 [00:01<00:00, 1600.18it/s]warmup should be done:  70%|██████▉   | 2093/3000 [00:01<00:00, 1739.11it/s]warmup should be done:  69%|██████▉   | 2069/3000 [00:01<00:00, 1730.99it/s]warmup should be done:  69%|██████▉   | 2076/3000 [00:01<00:00, 1723.69it/s]warmup should be done:  69%|██████▊   | 2056/3000 [00:01<00:00, 1702.52it/s]warmup should be done:  70%|██████▉   | 2085/3000 [00:01<00:00, 1723.17it/s]warmup should be done:  69%|██████▉   | 2075/3000 [00:01<00:00, 1711.05it/s]warmup should be done:  65%|██████▌   | 1953/3000 [00:01<00:00, 1624.90it/s]warmup should be done:  68%|██████▊   | 2055/3000 [00:01<00:00, 1635.51it/s]warmup should be done:  75%|███████▍  | 2243/3000 [00:01<00:00, 1732.65it/s]warmup should be done:  76%|███████▌  | 2268/3000 [00:01<00:00, 1739.30it/s]warmup should be done:  75%|███████▌  | 2250/3000 [00:01<00:00, 1725.81it/s]warmup should be done:  74%|███████▍  | 2227/3000 [00:01<00:00, 1703.19it/s]warmup should be done:  75%|███████▍  | 2248/3000 [00:01<00:00, 1716.36it/s]warmup should be done:  71%|███████   | 2120/3000 [00:01<00:00, 1637.49it/s]warmup should be done:  75%|███████▌  | 2258/3000 [00:01<00:00, 1721.87it/s]warmup should be done:  74%|███████▍  | 2228/3000 [00:01<00:00, 1660.70it/s]warmup should be done:  81%|████████  | 2417/3000 [00:01<00:00, 1726.37it/s]warmup should be done:  81%|████████  | 2423/3000 [00:01<00:00, 1725.59it/s]warmup should be done:  81%|████████▏ | 2442/3000 [00:01<00:00, 1731.34it/s]warmup should be done:  80%|███████▉  | 2398/3000 [00:01<00:00, 1703.80it/s]warmup should be done:  76%|███████▋  | 2295/3000 [00:01<00:00, 1670.44it/s]warmup should be done:  81%|████████  | 2431/3000 [00:01<00:00, 1720.30it/s]warmup should be done:  81%|████████  | 2420/3000 [00:01<00:00, 1677.57it/s]warmup should be done:  80%|████████  | 2401/3000 [00:01<00:00, 1678.26it/s]warmup should be done:  87%|████████▋ | 2596/3000 [00:01<00:00, 1726.46it/s]warmup should be done:  86%|████████▋ | 2590/3000 [00:01<00:00, 1723.47it/s]warmup should be done:  86%|████████▌ | 2570/3000 [00:01<00:00, 1706.83it/s]warmup should be done:  82%|████████▏ | 2470/3000 [00:01<00:00, 1694.00it/s]warmup should be done:  87%|████████▋ | 2616/3000 [00:01<00:00, 1722.49it/s]warmup should be done:  87%|████████▋ | 2605/3000 [00:01<00:00, 1724.19it/s]warmup should be done:  86%|████████▋ | 2593/3000 [00:01<00:00, 1692.60it/s]warmup should be done:  86%|████████▌ | 2574/3000 [00:01<00:00, 1692.50it/s]warmup should be done:  92%|█████████▏| 2770/3000 [00:01<00:00, 1727.97it/s]warmup should be done:  92%|█████████▏| 2764/3000 [00:01<00:00, 1727.47it/s]warmup should be done:  91%|█████████▏| 2742/3000 [00:01<00:00, 1708.90it/s]warmup should be done:  88%|████████▊ | 2645/3000 [00:01<00:00, 1710.46it/s]warmup should be done:  93%|█████████▎| 2789/3000 [00:01<00:00, 1718.21it/s]warmup should be done:  93%|█████████▎| 2781/3000 [00:01<00:00, 1732.11it/s]warmup should be done:  92%|█████████▏| 2767/3000 [00:01<00:00, 1704.57it/s]warmup should be done:  92%|█████████▏| 2747/3000 [00:01<00:00, 1703.37it/s]warmup should be done:  98%|█████████▊| 2944/3000 [00:01<00:00, 1729.39it/s]warmup should be done:  98%|█████████▊| 2939/3000 [00:01<00:00, 1732.99it/s]warmup should be done:  97%|█████████▋| 2914/3000 [00:01<00:00, 1709.53it/s]warmup should be done:  94%|█████████▍| 2817/3000 [00:01<00:00, 1704.57it/s]warmup should be done:  99%|█████████▊| 2961/3000 [00:01<00:00, 1716.69it/s]warmup should be done:  99%|█████████▊| 2956/3000 [00:01<00:00, 1735.76it/s]warmup should be done:  98%|█████████▊| 2941/3000 [00:01<00:00, 1712.67it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1729.99it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1727.82it/s]warmup should be done:  97%|█████████▋| 2920/3000 [00:01<00:00, 1709.62it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1723.89it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1721.02it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1712.41it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1704.59it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1678.38it/s]warmup should be done: 100%|█████████▉| 2989/3000 [00:01<00:00, 1706.27it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1650.25it/s]
WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f5d02a5a790>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f5d02a59b80>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f5d0299c160>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f5d0299c0d0>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f5d0299b2e0>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f5d02994310>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f5d0299d1f0>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f5d0299c1f0>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
2022-12-11 19:57:43.156956: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f583af90240 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 19:57:43.157018: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 19:57:43.157054: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f5836b89000 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 19:57:43.157109: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 19:57:43.166868: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 19:57:43.167402: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 19:57:43.188492: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f5836b86aa0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 19:57:43.188547: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 19:57:43.188543: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f5836b89bb0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 19:57:43.188588: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 19:57:43.197701: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 19:57:43.198138: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 19:57:43.648757: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f5832b85db0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 19:57:43.648821: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 19:57:43.658382: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 19:57:43.684939: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f5836b81280 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 19:57:43.684998: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 19:57:43.694051: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 19:57:43.695180: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f58428264b0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 19:57:43.695236: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 19:57:43.695241: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f5836b89e80 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 19:57:43.695280: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 19:57:43.704643: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 19:57:43.704707: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 19:57:45.834475: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 19:57:45.870419: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 19:57:45.890824: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 19:57:45.912860: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 19:57:46.091521: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 19:57:46.153428: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 19:57:46.154154: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 19:57:46.162456: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
[HCTR][19:58:07.596][ERROR][RK0][tid #140017192130304]: replica 5 reaches 1000, calling init pre replica
[HCTR][19:58:07.596][ERROR][RK0][tid #140017192130304]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][19:58:07.605][ERROR][RK0][tid #140017192130304]: coll ps creation done
[HCTR][19:58:07.605][ERROR][RK0][tid #140017192130304]: replica 5 waits for coll ps creation barrier
[HCTR][19:58:07.638][ERROR][RK0][tid #140017729001216]: replica 4 reaches 1000, calling init pre replica
[HCTR][19:58:07.638][ERROR][RK0][tid #140017729001216]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][19:58:07.646][ERROR][RK0][tid #140017729001216]: coll ps creation done
[HCTR][19:58:07.646][ERROR][RK0][tid #140017729001216]: replica 4 waits for coll ps creation barrier
[HCTR][19:58:07.754][ERROR][RK0][tid #140017317955328]: replica 6 reaches 1000, calling init pre replica
[HCTR][19:58:07.754][ERROR][RK0][tid #140017317955328]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][19:58:07.758][ERROR][RK0][main]: replica 7 reaches 1000, calling init pre replica
[HCTR][19:58:07.758][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][19:58:07.763][ERROR][RK0][tid #140017317955328]: coll ps creation done
[HCTR][19:58:07.763][ERROR][RK0][tid #140017317955328]: replica 6 waits for coll ps creation barrier
[HCTR][19:58:07.763][ERROR][RK0][main]: replica 0 reaches 1000, calling init pre replica
[HCTR][19:58:07.763][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][19:58:07.765][ERROR][RK0][main]: coll ps creation done
[HCTR][19:58:07.765][ERROR][RK0][main]: replica 7 waits for coll ps creation barrier
[HCTR][19:58:07.772][ERROR][RK0][main]: coll ps creation done
[HCTR][19:58:07.772][ERROR][RK0][main]: replica 0 waits for coll ps creation barrier
[HCTR][19:58:07.817][ERROR][RK0][main]: replica 3 reaches 1000, calling init pre replica
[HCTR][19:58:07.817][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][19:58:07.822][ERROR][RK0][main]: coll ps creation done
[HCTR][19:58:07.822][ERROR][RK0][main]: replica 3 waits for coll ps creation barrier
[HCTR][19:58:07.862][ERROR][RK0][main]: replica 2 reaches 1000, calling init pre replica
[HCTR][19:58:07.862][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][19:58:07.867][ERROR][RK0][main]: coll ps creation done
[HCTR][19:58:07.867][ERROR][RK0][main]: replica 2 waits for coll ps creation barrier
[HCTR][19:58:07.873][ERROR][RK0][main]: replica 1 reaches 1000, calling init pre replica
[HCTR][19:58:07.873][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][19:58:07.878][ERROR][RK0][main]: coll ps creation done
[HCTR][19:58:07.878][ERROR][RK0][main]: replica 1 waits for coll ps creation barrier
[HCTR][19:58:07.878][ERROR][RK0][main]: replica 0 preparing frequency
[HCTR][19:58:14.923][ERROR][RK0][main]: replica 0 preparing frequency done
[HCTR][19:58:14.966][ERROR][RK0][main]: replica 0 calling init per replica
[HCTR][19:58:14.966][ERROR][RK0][main]: replica 2 calling init per replica
[HCTR][19:58:14.966][ERROR][RK0][main]: replica 1 calling init per replica
[HCTR][19:58:14.966][ERROR][RK0][main]: replica 3 calling init per replica
[HCTR][19:58:14.966][ERROR][RK0][main]: replica 7 calling init per replica
[HCTR][19:58:14.966][ERROR][RK0][tid #140017729001216]: replica 4 calling init per replica
[HCTR][19:58:14.966][ERROR][RK0][main]: Calling build_v2
[HCTR][19:58:14.966][ERROR][RK0][tid #140017192130304]: replica 5 calling init per replica
[HCTR][19:58:14.966][ERROR][RK0][tid #140017317955328]: replica 6 calling init per replica
[HCTR][19:58:14.966][ERROR][RK0][main]: Calling build_v2
[HCTR][19:58:14.966][ERROR][RK0][main]: Calling build_v2
[HCTR][19:58:14.966][ERROR][RK0][main]: Calling build_v2
[HCTR][19:58:14.966][ERROR][RK0][main]: Calling build_v2
[HCTR][19:58:14.966][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][19:58:14.966][ERROR][RK0][tid #140017729001216]: Calling build_v2
[HCTR][19:58:14.966][ERROR][RK0][tid #140017192130304]: Calling build_v2
[HCTR][19:58:14.966][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][19:58:14.966][ERROR][RK0][tid #140017317955328]: Calling build_v2
[HCTR][19:58:14.966][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][19:58:14.966][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][19:58:14.966][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][19:58:14.966][ERROR][RK0][tid #140017729001216]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][19:58:14.966][ERROR][RK0][tid #140017192130304]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][19:58:14.966][ERROR][RK0][tid #140017317955328]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[[[[[[[2022-12-11 19:58:142022-12-11 19:58:142022-12-11 19:58:14.2022-12-11 19:58:14.2022-12-11 19:58:14.2022-12-11 19:58:142022-12-11 19:58:14966631.966627.966626..: 966635: 966626: 966638966626E: E: E: :  E E EE/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc /hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc [/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc  :/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc:/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc:/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc136:136:136::] 136] 136] 1361362022-12-11 19:58:14using concurrent impl MPS] using concurrent impl MPS] using concurrent impl MPS] ] .
using concurrent impl MPS
using concurrent impl MPS
using concurrent impl MPSusing concurrent impl MPS966736



: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc:136] using concurrent impl MPS
[2022-12-11 19:58:14.970634: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:178] v100x8, slow pcie
[2022-12-11 19:58:14.970671: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:[1962022-12-11 19:58:14] .assigning 8 to cpu970681
: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:178] v100x8, slow pcie
[2022-12-11 19:58:14.970724[: 2022-12-11 19:58:14E. 970726/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[: :2022-12-11 19:58:14E196. ] 970753/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccassigning 8 to cpu[: :
2022-12-11 19:58:14E178. ] 970773/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccv100x8, slow pcie: :
E212 ] [/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccbuild asymm link desc with 8X Tesla V100-SXM2-32GB out of 82022-12-11 19:58:14[:[
.2022-12-11 19:58:141782022-12-11 19:58:14970819.] .: 970823v100x8, slow pcie970820E[: 
:  2022-12-11 19:58:14E[E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.[ 2022-12-11 19:58:14 :9708532022-12-11 19:58:14/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc196: .:970863:] E970873212: 178[assigning 8 to cpu : ] E] 2022-12-11 19:58:14
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccEbuild asymm link desc with 8X Tesla V100-SXM2-32GB out of 8 v100x8, slow pcie.: 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
970907213/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:: ] :[178[[Eremote time is 8.684211962022-12-11 19:58:14] 2022-12-11 19:58:142022-12-11 19:58:14 
] [.v100x8, slow pcie../hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccassigning 8 to cpu2022-12-11 19:58:14970967[
970960970970:
.: 2022-12-11 19:58:14: : [178970991E.EE2022-12-11 19:58:14] :  971009  .v100x8, slow pcieE[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc971032
 2022-12-11 19:58:14:E::178: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.196[ 213] E:971066] 2022-12-11 19:58:14/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] v100x8, slow pcie 212: assigning 8 to cpu.:remote time is 8.68421
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] E
971100214
:build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8 : [] 196
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[E2022-12-11 19:58:14cpu time is 97.0588] :2022-12-11 19:58:14 .[
assigning 8 to cpu212[./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc9711732022-12-11 19:58:14
] 2022-12-11 19:58:14971186:: .build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8.: 196E971205
971211E]  : :  [assigning 8 to cpu/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[EE/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-11 19:58:14
:2022-12-11 19:58:14  :.196./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc214971267] 971274::] : assigning 8 to cpu: 213212cpu time is 97.0588E
E] ] 
 [ remote time is 8.68421build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-11 19:58:14/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc

:.:[2129713502132022-12-11 19:58:14] [: ] .[build asymm link desc with 8X Tesla V100-SXM2-32GB out of 82022-12-11 19:58:14Eremote time is 8.684219713812022-12-11 19:58:14
. 
: .971387/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE971389: [[: : E2022-12-11 19:58:142022-12-11 19:58:14212/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE ..] : /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc971430971430build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8214/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:: : 
] :213EEcpu time is 97.0588212]   [
] remote time is 8.68421/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-11 19:58:14build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8
::.
213214971513[] ] : 2022-12-11 19:58:14remote time is 8.68421cpu time is 97.0588E[.

 2022-12-11 19:58:14971548/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[.: :2022-12-11 19:58:14971562E213.:  ] 971583E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccremote time is 8.68421:  :
E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc214 :[] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2132022-12-11 19:58:14cpu time is 97.0588:] .
214remote time is 8.68421971635] 
: cpu time is 97.0588E
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:[2142022-12-11 19:58:14] .cpu time is 97.0588971684
: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:214] cpu time is 97.0588
[2022-12-11 19:59:57.907850: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:118] solver created. now build & solve
[2022-12-11 19:59:58.259734: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 3.29 GB
[2022-12-11 19:59:58.259850: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 3.29 GB
[2022-12-11 19:59:58.260959: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:74] mapping nid to rank...
[2022-12-11 19:59:58.968549: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:91] counting slots...
[2022-12-11 20:00:00.566279: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:105] Final num slot is 2
[2022-12-11 20:00:00.566386: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:109] counting blocks...
[2022-12-11 20:01:33.712607: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:118] Final num block is 1001
[2022-12-11 20:01:33.712703: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:123] counting freq and density...
[2022-12-11 20:01:47. 59310: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:154] averaging freq and density...
[2022-12-11 20:01:47. 59412: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:155] 1001
[2022-12-11 20:01:47. 78715: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:123] solver built. now solve
[2022-12-11 20:01:47. 78791: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:354] constructing optimal solver, device=8, stream=1
1001 blocks, 8 devices
[2022-12-11 20:01:47.348087: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:527] Add Var...
[2022-12-11 20:01:47.380526: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:545] Capacity...
[2022-12-11 20:01:47.381923: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:548] Connect CPU...
[2022-12-11 20:01:47.402742: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:550] Connect Access To Storage...
[2022-12-11 20:01:47.915976: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:554] Time...
[2022-12-11 20:01:52.672590: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:569] Coll Cache init block placement array
[2022-12-11 20:01:52.680038: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:645] Coll Cache init block placement array done
[2022-12-11 20:01:52.680374: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:647] Coll Cache model reset done
[2022-12-11 20:01:52.727583: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:127] solver solved
[2022-12-11 20:01:52.727680: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:195] 0 solved master
[2022-12-11 20:01:52.727712: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 0 solved
[2022-12-11 20:01:52.727741: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 0 initing device 0
[2022-12-11 20:01:52.728237: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-11 20:01:52.728576: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 3.29 GB
[2022-12-11 20:01:52.732721: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 3.29 GB
[2022-12-11 20:01:52.736622: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 3.29 GB
[2022-12-11 20:01:52.861022: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 7 solved
[2022-12-11 20:01:52.861097: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 7 initing device 7
[2022-12-11 20:01:52.861495: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-11 20:01:52.861549: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 3.29 GB
[2022-12-11 20:01:52.861827: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 3 solved
[2022-12-11 20:01:52.861899: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 3 initing device 3
[2022-12-11 20:01:52.862244: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 4 solved
[2022-12-11 20:01:52.862306: [E2022-12-11 20:01:52 ./hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc862312:: 205E]  worker 0 thread 4 initing device 4/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu
:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-11 20:01:52.862406: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 3.29 GB
[2022-12-11 20:01:52.862737: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-11 20:01:52.862781: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 3.29 GB
[2022-12-11 20:01:52.864076: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 5 solved
[2022-12-11 20:01:52.864134: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 5 initing device 5
[2022-12-11 20:01:52.864256: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 1 solved
[2022-12-11 20:01:52.864312: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 1 initing device 1
[2022-12-11 20:01:52.864509: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-11 20:01:52.864558: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 3.29 GB
[2022-12-11 20:01:52.864703: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-11 20:01:52.864745: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 3.29 GB
[2022-12-11 20:01:52.864944: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 6 solved
[2022-12-11 20:01:52.864995: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 6 initing device 6
[2022-12-11 20:01:52.865374: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-11 20:01:52.865414: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 3.29 GB
[2022-12-11 20:01:52.867087: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 2 solved
[2022-12-11 20:01:52.867151: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 2 initing device 2
[2022-12-11 20:01:52.867524: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-11 20:01:52.867573: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 3.29 GB
[2022-12-11 20:01:52.888159: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 3.29 GB
[2022-12-11 20:01:52.888393: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 3.29 GB
[2022-12-11 20:01:52.888498: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 3.29 GB
[2022-12-11 20:01:52.888577: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 3.29 GB
[2022-12-11 20:01:52.888721: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 3.29 GB
[2022-12-11 20:01:52.888773: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 3.29 GB
[2022-12-11 20:01:52.892507: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 3.29 GB
[2022-12-11 20:01:52.914975: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 3.29 GB
[2022-12-11 20:01:52.915104: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 3.29 GB
[2022-12-11 20:01:52.915221: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 3.29 GB
[2022-12-11 20:01:52.915288: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 3.29 GB
[2022-12-11 20:01:52.915424: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 3.29 GB
[2022-12-11 20:01:52.915479: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 3.29 GB
[2022-12-11 20:01:52.915582: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 3.29 GB
[2022-12-11 20:01:53.438796: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1001.00 Bytes
[2022-12-11 20:01:53.439003: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1855] using empty feat=27
[2022-12-11 20:01:53.456573: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1001
[2022-12-11 20:01:53.456729: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 3531098340
[2022-12-11 20:01:53.461003: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 138.07 MB
[2022-12-11 20:01:53.461820: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 20:01:53.469094: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 20:01:53.469473: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 127.97 MB
[2022-12-11 20:01:53.562941: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 20:01:53.567570: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 20:01:53.567650: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 16.04 GB
[[[2022-12-11 20:01:532022-12-11 20:01:532022-12-11 20:01:53...627811627811627810: : : EEE   /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:::198019801980] ] ] eager alloc mem 1001.00 Byteseager alloc mem 1001.00 Byteseager alloc mem 1001.00 Bytes


[2022-12-11 20:01:53[[.2022-12-11 20:01:532022-12-11 20:01:53628059..: 628061628061W: :  WW/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu  :/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu1855::] 18551855using empty feat=27] ] 
using empty feat=27using empty feat=27

[2022-12-11 20:01:53.645939: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1001
[[[[2022-12-11 20:01:532022-12-11 20:01:532022-12-11 20:01:53[2022-12-11 20:01:53...2022-12-11 20:01:53[.646002645993645993.2022-12-11 20:01:53645993: : : 646074.: EEE: 646085E   E:  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu E/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:::/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc :63819801980:/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc1980] ] ] 638:] eager release cuda mem 1001eager alloc mem 1001.00 Byteseager alloc mem 1001.00 Bytes] 638eager alloc mem 1001.00 Bytes


eager release cuda mem 3531098340] 

eager release cuda mem 1001
[2022-12-11 20:01:53.646362: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638[] 2022-12-11 20:01:53eager release cuda mem 3531098340.
646380: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 3531098340
[[[2022-12-11 20:01:532022-12-11 20:01:532022-12-11 20:01:53...646408646408646409: : : WWW   /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:::185518551855] ] ] using empty feat=27using empty feat=27using empty feat=27


[2022-12-11 20:01:53.648658: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1001.00 Bytes
[2022-12-11 20:01:53.648800: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1855] using empty feat=27
[2022-12-11 20:01:53.650816: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 138.07 MB
[2022-12-11 20:01:53.654749: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 138.07 MB
[2022-12-11 20:01:53.658892: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 138.07 MB
[2022-12-11 20:01:53.661706: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 20:01:53.661759: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 20:01:53.661800: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 20:01:53.669234: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 20:01:53.669349: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079[
2022-12-11 20:01:53.669375: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 20:01:53.669562: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 87.56 MB
[2022-12-11 20:01:53.669820: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 117.86 MB
[2022-12-11 20:01:53.669882: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 127.97 MB
[2022-12-11 20:01:53.674966: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1001
[2022-12-11 20:01:53.675051: E [/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc2022-12-11 20:01:53:.638675049] : eager release cuda mem 3531098340E
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1001
[[2022-12-11 20:01:532022-12-11 20:01:53..675141675152: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::638638] ] eager release cuda mem 1001eager release cuda mem 3531098340

[2022-12-11 20:01:53.675244: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 3531098340
[2022-12-11 20:01:53.675304: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1001
[2022-12-11 20:01:53.675383: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 3531098340
[2022-12-11 20:01:53.679411: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 138.07 MB
[2022-12-11 20:01:53.683906: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 138.07 MB
[2022-12-11 20:01:53.687809: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 138.07 MB
[2022-12-11 20:01:53.691957: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 138.07 MB
[2022-12-11 20:01:53.692950: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 20:01:53.693303: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 20:01:53.693373: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 20:01:53.693548: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 20:01:53.700630: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 20:01:53.700914: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 134.70 MB
[2022-12-11 20:01:53.701025: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 20:01:53.701060: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 20:01:53.701204: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 20:01:53.701492: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 121.23 MB
[2022-12-11 20:01:53.701578: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 84.19 MB
[2022-12-11 20:01:53.701747: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 134.70 MB
[2022-12-11 20:01:53.728623: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 20:01:53.734382: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 20:01:53.734431: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 10.99 GB
[2022-12-11 20:01:53.749028: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 20:01:53.753533: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 20:01:53.753591: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 14.77 GB
[2022-12-11 20:01:53.756799: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 20:01:53.759231: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 20:01:53.761301: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 20:01:53.761348: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 16.04 GB
[2022-12-11 20:01:53.764987: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 20:01:53.765037: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 10.57 GB
[2022-12-11 20:01:53.780199: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 20:01:53.784701: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 20:01:53.784749: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 15.20 GB
[2022-12-11 20:01:53.787021: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 20:01:53.788875: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 20:01:53.791516: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 20:01:53.791562: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 16.88 GB
[2022-12-11 20:01:53.793393: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 20:01:53.793440: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 16.88 GB
[[[[[[[[2022-12-11 20:01:562022-12-11 20:01:562022-12-11 20:01:562022-12-11 20:01:562022-12-11 20:01:562022-12-11 20:01:562022-12-11 20:01:562022-12-11 20:01:56........846114846121846121846116846121846115846123846123: : : : : : : : EEEEEEEE        /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::::::::19261926192619261926192619261926] ] ] ] ] ] ] ] Device 3 init p2p of link 2Device 5 init p2p of link 6Device 7 init p2p of link 4Device 2 init p2p of link 1Device 0 init p2p of link 3Device 1 init p2p of link 7Device 4 init p2p of link 5Device 6 init p2p of link 0







[[[[2022-12-11 20:01:56[[[2022-12-11 20:01:56[2022-12-11 20:01:562022-12-11 20:01:56.2022-12-11 20:01:562022-12-11 20:01:562022-12-11 20:01:56.2022-12-11 20:01:56..846687...846687.846687846691: 846687846687846688: 846692: : E: : : E: EE EEE E  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu   /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::1980:::1980:19801980] 198019801980] 1980] ] eager alloc mem 5.26 MB] ] ] eager alloc mem 5.26 MB] eager alloc mem 5.26 MBeager alloc mem 5.26 MB
eager alloc mem 5.26 MBeager alloc mem 5.26 MBeager alloc mem 5.26 MB
eager alloc mem 5.26 MB





[2022-12-11 20:01:56.855945: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 20:01:56[.2022-12-11 20:01:56856032.: 856038E:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc :/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc[638:2022-12-11 20:01:56] [638.eager release cuda mem 55180792022-12-11 20:01:56] 856072
.eager release cuda mem 5518079: 856083
E:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc :/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc638:] 638eager release cuda mem 5518079] 
eager release cuda mem 5518079
[2022-12-11 20:01:56.856219: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079[
2022-12-11 20:01:56.856247: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 20:01:56.856327: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 20:01:56.874815: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 0 init p2p of link 6
[2022-12-11 20:01:56.874968: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 20:01:56.875099: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 5 init p2p of link 4
[2022-12-11 20:01:56.875267: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 20:01:56.875726: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 2
[2022-12-11 20:01:56.875866: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 20:01:56.876527: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 4 init p2p of link 7
[2022-12-11 20:01:56.876684: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 20:01:56.878360: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 7 init p2p of link 1
[2022-12-11 20:01:56.878506: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 20:01:56.878645: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 6 init p2p of link 5
[2022-12-11 20:01:56.878797: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 20:01:56.879247: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 3 init p2p of link 0
[2022-12-11 20:01:56.879404: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 20:01:56.880674: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 2 init p2p of link 3
[2022-12-11 20:01:56.880833: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 20:01:56.883656: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 20:01:56.883755[: 2022-12-11 20:01:56E. 883767/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc: :E638 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cceager release cuda mem 5518079:
638[] 2022-12-11 20:01:56eager release cuda mem 5518079.
883805: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 20:01:56.884926: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 20:01:56.885325: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 20:01:56.885835: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 20:01:56.887258: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 20:01:56.900970: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 4 init p2p of link 2
[2022-12-11 20:01:56.901089: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 20:01:56.902567: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 3
[2022-12-11 20:01:56.902688: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 20:01:56.903787: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 2 init p2p of link 0
[2022-12-11 20:01:56.903911: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 20:01:56.904550: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 0 init p2p of link 1
[2022-12-11 20:01:56.904672: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 20:01:56.905663: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 5 init p2p of link 7
[2022-12-11 20:01:56.905785: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 20:01:56.907120: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 7 init p2p of link 6
[2022-12-11 20:01:56.907248: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 20:01:56.907515: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 6 init p2p of link 4
[2022-12-11 20:01:56.907609: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu[:2022-12-11 20:01:561926.] 907638Device 3 init p2p of link 5: 
E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 20:01:56.907745: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 20:01:56.909995: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 20:01:56.910030: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 20:01:56.910479: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 20:01:56.911282: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 20:01:56.912239: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 20:01:56.913641: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 20:01:56.914286: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 20:01:56.914357: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 20:01:56.924905: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 0
[2022-12-11 20:01:56.925023: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 20:01:56.931643: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 5 init p2p of link 3
[2022-12-11 20:01:56.931764: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 20:01:56.932917: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 3 init p2p of link 1
[2022-12-11 20:01:56.933043: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 20:01:56.936830: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 20:01:56.937871: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 6 init p2p of link 7
[2022-12-11 20:01:56.937903: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 2 init p2p of link 4
[2022-12-11 20:01:56.937986: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 20:01:56.938037: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 20:01:56.939665: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 20:01:56.939834: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 20:01:56.940519: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 7 init p2p of link 5
[2022-12-11 20:01:56.940642: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 20:01:56.943080: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 4 init p2p of link 6
[2022-12-11 20:01:56.943218: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 20:01:56.945777: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 20:01:56.945890: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 20:01:56.947865: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 0 init p2p of link 2
[2022-12-11 20:01:56.947999: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 20:01:56.949066: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 20:01:56.951576: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 20:01:56.954150: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 144775028
[2022-12-11 20:01:56.954620: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 144775028
[2022-12-11 20:01:56.956024: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 20:01:56.956142: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 35310960 / 882774585 nodes ( 4.00 %~4.00 %) | remote 24717672 / 882774585 nodes ( 2.80 %) | cpu 822745953 / 882774585 nodes ( 93.20 %) | 16.88 GB | 4.0914 secs 
[2022-12-11 20:01:56.956187: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 144775028
[2022-12-11 20:01:56.956375: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 31779864 / 882774585 nodes ( 3.60 %~4.00 %) | remote 28248768 / 882774585 nodes ( 3.20 %) | cpu 822745953 / 882774585 nodes ( 93.20 %) | 15.20 GB | 4.09097 secs 
[2022-12-11 20:01:56.957186: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 144775028
[2022-12-11 20:01:56.958978: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 35310960 / 882774585 nodes ( 4.00 %~4.00 %) | remote 24717672 / 882774585 nodes ( 2.80 %) | cpu 822745953 / 882774585 nodes ( 93.20 %) | 16.88 GB | 4.09443 secs 
[2022-12-11 20:01:56.959207: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 30897090 / 882774585 nodes ( 3.50 %~4.00 %) | remote 29131542 / 882774585 nodes ( 3.30 %) | cpu 822745953 / 882774585 nodes ( 93.20 %) | 14.77 GB | 4.09164 secs 
[2022-12-11 20:01:56.962298: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 144775028
[2022-12-11 20:01:56.967161: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 22069350 / 882774585 nodes ( 2.50 %~4.00 %) | remote 37959282 / 882774585 nodes ( 4.30 %) | cpu 822745953 / 882774585 nodes ( 93.20 %) | 10.57 GB | 4.10477 secs 
[2022-12-11 20:01:56.968503: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 144775028
[2022-12-11 20:01:56.970836: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 22952124 / 882774585 nodes ( 2.60 %~4.00 %) | remote 37076508 / 882774585 nodes ( 4.20 %) | cpu 822745953 / 882774585 nodes ( 93.20 %) | 10.99 GB | 4.10931 secs 
[2022-12-11 20:01:56.973834: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 144775028
[2022-12-11 20:01:56.976648: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 33545412 / 882774585 nodes ( 3.80 %~4.00 %) | remote 26483220 / 882774585 nodes ( 3.00 %) | cpu 822745953 / 882774585 nodes ( 93.20 %) | 16.04 GB | 4.11388 secs 
[2022-12-11 20:01:56.983052: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 144775028
[2022-12-11 20:01:56.985663: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 33545412 / 882774585 nodes ( 3.80 %~4.00 %) | remote 26483220 / 882774585 nodes ( 3.00 %) | cpu 822745953 / 882774585 nodes ( 93.20 %) | 16.04 GB | 4.25739 secs 
[2022-12-11 20:01:56.986524: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 27.43 GB
[2022-12-11 20:01:58.472302: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 27.69 GB
[2022-12-11 20:01:58.472978: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 27.69 GB
[2022-12-11 20:01:58.475324: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 27.69 GB
[2022-12-11 20:01:59.918308: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 27.95 GB
[2022-12-11 20:01:59.918500: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 27.95 GB
[2022-12-11 20:01:59.919545: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 27.95 GB
[2022-12-11 20:02:01.647452: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 28.17 GB
[2022-12-11 20:02:01.649213: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 28.17 GB
[2022-12-11 20:02:01.649972: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 28.17 GB
[2022-12-11 20:02:03.174974: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 28.38 GB
[2022-12-11 20:02:03.176104: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 28.38 GB
[2022-12-11 20:02:03.177590: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 28.38 GB
[2022-12-11 20:02:04.750040: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 28.84 GB
[2022-12-11 20:02:04.750988: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 28.84 GB
[2022-12-11 20:02:04.757578: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 28.84 GB
[2022-12-11 20:02:06.276183: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 29.04 GB
[2022-12-11 20:02:06.276588: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 29.04 GB
[HCTR][20:02:07.049][ERROR][RK0][tid #140017729001216]: replica 4 calling init per replica done, doing barrier
[HCTR][20:02:07.049][ERROR][RK0][tid #140017192130304]: replica 5 calling init per replica done, doing barrier
[HCTR][20:02:07.049][ERROR][RK0][main]: replica 3 calling init per replica done, doing barrier
[HCTR][20:02:07.049][ERROR][RK0][main]: replica 0 calling init per replica done, doing barrier
[HCTR][20:02:07.049][ERROR][RK0][tid #140017317955328]: replica 6 calling init per replica done, doing barrier
[HCTR][20:02:07.049][ERROR][RK0][main]: replica 1 calling init per replica done, doing barrier
[HCTR][20:02:07.049][ERROR][RK0][main]: replica 7 calling init per replica done, doing barrier
[HCTR][20:02:07.049][ERROR][RK0][main]: replica 2 calling init per replica done, doing barrier
[HCTR][20:02:07.049][ERROR][RK0][main]: replica 3 calling init per replica done, doing barrier done
[HCTR][20:02:07.049][ERROR][RK0][tid #140017729001216]: replica 4 calling init per replica done, doing barrier done
[HCTR][20:02:07.049][ERROR][RK0][main]: replica 2 calling init per replica done, doing barrier done
[HCTR][20:02:07.049][ERROR][RK0][main]: replica 1 calling init per replica done, doing barrier done
[HCTR][20:02:07.049][ERROR][RK0][main]: replica 7 calling init per replica done, doing barrier done
[HCTR][20:02:07.049][ERROR][RK0][tid #140017317955328]: replica 6 calling init per replica done, doing barrier done
[HCTR][20:02:07.049][ERROR][RK0][main]: replica 0 calling init per replica done, doing barrier done
[HCTR][20:02:07.049][ERROR][RK0][main]: init per replica done
[HCTR][20:02:07.049][ERROR][RK0][tid #140017192130304]: replica 5 calling init per replica done, doing barrier done
[HCTR][20:02:07.049][ERROR][RK0][main]: init per replica done
[HCTR][20:02:07.049][ERROR][RK0][tid #140017729001216]: init per replica done
[HCTR][20:02:07.049][ERROR][RK0][main]: init per replica done
[HCTR][20:02:07.049][ERROR][RK0][main]: init per replica done
[HCTR][20:02:07.049][ERROR][RK0][tid #140017317955328]: init per replica done
[HCTR][20:02:07.049][ERROR][RK0][tid #140017192130304]: init per replica done
[HCTR][20:02:07.070][ERROR][RK0][main]: init per replica done
