2022-12-12 00:20:52.589904: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.597254: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.601337: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.605654: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.609001: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.620188: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.635227: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.640624: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.687722: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.690117: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.692776: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.693967: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.695480: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.696543: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.696643: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.697982: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.698081: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.699290: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.699620: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.700523: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.701080: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.701968: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.702545: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.703466: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.703995: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.704835: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.705379: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.706145: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.706950: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.707491: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.708733: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.709626: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.711392: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.712444: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.713337: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.714247: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.715232: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.716168: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.717108: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.718024: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.723397: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 00:20:52.726664: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.728082: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.730422: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.731350: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.732857: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.733434: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.733710: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.733862: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.736056: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.736874: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.737517: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.737677: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.737953: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.739790: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.740609: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.741827: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.741880: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.742359: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.742577: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.743829: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.745695: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.745837: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.746323: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.746694: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.747753: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.749824: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.750057: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.750550: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.750930: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.752375: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.752482: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.753708: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.753986: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.754313: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.754827: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.756388: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.756662: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.757989: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.758172: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.758587: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.759002: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.760145: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.760723: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.762042: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.762623: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.762902: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.763650: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.764397: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.765926: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.766179: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.766606: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.767470: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.768964: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.769102: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.769969: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.771216: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.771899: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.772315: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.773025: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.773691: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.780809: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.781491: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.782568: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.796133: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.806083: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.809513: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.811829: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.814373: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.819606: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.820626: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.820819: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.820911: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.820959: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.820988: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.823612: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.824132: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.825512: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.825807: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.825887: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.825934: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.826062: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.829458: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.829646: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.831862: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.832154: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.832200: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.832336: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.832386: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.834855: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.834998: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.836589: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.836973: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.837063: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.837165: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.837315: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.840228: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.841199: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.841810: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.842328: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.842336: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.842468: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.842622: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.845573: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.846777: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.847011: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.847409: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.847708: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.847770: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.847878: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.850262: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.851646: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.851894: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.852429: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.852529: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.852716: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.852812: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.855458: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.856595: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.856770: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.857349: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.857589: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.857857: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.857886: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.860329: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.861339: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.861601: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.862180: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.862630: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.862733: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.863189: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.865364: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.866699: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.866702: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.867364: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.867715: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.867766: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.867995: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.869840: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.871149: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.871393: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.871750: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.872102: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.872233: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.872376: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.874917: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.876489: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.876758: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.876915: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.877410: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.877463: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.877598: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.879784: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.881163: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.881554: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.882086: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.882178: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.882264: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.884379: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.885265: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.885586: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.885796: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 00:20:52.886082: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.886219: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.886276: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.888250: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.889228: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.889403: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.889858: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.890074: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.890158: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.892753: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.895014: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.895523: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.895604: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.895742: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.895822: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.895946: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.896286: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.898731: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.899902: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.900100: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.900389: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.900445: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.901007: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.901118: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.903244: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.904283: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.904498: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.904665: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.904708: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.905393: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.905554: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.907729: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.908769: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.909060: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.909176: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.910036: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.910282: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.912338: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.913682: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.914122: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.914284: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.915033: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.915121: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.917392: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.918570: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.918798: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.919034: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.919723: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.919842: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.922459: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.923321: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.923886: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.924107: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.924498: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.924688: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.926924: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.928394: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.928869: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.929046: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.929407: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.929592: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.933731: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.935772: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 00:20:52.936491: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 00:20:52.936775: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 00:20:52.937017: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 00:20:52.937514: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 00:20:52.938968: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 00:20:52.946600: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.947056: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.947153: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.947413: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.948039: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.949525: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.978737: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.979994: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.979997: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.980211: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:52.981587: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:53.010723: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:53.012722: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:53.013843: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:53.013930: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:53.013938: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:53.015399: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:53.015793: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:54.047634: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:54.048742: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:54.049262: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:54.049941: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 00:20:54.049998: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-12 00:20:54.068260: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:54.068910: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:54.069427: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:54.070001: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:54.071338: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:54.071819: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-12 00:20:54.116662: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 00:20:54.116862: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 00:20:54.175787: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12340
2022-12-12 00:20:54.253681: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:54.254555: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:54.255081: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:54.255561: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 00:20:54.255619: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-12 00:20:54.272583: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:54.273522: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:54.274247: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:54.274831: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:54.275390: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:54.276070: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:1/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-12 00:20:54.319841: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:54.320458: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:54.320973: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:54.321440: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 00:20:54.321495: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-12 00:20:54.330977: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:54.331607: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:54.332285: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:54.332759: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 00:20:54.332814: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-12 00:20:54.337126: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:54.337721: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:54.338025: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:54.338025: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:54.338126: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:54.339832: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:54.340117: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:54.340281: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:54.340410: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:54.341873: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:54.342004: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 00:20:54.342069: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-12 00:20:54.342287: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:54.342510: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:54.343826: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:54.343888: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 00:20:54.343942: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-12 00:20:54.344029: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 00:20:54.344085: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-12 00:20:54.344942: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:54.345434: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:2/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-12 00:20:54.350135: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:54.350740: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:54.351257: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:54.351844: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:54.352370: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:54.352831: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:4/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-12 00:20:54.358041: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 00:20:54.358229: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 00:20:54.360235: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12341
2022-12-12 00:20:54.360796: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:54.361426: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:54.361911: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:54.361979: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:54.362227: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:54.362507: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:54.363539: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:54.363815: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:54.364247: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:54.364670: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:54.365688: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:54.365854: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:54.366220: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:54.366728: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:54.367600: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 00:20:54.367654: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-12 00:20:54.367788: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:7/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-12 00:20:54.368404: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:54.368537: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:54.369482: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:54.369477: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:54.370414: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:6/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-12 00:20:54.370455: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:5/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-12 00:20:54.385942: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:54.386609: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:54.387117: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:54.387715: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:54.388223: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:20:54.388697: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:3/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-12 00:20:54.396689: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 00:20:54.396906: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 00:20:54.398871: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12344
2022-12-12 00:20:54.412384: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 00:20:54.412580: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 00:20:54.414376: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12347
2022-12-12 00:20:54.414736: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 00:20:54.414919: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 00:20:54.416699: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 00:20:54.416836: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 00:20:54.416836: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12346
2022-12-12 00:20:54.418645: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12345
2022-12-12 00:20:54.433556: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 00:20:54.433740: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 00:20:54.435510: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12343
2022-12-12 00:20:54.436039: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 00:20:54.436185: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 00:20:54.438037: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12342
[HCTR][00:20:55.706][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][00:20:55.707][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][00:20:55.707][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][00:20:55.707][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][00:20:55.707][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][00:20:55.707][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][00:20:55.707][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][00:20:55.707][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 1it [00:01,  1.55s/it]warmup run: 1it [00:01,  1.55s/it]warmup run: 97it [00:01, 81.71it/s]warmup run: 1it [00:01,  1.56s/it]warmup run: 103it [00:01, 86.73it/s]warmup run: 194it [00:01, 177.38it/s]warmup run: 204it [00:01, 186.11it/s]warmup run: 92it [00:01, 76.68it/s]warmup run: 1it [00:01,  1.50s/it]warmup run: 292it [00:01, 284.31it/s]warmup run: 1it [00:01,  1.50s/it]warmup run: 1it [00:01,  1.52s/it]warmup run: 1it [00:01,  1.48s/it]warmup run: 183it [00:01, 165.57it/s]warmup run: 306it [00:01, 296.88it/s]warmup run: 97it [00:01, 84.11it/s]warmup run: 390it [00:01, 395.40it/s]warmup run: 101it [00:01, 87.59it/s]warmup run: 101it [00:01, 88.64it/s]warmup run: 98it [00:01, 83.90it/s]warmup run: 275it [00:01, 265.15it/s]warmup run: 406it [00:01, 408.60it/s]warmup run: 192it [00:01, 179.73it/s]warmup run: 487it [00:02, 502.23it/s]warmup run: 1it [00:01,  1.46s/it]warmup run: 202it [00:01, 189.45it/s]warmup run: 203it [00:01, 192.48it/s]warmup run: 196it [00:01, 181.75it/s]warmup run: 368it [00:01, 370.31it/s]warmup run: 507it [00:02, 519.66it/s]warmup run: 288it [00:01, 285.76it/s]warmup run: 587it [00:02, 605.45it/s]warmup run: 97it [00:01, 86.13it/s]warmup run: 301it [00:01, 298.41it/s]warmup run: 295it [00:01, 290.63it/s]warmup run: 305it [00:01, 305.92it/s]warmup run: 452it [00:02, 457.03it/s]warmup run: 610it [00:02, 625.41it/s]warmup run: 372it [00:01, 375.95it/s]warmup run: 685it [00:02, 690.52it/s]warmup run: 194it [00:01, 185.81it/s]warmup run: 401it [00:01, 412.39it/s]warmup run: 388it [00:01, 393.90it/s]warmup run: 406it [00:01, 421.29it/s]warmup run: 713it [00:02, 717.92it/s]warmup run: 455it [00:02, 449.07it/s]warmup run: 536it [00:02, 519.32it/s]warmup run: 784it [00:02, 763.09it/s]warmup run: 293it [00:01, 297.33it/s]warmup run: 501it [00:02, 522.53it/s]warmup run: 487it [00:02, 505.61it/s]warmup run: 504it [00:01, 526.71it/s]warmup run: 814it [00:02, 789.74it/s]warmup run: 552it [00:02, 556.90it/s]warmup run: 630it [00:02, 612.59it/s]warmup run: 882it [00:02, 818.48it/s]warmup run: 392it [00:01, 411.53it/s]warmup run: 600it [00:02, 621.51it/s]warmup run: 588it [00:02, 611.59it/s]warmup run: 604it [00:02, 627.07it/s]warmup run: 916it [00:02, 848.43it/s]warmup run: 651it [00:02, 655.74it/s]warmup run: 724it [00:02, 691.11it/s]warmup run: 980it [00:02, 860.90it/s]warmup run: 490it [00:01, 520.10it/s]warmup run: 700it [00:02, 708.47it/s]warmup run: 689it [00:02, 703.44it/s]warmup run: 703it [00:02, 710.89it/s]warmup run: 1017it [00:02, 886.14it/s]warmup run: 749it [00:02, 734.80it/s]warmup run: 815it [00:02, 746.51it/s]warmup run: 1077it [00:02, 885.10it/s]warmup run: 591it [00:02, 623.95it/s]warmup run: 800it [00:02, 779.92it/s]warmup run: 789it [00:02, 775.84it/s]warmup run: 802it [00:02, 779.12it/s]warmup run: 1119it [00:02, 921.58it/s]warmup run: 847it [00:02, 797.70it/s]warmup run: 905it [00:02, 787.13it/s]warmup run: 1174it [00:02, 907.63it/s]warmup run: 691it [00:02, 711.88it/s]warmup run: 899it [00:02, 833.69it/s]warmup run: 889it [00:02, 834.35it/s]warmup run: 900it [00:02, 830.84it/s]warmup run: 1222it [00:02, 950.20it/s]warmup run: 946it [00:02, 849.63it/s]warmup run: 996it [00:02, 819.35it/s]warmup run: 1272it [00:02, 928.36it/s]warmup run: 791it [00:02, 783.48it/s]warmup run: 997it [00:02, 872.76it/s]warmup run: 989it [00:02, 879.29it/s]warmup run: 998it [00:02, 866.83it/s]warmup run: 1324it [00:02, 968.16it/s]warmup run: 1047it [00:02, 893.94it/s]warmup run: 1088it [00:02, 846.71it/s]warmup run: 1369it [00:02, 932.86it/s]warmup run: 890it [00:02, 836.40it/s]warmup run: 1097it [00:02, 908.26it/s]warmup run: 1089it [00:02, 912.70it/s]warmup run: 1095it [00:02, 891.89it/s]warmup run: 1427it [00:02, 986.08it/s]warmup run: 1151it [00:02, 933.39it/s]warmup run: 1181it [00:02, 868.41it/s]warmup run: 1466it [00:03, 935.54it/s]warmup run: 990it [00:02, 880.99it/s]warmup run: 1197it [00:02, 933.76it/s]warmup run: 1189it [00:02, 932.27it/s]warmup run: 1193it [00:02, 914.85it/s]warmup run: 1531it [00:03, 999.53it/s]warmup run: 1254it [00:02, 959.58it/s]warmup run: 1273it [00:02, 880.63it/s]warmup run: 1562it [00:03, 938.14it/s]warmup run: 1090it [00:02, 913.03it/s]warmup run: 1297it [00:02, 952.39it/s]warmup run: 1288it [00:02, 943.56it/s]warmup run: 1290it [00:02, 928.47it/s]warmup run: 1635it [00:03, 1008.87it/s]warmup run: 1356it [00:02, 976.63it/s]warmup run: 1365it [00:03, 891.98it/s]warmup run: 1663it [00:03, 959.21it/s]warmup run: 1189it [00:02, 933.00it/s]warmup run: 1399it [00:02, 969.78it/s]warmup run: 1389it [00:02, 946.26it/s]warmup run: 1387it [00:02, 939.59it/s]warmup run: 1739it [00:03, 1016.48it/s]warmup run: 1457it [00:03, 985.47it/s]warmup run: 1458it [00:03, 902.20it/s]warmup run: 1760it [00:03, 955.27it/s]warmup run: 1290it [00:02, 954.84it/s]warmup run: 1500it [00:03, 980.56it/s]warmup run: 1488it [00:02, 957.94it/s]warmup run: 1486it [00:03, 953.48it/s]warmup run: 1842it [00:03, 1019.35it/s]warmup run: 1554it [00:03, 917.57it/s]warmup run: 1558it [00:03, 981.95it/s]warmup run: 1392it [00:02, 972.21it/s]warmup run: 1601it [00:03, 985.96it/s]warmup run: 1857it [00:03, 931.94it/s]warmup run: 1586it [00:03, 952.96it/s]warmup run: 1584it [00:03, 952.95it/s]warmup run: 1945it [00:03, 1007.55it/s]warmup run: 1650it [00:03, 927.51it/s]warmup run: 1658it [00:03, 977.68it/s]warmup run: 1495it [00:02, 986.63it/s]warmup run: 1702it [00:03, 990.37it/s]warmup run: 1959it [00:03, 956.80it/s]warmup run: 1687it [00:03, 966.93it/s]warmup run: 1681it [00:03, 949.03it/s]warmup run: 2047it [00:03, 1002.11it/s]warmup run: 1744it [00:03, 922.17it/s]warmup run: 1757it [00:03, 974.19it/s]warmup run: 1598it [00:03, 997.22it/s]warmup run: 1803it [00:03, 995.39it/s]warmup run: 2074it [00:03, 1011.84it/s]warmup run: 1792it [00:03, 989.85it/s]warmup run: 1784it [00:03, 971.83it/s]warmup run: 2166it [00:03, 1055.08it/s]warmup run: 1856it [00:03, 971.55it/s]warmup run: 1837it [00:03, 910.14it/s]warmup run: 1701it [00:03, 1004.03it/s]warmup run: 1905it [00:03, 1001.74it/s]warmup run: 2197it [00:03, 1075.33it/s]warmup run: 1896it [00:03, 1003.43it/s]warmup run: 1884it [00:03, 977.26it/s]warmup run: 2287it [00:03, 1099.10it/s]warmup run: 1954it [00:03, 973.14it/s]warmup run: 1804it [00:03, 1010.59it/s]warmup run: 2006it [00:03, 1003.41it/s]warmup run: 2320it [00:03, 1120.55it/s]warmup run: 1929it [00:03, 836.10it/s]warmup run: 1997it [00:03, 1004.91it/s]warmup run: 1987it [00:03, 991.17it/s]warmup run: 2408it [00:03, 1129.97it/s]warmup run: 2064it [00:03, 1009.20it/s]warmup run: 1907it [00:03, 1014.96it/s]warmup run: 2129it [00:03, 1069.85it/s]warmup run: 2443it [00:03, 1152.22it/s]warmup run: 2030it [00:03, 882.61it/s]warmup run: 2119it [00:03, 1068.56it/s]warmup run: 2104it [00:03, 1043.98it/s]warmup run: 2531it [00:03, 1157.53it/s]warmup run: 2187it [00:03, 1074.06it/s]warmup run: 2010it [00:03, 1018.32it/s]warmup run: 2252it [00:03, 1117.61it/s]warmup run: 2567it [00:04, 1175.88it/s]warmup run: 2152it [00:03, 976.36it/s]warmup run: 2243it [00:03, 1118.66it/s]warmup run: 2225it [00:03, 1091.04it/s]warmup run: 2655it [00:04, 1181.01it/s]warmup run: 2310it [00:03, 1120.32it/s]warmup run: 2131it [00:03, 1073.51it/s]warmup run: 2376it [00:03, 1151.17it/s]warmup run: 2691it [00:04, 1192.63it/s]warmup run: 2274it [00:04, 1044.13it/s]warmup run: 2367it [00:03, 1153.56it/s]warmup run: 2345it [00:03, 1116.16it/s]warmup run: 2777it [00:04, 1192.48it/s]warmup run: 2433it [00:03, 1151.84it/s]warmup run: 2253it [00:03, 1114.55it/s]warmup run: 2499it [00:03, 1174.26it/s]warmup run: 2814it [00:04, 1201.59it/s]warmup run: 2396it [00:04, 1094.52it/s]warmup run: 2491it [00:03, 1176.74it/s]warmup run: 2465it [00:03, 1139.59it/s]warmup run: 2901it [00:04, 1205.18it/s]warmup run: 2556it [00:04, 1174.29it/s]warmup run: 2374it [00:03, 1142.89it/s]warmup run: 2622it [00:04, 1188.77it/s]warmup run: 2938it [00:04, 1211.00it/s]warmup run: 2513it [00:04, 1116.43it/s]warmup run: 2613it [00:03, 1189.27it/s]warmup run: 2585it [00:04, 1155.14it/s]warmup run: 3000it [00:04, 675.12it/s] warmup run: 2679it [00:04, 1188.09it/s]warmup run: 3000it [00:04, 684.03it/s] warmup run: 2494it [00:03, 1159.03it/s]warmup run: 2742it [00:04, 1190.86it/s]warmup run: 2631it [00:04, 1132.97it/s]warmup run: 2733it [00:04, 1188.00it/s]warmup run: 2705it [00:04, 1167.00it/s]warmup run: 2801it [00:04, 1194.87it/s]warmup run: 2614it [00:03, 1169.59it/s]warmup run: 2862it [00:04, 1187.78it/s]warmup run: 2752it [00:04, 1154.81it/s]warmup run: 2853it [00:04, 1190.89it/s]warmup run: 2827it [00:04, 1181.23it/s]warmup run: 2924it [00:04, 1203.82it/s]warmup run: 2734it [00:04, 1176.48it/s]warmup run: 2982it [00:04, 1189.55it/s]warmup run: 2871it [00:04, 1163.78it/s]warmup run: 3000it [00:04, 691.90it/s] warmup run: 2974it [00:04, 1195.16it/s]warmup run: 2949it [00:04, 1190.03it/s]warmup run: 3000it [00:04, 681.42it/s] warmup run: 3000it [00:04, 693.04it/s] warmup run: 3000it [00:04, 682.23it/s] warmup run: 2852it [00:04, 1174.68it/s]warmup run: 2993it [00:04, 1179.17it/s]warmup run: 3000it [00:04, 645.65it/s] warmup run: 2970it [00:04, 1173.90it/s]warmup run: 3000it [00:04, 696.59it/s] 


warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]



warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   6%|▌         | 166/3000 [00:00<00:01, 1657.61it/s]warmup should be done:   5%|▌         | 164/3000 [00:00<00:01, 1635.88it/s]warmup should be done:   6%|▌         | 168/3000 [00:00<00:01, 1674.07it/s]warmup should be done:   5%|▌         | 158/3000 [00:00<00:01, 1571.27it/s]warmup should be done:   6%|▌         | 168/3000 [00:00<00:01, 1672.97it/s]warmup should be done:   6%|▌         | 170/3000 [00:00<00:01, 1690.50it/s]warmup should be done:   5%|▌         | 160/3000 [00:00<00:01, 1590.51it/s]warmup should be done:   5%|▍         | 146/3000 [00:00<00:01, 1451.43it/s]warmup should be done:  11%|█         | 332/3000 [00:00<00:01, 1658.53it/s]warmup should be done:  11%|█         | 336/3000 [00:00<00:01, 1676.24it/s]warmup should be done:  11%|█         | 319/3000 [00:00<00:01, 1592.61it/s]warmup should be done:  11%|█         | 320/3000 [00:00<00:01, 1595.74it/s]warmup should be done:  11%|█         | 337/3000 [00:00<00:01, 1681.46it/s]warmup should be done:  11%|█         | 330/3000 [00:00<00:01, 1645.68it/s]warmup should be done:  11%|█▏        | 340/3000 [00:00<00:01, 1692.88it/s]warmup should be done:  10%|▉         | 294/3000 [00:00<00:01, 1462.81it/s]warmup should be done:  16%|█▌        | 479/3000 [00:00<00:01, 1593.90it/s]warmup should be done:  16%|█▋        | 495/3000 [00:00<00:01, 1646.07it/s]warmup should be done:  16%|█▌        | 480/3000 [00:00<00:01, 1594.04it/s]warmup should be done:  17%|█▋        | 498/3000 [00:00<00:01, 1650.76it/s]warmup should be done:  17%|█▋        | 506/3000 [00:00<00:01, 1678.71it/s]warmup should be done:  15%|█▍        | 441/3000 [00:00<00:01, 1462.54it/s]warmup should be done:  17%|█▋        | 510/3000 [00:00<00:01, 1685.74it/s]warmup should be done:  17%|█▋        | 504/3000 [00:00<00:01, 1618.63it/s]warmup should be done:  22%|██▏       | 661/3000 [00:00<00:01, 1649.87it/s]warmup should be done:  21%|██▏       | 639/3000 [00:00<00:01, 1592.07it/s]warmup should be done:  21%|██▏       | 640/3000 [00:00<00:01, 1593.56it/s]warmup should be done:  22%|██▏       | 674/3000 [00:00<00:01, 1678.07it/s]warmup should be done:  22%|██▏       | 664/3000 [00:00<00:01, 1648.28it/s]warmup should be done:  20%|█▉        | 588/3000 [00:00<00:01, 1458.95it/s]warmup should be done:  23%|██▎       | 679/3000 [00:00<00:01, 1652.70it/s]warmup should be done:  22%|██▏       | 671/3000 [00:00<00:01, 1637.63it/s]warmup should be done:  28%|██▊       | 826/3000 [00:00<00:01, 1648.99it/s]warmup should be done:  27%|██▋       | 804/3000 [00:00<00:01, 1608.59it/s]warmup should be done:  28%|██▊       | 842/3000 [00:00<00:01, 1676.95it/s]warmup should be done:  27%|██▋       | 801/3000 [00:00<00:01, 1599.61it/s]warmup should be done:  28%|██▊       | 829/3000 [00:00<00:01, 1645.77it/s]warmup should be done:  25%|██▍       | 744/3000 [00:00<00:01, 1494.88it/s]warmup should be done:  28%|██▊       | 838/3000 [00:00<00:01, 1647.36it/s]warmup should be done:  28%|██▊       | 845/3000 [00:00<00:01, 1645.90it/s]warmup should be done:  33%|███▎      | 991/3000 [00:00<00:01, 1648.24it/s]warmup should be done:  32%|███▏      | 967/3000 [00:00<00:01, 1614.01it/s]warmup should be done:  34%|███▎      | 1010/3000 [00:00<00:01, 1675.39it/s]warmup should be done:  32%|███▏      | 963/3000 [00:00<00:01, 1603.45it/s]warmup should be done:  33%|███▎      | 994/3000 [00:00<00:01, 1646.02it/s]warmup should be done:  30%|██▉       | 899/3000 [00:00<00:01, 1513.13it/s]warmup should be done:  33%|███▎      | 1004/3000 [00:00<00:01, 1648.77it/s]warmup should be done:  34%|███▎      | 1010/3000 [00:00<00:01, 1641.73it/s]warmup should be done:  39%|███▊      | 1156/3000 [00:00<00:01, 1647.71it/s]warmup should be done:  38%|███▊      | 1129/3000 [00:00<00:01, 1613.64it/s]warmup should be done:  39%|███▊      | 1159/3000 [00:00<00:01, 1645.00it/s]warmup should be done:  39%|███▉      | 1178/3000 [00:00<00:01, 1670.93it/s]warmup should be done:  37%|███▋      | 1124/3000 [00:00<00:01, 1599.79it/s]warmup should be done:  35%|███▌      | 1059/3000 [00:00<00:01, 1538.97it/s]warmup should be done:  39%|███▉      | 1169/3000 [00:00<00:01, 1648.15it/s]warmup should be done:  39%|███▉      | 1175/3000 [00:00<00:01, 1630.77it/s]warmup should be done:  44%|████▍     | 1321/3000 [00:00<00:01, 1646.59it/s]warmup should be done:  43%|████▎     | 1291/3000 [00:00<00:01, 1615.24it/s]warmup should be done:  43%|████▎     | 1285/3000 [00:00<00:01, 1601.90it/s]warmup should be done:  41%|████      | 1218/3000 [00:00<00:01, 1553.55it/s]warmup should be done:  45%|████▍     | 1346/3000 [00:00<00:00, 1669.21it/s]warmup should be done:  44%|████▍     | 1324/3000 [00:00<00:01, 1641.19it/s]warmup should be done:  44%|████▍     | 1335/3000 [00:00<00:01, 1651.01it/s]warmup should be done:  45%|████▍     | 1339/3000 [00:00<00:01, 1628.96it/s]warmup should be done:  50%|████▉     | 1486/3000 [00:00<00:00, 1647.12it/s]warmup should be done:  48%|████▊     | 1454/3000 [00:00<00:00, 1616.82it/s]warmup should be done:  48%|████▊     | 1447/3000 [00:00<00:00, 1604.99it/s]warmup should be done:  46%|████▌     | 1379/3000 [00:00<00:01, 1568.58it/s]warmup should be done:  50%|█████     | 1513/3000 [00:00<00:00, 1667.56it/s]warmup should be done:  50%|████▉     | 1489/3000 [00:00<00:00, 1637.91it/s]warmup should be done:  50%|█████     | 1501/3000 [00:00<00:00, 1652.44it/s]warmup should be done:  50%|█████     | 1504/3000 [00:00<00:00, 1633.71it/s]warmup should be done:  55%|█████▌    | 1651/3000 [00:01<00:00, 1645.68it/s]warmup should be done:  54%|█████▍    | 1617/3000 [00:01<00:00, 1618.03it/s]warmup should be done:  51%|█████▏    | 1539/3000 [00:01<00:00, 1577.95it/s]warmup should be done:  54%|█████▎    | 1609/3000 [00:01<00:00, 1607.82it/s]warmup should be done:  56%|█████▌    | 1680/3000 [00:01<00:00, 1666.20it/s]warmup should be done:  55%|█████▌    | 1653/3000 [00:01<00:00, 1636.15it/s]warmup should be done:  56%|█████▌    | 1667/3000 [00:01<00:00, 1648.02it/s]warmup should be done:  56%|█████▌    | 1669/3000 [00:01<00:00, 1636.13it/s]warmup should be done:  59%|█████▉    | 1780/3000 [00:01<00:00, 1619.87it/s]warmup should be done:  61%|██████    | 1816/3000 [00:01<00:00, 1638.38it/s]warmup should be done:  57%|█████▋    | 1699/3000 [00:01<00:00, 1583.04it/s]warmup should be done:  59%|█████▉    | 1770/3000 [00:01<00:00, 1607.15it/s]warmup should be done:  62%|██████▏   | 1847/3000 [00:01<00:00, 1667.17it/s]warmup should be done:  61%|██████    | 1817/3000 [00:01<00:00, 1635.26it/s]warmup should be done:  61%|██████    | 1834/3000 [00:01<00:00, 1652.85it/s]warmup should be done:  61%|██████    | 1833/3000 [00:01<00:00, 1634.25it/s]warmup should be done:  65%|██████▍   | 1944/3000 [00:01<00:00, 1625.90it/s]warmup should be done:  66%|██████▌   | 1980/3000 [00:01<00:00, 1638.08it/s]warmup should be done:  67%|██████▋   | 2015/3000 [00:01<00:00, 1668.38it/s]warmup should be done:  62%|██████▏   | 1858/3000 [00:01<00:00, 1580.94it/s]warmup should be done:  66%|██████▌   | 1981/3000 [00:01<00:00, 1636.23it/s]warmup should be done:  64%|██████▍   | 1931/3000 [00:01<00:00, 1595.79it/s]warmup should be done:  67%|██████▋   | 2001/3000 [00:01<00:00, 1656.36it/s]warmup should be done:  67%|██████▋   | 1997/3000 [00:01<00:00, 1633.77it/s]warmup should be done:  71%|███████▏  | 2144/3000 [00:01<00:00, 1638.50it/s]warmup should be done:  70%|███████   | 2110/3000 [00:01<00:00, 1633.54it/s]warmup should be done:  67%|██████▋   | 2020/3000 [00:01<00:00, 1590.70it/s]warmup should be done:  72%|███████▏  | 2145/3000 [00:01<00:00, 1633.44it/s]warmup should be done:  70%|██████▉   | 2092/3000 [00:01<00:00, 1598.90it/s]warmup should be done:  72%|███████▏  | 2168/3000 [00:01<00:00, 1657.52it/s]warmup should be done:  72%|███████▏  | 2161/3000 [00:01<00:00, 1629.09it/s]warmup should be done:  73%|███████▎  | 2182/3000 [00:01<00:00, 1463.30it/s]warmup should be done:  77%|███████▋  | 2309/3000 [00:01<00:00, 1640.60it/s]warmup should be done:  76%|███████▌  | 2276/3000 [00:01<00:00, 1640.18it/s]warmup should be done:  73%|███████▎  | 2182/3000 [00:01<00:00, 1596.67it/s]warmup should be done:  77%|███████▋  | 2309/3000 [00:01<00:00, 1631.81it/s]warmup should be done:  75%|███████▌  | 2253/3000 [00:01<00:00, 1600.10it/s]warmup should be done:  78%|███████▊  | 2334/3000 [00:01<00:00, 1658.13it/s]warmup should be done:  78%|███████▊  | 2326/3000 [00:01<00:00, 1635.14it/s]warmup should be done:  78%|███████▊  | 2333/3000 [00:01<00:00, 1429.73it/s]warmup should be done:  81%|████████▏ | 2441/3000 [00:01<00:00, 1642.94it/s]warmup should be done:  82%|████████▏ | 2474/3000 [00:01<00:00, 1638.86it/s]warmup should be done:  78%|███████▊  | 2342/3000 [00:01<00:00, 1597.06it/s]warmup should be done:  82%|████████▏ | 2473/3000 [00:01<00:00, 1633.72it/s]warmup should be done:  80%|████████  | 2414/3000 [00:01<00:00, 1598.20it/s]warmup should be done:  83%|████████▎ | 2500/3000 [00:01<00:00, 1652.07it/s]warmup should be done:  83%|████████▎ | 2490/3000 [00:01<00:00, 1631.84it/s]warmup should be done:  83%|████████▎ | 2501/3000 [00:01<00:00, 1497.56it/s]warmup should be done:  87%|████████▋ | 2608/3000 [00:01<00:00, 1650.28it/s]warmup should be done:  88%|████████▊ | 2639/3000 [00:01<00:00, 1641.63it/s]warmup should be done:  83%|████████▎ | 2504/3000 [00:01<00:00, 1601.39it/s]warmup should be done:  86%|████████▌ | 2575/3000 [00:01<00:00, 1599.49it/s]warmup should be done:  88%|████████▊ | 2637/3000 [00:01<00:00, 1625.21it/s]warmup should be done:  89%|████████▉ | 2666/3000 [00:01<00:00, 1654.08it/s]warmup should be done:  88%|████████▊ | 2654/3000 [00:01<00:00, 1595.28it/s]warmup should be done:  89%|████████▉ | 2670/3000 [00:01<00:00, 1551.31it/s]warmup should be done:  92%|█████████▎| 2775/3000 [00:01<00:00, 1654.45it/s]warmup should be done:  93%|█████████▎| 2804/3000 [00:01<00:00, 1643.16it/s]warmup should be done:  89%|████████▉ | 2667/3000 [00:01<00:00, 1607.68it/s]warmup should be done:  91%|█████████ | 2736/3000 [00:01<00:00, 1600.25it/s]warmup should be done:  93%|█████████▎| 2801/3000 [00:01<00:00, 1627.82it/s]warmup should be done:  94%|█████████▍| 2833/3000 [00:01<00:00, 1657.97it/s]warmup should be done:  94%|█████████▍| 2816/3000 [00:01<00:00, 1600.99it/s]warmup should be done:  95%|█████████▍| 2840/3000 [00:01<00:00, 1592.90it/s]warmup should be done:  98%|█████████▊| 2944/3000 [00:01<00:00, 1663.53it/s]warmup should be done:  99%|█████████▉| 2970/3000 [00:01<00:00, 1647.98it/s]warmup should be done:  94%|█████████▍| 2832/3000 [00:01<00:00, 1618.47it/s]warmup should be done:  97%|█████████▋| 2898/3000 [00:01<00:00, 1606.01it/s]warmup should be done:  99%|█████████▉| 2968/3000 [00:01<00:00, 1637.57it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1653.75it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1644.19it/s]warmup should be done:  99%|█████████▉| 2980/3000 [00:01<00:00, 1611.33it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1638.70it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1630.99it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1630.64it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1614.01it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1601.05it/s]warmup should be done: 100%|█████████▉| 2998/3000 [00:01<00:00, 1629.26it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1570.66it/s]





warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   6%|▌         | 168/3000 [00:00<00:01, 1678.56it/s]warmup should be done:   6%|▌         | 171/3000 [00:00<00:01, 1706.26it/s]warmup should be done:   6%|▌         | 174/3000 [00:00<00:01, 1733.81it/s]warmup should be done:   6%|▌         | 173/3000 [00:00<00:01, 1721.39it/s]warmup should be done:   6%|▌         | 171/3000 [00:00<00:01, 1701.21it/s]warmup should be done:   6%|▌         | 166/3000 [00:00<00:01, 1653.09it/s]warmup should be done:   6%|▌         | 167/3000 [00:00<00:01, 1661.01it/s]warmup should be done:   6%|▌         | 172/3000 [00:00<00:01, 1709.98it/s]warmup should be done:  11%|█▏        | 342/3000 [00:00<00:01, 1706.44it/s]warmup should be done:  11%|█▏        | 338/3000 [00:00<00:01, 1686.95it/s]warmup should be done:  11%|█▏        | 344/3000 [00:00<00:01, 1714.00it/s]warmup should be done:  12%|█▏        | 348/3000 [00:00<00:01, 1731.56it/s]warmup should be done:  11%|█         | 332/3000 [00:00<00:01, 1652.35it/s]warmup should be done:  11%|█▏        | 344/3000 [00:00<00:01, 1710.28it/s]warmup should be done:  11%|█         | 334/3000 [00:00<00:01, 1658.15it/s]warmup should be done:  12%|█▏        | 346/3000 [00:00<00:01, 1692.04it/s]warmup should be done:  17%|█▋        | 514/3000 [00:00<00:01, 1709.16it/s]warmup should be done:  17%|█▋        | 517/3000 [00:00<00:01, 1721.09it/s]warmup should be done:  17%|█▋        | 509/3000 [00:00<00:01, 1693.68it/s]warmup should be done:  17%|█▋        | 498/3000 [00:00<00:01, 1653.72it/s]warmup should be done:  17%|█▋        | 522/3000 [00:00<00:01, 1731.96it/s]warmup should be done:  17%|█▋        | 516/3000 [00:00<00:01, 1712.42it/s]warmup should be done:  17%|█▋        | 501/3000 [00:00<00:01, 1660.82it/s]warmup should be done:  17%|█▋        | 516/3000 [00:00<00:01, 1687.64it/s]warmup should be done:  23%|██▎       | 690/3000 [00:00<00:01, 1724.41it/s]warmup should be done:  23%|██▎       | 686/3000 [00:00<00:01, 1712.77it/s]warmup should be done:  23%|██▎       | 679/3000 [00:00<00:01, 1693.78it/s]warmup should be done:  23%|██▎       | 696/3000 [00:00<00:01, 1734.62it/s]warmup should be done:  22%|██▏       | 665/3000 [00:00<00:01, 1656.98it/s]warmup should be done:  23%|██▎       | 689/3000 [00:00<00:01, 1716.60it/s]warmup should be done:  22%|██▏       | 669/3000 [00:00<00:01, 1664.70it/s]warmup should be done:  23%|██▎       | 685/3000 [00:00<00:01, 1684.32it/s]warmup should be done:  29%|██▉       | 863/3000 [00:00<00:01, 1723.87it/s]warmup should be done:  29%|██▊       | 858/3000 [00:00<00:01, 1710.35it/s]warmup should be done:  28%|██▊       | 831/3000 [00:00<00:01, 1657.89it/s]warmup should be done:  29%|██▉       | 871/3000 [00:00<00:01, 1736.69it/s]warmup should be done:  29%|██▊       | 862/3000 [00:00<00:01, 1719.38it/s]warmup should be done:  28%|██▊       | 836/3000 [00:00<00:01, 1666.11it/s]warmup should be done:  28%|██▊       | 849/3000 [00:00<00:01, 1686.98it/s]warmup should be done:  28%|██▊       | 854/3000 [00:00<00:01, 1670.64it/s]warmup should be done:  35%|███▍      | 1036/3000 [00:00<00:01, 1724.79it/s]warmup should be done:  35%|███▍      | 1045/3000 [00:00<00:01, 1736.86it/s]warmup should be done:  33%|███▎      | 997/3000 [00:00<00:01, 1655.18it/s]warmup should be done:  34%|███▍      | 1034/3000 [00:00<00:01, 1718.16it/s]warmup should be done:  33%|███▎      | 1003/3000 [00:00<00:01, 1663.96it/s]warmup should be done:  34%|███▍      | 1030/3000 [00:00<00:01, 1706.06it/s]warmup should be done:  34%|███▍      | 1018/3000 [00:00<00:01, 1685.42it/s]warmup should be done:  34%|███▍      | 1022/3000 [00:00<00:01, 1672.86it/s]warmup should be done:  40%|████      | 1209/3000 [00:00<00:01, 1725.57it/s]warmup should be done:  39%|███▉      | 1163/3000 [00:00<00:01, 1656.39it/s]warmup should be done:  39%|███▉      | 1170/3000 [00:00<00:01, 1665.61it/s]warmup should be done:  40%|███▉      | 1187/3000 [00:00<00:01, 1685.26it/s]warmup should be done:  40%|████      | 1206/3000 [00:00<00:01, 1710.94it/s]warmup should be done:  40%|████      | 1201/3000 [00:00<00:01, 1700.23it/s]warmup should be done:  41%|████      | 1219/3000 [00:00<00:01, 1718.43it/s]warmup should be done:  40%|███▉      | 1190/3000 [00:00<00:01, 1672.31it/s]warmup should be done:  46%|████▌     | 1383/3000 [00:00<00:00, 1728.08it/s]warmup should be done:  44%|████▍     | 1329/3000 [00:00<00:01, 1655.48it/s]warmup should be done:  45%|████▍     | 1339/3000 [00:00<00:00, 1670.60it/s]warmup should be done:  45%|████▌     | 1356/3000 [00:00<00:00, 1686.20it/s]warmup should be done:  46%|████▌     | 1372/3000 [00:00<00:00, 1702.05it/s]warmup should be done:  46%|████▌     | 1378/3000 [00:00<00:00, 1708.27it/s]warmup should be done:  46%|████▋     | 1391/3000 [00:00<00:00, 1716.12it/s]warmup should be done:  45%|████▌     | 1359/3000 [00:00<00:00, 1676.50it/s]warmup should be done:  52%|█████▏    | 1556/3000 [00:00<00:00, 1727.41it/s]warmup should be done:  50%|████▉     | 1496/3000 [00:00<00:00, 1657.26it/s]warmup should be done:  50%|█████     | 1507/3000 [00:00<00:00, 1673.16it/s]warmup should be done:  51%|█████     | 1525/3000 [00:00<00:00, 1685.44it/s]warmup should be done:  52%|█████▏    | 1549/3000 [00:00<00:00, 1700.58it/s]warmup should be done:  52%|█████▏    | 1565/3000 [00:00<00:00, 1722.11it/s]warmup should be done:  51%|█████▏    | 1543/3000 [00:00<00:00, 1694.68it/s]warmup should be done:  51%|█████     | 1527/3000 [00:00<00:00, 1676.60it/s]warmup should be done:  58%|█████▊    | 1729/3000 [00:01<00:00, 1726.05it/s]warmup should be done:  55%|█████▌    | 1663/3000 [00:01<00:00, 1660.33it/s]warmup should be done:  56%|█████▌    | 1676/3000 [00:01<00:00, 1675.89it/s]warmup should be done:  56%|█████▋    | 1694/3000 [00:01<00:00, 1685.15it/s]warmup should be done:  57%|█████▋    | 1713/3000 [00:01<00:00, 1696.26it/s]warmup should be done:  58%|█████▊    | 1739/3000 [00:01<00:00, 1725.17it/s]warmup should be done:  57%|█████▋    | 1720/3000 [00:01<00:00, 1696.65it/s]warmup should be done:  57%|█████▋    | 1696/3000 [00:01<00:00, 1677.65it/s]warmup should be done:  63%|██████▎   | 1902/3000 [00:01<00:00, 1726.67it/s]warmup should be done:  61%|██████    | 1830/3000 [00:01<00:00, 1660.93it/s]warmup should be done:  62%|██████▏   | 1846/3000 [00:01<00:00, 1682.83it/s]warmup should be done:  62%|██████▏   | 1863/3000 [00:01<00:00, 1685.25it/s]warmup should be done:  63%|██████▎   | 1884/3000 [00:01<00:00, 1699.11it/s]warmup should be done:  64%|██████▎   | 1912/3000 [00:01<00:00, 1721.82it/s]warmup should be done:  63%|██████▎   | 1890/3000 [00:01<00:00, 1693.59it/s]warmup should be done:  62%|██████▏   | 1864/3000 [00:01<00:00, 1673.24it/s]warmup should be done:  69%|██████▉   | 2075/3000 [00:01<00:00, 1726.39it/s]warmup should be done:  67%|██████▋   | 2017/3000 [00:01<00:00, 1688.43it/s]warmup should be done:  68%|██████▊   | 2032/3000 [00:01<00:00, 1685.60it/s]warmup should be done:  67%|██████▋   | 1997/3000 [00:01<00:00, 1658.72it/s]warmup should be done:  68%|██████▊   | 2055/3000 [00:01<00:00, 1700.49it/s]warmup should be done:  70%|██████▉   | 2085/3000 [00:01<00:00, 1721.26it/s]warmup should be done:  69%|██████▊   | 2060/3000 [00:01<00:00, 1692.59it/s]warmup should be done:  68%|██████▊   | 2032/3000 [00:01<00:00, 1674.10it/s]warmup should be done:  75%|███████▍  | 2248/3000 [00:01<00:00, 1725.56it/s]warmup should be done:  73%|███████▎  | 2187/3000 [00:01<00:00, 1691.65it/s]warmup should be done:  72%|███████▏  | 2163/3000 [00:01<00:00, 1657.94it/s]warmup should be done:  73%|███████▎  | 2201/3000 [00:01<00:00, 1683.26it/s]warmup should be done:  74%|███████▍  | 2226/3000 [00:01<00:00, 1700.30it/s]warmup should be done:  75%|███████▌  | 2258/3000 [00:01<00:00, 1719.08it/s]warmup should be done:  74%|███████▍  | 2230/3000 [00:01<00:00, 1691.29it/s]warmup should be done:  73%|███████▎  | 2200/3000 [00:01<00:00, 1675.11it/s]warmup should be done:  79%|███████▊  | 2358/3000 [00:01<00:00, 1696.32it/s]warmup should be done:  78%|███████▊  | 2330/3000 [00:01<00:00, 1659.83it/s]warmup should be done:  79%|███████▉  | 2370/3000 [00:01<00:00, 1682.52it/s]warmup should be done:  81%|████████  | 2421/3000 [00:01<00:00, 1712.20it/s]warmup should be done:  80%|███████▉  | 2397/3000 [00:01<00:00, 1701.43it/s]warmup should be done:  81%|████████  | 2430/3000 [00:01<00:00, 1716.91it/s]warmup should be done:  80%|████████  | 2400/3000 [00:01<00:00, 1690.41it/s]warmup should be done:  79%|███████▉  | 2368/3000 [00:01<00:00, 1675.89it/s]warmup should be done:  84%|████████▍ | 2530/3000 [00:01<00:00, 1700.45it/s]warmup should be done:  83%|████████▎ | 2497/3000 [00:01<00:00, 1660.79it/s]warmup should be done:  86%|████████▋ | 2594/3000 [00:01<00:00, 1717.19it/s]warmup should be done:  85%|████████▍ | 2539/3000 [00:01<00:00, 1684.00it/s]warmup should be done:  86%|████████▌ | 2569/3000 [00:01<00:00, 1704.39it/s]warmup should be done:  87%|████████▋ | 2602/3000 [00:01<00:00, 1715.64it/s]warmup should be done:  86%|████████▌ | 2570/3000 [00:01<00:00, 1691.86it/s]warmup should be done:  85%|████████▍ | 2537/3000 [00:01<00:00, 1677.32it/s]warmup should be done:  90%|█████████ | 2702/3000 [00:01<00:00, 1703.87it/s]warmup should be done:  90%|█████████ | 2708/3000 [00:01<00:00, 1684.32it/s]warmup should be done:  92%|█████████▏| 2768/3000 [00:01<00:00, 1721.36it/s]warmup should be done:  89%|████████▉ | 2664/3000 [00:01<00:00, 1658.63it/s]warmup should be done:  91%|█████████▏| 2740/3000 [00:01<00:00, 1703.42it/s]warmup should be done:  92%|█████████▎| 2775/3000 [00:01<00:00, 1717.56it/s]warmup should be done:  91%|█████████▏| 2740/3000 [00:01<00:00, 1693.31it/s]warmup should be done:  90%|█████████ | 2706/3000 [00:01<00:00, 1678.52it/s]warmup should be done:  96%|█████████▌| 2873/3000 [00:01<00:00, 1704.00it/s]warmup should be done:  98%|█████████▊| 2942/3000 [00:01<00:00, 1724.93it/s]warmup should be done:  94%|█████████▍| 2830/3000 [00:01<00:00, 1657.11it/s]warmup should be done:  96%|█████████▌| 2877/3000 [00:01<00:00, 1681.79it/s]warmup should be done:  97%|█████████▋| 2911/3000 [00:01<00:00, 1704.09it/s]warmup should be done:  98%|█████████▊| 2948/3000 [00:01<00:00, 1718.81it/s]warmup should be done:  97%|█████████▋| 2910/3000 [00:01<00:00, 1691.91it/s]warmup should be done:  96%|█████████▌| 2874/3000 [00:01<00:00, 1677.10it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1722.78it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1722.20it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1702.95it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1699.20it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1684.86it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1684.51it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1677.71it/s]warmup should be done: 100%|█████████▉| 2997/3000 [00:01<00:00, 1659.74it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1657.74it/s]
WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f2e5ea541f0>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f2e5fda9b20>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f2e5ea540d0>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f2e5fdacd30>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f2e5fdab9d0>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f2e5ea532b0>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f2e5ea44190>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f2e5ea531c0>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
2022-12-12 00:22:26.281688: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f299702db60 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 00:22:26.281752: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 00:22:26.281725: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f29a282ca70 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 00:22:26.281797: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 00:22:26.291026: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 00:22:26.292356: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 00:22:26.796780: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f2992834e80 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 00:22:26.796846: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 00:22:26.804376: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f299a834b60 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 00:22:26.804437: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 00:22:26.806869: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 00:22:26.814676: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 00:22:26.829072: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f297e8be9a0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 00:22:26.829132: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 00:22:26.838298: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 00:22:26.977755: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f299a82ced0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 00:22:26.977827: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 00:22:26.977913: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f29a2834800 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 00:22:26.977965: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 00:22:26.986052: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 00:22:26.987442: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 00:22:26.996076: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f299e8382d0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 00:22:26.996142: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 00:22:27.003876: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 00:22:33.300937: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 00:22:33.505062: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 00:22:33.694037: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 00:22:33.745705: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 00:22:33.752068: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 00:22:33.815817: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 00:22:33.888431: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 00:22:33.893522: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
[HCTR][00:23:31.634][ERROR][RK0][main]: replica 2 reaches 1000, calling init pre replica
[HCTR][00:23:31.634][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][00:23:31.643][ERROR][RK0][main]: coll ps creation done
[HCTR][00:23:31.643][ERROR][RK0][main]: replica 2 waits for coll ps creation barrier
[HCTR][00:23:31.695][ERROR][RK0][main]: replica 5 reaches 1000, calling init pre replica
[HCTR][00:23:31.696][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][00:23:31.698][ERROR][RK0][main]: replica 3 reaches 1000, calling init pre replica
[HCTR][00:23:31.699][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][00:23:31.700][ERROR][RK0][main]: coll ps creation done
[HCTR][00:23:31.700][ERROR][RK0][main]: replica 5 waits for coll ps creation barrier
[HCTR][00:23:31.707][ERROR][RK0][main]: coll ps creation done
[HCTR][00:23:31.707][ERROR][RK0][main]: replica 3 waits for coll ps creation barrier
[HCTR][00:23:31.724][ERROR][RK0][main]: replica 1 reaches 1000, calling init pre replica
[HCTR][00:23:31.724][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][00:23:31.732][ERROR][RK0][main]: coll ps creation done
[HCTR][00:23:31.732][ERROR][RK0][main]: replica 1 waits for coll ps creation barrier
[HCTR][00:23:31.780][ERROR][RK0][tid #139817551652608]: replica 7 reaches 1000, calling init pre replica
[HCTR][00:23:31.780][ERROR][RK0][tid #139817551652608]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][00:23:31.785][ERROR][RK0][tid #139817551652608]: coll ps creation done
[HCTR][00:23:31.785][ERROR][RK0][tid #139817551652608]: replica 7 waits for coll ps creation barrier
[HCTR][00:23:31.786][ERROR][RK0][main]: replica 6 reaches 1000, calling init pre replica
[HCTR][00:23:31.786][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][00:23:31.793][ERROR][RK0][main]: coll ps creation done
[HCTR][00:23:31.793][ERROR][RK0][main]: replica 6 waits for coll ps creation barrier
[HCTR][00:23:31.834][ERROR][RK0][main]: replica 4 reaches 1000, calling init pre replica
[HCTR][00:23:31.834][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][00:23:31.841][ERROR][RK0][main]: coll ps creation done
[HCTR][00:23:31.841][ERROR][RK0][main]: replica 4 waits for coll ps creation barrier
[HCTR][00:23:31.846][ERROR][RK0][tid #139818281457408]: replica 0 reaches 1000, calling init pre replica
[HCTR][00:23:31.846][ERROR][RK0][tid #139818281457408]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][00:23:31.853][ERROR][RK0][tid #139818281457408]: coll ps creation done
[HCTR][00:23:31.853][ERROR][RK0][tid #139818281457408]: replica 0 waits for coll ps creation barrier
[HCTR][00:23:31.853][ERROR][RK0][tid #139818281457408]: replica 0 preparing frequency
[HCTR][00:23:32.693][ERROR][RK0][tid #139818281457408]: replica 0 preparing frequency done
[HCTR][00:23:32.727][ERROR][RK0][tid #139818281457408]: replica 0 calling init per replica
[HCTR][00:23:32.727][ERROR][RK0][main]: replica 3 calling init per replica
[HCTR][00:23:32.727][ERROR][RK0][tid #139818281457408]: Calling build_v2
[HCTR][00:23:32.727][ERROR][RK0][main]: replica 1 calling init per replica
[HCTR][00:23:32.727][ERROR][RK0][main]: replica 2 calling init per replica
[HCTR][00:23:32.727][ERROR][RK0][main]: replica 4 calling init per replica
[HCTR][00:23:32.727][ERROR][RK0][main]: replica 5 calling init per replica
[HCTR][00:23:32.727][ERROR][RK0][main]: replica 6 calling init per replica
[HCTR][00:23:32.727][ERROR][RK0][tid #139817551652608]: replica 7 calling init per replica
[HCTR][00:23:32.727][ERROR][RK0][tid #139818281457408]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][00:23:32.727][ERROR][RK0][main]: Calling build_v2
[HCTR][00:23:32.727][ERROR][RK0][main]: Calling build_v2
[HCTR][00:23:32.727][ERROR][RK0][main]: Calling build_v2
[HCTR][00:23:32.727][ERROR][RK0][main]: Calling build_v2
[HCTR][00:23:32.727][ERROR][RK0][main]: Calling build_v2
[HCTR][00:23:32.727][ERROR][RK0][main]: Calling build_v2
[HCTR][00:23:32.727][ERROR][RK0][tid #139817551652608]: Calling build_v2
[HCTR][00:23:32.727][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][00:23:32.727][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][00:23:32.727][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][00:23:32.727][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][00:23:32.727][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][00:23:32.727][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][00:23:32.727][ERROR][RK0][tid #139817551652608]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[2022-12-12 00:23:32[.731373: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 00:23:32:.178731419] : v100x8, slow pcieE
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:178[] [v100x8, slow pcie2022-12-12 00:23:32
.7314822022-12-12 00:23:32: .E[731467 2022-12-12 00:23:32: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.E:731503 196: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] E:assigning 0 to cpu 178
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[] :v100x8, slow pcie196
] assigning 0 to cpu2022-12-12 00:23:32
.[[7315212022-12-12 00:23:32[2022-12-12 00:23:32: ..2022-12-12 00:23:32E731573731575.[ [: : 7315652022-12-12 00:23:32/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccEE: 2022-12-12 00:23:32.:  [E.731619178/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc 731609: 2022-12-12 00:23:32] ::/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: E[.v100x8, slow pcie196212:E 731654
2022-12-12 00:23:32] ] 178 /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: .assigning 0 to cpu[build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:E731697
2022-12-12 00:23:32
v100x8, slow pcie:212 : .
[178] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE7317832022-12-12 00:23:32] [build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8[: : .v100x8, slow pcie2022-12-12 00:23:32
2022-12-12 00:23:32178/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE731859
..] [: [: 731868731873v100x8, slow pcie2022-12-12 00:23:32178/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 00:23:32E: : 
.] :. EE731945v100x8, slow pcie196[731933/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc  : 
] 2022-12-12 00:23:32: :/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccEassigning 0 to cpu.[E213:: 
7320042022-12-12 00:23:32 ] 196212/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: ./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccremote time is 8.68421] ] :E732055:
assigning 0 to cpubuild asymm link desc with 8X Tesla V100-SXM2-32GB out of 8196 : 213[
[
] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE] 2022-12-12 00:23:322022-12-12 00:23:32assigning 0 to cpu: [remote time is 8.68421..
196/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 00:23:32
732158[732170] :.: 2022-12-12 00:23:32[: assigning 0 to cpu196732209E.2022-12-12 00:23:32[E
] :  732235.2022-12-12 00:23:32 assigning 0 to cpuE/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: 732261./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
 :E[: 732284:/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc212 2022-12-12 00:23:32E: 214:] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc. [E] 213build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8:732357/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 00:23:32 cpu time is 97.0588] 
212: :./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
remote time is 8.68421] E214732411[:
build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8 ] : 2022-12-12 00:23:32212
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[cpu time is 97.0588E.] :2022-12-12 00:23:32
 732487[build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8212./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: 2022-12-12 00:23:32
] 732519:E.build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8: 212 [732543
E] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 00:23:32:  build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8:.[E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
2137325902022-12-12 00:23:32 :] : ./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[214remote time is 8.68421E732622:2022-12-12 00:23:32] 
 : 213.cpu time is 97.0588/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE] [732658
: remote time is 8.684212022-12-12 00:23:32: 213/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
.E] :732702 remote time is 8.68421[213: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
2022-12-12 00:23:32] E:.remote time is 8.68421 [213732746
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 00:23:32] : :.remote time is 8.68421[E214732767
2022-12-12 00:23:32 ] : ./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cccpu time is 97.0588[E732787:
2022-12-12 00:23:32 : 214./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE] 732809: cpu time is 97.0588: 214/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
E] : cpu time is 97.0588214/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
] :cpu time is 97.0588214
] cpu time is 97.0588
[2022-12-12 00:24:50.121660: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:118] solver created. now build & solve
[2022-12-12 00:24:50.161891: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:123] solver built. now solve
block 0 storage is 00010001
	access is	0	0	0	0	4	4	4	4	
block 1 storage is 00100010
	access is	1	1	1	1	5	5	5	5	
block 2 storage is 01000100
	access is	2	2	2	2	6	6	6	6	
block 3 storage is 10001000
	access is	3	3	3	3	7	7	7	7	
block 4 storage is 00000000
	access is	8	8	8	8	8	8	8	8	
[2022-12-12 00:24:50.278512: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:127] solver solved
[2022-12-12 00:24:50.278571: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:195] 0 solved master
[2022-12-12 00:24:50.278602: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 0 solved
[2022-12-12 00:24:50.278632: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 0 initing device 0
[2022-12-12 00:24:50.279221: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-12 00:24:50.279264: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 00:24:50.280134: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 00:24:50.280803: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 00:24:50.293901: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 5 solved
[2022-12-12 00:24:50.293962: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 5 initing device 5
[2022-12-12 00:24:50.294391: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-12 00:24:50.294437: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 00:24:50.295291: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 00:24:50.295826: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 3 solved
[[2022-12-12 00:24:502022-12-12 00:24:50..295859295881: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc::202205] ] 1 solvedworker 0 thread 3 initing device 3[

2022-12-12 00:24:50.295964: [E2022-12-12 00:24:50 ./hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu295988:: 1980E]  eager alloc mem 381.47 MB/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc
:205] worker 0 thread 1 initing device 1
[2022-12-12 00:24:50.296386: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-12 00:24:50.296432: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 00:24:50.296470: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-12 00:24:50.296519: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 00:24:50.297872: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 2 solved
[2022-12-12 00:24:50.297931: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 2 initing device 2
[[2022-12-12 00:24:502022-12-12 00:24:50..297992298000: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::2021980] ] eager alloc mem 381.47 MB4 solved

[[2022-12-12 00:24:502022-12-12 00:24:50..298059298070: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc::1980205] ] eager alloc mem 381.47 MBworker 0 thread 4 initing device 4

[2022-12-12 00:24:50.298368: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-12 00:24:50.298414: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 00:24:50.298573: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-12 00:24:50.298628: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[[2022-12-12 00:24:502022-12-12 00:24:50..300119300123: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc::202202] ] 7 solved6 solved

[[2022-12-12 00:24:502022-12-12 00:24:50..300240300243: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc::205205] ] worker 0 thread 7 initing device 7worker 0 thread 6 initing device 6

[2022-12-12 00:24:50.300441: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 00:24:50.300581: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 00:24:50.300645: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[[2022-12-12 00:24:502022-12-12 00:24:50..300717300719: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::18151815] ] Building Coll Cache with ... num gpu device is 8Building Coll Cache with ... num gpu device is 8

[[2022-12-12 00:24:502022-12-12 00:24:50..300814300816: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::19801980] ] eager alloc mem 381.47 MBeager alloc mem 381.47 MB

[2022-12-12 00:24:50.301144: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 00:24:50.304311: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 00:24:50.304397: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 00:24:50.304459: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 00:24:50.304510: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 00:24:50.306849: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 00:24:50.306910: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 00:24:50.360464: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.00 Bytes
[2022-12-12 00:24:50.374385: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5
[2022-12-12 00:24:50.374527: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 00:24:50.376547: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.00 Bytes
[2022-12-12 00:24:50.377725: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 15.64 MB
[2022-12-12 00:24:50.378473: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:24:50.379477: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:24:50.379529: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 15.26 MB
[2022-12-12 00:24:50.381583: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5
[2022-12-12 00:24:50.381680: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 00:24:50.382465: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 15.64 MB
[2022-12-12 00:24:50.382957: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:24:50.383956: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:24:50[.2022-12-12 00:24:50383973.: 384003E:  W/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu :/hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc1980:] 43eager alloc mem 5.00 Bytes] 
WORKER[0] alloc host memory 15.26 MB
[2022-12-12 00:24:50.386966: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 00:24:50.387565: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[[2022-12-12 00:24:502022-12-12 00:24:50..387712387729: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::1980638] ] eager alloc mem 5.00 Byteseager release cuda mem 25855

[2022-12-12 00:24:50.387767: E[ 2022-12-12 00:24:50/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu.:3878021980: ] Eeager alloc mem 5.00 Bytes 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1.91 GB
[2022-12-12 00:24:50.388294: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 00:24:50.388338: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1.91 GB
[2022-12-12 00:24:50.388746: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5
[2022-12-12 00:24:50.388828: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[[[2022-12-12 00:24:502022-12-12 00:24:502022-12-12 00:24:50...390651390651390651: : : EEE   /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:::198019801980] ] ] eager alloc mem 5.00 Byteseager alloc mem 5.00 Byteseager alloc mem 5.00 Bytes


[2022-12-12 00:24:50.393641: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 15.64 MB
[2022-12-12 00:24:50.395460: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5
[2022-12-12 00:24:50.395502: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[[2022-12-12 00:24:502022-12-12 00:24:50..395545395537: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::638638] ] eager release cuda mem 400000000eager release cuda mem 5

[2022-12-12 00:24:50.395633: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 00:24:50.396451: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] [eager alloc mem 15.64 MB2022-12-12 00:24:50
.396474: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:24:50.396521: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 15.26 MB
[2022-12-12 00:24:50.396705: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5
[2022-12-12 00:24:50.396792: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000[
2022-12-12 00:24:50.396799: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5
[2022-12-12 00:24:50.396882: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 00:24:50.396899: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5
[2022-12-12 00:24:50.396954: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu[:2022-12-12 00:24:501980.] 396977eager alloc mem 15.64 MB: 
E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 00:24:50.398218: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 15.64 MB
[2022-12-12 00:24:50.398724: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 15.64 MB
[2022-12-12 00:24:50.398960: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:24:50.399431: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 15.64 MB
[2022-12-12 00:24:50.399794: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:24:50.399936: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:24:50.399981: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 15.26 MB
[2022-12-12 00:24:50.400183: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:24:50.400291: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:24:50.400341: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:24:50.400747: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:24:50.400792: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 15.26 MB
[2022-12-12 00:24:50.401151: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:24:50.401203: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 15.26 MB
[2022-12-12 00:24:50.401272: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[[2022-12-12 00:24:502022-12-12 00:24:50..401312401316: : EW  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc::63843] ] eager release cuda mem 625663WORKER[0] alloc host memory 15.26 MB

[2022-12-12 00:24:50.401380: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 15.26 MB
[2022-12-12 00:24:50.405162: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 00:24:50.405892: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 00:24:50.405934: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1.91 GB
[2022-12-12 00:24:50.407363: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 00:24:50.407963: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 00:24:50.408004: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1.91 GB
[2022-12-12 00:24:50.410759: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 00:24:50.411365: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 00:24:50.411387: E[ 2022-12-12 00:24:50/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu.:4114101980: ] Eeager alloc mem 25.25 KB 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1.91 GB
[2022-12-12 00:24:50.411993: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 00:24:50.412033: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1.91 GB
[2022-12-12 00:24:50.412083: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 00:24:50.412118: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 00:24:50.412692: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 00:24:50.412727[: 2022-12-12 00:24:50E. 412736/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc: :E638 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cueager release cuda mem 25855:
1980] eager alloc mem 1.91 GB
[2022-12-12 00:24:50.412789: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1.91 GB
[[[[[[[[2022-12-12 00:24:512022-12-12 00:24:512022-12-12 00:24:512022-12-12 00:24:512022-12-12 00:24:512022-12-12 00:24:512022-12-12 00:24:512022-12-12 00:24:51........ 94854 94854 94854 94854 94854 94853 94854 94854: : : : : : : : EEEEEEEE        /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::::::::19261926192619261926192619261926] ] ] ] ] ] ] Device 3 init p2p of link 2Device 4 init p2p of link 5] Device 1 init p2p of link 7Device 7 init p2p of link 4Device 2 init p2p of link 1Device 0 init p2p of link 3Device 5 init p2p of link 6

Device 6 init p2p of link 0





[[[[2022-12-12 00:24:512022-12-12 00:24:51[2022-12-12 00:24:51[[[2022-12-12 00:24:51..2022-12-12 00:24:51.2022-12-12 00:24:512022-12-12 00:24:512022-12-12 00:24:51. 95363 95364. 95365... 95366: :  95374:  95375 95374 95374: EE: E: : : E  E EEE /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu   /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:19801980:1980:::1980] ] 1980] 198019801980] eager alloc mem 611.00 KBeager alloc mem 611.00 KB] eager alloc mem 611.00 KB] ] ] eager alloc mem 611.00 KB

eager alloc mem 611.00 KB
eager alloc mem 611.00 KBeager alloc mem 611.00 KBeager alloc mem 611.00 KB




[2022-12-12 00:24:51. 96432[: 2022-12-12 00:24:51E[.[ 2022-12-12 00:24:51 964412022-12-12 00:24:51/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc[[.: .[:2022-12-12 00:24:512022-12-12 00:24:51[ 96451E 964532022-12-12 00:24:51638..2022-12-12 00:24:51:  : .]  96470 96483.E/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccE 96478eager release cuda mem 625663: :  96489 : : 
EE: /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc638/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccE  E:] : /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc 638eager release cuda mem 625663638/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc] 
] :638638:eager release cuda mem 625663eager release cuda mem 625663638] ] 638

] eager release cuda mem 625663eager release cuda mem 625663] eager release cuda mem 625663

eager release cuda mem 625663

[2022-12-12 00:24:51.109239: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 2
[2022-12-12 00:24:51.109412: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] [eager alloc mem 611.00 KB2022-12-12 00:24:51
.109422: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 6 init p2p of link 5
[2022-12-12 00:24:51.109559: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:24:51.110238: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:24:51.110365: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:24:51.111312: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 7 init p2p of link 1
[2022-12-12 00:24:51.111473: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:24:51.111540: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 3 init p2p of link 0
[2022-12-12 00:24:51.111687: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:24:51.111777: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 2 init p2p of link 3
[2022-12-12 00:24:51.111936: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:24:51.111958: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 5 init p2p of link 4
[2022-12-12 00:24:51.112110: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:24:51.112142: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 0 init p2p of link 6
[2022-12-12 00:24:51.112241: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:24:51.112295: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:24:51.112313: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 4 init p2p of link 7
[2022-12-12 00:24:51.112462: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:24:51.112500: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:24:51.112741: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:24:51.112921: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:24:51.113065: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:24:51.113261: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:24:51.122090: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 3
[2022-12-12 00:24:51.122211: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:24:51.122380: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 6 init p2p of link 4
[2022-12-12 00:24:51.122492: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:24:51.123008: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:24:51.123121: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 7 init p2p of link 6
[2022-12-12 00:24:51.123254: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:24:51.123289: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:24:51.123961: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 0 init p2p of link 1
[2022-12-12 00:24:51.124060: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] [eager release cuda mem 6256632022-12-12 00:24:51
.124081: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:24:51.124933: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:24:51.124966: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 3 init p2p of link 5
[2022-12-12 00:24:51.125082: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:24:51.125149: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 2 init p2p of link 0
[2022-12-12 00:24:51.125266: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:24:51.125379: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 5 init p2p of link 7
[2022-12-12 00:24:51.125493: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:24:51.125845[: 2022-12-12 00:24:51E. 125845/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc: :E638 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cueager release cuda mem 625663:
1926] Device 4 init p2p of link 2
[2022-12-12 00:24:51.125974: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:24:51.126066: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:24:51.126295: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:24:51.126737: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:24:51.139206: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 3 init p2p of link 1
[2022-12-12 00:24:51.139320: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:24:51.140147: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:24:51.140172: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 4 init p2p of link 6
[2022-12-12 00:24:51.140285: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:24:51.140814: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 0
[2022-12-12 00:24:51.140929: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:24:51.141103: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:24:51.141323: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 6 init p2p of link 7
[2022-12-12 00:24:51.141433: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:24:51.141739: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:24:51.141785: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 7 init p2p of link 5
[2022-12-12 00:24:51.141899: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:24:51.142195: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 0 init p2p of link 2
[2022-12-12 00:24:51.142238: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:24:51.142313: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:24:51.142710: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:24:51.142822: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 2 init p2p of link 4
[2022-12-12 00:24:51.142939: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[[2022-12-12 00:24:512022-12-12 00:24:51..143108143118: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::1926638] ] Device 5 init p2p of link 3eager release cuda mem 625663

[2022-12-12 00:24:51.143260: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:24:51.143703: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:24:51.144028: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:24:51.152633: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 16399996
[2022-12-12 00:24:51.155556: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 3999999 / 100000000 nodes ( 4.00 %~4.00 %) | remote 11999997 / 100000000 nodes ( 12.00 %) | cpu 84000004 / 100000000 nodes ( 84.00 %) | 1.91 GB | 0.85913 secs 
[2022-12-12 00:24:51.156178: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 16399996
[2022-12-12 00:24:51.156514: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 16399996
[2022-12-12 00:24:51.156808: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 3999999 / 100000000 nodes ( 4.00 %~4.00 %) | remote 11999997 / 100000000 nodes ( 12.00 %) | cpu 84000004 / 100000000 nodes ( 84.00 %) | 1.91 GB | 0.858401 secs 
[2022-12-12 00:24:51.157107: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 16399996
[2022-12-12 00:24:51.157326: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 3999999 / 100000000 nodes ( 4.00 %~4.00 %) | remote 11999997 / 100000000 nodes ( 12.00 %) | cpu 84000004 / 100000000 nodes ( 84.00 %) | 1.91 GB | 0.862899 secs 
[2022-12-12 00:24:51.157575: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 3999999 / 100000000 nodes ( 4.00 %~4.00 %) | remote 11999997 / 100000000 nodes ( 12.00 %) | cpu 84000004 / 100000000 nodes ( 84.00 %) | 1.91 GB | 0.858955 secs 
[2022-12-12 00:24:51.157704: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 16399996
[2022-12-12 00:24:51.158021: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 3999999 / 100000000 nodes ( 4.00 %~4.00 %) | remote 11999997 / 100000000 nodes ( 12.00 %) | cpu 84000004 / 100000000 nodes ( 84.00 %) | 1.91 GB | 0.861514 secs 
[2022-12-12 00:24:51.158316: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 16399996
[2022-12-12 00:24:51.158635: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 3999999 / 100000000 nodes ( 4.00 %~4.00 %) | remote 11999997 / 100000000 nodes ( 12.00 %) | cpu 84000004 / 100000000 nodes ( 84.00 %) | 1.91 GB | 0.857828 secs 
[2022-12-12 00:24:51.158738: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 16399996
[2022-12-12 00:24:51.159172: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 16399996
[2022-12-12 00:24:51.159403: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 3999999 / 100000000 nodes ( 4.00 %~4.00 %) | remote 11999997 / 100000000 nodes ( 12.00 %) | cpu 84000004 / 100000000 nodes ( 84.00 %) | 1.91 GB | 0.858593 secs 
[2022-12-12 00:24:51.159986: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 3999999 / 100000000 nodes ( 4.00 %~4.00 %) | remote 11999997 / 100000000 nodes ( 12.00 %) | cpu 84000004 / 100000000 nodes ( 84.00 %) | 1.91 GB | 0.880728 secs 
[HCTR][00:24:51.160][ERROR][RK0][tid #139817551652608]: replica 7 calling init per replica done, doing barrier
[HCTR][00:24:51.160][ERROR][RK0][tid #139818281457408]: replica 0 calling init per replica done, doing barrier
[HCTR][00:24:51.160][ERROR][RK0][main]: replica 1 calling init per replica done, doing barrier
[HCTR][00:24:51.160][ERROR][RK0][main]: replica 5 calling init per replica done, doing barrier
[HCTR][00:24:51.160][ERROR][RK0][main]: replica 3 calling init per replica done, doing barrier
[HCTR][00:24:51.160][ERROR][RK0][main]: replica 6 calling init per replica done, doing barrier
[HCTR][00:24:51.160][ERROR][RK0][main]: replica 4 calling init per replica done, doing barrier
[HCTR][00:24:51.160][ERROR][RK0][main]: replica 2 calling init per replica done, doing barrier
[HCTR][00:24:51.160][ERROR][RK0][main]: replica 2 calling init per replica done, doing barrier done
[HCTR][00:24:51.160][ERROR][RK0][main]: replica 5 calling init per replica done, doing barrier done
[HCTR][00:24:51.160][ERROR][RK0][main]: replica 1 calling init per replica done, doing barrier done
[HCTR][00:24:51.160][ERROR][RK0][main]: replica 3 calling init per replica done, doing barrier done
[HCTR][00:24:51.160][ERROR][RK0][main]: replica 6 calling init per replica done, doing barrier done
[HCTR][00:24:51.160][ERROR][RK0][tid #139818281457408]: replica 0 calling init per replica done, doing barrier done
[HCTR][00:24:51.160][ERROR][RK0][main]: replica 4 calling init per replica done, doing barrier done
[HCTR][00:24:51.160][ERROR][RK0][tid #139817551652608]: replica 7 calling init per replica done, doing barrier done
[HCTR][00:24:51.160][ERROR][RK0][main]: init per replica done
[HCTR][00:24:51.160][ERROR][RK0][main]: init per replica done
[HCTR][00:24:51.160][ERROR][RK0][main]: init per replica done
[HCTR][00:24:51.160][ERROR][RK0][main]: init per replica done
[HCTR][00:24:51.160][ERROR][RK0][main]: init per replica done
[HCTR][00:24:51.160][ERROR][RK0][main]: init per replica done
[HCTR][00:24:51.160][ERROR][RK0][tid #139817551652608]: init per replica done
[HCTR][00:24:51.162][ERROR][RK0][tid #139818281457408]: init per replica done
