2022-12-11 21:18:46.413041: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.427224: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.439719: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.445077: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.449060: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.455280: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.473962: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.483359: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.532597: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.538807: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.541719: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.542676: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.543740: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.544737: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.545919: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.547490: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.548372: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.548671: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.550226: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.550236: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.552010: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.552265: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.553589: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.554075: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.555100: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.555547: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.556569: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.557128: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.557904: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.558810: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.559285: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.560449: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.561426: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.562431: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.563462: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.564490: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.565521: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.566549: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.567592: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.568530: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.579354: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 21:18:46.582043: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.583152: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.584460: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.585108: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.585260: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.586141: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.586863: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.587626: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.588642: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.589128: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.589194: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.590695: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.591157: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.591178: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.592924: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.593209: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.594919: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.596978: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.598916: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.599439: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.601732: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.601754: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.604016: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.604106: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.605575: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.606591: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.606812: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.608599: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.609855: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.610136: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.611460: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.611529: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.612984: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.613304: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.614662: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.614889: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.616415: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.616734: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.617831: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.618223: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.619522: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.619642: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.619882: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.626558: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.628030: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.629738: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.629956: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.630449: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.631267: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.632508: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.633228: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.633954: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.637282: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.645491: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.669134: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.669858: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.670790: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.670830: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.670906: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.672335: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.673010: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.673397: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.674570: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.674800: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.674843: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.676283: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.676615: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.677736: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.678364: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.679396: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.679587: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.682225: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.682475: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.682706: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.683338: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.684093: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.684552: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.686257: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.686787: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.686895: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.687997: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.688642: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.688948: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.690587: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.691194: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.692517: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.693148: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.693304: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.694799: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.695065: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.695842: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.696613: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.696769: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.698320: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.698492: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.699430: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.700102: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.700183: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.701843: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.702582: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.702841: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.703179: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.703270: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.704933: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.706488: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.706740: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.707332: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.707467: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.709772: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.710756: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.710880: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.711378: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.711495: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.713199: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.714689: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.714727: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.715185: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.715229: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.716787: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.718356: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.718378: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.718553: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.718664: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.720283: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.721860: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.721942: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.721947: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.722081: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.723798: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.724757: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.725329: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.725462: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.725463: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.725637: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.727576: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.729039: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.730241: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.730506: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.731787: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.731881: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.732962: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.733881: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.734661: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.735285: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.735636: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.735875: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.737046: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.738201: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.739192: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.739270: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.739360: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 21:18:46.739685: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.739973: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.741385: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.741945: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.743124: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.743438: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.743568: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.743967: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.745395: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.745722: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.747167: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.747471: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.747612: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.748261: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.750141: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.750660: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.750862: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.752813: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.753253: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.753292: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.753524: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.755046: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.755552: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.755647: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.757376: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.758093: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.758135: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.758199: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.759743: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.760388: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.760510: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.761923: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.762676: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.762861: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.762947: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.765139: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.765283: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.766757: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.768696: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.769009: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.769202: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.771089: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 21:18:46.771156: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.771420: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 21:18:46.773727: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.773958: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.773996: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.775364: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.777979: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.778525: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.780177: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.780512: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.780841: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.780967: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.781368: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 21:18:46.782097: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 21:18:46.783308: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.783610: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.783943: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.784153: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.788163: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.788458: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.789008: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.789007: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.791218: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.791402: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.791870: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.792589: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.795879: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.796008: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.796432: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.797025: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.800177: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.800312: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.800763: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.831363: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.837074: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.838781: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.843191: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.845359: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.878642: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.879775: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.883857: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.886724: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.889815: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.892041: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.897903: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.900368: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 21:18:46.902237: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.910107: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.913786: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 21:18:46.913936: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.917297: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.923688: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.984361: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:46.991231: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:47.917566: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:47.918586: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:47.919114: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:47.919966: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 21:18:47.920026: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-11 21:18:47.938754: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:47.939638: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:47.940149: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:47.940736: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:47.941246: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:47.941724: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-11 21:18:47.988045: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 21:18:47.988265: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 21:18:48.027814: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12340
2022-12-11 21:18:48.191209: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:48.191838: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:48.192372: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:48.192844: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 21:18:48.192897: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-11 21:18:48.215761: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:48.216788: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:48.217510: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:48.218284: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:48.218818: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:48.219306: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:1/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-11 21:18:48.245931: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:48.246559: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:48.247076: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:48.247742: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 21:18:48.247796: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-11 21:18:48.263258: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:48.263884: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:48.264637: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:48.265104: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 21:18:48.265155: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-11 21:18:48.265484: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:48.265720: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:48.266496: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:48.266649: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:48.267404: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:48.267652: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:48.268507: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:48.268751: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 21:18:48.268797: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-11 21:18:48.269363: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:48.269826: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:5/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-11 21:18:48.274613: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:48.275252: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:48.275782: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:48.276253: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 21:18:48.276302: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-11 21:18:48.283422: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:48.284065: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:48.284594: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:48.285160: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:48.285682: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:48.286147: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:4/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-11 21:18:48.286634: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:48.287299: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:48.287830: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:48.288417: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:48.288550: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:48.289322: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:48.289616: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:48.290093: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:2/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-11 21:18:48.290407: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:48.291041: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 21:18:48.291082: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-11 21:18:48.293509: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:48.294114: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:48.294633: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:48.295217: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:48.295738: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:48.296198: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:3/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-11 21:18:48.307928: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:48.308588: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:48.309096: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:48.309704: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:48.310210: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:48.310675: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:6/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-11 21:18:48.313272: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:48.313889: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:48.314420: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:48.314893: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 21:18:48.314940: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-11 21:18:48.315665: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 21:18:48.315828: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 21:18:48.317704: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12345
2022-12-11 21:18:48.325404: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 21:18:48.325595: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 21:18:48.327590: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12341
2022-12-11 21:18:48.332472: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 21:18:48.332639: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 21:18:48.333984: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:48.334482: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12344
2022-12-11 21:18:48.334668: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:48.335206: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:48.335803: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:48.336326: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:18:48.336789: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:7/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-11 21:18:48.342204: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 21:18:48.342367: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 21:18:48.344275: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12343
2022-12-11 21:18:48.355883: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 21:18:48.356075: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 21:18:48.357811: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12346
2022-12-11 21:18:48.380708: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 21:18:48.380917: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 21:18:48.382705: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12347
2022-12-11 21:18:48.402747: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 21:18:48.402946: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 21:18:48.404752: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12342
[HCTR][21:18:49.670][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][21:18:49.670][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][21:18:49.670][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][21:18:49.670][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][21:18:49.670][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][21:18:49.670][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][21:18:49.712][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][21:18:49.713][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 1it [00:01,  1.53s/it]warmup run: 1it [00:01,  1.51s/it]warmup run: 1it [00:01,  1.52s/it]warmup run: 100it [00:01, 84.95it/s]warmup run: 1it [00:01,  1.52s/it]warmup run: 99it [00:01, 85.13it/s]warmup run: 1it [00:01,  1.55s/it]warmup run: 1it [00:01,  1.51s/it]warmup run: 1it [00:01,  1.54s/it]warmup run: 1it [00:01,  1.49s/it]warmup run: 91it [00:01, 77.76it/s]warmup run: 200it [00:01, 184.20it/s]warmup run: 97it [00:01, 82.92it/s]warmup run: 197it [00:01, 183.14it/s]warmup run: 97it [00:01, 81.60it/s]warmup run: 96it [00:01, 82.89it/s]warmup run: 96it [00:01, 81.14it/s]warmup run: 100it [00:01, 87.07it/s]warmup run: 186it [00:01, 172.89it/s]warmup run: 300it [00:01, 293.39it/s]warmup run: 195it [00:01, 180.76it/s]warmup run: 295it [00:01, 290.62it/s]warmup run: 196it [00:01, 179.16it/s]warmup run: 192it [00:01, 179.46it/s]warmup run: 197it [00:01, 181.40it/s]warmup run: 200it [00:01, 188.13it/s]warmup run: 281it [00:01, 277.38it/s]warmup run: 400it [00:01, 406.94it/s]warmup run: 296it [00:01, 292.17it/s]warmup run: 286it [00:01, 282.76it/s]warmup run: 393it [00:01, 401.65it/s]warmup run: 296it [00:01, 288.07it/s]warmup run: 298it [00:01, 292.08it/s]warmup run: 302it [00:01, 301.40it/s]warmup run: 376it [00:01, 385.60it/s]warmup run: 501it [00:02, 518.63it/s]warmup run: 396it [00:01, 406.03it/s]warmup run: 382it [00:01, 392.82it/s]warmup run: 491it [00:02, 510.01it/s]warmup run: 396it [00:01, 401.14it/s]warmup run: 400it [00:01, 408.21it/s]warmup run: 404it [00:01, 418.15it/s]warmup run: 469it [00:02, 487.49it/s]warmup run: 603it [00:02, 623.58it/s]warmup run: 491it [00:02, 507.15it/s]warmup run: 591it [00:02, 613.20it/s]warmup run: 480it [00:02, 502.34it/s]warmup run: 496it [00:02, 511.98it/s]warmup run: 493it [00:02, 504.79it/s]warmup run: 505it [00:01, 529.59it/s]warmup run: 565it [00:02, 586.62it/s]warmup run: 705it [00:02, 714.24it/s]warmup run: 586it [00:02, 599.84it/s]warmup run: 692it [00:02, 703.95it/s]warmup run: 577it [00:02, 600.77it/s]warmup run: 596it [00:02, 613.54it/s]warmup run: 585it [00:02, 587.88it/s]warmup run: 608it [00:02, 635.39it/s]warmup run: 656it [00:02, 648.41it/s]warmup run: 806it [00:02, 786.89it/s]warmup run: 687it [00:02, 693.55it/s]warmup run: 791it [00:02, 774.99it/s]warmup run: 677it [00:02, 693.32it/s]warmup run: 696it [00:02, 702.43it/s]warmup run: 679it [00:02, 667.84it/s]warmup run: 711it [00:02, 726.07it/s]warmup run: 752it [00:02, 723.58it/s]warmup run: 907it [00:02, 844.93it/s]warmup run: 788it [00:02, 771.56it/s]warmup run: 776it [00:02, 767.06it/s]warmup run: 796it [00:02, 775.34it/s]warmup run: 781it [00:02, 753.66it/s]warmup run: 810it [00:02, 782.41it/s]warmup run: 889it [00:02, 779.45it/s]warmup run: 849it [00:02, 787.24it/s]warmup run: 1008it [00:02, 887.98it/s]warmup run: 889it [00:02, 833.85it/s]warmup run: 875it [00:02, 823.21it/s]warmup run: 896it [00:02, 832.02it/s]warmup run: 884it [00:02, 824.46it/s]warmup run: 908it [00:02, 832.11it/s]warmup run: 981it [00:02, 812.64it/s]warmup run: 946it [00:02, 835.54it/s]warmup run: 1110it [00:02, 923.02it/s]warmup run: 991it [00:02, 884.27it/s]warmup run: 974it [00:02, 866.83it/s]warmup run: 996it [00:02, 876.33it/s]warmup run: 986it [00:02, 876.12it/s]warmup run: 1006it [00:02, 870.77it/s]warmup run: 1084it [00:02, 870.02it/s]warmup run: 1046it [00:02, 879.83it/s]warmup run: 1211it [00:02, 940.28it/s]warmup run: 1093it [00:02, 920.09it/s]warmup run: 1096it [00:02, 909.07it/s]warmup run: 1072it [00:02, 891.42it/s]warmup run: 1087it [00:02, 911.61it/s]warmup run: 1104it [00:02, 896.10it/s]warmup run: 1187it [00:02, 914.50it/s]warmup run: 1146it [00:02, 913.02it/s]warmup run: 1311it [00:02, 942.83it/s]warmup run: 1195it [00:02, 946.54it/s]warmup run: 1195it [00:02, 930.19it/s]warmup run: 1172it [00:02, 920.28it/s]warmup run: 1187it [00:02, 933.92it/s]warmup run: 1202it [00:02, 913.96it/s]warmup run: 1289it [00:02, 944.22it/s]warmup run: 1246it [00:02, 936.53it/s]warmup run: 1410it [00:02, 945.69it/s]warmup run: 1296it [00:02, 964.01it/s]warmup run: 1294it [00:02, 946.49it/s]warmup run: 1271it [00:02, 938.82it/s]warmup run: 1287it [00:02, 951.40it/s]warmup run: 1392it [00:02, 968.19it/s]warmup run: 1299it [00:02, 921.01it/s]warmup run: 1347it [00:02, 956.30it/s]warmup run: 1508it [00:03, 951.05it/s]warmup run: 1397it [00:02, 973.60it/s]warmup run: 1394it [00:02, 959.29it/s]warmup run: 1371it [00:02, 954.63it/s]warmup run: 1387it [00:02, 964.34it/s]warmup run: 1395it [00:02, 931.65it/s]warmup run: 1496it [00:03, 986.55it/s]warmup run: 1447it [00:03, 967.55it/s]warmup run: 1606it [00:03, 954.14it/s]warmup run: 1498it [00:03, 980.99it/s]warmup run: 1471it [00:03, 967.06it/s]warmup run: 1493it [00:03, 960.98it/s]warmup run: 1487it [00:03, 973.16it/s]warmup run: 1491it [00:03, 939.87it/s]warmup run: 1599it [00:03, 998.21it/s]warmup run: 1548it [00:03, 977.75it/s]warmup run: 1703it [00:03, 957.17it/s]warmup run: 1600it [00:03, 990.58it/s]warmup run: 1571it [00:03, 975.65it/s]warmup run: 1587it [00:03, 979.92it/s]warmup run: 1592it [00:03, 956.61it/s]warmup run: 1587it [00:03, 945.02it/s]warmup run: 1702it [00:03, 1007.21it/s]warmup run: 1649it [00:03, 985.22it/s]warmup run: 1800it [00:03, 958.95it/s]warmup run: 1702it [00:03, 998.77it/s]warmup run: 1671it [00:03, 981.56it/s]warmup run: 1687it [00:03, 985.80it/s]warmup run: 1690it [00:03, 952.60it/s]warmup run: 1805it [00:03, 1011.96it/s]warmup run: 1683it [00:03, 943.55it/s]warmup run: 1751it [00:03, 993.29it/s]warmup run: 1897it [00:03, 960.45it/s]warmup run: 1803it [00:03, 1000.45it/s]warmup run: 1771it [00:03, 984.19it/s]warmup run: 1787it [00:03, 986.98it/s]warmup run: 1787it [00:03, 947.99it/s]warmup run: 1779it [00:03, 943.09it/s]warmup run: 1907it [00:03, 1000.57it/s]warmup run: 1852it [00:03, 995.78it/s]warmup run: 1995it [00:03, 965.26it/s]warmup run: 1904it [00:03, 999.18it/s] warmup run: 1872it [00:03, 988.93it/s]warmup run: 1887it [00:03, 989.52it/s]warmup run: 1883it [00:03, 923.71it/s]warmup run: 1876it [00:03, 949.76it/s]warmup run: 2008it [00:03, 987.70it/s] warmup run: 1953it [00:03, 990.80it/s]warmup run: 2115it [00:03, 1032.71it/s]warmup run: 2006it [00:03, 1003.50it/s]warmup run: 1972it [00:03, 990.61it/s]warmup run: 1987it [00:03, 986.16it/s]warmup run: 1976it [00:03, 925.10it/s]warmup run: 1973it [00:03, 954.34it/s]warmup run: 2129it [00:03, 1052.76it/s]warmup run: 2064it [00:03, 1025.38it/s]warmup run: 2237it [00:03, 1087.12it/s]warmup run: 2125it [00:03, 1058.95it/s]warmup run: 2085it [00:03, 1031.75it/s]warmup run: 2101it [00:03, 1031.81it/s]warmup run: 2086it [00:03, 974.43it/s]warmup run: 2086it [00:03, 1004.74it/s]warmup run: 2251it [00:03, 1099.68it/s]warmup run: 2186it [00:03, 1081.66it/s]warmup run: 2359it [00:03, 1125.93it/s]warmup run: 2246it [00:03, 1101.42it/s]warmup run: 2204it [00:03, 1078.56it/s]warmup run: 2220it [00:03, 1078.01it/s]warmup run: 2202it [00:03, 1028.76it/s]warmup run: 2205it [00:03, 1057.61it/s]warmup run: 2373it [00:03, 1133.75it/s]warmup run: 2308it [00:03, 1122.19it/s]warmup run: 2481it [00:03, 1152.81it/s]warmup run: 2366it [00:03, 1130.58it/s]warmup run: 2323it [00:03, 1111.74it/s]warmup run: 2340it [00:03, 1113.46it/s]warmup run: 2318it [00:03, 1067.31it/s]warmup run: 2322it [00:03, 1090.10it/s]warmup run: 2495it [00:03, 1156.56it/s]warmup run: 2430it [00:03, 1150.50it/s]warmup run: 2603it [00:04, 1170.20it/s]warmup run: 2487it [00:03, 1153.24it/s]warmup run: 2442it [00:03, 1134.90it/s]warmup run: 2460it [00:03, 1136.97it/s]warmup run: 2435it [00:03, 1094.97it/s]warmup run: 2439it [00:03, 1113.43it/s]warmup run: 2617it [00:04, 1173.95it/s]warmup run: 2552it [00:04, 1165.84it/s]warmup run: 2725it [00:04, 1182.79it/s]warmup run: 2608it [00:04, 1168.23it/s]warmup run: 2561it [00:04, 1151.37it/s]warmup run: 2579it [00:04, 1149.97it/s]warmup run: 2551it [00:04, 1114.06it/s]warmup run: 2556it [00:04, 1130.22it/s]warmup run: 2737it [00:04, 1180.87it/s]warmup run: 2673it [00:04, 1176.61it/s]warmup run: 2847it [00:04, 1191.11it/s]warmup run: 2729it [00:04, 1179.34it/s]warmup run: 2680it [00:04, 1161.25it/s]warmup run: 2699it [00:04, 1163.01it/s]warmup run: 2668it [00:04, 1128.39it/s]warmup run: 2673it [00:04, 1141.66it/s]warmup run: 2857it [00:04, 1185.57it/s]warmup run: 2792it [00:04, 1180.56it/s]warmup run: 2968it [00:04, 1194.19it/s]warmup run: 2849it [00:04, 1184.54it/s]warmup run: 2797it [00:04, 1162.87it/s]warmup run: 2818it [00:04, 1168.65it/s]warmup run: 2783it [00:04, 1132.49it/s]warmup run: 2788it [00:04, 1143.49it/s]warmup run: 3000it [00:04, 682.72it/s] warmup run: 2978it [00:04, 1192.34it/s]warmup run: 3000it [00:04, 684.20it/s] warmup run: 2913it [00:04, 1186.59it/s]warmup run: 2971it [00:04, 1192.39it/s]warmup run: 2915it [00:04, 1166.53it/s]warmup run: 2937it [00:04, 1174.28it/s]warmup run: 2899it [00:04, 1139.85it/s]warmup run: 2906it [00:04, 1152.40it/s]warmup run: 3000it [00:04, 686.59it/s] warmup run: 3000it [00:04, 677.48it/s] warmup run: 3000it [00:04, 678.66it/s] warmup run: 3000it [00:04, 682.04it/s] warmup run: 3000it [00:04, 681.05it/s] warmup run: 3000it [00:04, 669.28it/s] 

warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]




warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   5%|▌         | 162/3000 [00:00<00:01, 1619.42it/s]warmup should be done:   5%|▌         | 160/3000 [00:00<00:01, 1595.70it/s]warmup should be done:   5%|▌         | 163/3000 [00:00<00:01, 1627.56it/s]warmup should be done:   6%|▌         | 165/3000 [00:00<00:01, 1647.61it/s]warmup should be done:   5%|▌         | 162/3000 [00:00<00:01, 1618.60it/s]warmup should be done:   5%|▌         | 164/3000 [00:00<00:01, 1636.23it/s]warmup should be done:   6%|▌         | 166/3000 [00:00<00:01, 1653.97it/s]warmup should be done:   5%|▌         | 163/3000 [00:00<00:01, 1622.54it/s]warmup should be done:  11%|█         | 326/3000 [00:00<00:01, 1627.96it/s]warmup should be done:  11%|█         | 327/3000 [00:00<00:01, 1632.10it/s]warmup should be done:  11%|█         | 329/3000 [00:00<00:01, 1642.50it/s]warmup should be done:  11%|█         | 326/3000 [00:00<00:01, 1626.32it/s]warmup should be done:  11%|█         | 332/3000 [00:00<00:01, 1656.70it/s]warmup should be done:  11%|█         | 321/3000 [00:00<00:01, 1599.48it/s]warmup should be done:  11%|█         | 333/3000 [00:00<00:01, 1659.56it/s]warmup should be done:  11%|█         | 328/3000 [00:00<00:01, 1637.30it/s]warmup should be done:  16%|█▋        | 489/3000 [00:00<00:01, 1628.49it/s]warmup should be done:  17%|█▋        | 499/3000 [00:00<00:01, 1656.77it/s]warmup should be done:  17%|█▋        | 498/3000 [00:00<00:01, 1652.91it/s]warmup should be done:  16%|█▋        | 493/3000 [00:00<00:01, 1639.13it/s]warmup should be done:  16%|█▋        | 494/3000 [00:00<00:01, 1638.84it/s]warmup should be done:  16%|█▌        | 481/3000 [00:00<00:01, 1593.66it/s]warmup should be done:  16%|█▋        | 491/3000 [00:00<00:01, 1627.63it/s]warmup should be done:  16%|█▋        | 489/3000 [00:00<00:01, 1618.56it/s]warmup should be done:  22%|██▏       | 652/3000 [00:00<00:01, 1628.09it/s]warmup should be done:  22%|██▏       | 665/3000 [00:00<00:01, 1657.14it/s]warmup should be done:  22%|██▏       | 657/3000 [00:00<00:01, 1638.46it/s]warmup should be done:  22%|██▏       | 659/3000 [00:00<00:01, 1641.28it/s]warmup should be done:  22%|██▏       | 654/3000 [00:00<00:01, 1626.19it/s]warmup should be done:  21%|██▏       | 641/3000 [00:00<00:01, 1592.01it/s]warmup should be done:  22%|██▏       | 664/3000 [00:00<00:01, 1650.19it/s]warmup should be done:  22%|██▏       | 651/3000 [00:00<00:01, 1608.90it/s]warmup should be done:  27%|██▋       | 815/3000 [00:00<00:01, 1627.02it/s]warmup should be done:  28%|██▊       | 825/3000 [00:00<00:01, 1647.66it/s]warmup should be done:  27%|██▋       | 821/3000 [00:00<00:01, 1636.11it/s]warmup should be done:  28%|██▊       | 831/3000 [00:00<00:01, 1653.08it/s]warmup should be done:  27%|██▋       | 817/3000 [00:00<00:01, 1625.08it/s]warmup should be done:  27%|██▋       | 801/3000 [00:00<00:01, 1591.39it/s]warmup should be done:  28%|██▊       | 830/3000 [00:00<00:01, 1647.63it/s]warmup should be done:  27%|██▋       | 812/3000 [00:00<00:01, 1603.44it/s]warmup should be done:  33%|███▎      | 978/3000 [00:00<00:01, 1624.97it/s]warmup should be done:  33%|███▎      | 990/3000 [00:00<00:01, 1645.28it/s]warmup should be done:  33%|███▎      | 985/3000 [00:00<00:01, 1635.43it/s]warmup should be done:  33%|███▎      | 980/3000 [00:00<00:01, 1620.83it/s]warmup should be done:  33%|███▎      | 995/3000 [00:00<00:01, 1646.85it/s]warmup should be done:  32%|███▏      | 961/3000 [00:00<00:01, 1588.17it/s]warmup should be done:  33%|███▎      | 997/3000 [00:00<00:01, 1645.23it/s]warmup should be done:  32%|███▏      | 973/3000 [00:00<00:01, 1602.75it/s]warmup should be done:  38%|███▊      | 1141/3000 [00:00<00:01, 1619.20it/s]warmup should be done:  38%|███▊      | 1149/3000 [00:00<00:01, 1628.87it/s]warmup should be done:  38%|███▊      | 1155/3000 [00:00<00:01, 1636.72it/s]warmup should be done:  39%|███▊      | 1160/3000 [00:00<00:01, 1641.27it/s]warmup should be done:  37%|███▋      | 1120/3000 [00:00<00:01, 1583.52it/s]warmup should be done:  38%|███▊      | 1143/3000 [00:00<00:01, 1615.39it/s]warmup should be done:  38%|███▊      | 1134/3000 [00:00<00:01, 1602.05it/s]warmup should be done:  39%|███▊      | 1162/3000 [00:00<00:01, 1625.86it/s]warmup should be done:  43%|████▎     | 1303/3000 [00:00<00:01, 1615.74it/s]warmup should be done:  44%|████▎     | 1312/3000 [00:00<00:01, 1628.03it/s]warmup should be done:  43%|████▎     | 1279/3000 [00:00<00:01, 1585.03it/s]warmup should be done:  44%|████▎     | 1305/3000 [00:00<00:01, 1615.10it/s]warmup should be done:  44%|████▍     | 1325/3000 [00:00<00:01, 1639.49it/s]warmup should be done:  44%|████▍     | 1319/3000 [00:00<00:01, 1629.92it/s]warmup should be done:  43%|████▎     | 1295/3000 [00:00<00:01, 1603.50it/s]warmup should be done:  44%|████▍     | 1326/3000 [00:00<00:01, 1627.51it/s]warmup should be done:  49%|████▉     | 1466/3000 [00:00<00:00, 1618.13it/s]warmup should be done:  49%|████▉     | 1475/3000 [00:00<00:00, 1628.11it/s]warmup should be done:  48%|████▊     | 1439/3000 [00:00<00:00, 1588.92it/s]warmup should be done:  49%|████▊     | 1456/3000 [00:00<00:00, 1604.88it/s]warmup should be done:  50%|████▉     | 1489/3000 [00:00<00:00, 1637.67it/s]warmup should be done:  49%|████▉     | 1483/3000 [00:00<00:00, 1630.91it/s]warmup should be done:  49%|████▉     | 1467/3000 [00:00<00:00, 1610.20it/s]warmup should be done:  50%|████▉     | 1490/3000 [00:00<00:00, 1631.00it/s]warmup should be done:  55%|█████▍    | 1638/3000 [00:01<00:00, 1627.64it/s]warmup should be done:  54%|█████▍    | 1629/3000 [00:01<00:00, 1618.84it/s]warmup should be done:  53%|█████▎    | 1600/3000 [00:01<00:00, 1593.65it/s]warmup should be done:  55%|█████▌    | 1653/3000 [00:01<00:00, 1636.82it/s]warmup should be done:  54%|█████▍    | 1617/3000 [00:01<00:00, 1603.57it/s]warmup should be done:  55%|█████▍    | 1647/3000 [00:01<00:00, 1630.82it/s]warmup should be done:  54%|█████▍    | 1629/3000 [00:01<00:00, 1610.57it/s]warmup should be done:  55%|█████▌    | 1655/3000 [00:01<00:00, 1636.53it/s]warmup should be done:  60%|██████    | 1801/3000 [00:01<00:00, 1627.07it/s]warmup should be done:  60%|█████▉    | 1792/3000 [00:01<00:00, 1619.81it/s]warmup should be done:  59%|█████▊    | 1761/3000 [00:01<00:00, 1597.76it/s]warmup should be done:  59%|█████▉    | 1778/3000 [00:01<00:00, 1605.44it/s]warmup should be done:  61%|██████    | 1817/3000 [00:01<00:00, 1635.03it/s]warmup should be done:  60%|██████    | 1811/3000 [00:01<00:00, 1630.92it/s]warmup should be done:  60%|█████▉    | 1791/3000 [00:01<00:00, 1612.40it/s]warmup should be done:  61%|██████    | 1820/3000 [00:01<00:00, 1639.64it/s]warmup should be done:  65%|██████▌   | 1955/3000 [00:01<00:00, 1620.71it/s]warmup should be done:  65%|██████▌   | 1964/3000 [00:01<00:00, 1622.63it/s]warmup should be done:  64%|██████▍   | 1921/3000 [00:01<00:00, 1597.50it/s]warmup should be done:  65%|██████▍   | 1939/3000 [00:01<00:00, 1605.32it/s]warmup should be done:  65%|██████▌   | 1953/3000 [00:01<00:00, 1613.33it/s]warmup should be done:  66%|██████▌   | 1981/3000 [00:01<00:00, 1628.27it/s]warmup should be done:  66%|██████▌   | 1985/3000 [00:01<00:00, 1642.25it/s]warmup should be done:  66%|██████▌   | 1975/3000 [00:01<00:00, 1598.71it/s]warmup should be done:  71%|███████   | 2127/3000 [00:01<00:00, 1623.71it/s]warmup should be done:  71%|███████   | 2118/3000 [00:01<00:00, 1620.32it/s]warmup should be done:  70%|███████   | 2100/3000 [00:01<00:00, 1606.11it/s]warmup should be done:  69%|██████▉   | 2081/3000 [00:01<00:00, 1592.72it/s]warmup should be done:  70%|███████   | 2115/3000 [00:01<00:00, 1613.78it/s]warmup should be done:  72%|███████▏  | 2150/3000 [00:01<00:00, 1643.31it/s]warmup should be done:  71%|███████▏  | 2144/3000 [00:01<00:00, 1619.09it/s]warmup should be done:  71%|███████   | 2136/3000 [00:01<00:00, 1588.12it/s]warmup should be done:  76%|███████▋  | 2291/3000 [00:01<00:00, 1625.73it/s]warmup should be done:  75%|███████▌  | 2261/3000 [00:01<00:00, 1606.82it/s]warmup should be done:  76%|███████▌  | 2281/3000 [00:01<00:00, 1615.50it/s]warmup should be done:  76%|███████▌  | 2277/3000 [00:01<00:00, 1612.78it/s]warmup should be done:  75%|███████▍  | 2241/3000 [00:01<00:00, 1584.75it/s]warmup should be done:  77%|███████▋  | 2315/3000 [00:01<00:00, 1644.08it/s]warmup should be done:  77%|███████▋  | 2306/3000 [00:01<00:00, 1613.28it/s]warmup should be done:  77%|███████▋  | 2298/3000 [00:01<00:00, 1595.89it/s]warmup should be done:  82%|████████▏ | 2454/3000 [00:01<00:00, 1624.22it/s]warmup should be done:  81%|████████  | 2422/3000 [00:01<00:00, 1605.03it/s]warmup should be done:  81%|████████▏ | 2443/3000 [00:01<00:00, 1609.07it/s]warmup should be done:  81%|████████▏ | 2439/3000 [00:01<00:00, 1610.42it/s]warmup should be done:  83%|████████▎ | 2480/3000 [00:01<00:00, 1638.69it/s]warmup should be done:  80%|████████  | 2400/3000 [00:01<00:00, 1576.84it/s]warmup should be done:  82%|████████▏ | 2468/3000 [00:01<00:00, 1606.75it/s]warmup should be done:  82%|████████▏ | 2461/3000 [00:01<00:00, 1603.73it/s]warmup should be done:  87%|████████▋ | 2617/3000 [00:01<00:00, 1624.88it/s]warmup should be done:  86%|████████▌ | 2583/3000 [00:01<00:00, 1601.45it/s]warmup should be done:  87%|████████▋ | 2604/3000 [00:01<00:00, 1608.85it/s]warmup should be done:  87%|████████▋ | 2601/3000 [00:01<00:00, 1609.25it/s]warmup should be done:  88%|████████▊ | 2645/3000 [00:01<00:00, 1640.95it/s]warmup should be done:  85%|████████▌ | 2558/3000 [00:01<00:00, 1572.95it/s]warmup should be done:  88%|████████▊ | 2629/3000 [00:01<00:00, 1605.20it/s]warmup should be done:  87%|████████▋ | 2624/3000 [00:01<00:00, 1611.19it/s]warmup should be done:  93%|█████████▎| 2780/3000 [00:01<00:00, 1626.30it/s]warmup should be done:  91%|█████████▏| 2744/3000 [00:01<00:00, 1600.23it/s]warmup should be done:  92%|█████████▏| 2767/3000 [00:01<00:00, 1612.54it/s]warmup should be done:  92%|█████████▏| 2763/3000 [00:01<00:00, 1610.83it/s]warmup should be done:  94%|█████████▎| 2811/3000 [00:01<00:00, 1644.91it/s]warmup should be done:  91%|█████████ | 2716/3000 [00:01<00:00, 1572.94it/s]warmup should be done:  93%|█████████▎| 2790/3000 [00:01<00:00, 1604.97it/s]warmup should be done:  93%|█████████▎| 2788/3000 [00:01<00:00, 1617.23it/s]warmup should be done:  98%|█████████▊| 2945/3000 [00:01<00:00, 1632.03it/s]warmup should be done:  97%|█████████▋| 2906/3000 [00:01<00:00, 1604.25it/s]warmup should be done:  98%|█████████▊| 2932/3000 [00:01<00:00, 1621.57it/s]warmup should be done:  98%|█████████▊| 2927/3000 [00:01<00:00, 1617.38it/s]warmup should be done:  99%|█████████▉| 2978/3000 [00:01<00:00, 1650.80it/s]warmup should be done:  96%|█████████▌| 2875/3000 [00:01<00:00, 1575.75it/s]warmup should be done:  98%|█████████▊| 2954/3000 [00:01<00:00, 1612.99it/s]warmup should be done:  98%|█████████▊| 2954/3000 [00:01<00:00, 1629.91it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1643.57it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1629.37it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1626.87it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1623.97it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1619.52it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1616.55it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1605.14it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1585.44it/s]


warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]


warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   6%|▌         | 168/3000 [00:00<00:01, 1678.99it/s]warmup should be done:   6%|▌         | 170/3000 [00:00<00:01, 1697.19it/s]warmup should be done:   6%|▌         | 166/3000 [00:00<00:01, 1654.07it/s]warmup should be done:   5%|▌         | 162/3000 [00:00<00:01, 1612.73it/s]warmup should be done:   6%|▌         | 168/3000 [00:00<00:01, 1673.18it/s]warmup should be done:   6%|▌         | 165/3000 [00:00<00:01, 1645.08it/s]warmup should be done:   6%|▌         | 167/3000 [00:00<00:01, 1661.36it/s]warmup should be done:   6%|▌         | 165/3000 [00:00<00:01, 1640.42it/s]warmup should be done:  11%|█         | 332/3000 [00:00<00:01, 1657.26it/s]warmup should be done:  11%|█         | 336/3000 [00:00<00:01, 1675.78it/s]warmup should be done:  11%|█▏        | 340/3000 [00:00<00:01, 1694.66it/s]warmup should be done:  11%|█         | 330/3000 [00:00<00:01, 1645.21it/s]warmup should be done:  11%|█         | 335/3000 [00:00<00:01, 1670.95it/s]warmup should be done:  11%|█         | 324/3000 [00:00<00:01, 1611.20it/s]warmup should be done:  11%|█         | 336/3000 [00:00<00:01, 1668.05it/s]warmup should be done:  11%|█         | 330/3000 [00:00<00:01, 1639.63it/s]warmup should be done:  17%|█▋        | 504/3000 [00:00<00:01, 1675.72it/s]warmup should be done:  17%|█▋        | 496/3000 [00:00<00:01, 1650.50it/s]warmup should be done:  17%|█▋        | 499/3000 [00:00<00:01, 1659.61it/s]warmup should be done:  17%|█▋        | 504/3000 [00:00<00:01, 1677.69it/s]warmup should be done:  17%|█▋        | 510/3000 [00:00<00:01, 1693.63it/s]warmup should be done:  16%|█▌        | 486/3000 [00:00<00:01, 1613.44it/s]warmup should be done:  17%|█▋        | 503/3000 [00:00<00:01, 1667.19it/s]warmup should be done:  16%|█▋        | 495/3000 [00:00<00:01, 1640.48it/s]warmup should be done:  22%|██▏       | 664/3000 [00:00<00:01, 1660.27it/s]warmup should be done:  22%|██▏       | 673/3000 [00:00<00:01, 1681.13it/s]warmup should be done:  22%|██▏       | 673/3000 [00:00<00:01, 1677.65it/s]warmup should be done:  23%|██▎       | 680/3000 [00:00<00:01, 1694.33it/s]warmup should be done:  22%|██▏       | 666/3000 [00:00<00:01, 1660.95it/s]warmup should be done:  22%|██▏       | 670/3000 [00:00<00:01, 1668.10it/s]warmup should be done:  22%|██▏       | 649/3000 [00:00<00:01, 1617.04it/s]warmup should be done:  22%|██▏       | 661/3000 [00:00<00:01, 1644.75it/s]warmup should be done:  28%|██▊       | 832/3000 [00:00<00:01, 1666.37it/s]warmup should be done:  28%|██▊       | 842/3000 [00:00<00:01, 1680.01it/s]warmup should be done:  28%|██▊       | 851/3000 [00:00<00:01, 1696.95it/s]warmup should be done:  27%|██▋       | 811/3000 [00:00<00:01, 1617.40it/s]warmup should be done:  28%|██▊       | 842/3000 [00:00<00:01, 1679.74it/s]warmup should be done:  28%|██▊       | 838/3000 [00:00<00:01, 1670.33it/s]warmup should be done:  28%|██▊       | 833/3000 [00:00<00:01, 1659.14it/s]warmup should be done:  28%|██▊       | 827/3000 [00:00<00:01, 1647.45it/s]warmup should be done:  33%|███▎      | 999/3000 [00:00<00:01, 1664.66it/s]warmup should be done:  34%|███▎      | 1010/3000 [00:00<00:01, 1678.77it/s]warmup should be done:  34%|███▍      | 1022/3000 [00:00<00:01, 1698.84it/s]warmup should be done:  32%|███▏      | 973/3000 [00:00<00:01, 1616.45it/s]warmup should be done:  33%|███▎      | 1000/3000 [00:00<00:01, 1662.07it/s]warmup should be done:  34%|███▎      | 1011/3000 [00:00<00:01, 1679.11it/s]warmup should be done:  34%|███▎      | 1006/3000 [00:00<00:01, 1670.59it/s]warmup should be done:  33%|███▎      | 992/3000 [00:00<00:01, 1645.01it/s]warmup should be done:  39%|███▉      | 1178/3000 [00:00<00:01, 1678.96it/s]warmup should be done:  39%|███▉      | 1166/3000 [00:00<00:01, 1662.84it/s]warmup should be done:  39%|███▉      | 1167/3000 [00:00<00:01, 1663.32it/s]warmup should be done:  38%|███▊      | 1136/3000 [00:00<00:01, 1618.15it/s]warmup should be done:  40%|███▉      | 1192/3000 [00:00<00:01, 1696.11it/s]warmup should be done:  39%|███▉      | 1174/3000 [00:00<00:01, 1669.11it/s]warmup should be done:  39%|███▉      | 1179/3000 [00:00<00:01, 1671.81it/s]warmup should be done:  39%|███▊      | 1157/3000 [00:00<00:01, 1644.04it/s]warmup should be done:  45%|████▍     | 1347/3000 [00:00<00:00, 1680.07it/s]warmup should be done:  44%|████▍     | 1335/3000 [00:00<00:00, 1669.63it/s]warmup should be done:  44%|████▍     | 1334/3000 [00:00<00:01, 1663.52it/s]warmup should be done:  45%|████▌     | 1363/3000 [00:00<00:00, 1699.42it/s]warmup should be done:  43%|████▎     | 1298/3000 [00:00<00:01, 1615.50it/s]warmup should be done:  45%|████▍     | 1342/3000 [00:00<00:00, 1670.90it/s]warmup should be done:  45%|████▍     | 1347/3000 [00:00<00:00, 1669.97it/s]warmup should be done:  44%|████▍     | 1322/3000 [00:00<00:01, 1642.29it/s]warmup should be done:  50%|█████     | 1504/3000 [00:00<00:00, 1675.15it/s]warmup should be done:  50%|█████     | 1501/3000 [00:00<00:00, 1664.57it/s]warmup should be done:  49%|████▊     | 1462/3000 [00:00<00:00, 1623.12it/s]warmup should be done:  51%|█████     | 1516/3000 [00:00<00:00, 1679.48it/s]warmup should be done:  51%|█████     | 1534/3000 [00:00<00:00, 1700.05it/s]warmup should be done:  50%|█████     | 1510/3000 [00:00<00:00, 1672.11it/s]warmup should be done:  50%|████▉     | 1487/3000 [00:00<00:00, 1642.36it/s]warmup should be done:  50%|█████     | 1515/3000 [00:00<00:00, 1668.26it/s]warmup should be done:  56%|█████▌    | 1674/3000 [00:01<00:00, 1680.60it/s]warmup should be done:  56%|█████▌    | 1684/3000 [00:01<00:00, 1678.97it/s]warmup should be done:  56%|█████▌    | 1669/3000 [00:01<00:00, 1667.14it/s]warmup should be done:  54%|█████▍    | 1627/3000 [00:01<00:00, 1628.40it/s]warmup should be done:  57%|█████▋    | 1705/3000 [00:01<00:00, 1700.21it/s]warmup should be done:  56%|█████▌    | 1678/3000 [00:01<00:00, 1673.35it/s]warmup should be done:  55%|█████▌    | 1652/3000 [00:01<00:00, 1643.76it/s]warmup should be done:  56%|█████▌    | 1682/3000 [00:01<00:00, 1668.57it/s]warmup should be done:  61%|██████▏   | 1844/3000 [00:01<00:00, 1684.87it/s]warmup should be done:  61%|██████    | 1837/3000 [00:01<00:00, 1668.24it/s]warmup should be done:  63%|██████▎   | 1876/3000 [00:01<00:00, 1701.72it/s]warmup should be done:  62%|██████▏   | 1852/3000 [00:01<00:00, 1674.84it/s]warmup should be done:  62%|██████▏   | 1846/3000 [00:01<00:00, 1674.53it/s]warmup should be done:  60%|█████▉    | 1790/3000 [00:01<00:00, 1624.48it/s]warmup should be done:  61%|██████    | 1817/3000 [00:01<00:00, 1644.91it/s]warmup should be done:  62%|██████▏   | 1850/3000 [00:01<00:00, 1669.08it/s]warmup should be done:  67%|██████▋   | 2013/3000 [00:01<00:00, 1686.03it/s]warmup should be done:  67%|██████▋   | 2004/3000 [00:01<00:00, 1666.83it/s]warmup should be done:  68%|██████▊   | 2047/3000 [00:01<00:00, 1702.25it/s]warmup should be done:  67%|██████▋   | 2014/3000 [00:01<00:00, 1673.13it/s]warmup should be done:  67%|██████▋   | 2020/3000 [00:01<00:00, 1669.67it/s]warmup should be done:  65%|██████▌   | 1953/3000 [00:01<00:00, 1619.30it/s]warmup should be done:  66%|██████▌   | 1982/3000 [00:01<00:00, 1642.25it/s]warmup should be done:  67%|██████▋   | 2017/3000 [00:01<00:00, 1667.46it/s]warmup should be done:  73%|███████▎  | 2182/3000 [00:01<00:00, 1683.90it/s]warmup should be done:  72%|███████▏  | 2171/3000 [00:01<00:00, 1664.06it/s]warmup should be done:  74%|███████▍  | 2218/3000 [00:01<00:00, 1701.09it/s]warmup should be done:  73%|███████▎  | 2182/3000 [00:01<00:00, 1672.11it/s]warmup should be done:  70%|███████   | 2115/3000 [00:01<00:00, 1619.33it/s]warmup should be done:  73%|███████▎  | 2187/3000 [00:01<00:00, 1664.47it/s]warmup should be done:  73%|███████▎  | 2184/3000 [00:01<00:00, 1665.32it/s]warmup should be done:  72%|███████▏  | 2147/3000 [00:01<00:00, 1638.49it/s]warmup should be done:  78%|███████▊  | 2351/3000 [00:01<00:00, 1685.12it/s]warmup should be done:  78%|███████▊  | 2338/3000 [00:01<00:00, 1665.24it/s]warmup should be done:  78%|███████▊  | 2350/3000 [00:01<00:00, 1672.87it/s]warmup should be done:  80%|███████▉  | 2389/3000 [00:01<00:00, 1700.33it/s]warmup should be done:  76%|███████▌  | 2278/3000 [00:01<00:00, 1619.51it/s]warmup should be done:  78%|███████▊  | 2351/3000 [00:01<00:00, 1666.62it/s]warmup should be done:  78%|███████▊  | 2354/3000 [00:01<00:00, 1664.04it/s]warmup should be done:  77%|███████▋  | 2311/3000 [00:01<00:00, 1636.94it/s]warmup should be done:  84%|████████▎ | 2506/3000 [00:01<00:00, 1666.95it/s]warmup should be done:  84%|████████▍ | 2520/3000 [00:01<00:00, 1680.19it/s]warmup should be done:  84%|████████▍ | 2518/3000 [00:01<00:00, 1673.83it/s]warmup should be done:  85%|████████▌ | 2560/3000 [00:01<00:00, 1701.67it/s]warmup should be done:  84%|████████▍ | 2522/3000 [00:01<00:00, 1668.49it/s]warmup should be done:  81%|████████▏ | 2440/3000 [00:01<00:00, 1617.93it/s]warmup should be done:  84%|████████▍ | 2519/3000 [00:01<00:00, 1668.70it/s]warmup should be done:  82%|████████▎ | 2475/3000 [00:01<00:00, 1634.58it/s]warmup should be done:  89%|████████▉ | 2673/3000 [00:01<00:00, 1665.66it/s]warmup should be done:  90%|████████▉ | 2686/3000 [00:01<00:00, 1672.90it/s]warmup should be done:  91%|█████████ | 2731/3000 [00:01<00:00, 1701.71it/s]warmup should be done:  90%|████████▉ | 2689/3000 [00:01<00:00, 1676.30it/s]warmup should be done:  90%|████████▉ | 2690/3000 [00:01<00:00, 1671.02it/s]warmup should be done:  87%|████████▋ | 2602/3000 [00:01<00:00, 1616.37it/s]warmup should be done:  90%|████████▉ | 2686/3000 [00:01<00:00, 1667.49it/s]warmup should be done:  88%|████████▊ | 2639/3000 [00:01<00:00, 1630.92it/s]warmup should be done:  95%|█████████▍| 2840/3000 [00:01<00:00, 1663.06it/s]warmup should be done:  95%|█████████▌| 2854/3000 [00:01<00:00, 1671.77it/s]warmup should be done:  97%|█████████▋| 2902/3000 [00:01<00:00, 1700.34it/s]warmup should be done:  95%|█████████▌| 2857/3000 [00:01<00:00, 1674.58it/s]warmup should be done:  95%|█████████▌| 2858/3000 [00:01<00:00, 1671.23it/s]warmup should be done:  92%|█████████▏| 2765/3000 [00:01<00:00, 1617.56it/s]warmup should be done:  95%|█████████▌| 2853/3000 [00:01<00:00, 1664.67it/s]warmup should be done:  93%|█████████▎| 2803/3000 [00:01<00:00, 1630.06it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1699.20it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1673.54it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1672.48it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1671.58it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1669.63it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1663.93it/s]warmup should be done:  98%|█████████▊| 2928/3000 [00:01<00:00, 1618.91it/s]warmup should be done:  99%|█████████▉| 2967/3000 [00:01<00:00, 1631.18it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1638.07it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1618.11it/s]
WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7efcc4f210d0>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7efcc4f1f1f0>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7efcc5294d30>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7efcc4f1e190>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7efcc5292730>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7efcc4f201c0>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7efcc5291e80>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7efcc4f2d2b0>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
2022-12-11 21:20:20.784730: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7ef7f702c770 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 21:20:20.784793: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 21:20:20.792703: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7ef7f3029920 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 21:20:20.792758: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 21:20:20.795053: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 21:20:20.801637: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 21:20:20.891848: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7ef7fef92d10 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 21:20:20.891911: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 21:20:20.899746: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 21:20:21.285911: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7ef7ef0319a0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 21:20:21.285977: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 21:20:21.295582: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 21:20:21.316002: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7ef7fa8300f0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 21:20:21.316072: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 21:20:21.326499: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 21:20:21.501609: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7ef7ff02d5f0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 21:20:21.501672: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 21:20:21.501869: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7ef7f7029820 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 21:20:21.501933: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 21:20:21.511498: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 21:20:21.511830: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 21:20:21.520004: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7ef7f6837c70 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 21:20:21.520069: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 21:20:21.529711: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 21:20:27.849885: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 21:20:28.045266: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 21:20:28.066213: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 21:20:28.174817: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 21:20:28.295057: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 21:20:28.355468: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 21:20:28.379604: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 21:20:28.379627: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
[HCTR][21:21:32.796][ERROR][RK0][tid #139604472616704]: replica 5 reaches 1000, calling init pre replica
[HCTR][21:21:32.796][ERROR][RK0][tid #139604472616704]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][21:21:32.805][ERROR][RK0][tid #139604472616704]: coll ps creation done
[HCTR][21:21:32.805][ERROR][RK0][tid #139604472616704]: replica 5 waits for coll ps creation barrier
[HCTR][21:21:33.066][ERROR][RK0][tid #139604002854656]: replica 1 reaches 1000, calling init pre replica
[HCTR][21:21:33.066][ERROR][RK0][tid #139604002854656]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][21:21:33.076][ERROR][RK0][tid #139604002854656]: coll ps creation done
[HCTR][21:21:33.076][ERROR][RK0][tid #139604002854656]: replica 1 waits for coll ps creation barrier
[HCTR][21:21:33.092][ERROR][RK0][tid #139604002854656]: replica 7 reaches 1000, calling init pre replica
[HCTR][21:21:33.092][ERROR][RK0][tid #139604002854656]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][21:21:33.097][ERROR][RK0][tid #139604128679680]: replica 0 reaches 1000, calling init pre replica
[HCTR][21:21:33.097][ERROR][RK0][tid #139604128679680]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][21:21:33.100][ERROR][RK0][main]: replica 4 reaches 1000, calling init pre replica
[HCTR][21:21:33.100][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][21:21:33.101][ERROR][RK0][tid #139604002854656]: coll ps creation done
[HCTR][21:21:33.101][ERROR][RK0][tid #139604002854656]: replica 7 waits for coll ps creation barrier
[HCTR][21:21:33.104][ERROR][RK0][tid #139604128679680]: coll ps creation done
[HCTR][21:21:33.104][ERROR][RK0][tid #139604128679680]: replica 0 waits for coll ps creation barrier
[HCTR][21:21:33.106][ERROR][RK0][main]: coll ps creation done
[HCTR][21:21:33.106][ERROR][RK0][main]: replica 4 waits for coll ps creation barrier
[HCTR][21:21:33.346][ERROR][RK0][main]: replica 6 reaches 1000, calling init pre replica
[HCTR][21:21:33.346][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][21:21:33.351][ERROR][RK0][main]: coll ps creation done
[HCTR][21:21:33.351][ERROR][RK0][main]: replica 6 waits for coll ps creation barrier
[HCTR][21:21:33.472][ERROR][RK0][main]: replica 2 reaches 1000, calling init pre replica
[HCTR][21:21:33.472][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][21:21:33.478][ERROR][RK0][main]: replica 3 reaches 1000, calling init pre replica
[HCTR][21:21:33.478][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][21:21:33.479][ERROR][RK0][main]: coll ps creation done
[HCTR][21:21:33.479][ERROR][RK0][main]: replica 2 waits for coll ps creation barrier
[HCTR][21:21:33.482][ERROR][RK0][main]: coll ps creation done
[HCTR][21:21:33.482][ERROR][RK0][main]: replica 3 waits for coll ps creation barrier
[HCTR][21:21:33.482][ERROR][RK0][tid #139604128679680]: replica 0 preparing frequency
[HCTR][21:21:34.462][ERROR][RK0][tid #139604128679680]: replica 0 preparing frequency done
[HCTR][21:21:34.498][ERROR][RK0][tid #139604128679680]: replica 0 calling init per replica
[HCTR][21:21:34.498][ERROR][RK0][tid #139604002854656]: replica 7 calling init per replica
[HCTR][21:21:34.498][ERROR][RK0][main]: replica 2 calling init per replica
[HCTR][21:21:34.498][ERROR][RK0][main]: replica 6 calling init per replica
[HCTR][21:21:34.498][ERROR][RK0][tid #139604002854656]: replica 1 calling init per replica
[HCTR][21:21:34.498][ERROR][RK0][main]: replica 3 calling init per replica
[HCTR][21:21:34.498][ERROR][RK0][main]: replica 4 calling init per replica
[HCTR][21:21:34.498][ERROR][RK0][tid #139604472616704]: replica 5 calling init per replica
[HCTR][21:21:34.498][ERROR][RK0][tid #139604128679680]: Calling build_v2
[HCTR][21:21:34.498][ERROR][RK0][tid #139604002854656]: Calling build_v2
[HCTR][21:21:34.498][ERROR][RK0][main]: Calling build_v2
[HCTR][21:21:34.498][ERROR][RK0][main]: Calling build_v2
[HCTR][21:21:34.498][ERROR][RK0][tid #139604002854656]: Calling build_v2
[HCTR][21:21:34.498][ERROR][RK0][main]: Calling build_v2
[HCTR][21:21:34.499][ERROR][RK0][main]: Calling build_v2
[HCTR][21:21:34.499][ERROR][RK0][tid #139604128679680]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][21:21:34.499][ERROR][RK0][tid #139604472616704]: Calling build_v2
[HCTR][21:21:34.499][ERROR][RK0][tid #139604002854656]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][21:21:34.499][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][21:21:34.499][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][21:21:34.499][ERROR][RK0][tid #139604002854656]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][21:21:34.499][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][21:21:34.499][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][21:21:34.499][ERROR][RK0][tid #139604472616704]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[[[[[[[[2022-12-11 21:21:342022-12-11 21:21:342022-12-11 21:21:342022-12-11 21:21:342022-12-11 21:21:342022-12-11 21:21:34.2022-12-11 21:21:34.2022-12-11 21:21:34....499085.499085.499104499102499105499097: 499111: 499099: : : : E: E: EEEE E E    /hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc /hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc /hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc:/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc:/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc::::136:136:136136136136] 136] 136] ] ] ] using concurrent impl MPSPhase] using concurrent impl MPSPhase] using concurrent impl MPSPhaseusing concurrent impl MPSPhaseusing concurrent impl MPSPhaseusing concurrent impl MPSPhase
using concurrent impl MPSPhase
using concurrent impl MPSPhase





[2022-12-11 21:21:34.503455: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:178] v100x8, slow pcie
[2022-12-11 21:21:34.503495: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[:2022-12-11 21:21:34196.] 503504assigning 8 to cpu: 
E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:178] v100x8, slow pcie
[2022-12-11 21:21:34.503550: E [/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[2022-12-11 21:21:34:2022-12-11 21:21:34.196.503555] 503564: assigning 8 to cpu: [E
E2022-12-11 21:21:34  ./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc503602:[[:: 1782022-12-11 21:21:342022-12-11 21:21:34212E] ..]  v100x8, slow pcie503647[503651build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
: 2022-12-11 21:21:34: 
:E[.E[178 2022-12-11 21:21:34503702 [2022-12-11 21:21:34] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[2022-12-11 21:21:34.v100x8, slow pcie:503746E:2022-12-11 21:21:34.503747
212:  178.503771: ] E[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] 503789: Ebuild asymm link desc with 8X Tesla V100-SXM2-32GB out of 8 2022-12-11 21:21:34:v100x8, slow pcie: E 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.178
E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:503867] [ /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[:196: v100x8, slow pcie2022-12-11 21:21:34/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:2022-12-11 21:21:34178] E
.:213.] assigning 8 to cpu 503959178[] 503963v100x8, slow pcie
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: ] 2022-12-11 21:21:34remote time is 8.68421: 
:E[v100x8, slow pcie.
E196[ 2022-12-11 21:21:34
504028 ] [2022-12-11 21:21:34/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.: [/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccassigning 8 to cpu2022-12-11 21:21:34.:504094E2022-12-11 21:21:34:
.504089213:  .196504114: ] E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc504131] : Eremote time is 8.68421 :: assigning 8 to cpuE[ 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc196E
 2022-12-11 21:21:34/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:] [ /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.:196assigning 8 to cpu2022-12-11 21:21:34[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:504244212] 
.2022-12-11 21:21:34:214: ] assigning 8 to cpu504279.196] Ebuild asymm link desc with 8X Tesla V100-SXM2-32GB out of 8
: 504301] cpu time is 97.0588[ 
E: assigning 8 to cpu
2022-12-11 21:21:34/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[ E
[.:2022-12-11 21:21:34/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc 2022-12-11 21:21:34504376212.:/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.: ] 504406214:504422Ebuild asymm link desc with 8X Tesla V100-SXM2-32GB out of 8: ] [212:  
Ecpu time is 97.05882022-12-11 21:21:34] E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc 
.[build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8 :/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc5044842022-12-11 21:21:34
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc212:: .:] 212E504528213[build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8]  : ] 2022-12-11 21:21:34
build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccEremote time is 8.68421.
: 
[504582212/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-11 21:21:34[[: ] :.2022-12-11 21:21:342022-12-11 21:21:34Ebuild asymm link desc with 8X Tesla V100-SXM2-32GB out of 8213504628.. 
] : 504643504645/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[remote time is 8.68421E: : :2022-12-11 21:21:34
 EE213./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc  [] 504715:/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-11 21:21:34remote time is 8.68421: 213::.
E] 213214504745[ remote time is 8.68421] ] : 2022-12-11 21:21:34/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
remote time is 8.68421cpu time is 97.0588E.:

 [504818213/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-11 21:21:34[: ] :.2022-12-11 21:21:34Eremote time is 8.68421214504854. 
] : 504871/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cccpu time is 97.0588E[: :
 2022-12-11 21:21:34E214/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc. ] :504926/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cccpu time is 97.0588214: :
] E214cpu time is 97.0588 ] 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cccpu time is 97.0588:
214] cpu time is 97.0588
[2022-12-11 21:22:52.248338: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:118] solver created. now build & solve
[2022-12-11 21:22:52.287920: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 381.47 MB
[2022-12-11 21:22:52.288008: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 381.47 MB
[2022-12-11 21:22:52.289138: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:74] mapping nid to rank...
[2022-12-11 21:22:52.355701: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:91] counting slots...
[2022-12-11 21:22:52.762880: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:105] Final num slot is 49
[2022-12-11 21:22:52.762967: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:109] counting blocks...
[2022-12-11 21:22:59.140830: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:118] Final num block is 1024
[2022-12-11 21:22:59.140920: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:123] counting freq and density...
[2022-12-11 21:23:00.873190: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:154] averaging freq and density...
[2022-12-11 21:23:00.873291: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:155] 1024
[2022-12-11 21:23:00.876006: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:123] solver built. now solve
[2022-12-11 21:23:00.876066: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:354] constructing optimal solver, device=8, stream=1
1024 blocks, 8 devices
[2022-12-11 21:23:01.150200: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:527] Add Var...
[2022-12-11 21:23:01.178743: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:545] Capacity...
[2022-12-11 21:23:01.180193: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:548] Connect CPU...
[2022-12-11 21:23:01.201398: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:550] Connect Access To Storage...
[2022-12-11 21:23:01.718966: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:554] Time...
[2022-12-11 21:23:01.721204: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:516] dst 0, total sm is 80
[2022-12-11 21:23:01.724220: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:516] dst 1, total sm is 80
[2022-12-11 21:23:01.727072: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:516] dst 2, total sm is 80
[2022-12-11 21:23:01.729944: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:516] dst 3, total sm is 80
[2022-12-11 21:23:01.732783: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:516] dst 4, total sm is 80
[2022-12-11 21:23:01.735607: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:516] dst 5, total sm is 80
[2022-12-11 21:23:01.738412: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:516] dst 6, total sm is 80
[2022-12-11 21:23:01.741223: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:516] dst 7, total sm is 80
[2022-12-11 21:23:12.845380: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:569] Coll Cache init block placement array
[2022-12-11 21:23:12.852951: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:645] Coll Cache init block placement array done
[2022-12-11 21:23:12.853557: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:647] Coll Cache model reset done
[2022-12-11 21:23:12.899701: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:127] solver solved
[2022-12-11 21:23:12.899801: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:195] 0 solved master
[2022-12-11 21:23:12.899833: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 0 solved
[2022-12-11 21:23:12.899861: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 0 initing device 0
[2022-12-11 21:23:12.900539: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-11 21:23:12.900590: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 21:23:12.901737: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 21:23:12.902485: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 21:23:12.915118: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 1 solved
[2022-12-11 21:23:12.915199: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 1 initing device 1
[2022-12-11 21:23:12.915451: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc[:2022022-12-11 21:23:12] .6 solved915477
: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc[:2022-12-11 21:23:12202.] 9155247 solved: 
E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:[2052022-12-11 21:23:12] .worker 0 thread 6 initing device 6915550
: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 7 initing device 7
[2022-12-11 21:23:12.915639: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-11 21:23:12.915691: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 21:23:12.915972: [E2022-12-11 21:23:12 ./hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu915982:: 1815E]  Building Coll Cache with ... num gpu device is 8/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu
:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-11 21:23:12.916036[: 2022-12-11 21:23:12E. 916042/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu: :E1980 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cueager alloc mem 381.47 MB:
1980] eager alloc mem 381.47 MB
[2022-12-11 21:23:12.917357: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc[:2022-12-11 21:23:12202.] 9173724 solved: 
E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202[] 2022-12-11 21:23:122 solved.
917443: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc[:2022-12-11 21:23:12205.] 917473worker 0 thread 4 initing device 4: 
E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 2 initing device 2
[2022-12-11 21:23:12.917745: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 21:23:12.917801: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 21:23:12.917854: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 21:23:12.917917: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-11 21:23:12.917952[: 2022-12-11 21:23:12E. 917964/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu: :E1815 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuBuilding Coll Cache with ... num gpu device is 8:
1980] eager alloc mem 381.47 MB
[2022-12-11 21:23:12.918045: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 21:23:12.918895: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] [3 solved2022-12-11 21:23:12
.918918: E[ 2022-12-11 21:23:12/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc.:918963202: ] E5 solved 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 3 initing device 3[
2022-12-11 21:23:12.919022: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 5 initing device 5
[2022-12-11 21:23:12.919444: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-11 21:23:12.919471: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] [Building Coll Cache with ... num gpu device is 82022-12-11 21:23:12
.919494: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 21:23:12.919532: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 21:23:12.921994: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 21:23:12.922050: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 21:23:12.922100: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 21:23:12.922393: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 21:23:12.922448: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 21:23:12.924347: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 21:23:12.924397: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 21:23:12.927401: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 21:23:12.927446: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 21:23:12.927563: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 21:23:12.927606: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 21:23:12.979826: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1024.00 Bytes
[2022-12-11 21:23:12.997104: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-11 21:23:12.997259: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-11 21:23:13.  2272: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 4.20 MB
[2022-12-11 21:23:13.  2772: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1024.00 Bytes
[[[2022-12-11 21:23:13[2022-12-11 21:23:132022-12-11 21:23:13.[[2022-12-11 21:23:13..  28482022-12-11 21:23:132022-12-11 21:23:13.  2849  2854: ..  2857: : E  2864  2864: EE : : E  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuEE /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::1980/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:19801980] ::1980] ] [eager alloc mem 1024.00 Bytes19801980] eager alloc mem 1024.00 Byteseager alloc mem 1024.00 Bytes2022-12-11 21:23:13
] ] eager alloc mem 1024.00 Byteseager alloc mem 1024.00 Bytes

.eager alloc mem 1024.00 Bytes

  3044
: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:23:13.  4114: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:23:13.  5146: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 21:23:13.  5882: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 21:23:13.  5927: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 491.68 MB
[2022-12-11 21:23:13.  9638: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-11 21:23:13.  9722: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-11 21:23:13. 10130: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-11 21:23:13. 10208: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-11 21:23:13. 10222: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-11 21:23:13. 10301: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-11 21:23:13. 10312: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-11 21:23:13. 10391: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] [eager release cuda mem 4000000002022-12-11 21:23:13
. 10398: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-11 21:23:13. 10480: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[[2022-12-11 21:23:132022-12-11 21:23:13.. 10501 10494: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::1980638] ] eager alloc mem 4.20 MBeager release cuda mem 1024

[2022-12-11 21:23:13. 10632: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-11 21:23:13. 10641: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-11 21:23:13. 10724: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-11 21:23:13. 11789: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 4.20 MB
[2022-12-11 21:23:13. 12374: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 4.20 MB
[2022-12-11 21:23:13. 12887: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 4.20 MB
[2022-12-11 21:23:13. 13486: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 4.20 MB
[2022-12-11 21:23:13. 14341: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 4.20 MB
[2022-12-11 21:23:13. 14899: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 4.20 MB
[2022-12-11 21:23:13. 15140: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:23:13. 15824: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:23:13. 15876: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:23:13. 15933: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:23:13. 15972: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:23:13. 16099: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:23:13. 16141: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:23:13. 16180: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:23:13. 16777: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:23:13. 16818: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:23:13. 16887: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:23:13. 16923: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:23:13. 17049: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 21:23:13. 17098: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:23:13. 17129: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:23:13. 17754: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 21:23:13. 17800: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 491.07 MB
[2022-12-11 21:23:13. 17833: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 21:23:13. 17909: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 21:23:13. 18082: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 21:23:13. 18534: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 21:23:13. 18577: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 492.49 MB
[2022-12-11 21:23:13. 18608: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 21:23:13. 18659: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 491.71 MB
[2022-12-11 21:23:13. 18789: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 21:23:13. 18835: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 493.02 MB
[2022-12-11 21:23:13. 20242: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 21:23:13. 20311: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 21:23:13. 20709: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 21:23:13. 20951: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 21:23:13. 20997: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 490.48 MB
[2022-12-11 21:23:13. 21018: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 21:23:13. 21064: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 488.72 MB
[2022-12-11 21:23:13. 21420: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 21:23:13. 21461: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 492.76 MB
[[[[[[[[2022-12-11 21:23:132022-12-11 21:23:132022-12-11 21:23:132022-12-11 21:23:132022-12-11 21:23:132022-12-11 21:23:132022-12-11 21:23:132022-12-11 21:23:13........162690162689162690162689162690162690162690162690: : : : : : : : EEEEEEEE        /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::::::::19261926192619261926192619261926] ] ] ] ] ] ] ] Device 2 init p2p of link 1Device 6 init p2p of link 0Device 4 init p2p of link 5Device 7 init p2p of link 4Device 1 init p2p of link 7Device 0 init p2p of link 3Device 3 init p2p of link 2Device 5 init p2p of link 6







[2022-12-11 21:23:13.163115: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] [eager alloc mem 611.00 KB[2022-12-11 21:23:13[[
2022-12-11 21:23:13[[.2022-12-11 21:23:132022-12-11 21:23:13.2022-12-11 21:23:132022-12-11 21:23:13163141..[163142..: 1631551631482022-12-11 21:23:13: 163152163152E: : .E: :  EE163183 EE/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu  : /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu  :/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE:/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu1980:: 1980::] 19801980/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu] 19801980eager alloc mem 611.00 KB] ] :eager alloc mem 611.00 KB] ] 
eager alloc mem 611.00 KBeager alloc mem 611.00 KB1980
eager alloc mem 611.00 KBeager alloc mem 611.00 KB

] 

eager alloc mem 611.00 KB
[2022-12-11 21:23:13.164009: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:23:13.164140: E [/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc2022-12-11 21:23:13:.638164154] : eager release cuda mem 625663E[
 [2022-12-11 21:23:13/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc2022-12-11 21:23:13.:[.1641746382022-12-11 21:23:13164178: ] [.: Eeager release cuda mem 6256632022-12-11 21:23:13164189E 
[.:  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc2022-12-11 21:23:13164204E/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:.:  :638164229E/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc638] :  :] eager release cuda mem 625663E/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc638eager release cuda mem 625663
 :] 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc638eager release cuda mem 625663:] 
638eager release cuda mem 625663] 
eager release cuda mem 625663
[2022-12-11 21:23:13.195056: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 2 init p2p of link 3
[2022-12-11 21:23:13.195225: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:23:13.195243: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 7 init p2p of link 1
[2022-12-11 21:23:13.195390: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:23:13.195475: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 2
[2022-12-11 21:23:13.195622: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:23:13.195635: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 6 init p2p of link 5
[2022-12-11 21:23:13.195793: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:23:13.195886: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 4 init p2p of link 7
[2022-12-11 21:23:13.196030: E [/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu2022-12-11 21:23:13:.1980196041] : eager alloc mem 611.00 KBE
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:23:13.196125: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 3 init p2p of link 0[
2022-12-11 21:23:13.196163: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:23:13.196277: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:23:13.196336: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 5 init p2p of link 4
[2022-12-11 21:23:13.196425: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:23:13[.2022-12-11 21:23:13196462.: 196481E:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu :/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu1926:] 1980Device 0 init p2p of link 6] 
eager alloc mem 611.00 KB
[2022-12-11 21:23:13.196575: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:23:13.196694: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:23:13.196861: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:23:13.197087: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:23:13.197337: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:23:13.197506: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:23:13.208205: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 2 init p2p of link 0
[2022-12-11 21:23:13.208326: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:23:13.208469: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 7 init p2p of link 6
[2022-12-11 21:23:13.208583: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:23:13.208825: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 3
[2022-12-11 21:23:13.208934: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:23:13.209037: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 6 init p2p of link 4
[2022-12-11 21:23:13.209131: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc[:2022-12-11 21:23:13638.] 209152eager release cuda mem 625663: 
E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:23:13.209303: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 4 init p2p of link 2
[2022-12-11 21:23:13.209368: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:23:13.209427: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:23:13.209481: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 3 init p2p of link 5
[2022-12-11 21:23:13.209598: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:23:13.209656: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 5 init p2p of link 7
[2022-12-11 21:23:13.209736: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:23:13.209774: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:23:13.209897: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 0 init p2p of link 1
[2022-12-11 21:23:13.209960: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:23:13.210019: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:23:13.210230: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:23:13.210397: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:23:13.210575: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:23:13.210823: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:23:13.229208: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 2 init p2p of link 4
[2022-12-11 21:23:13.229326: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:23:13.229816: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 7 init p2p of link 5
[2022-12-11 21:23:13.229931: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:23:13.229991: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 0
[2022-12-11 21:23:13.230099: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] [eager alloc mem 611.00 KB2022-12-11 21:23:13
.230116: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:23:13.230317: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 6 init p2p of link 7
[2022-12-11 21:23:13.230429: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:23:13.230684[: 2022-12-11 21:23:13E. 230704/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu: [:E2022-12-11 21:23:131926 .] /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc230709Device 4 init p2p of link 6:: 
638E]  eager release cuda mem 625663/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu
:1926] Device 3 init p2p of link 1
[2022-12-11 21:23:13.230830: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:23:13.230855: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:23:13.230908: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:23:13.231200: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:23:13.231288: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 5 init p2p of link 3
[2022-12-11 21:23:13.231401: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:23:13.231607: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:23:13.231635: [E2022-12-11 21:23:13 ./hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu231656:: 1926E]  Device 0 init p2p of link 2/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc
:638] eager release cuda mem 625663
[2022-12-11 21:23:13.231771: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:23:13.232193: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:23:13.232572: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:23:13.245291: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 4399996
[2022-12-11 21:23:13.245979: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 4399996
[2022-12-11 21:23:13.246389: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 4399996
[2022-12-11 21:23:13.246721: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 4399996
[2022-12-11 21:23:13.247047: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 4399996
[2022-12-11 21:23:13.247364: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 4399996
[2022-12-11 21:23:13.247518: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 4399996
[2022-12-11 21:23:13.248367: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 4399996
[2022-12-11 21:23:13.249373: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 995706 / 100000000 nodes ( 1.00 %~1.00 %) | remote 2986470 / 100000000 nodes ( 2.99 %) | cpu 96017824 / 100000000 nodes ( 96.02 %) | 491.07 MB | 0.331342 secs 
[2022-12-11 21:23:13.250229: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 999180 / 100000000 nodes ( 1.00 %~1.00 %) | remote 2982996 / 100000000 nodes ( 2.98 %) | cpu 96017824 / 100000000 nodes ( 96.02 %) | 492.76 MB | 0.334196 secs 
[2022-12-11 21:23:13.250307: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 990901 / 100000000 nodes ( 0.99 %~1.00 %) | remote 2991275 / 100000000 nodes ( 2.99 %) | cpu 96017824 / 100000000 nodes ( 96.02 %) | 488.72 MB | 0.334634 secs 
[2022-12-11 21:23:13.250719: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 997020 / 100000000 nodes ( 1.00 %~1.00 %) | remote 2985156 / 100000000 nodes ( 2.99 %) | cpu 96017824 / 100000000 nodes ( 96.02 %) | 491.71 MB | 0.334696 secs 
[2022-12-11 21:23:13.251145: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 999707 / 100000000 nodes ( 1.00 %~1.00 %) | remote 2982469 / 100000000 nodes ( 2.98 %) | cpu 96017824 / 100000000 nodes ( 96.02 %) | 493.02 MB | 0.333192 secs 
[2022-12-11 21:23:13.251552: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 998617 / 100000000 nodes ( 1.00 %~1.00 %) | remote 2983559 / 100000000 nodes ( 2.98 %) | cpu 96017824 / 100000000 nodes ( 96.02 %) | 492.49 MB | 0.332068 secs 
[2022-12-11 21:23:13.251638: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 994509 / 100000000 nodes ( 0.99 %~1.00 %) | remote 2987667 / 100000000 nodes ( 2.99 %) | cpu 96017824 / 100000000 nodes ( 96.02 %) | 490.48 MB | 0.332119 secs 
[2022-12-11 21:23:13.252271: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 996952 / 100000000 nodes ( 1.00 %~1.00 %) | remote 2985224 / 100000000 nodes ( 2.99 %) | cpu 96017824 / 100000000 nodes ( 96.02 %) | 491.68 MB | 0.351695 secs 
[2022-12-11 21:23:13.253657: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2213] before create ctx, mem is 8.80 GB
[2022-12-11 21:23:14.737934: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2219] after create ctx, mem is 9.06 GB
[2022-12-11 21:23:14.738073: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2226] after create stream, mem is 9.06 GB
[2022-12-11 21:23:14.738334: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2213] before create ctx, mem is 9.06 GB
[2022-12-11 21:23:16.104218: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2219] after create ctx, mem is 9.33 GB
[2022-12-11 21:23:16.104349: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2226] after create stream, mem is 9.33 GB
[2022-12-11 21:23:16.104709: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2213] before create ctx, mem is 9.33 GB
[2022-12-11 21:23:17.347111: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2219] after create ctx, mem is 9.54 GB
[2022-12-11 21:23:17.347254: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2226] after create stream, mem is 9.54 GB
[2022-12-11 21:23:17.347589: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2213] before create ctx, mem is 9.54 GB
[2022-12-11 21:23:18.437761: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2219] after create ctx, mem is 9.75 GB
[2022-12-11 21:23:18.438507: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2226] after create stream, mem is 9.75 GB
[2022-12-11 21:23:18.440071: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2243] before create stream, mem is 9.75 GB
[2022-12-11 21:23:18.441458: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2249] after create stream, mem is 9.75 GB
[2022-12-11 21:23:18.442253: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2213] before create ctx, mem is 9.75 GB
[2022-12-11 21:23:19.860575: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2219] after create ctx, mem is 9.95 GB
[2022-12-11 21:23:19.861163: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2226] after create stream, mem is 9.95 GB
[HCTR][21:23:21.154][ERROR][RK0][tid #139604472616704]: replica 5 calling init per replica done, doing barrier
[HCTR][21:23:21.154][ERROR][RK0][main]: replica 2 calling init per replica done, doing barrier
[HCTR][21:23:21.154][ERROR][RK0][tid #139604128679680]: replica 0 calling init per replica done, doing barrier
[HCTR][21:23:21.154][ERROR][RK0][main]: replica 6 calling init per replica done, doing barrier
[HCTR][21:23:21.154][ERROR][RK0][tid #139604002854656]: replica 1 calling init per replica done, doing barrier
[HCTR][21:23:21.154][ERROR][RK0][main]: replica 4 calling init per replica done, doing barrier
[HCTR][21:23:21.154][ERROR][RK0][tid #139604002854656]: replica 7 calling init per replica done, doing barrier
[HCTR][21:23:21.154][ERROR][RK0][main]: replica 3 calling init per replica done, doing barrier
[HCTR][21:23:21.154][ERROR][RK0][main]: replica 2 calling init per replica done, doing barrier done
[HCTR][21:23:21.154][ERROR][RK0][tid #139604128679680]: replica 0 calling init per replica done, doing barrier done
[HCTR][21:23:21.154][ERROR][RK0][tid #139604472616704]: replica 5 calling init per replica done, doing barrier done
[HCTR][21:23:21.154][ERROR][RK0][main]: replica 3 calling init per replica done, doing barrier done
[HCTR][21:23:21.154][ERROR][RK0][main]: replica 6 calling init per replica done, doing barrier done
[HCTR][21:23:21.154][ERROR][RK0][main]: replica 4 calling init per replica done, doing barrier done
[HCTR][21:23:21.154][ERROR][RK0][tid #139604002854656]: replica 1 calling init per replica done, doing barrier done
[HCTR][21:23:21.154][ERROR][RK0][tid #139604002854656]: replica 7 calling init per replica done, doing barrier done
[HCTR][21:23:21.154][ERROR][RK0][main]: init per replica done
[HCTR][21:23:21.154][ERROR][RK0][tid #139604472616704]: init per replica done
[HCTR][21:23:21.154][ERROR][RK0][main]: init per replica done
[HCTR][21:23:21.154][ERROR][RK0][main]: init per replica done
[HCTR][21:23:21.154][ERROR][RK0][main]: init per replica done
[HCTR][21:23:21.154][ERROR][RK0][tid #139604002854656]: init per replica done
[HCTR][21:23:21.154][ERROR][RK0][tid #139604002854656]: init per replica done
[HCTR][21:23:21.157][ERROR][RK0][tid #139604128679680]: init per replica done
[HCTR][21:23:21.193][ERROR][RK0][tid #139604472616704]: 5 allocated 3276800 at 0x7edc24238400
[HCTR][21:23:21.193][ERROR][RK0][tid #139604472616704]: 5 allocated 6553600 at 0x7edc24558400
[HCTR][21:23:21.193][ERROR][RK0][tid #139604472616704]: 5 allocated 3276800 at 0x7edc24b98400
[HCTR][21:23:21.193][ERROR][RK0][tid #139604472616704]: 5 allocated 6553600 at 0x7edc24eb8400
[HCTR][21:23:21.193][ERROR][RK0][main]: 7 allocated 3276800 at 0x7edb68238400
[HCTR][21:23:21.193][ERROR][RK0][main]: 7 allocated 6553600 at 0x7edb68558400
[HCTR][21:23:21.193][ERROR][RK0][main]: 7 allocated 3276800 at 0x7edb68b98400
[HCTR][21:23:21.193][ERROR][RK0][main]: 7 allocated 6553600 at 0x7edb68eb8400
[HCTR][21:23:21.193][ERROR][RK0][main]: 6 allocated 3276800 at 0x7edc6c238400
[HCTR][21:23:21.193][ERROR][RK0][main]: 6 allocated 6553600 at 0x7edc6c558400
[HCTR][21:23:21.193][ERROR][RK0][main]: 6 allocated 3276800 at 0x7edc6cb98400
[HCTR][21:23:21.193][ERROR][RK0][main]: 6 allocated 6553600 at 0x7edc6ceb8400
[HCTR][21:23:21.193][ERROR][RK0][tid #139603860244224]: 4 allocated 3276800 at 0x7edca8238400
[HCTR][21:23:21.193][ERROR][RK0][tid #139603860244224]: 4 allocated 6553600 at 0x7edca8558400
[HCTR][21:23:21.193][ERROR][RK0][tid #139603860244224]: 4 allocated 3276800 at 0x7edca8b98400
[HCTR][21:23:21.193][ERROR][RK0][tid #139603860244224]: 4 allocated 6553600 at 0x7edca8eb8400
[HCTR][21:23:21.193][ERROR][RK0][tid #139604002854656]: 1 allocated 3276800 at 0x7edc38238400
[HCTR][21:23:21.193][ERROR][RK0][tid #139604002854656]: 1 allocated 6553600 at 0x7edc38558400
[HCTR][21:23:21.193][ERROR][RK0][tid #139604002854656]: 1 allocated 3276800 at 0x7edc38b98400
[HCTR][21:23:21.193][ERROR][RK0][tid #139604002854656]: 1 allocated 6553600 at 0x7edc38eb8400
[HCTR][21:23:21.193][ERROR][RK0][main]: 2 allocated 3276800 at 0x7edb74238400
[HCTR][21:23:21.193][ERROR][RK0][main]: 2 allocated 6553600 at 0x7edb74558400
[HCTR][21:23:21.193][ERROR][RK0][main]: 2 allocated 3276800 at 0x7edb74b98400
[HCTR][21:23:21.193][ERROR][RK0][main]: 2 allocated 6553600 at 0x7edb74eb8400
[HCTR][21:23:21.194][ERROR][RK0][tid #139603868636928]: 3 allocated 3276800 at 0x7edc4c238400
[HCTR][21:23:21.194][ERROR][RK0][tid #139603868636928]: 3 allocated 6553600 at 0x7edc4c558400
[HCTR][21:23:21.194][ERROR][RK0][tid #139603868636928]: 3 allocated 3276800 at 0x7edc4cb98400
[HCTR][21:23:21.194][ERROR][RK0][tid #139603868636928]: 3 allocated 6553600 at 0x7edc4ceb8400
[HCTR][21:23:21.196][ERROR][RK0][tid #139604128679680]: 0 allocated 3276800 at 0x7edc80320000
[HCTR][21:23:21.196][ERROR][RK0][tid #139604128679680]: 0 allocated 6553600 at 0x7edc80640000
[HCTR][21:23:21.196][ERROR][RK0][tid #139604128679680]: 0 allocated 3276800 at 0x7edc80c80000
[HCTR][21:23:21.196][ERROR][RK0][tid #139604128679680]: 0 allocated 6553600 at 0x7edc80fa0000
