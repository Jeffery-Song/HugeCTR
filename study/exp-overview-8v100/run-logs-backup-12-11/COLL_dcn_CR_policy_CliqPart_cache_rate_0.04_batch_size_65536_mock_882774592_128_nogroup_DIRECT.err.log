2022-12-11 19:35:52.868987: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:52.878303: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:52.884619: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:52.890528: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:52.894464: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:52.906653: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:52.921967: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:52.929377: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:52.977374: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:52.987364: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:52.990794: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:52.991294: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:52.992025: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:52.993062: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:52.993693: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:52.994853: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:52.995354: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:52.996475: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:52.996784: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:52.998017: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:52.998247: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:52.999801: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:52.999854: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.001502: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.001540: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.003117: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.003239: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.004718: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.005775: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.006704: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.007614: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.008584: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.010364: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.011454: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.012372: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.013297: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.014316: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.015360: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.016374: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.017554: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.019901: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.021134: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.022204: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.022817: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 19:35:53.023213: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.024175: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.025284: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.026349: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.027528: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.031052: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.032225: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.033006: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.033372: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.034421: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.034925: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.036109: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.036864: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.038846: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.039252: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.040943: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.041375: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.043250: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.043834: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.046752: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.047627: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.048721: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.049333: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.050746: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.050874: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.051583: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.052405: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.054152: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.054233: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.054551: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.055784: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.057730: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.057838: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.057911: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.058885: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.059228: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.060856: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.060879: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.061027: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.062049: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.070534: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.070694: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.071685: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.072161: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.072699: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.074259: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.074576: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.074809: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.075253: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.076002: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.077249: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.078065: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.078160: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.078499: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.088779: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.105866: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.112803: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.113956: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.115906: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.115925: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.116846: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.116896: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.118035: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.119033: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.120544: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.120879: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.120934: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.122910: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.123206: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.125340: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.125535: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.125639: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.127271: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.127321: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.128630: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.128842: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.129087: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.131312: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.132277: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.132489: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.132686: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.134466: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.135771: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.135915: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.135990: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.137479: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.138880: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.138957: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.139098: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.140566: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.141663: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.141819: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.142009: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.144257: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.144277: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.144400: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.144491: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.148306: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.148333: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.148433: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.148498: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.151344: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.151370: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.151406: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.151458: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.154468: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.154525: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.154566: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.154633: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.157556: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.157618: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.157659: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.157763: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.159142: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.160536: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.160755: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.160848: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.161592: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.162416: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.163765: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.163915: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.164009: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.164480: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.165125: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.166561: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.169105: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.169876: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.169928: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.169993: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.170502: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.171280: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.173555: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.173925: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.173936: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.174103: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.174483: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.175103: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.177329: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.177428: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 19:35:53.177850: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.177882: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.178073: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.178367: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.178949: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.181225: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.181819: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.181817: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.181903: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.182325: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.182692: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.185694: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.186588: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.186724: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.186761: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.187481: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.187789: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.187946: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.191284: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.191990: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.192220: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.192235: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.192624: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.192917: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.193112: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.196404: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.196739: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.196929: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.196941: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.197870: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.198011: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.200351: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.200638: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.200822: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.201065: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.201858: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 19:35:53.201928: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.204792: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.205030: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.205546: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.208610: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 19:35:53.209298: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 19:35:53.209337: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.209723: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.210098: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.211849: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.212454: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.212930: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.213375: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.216189: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.216450: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.216926: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.217193: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.218832: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.219400: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.219460: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.219777: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.220521: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.220873: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.223246: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.224273: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.224626: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.225605: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.227168: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.229057: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.229434: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.229818: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.230784: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.231999: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.235614: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.235685: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.237073: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.240286: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.240589: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.241524: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.280123: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.280233: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.281822: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.286217: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.286336: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.287715: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.291987: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.292079: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.292961: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.330278: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.330434: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.332469: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.337684: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.337880: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.343175: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 19:35:53.344631: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.344717: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.353245: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.356770: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.358417: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.360838: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.361015: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 19:35:53.371094: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.385142: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.416998: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.420656: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 19:35:53.421483: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.431189: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.483227: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:53.489967: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:54.396750: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:54.397582: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:54.398179: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:54.398665: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 19:35:54.398717: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-11 19:35:54.416004: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:54.416641: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:54.417147: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:54.417726: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:54.418467: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:54.419382: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-11 19:35:54.463343: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 19:35:54.463544: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 19:35:54.500340: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12340
2022-12-11 19:35:54.625008: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:54.625717: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:54.626248: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:54.626729: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 19:35:54.626781: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-11 19:35:54.643630: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:54.644262: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:54.644781: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:54.645348: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:54.645884: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:54.646356: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:1/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-11 19:35:54.681313: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:54.681944: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:54.682525: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:54.683003: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 19:35:54.683056: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-11 19:35:54.700003: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:54.700003: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:54.700080: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:54.701754: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:54.701900: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:54.701966: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:54.703387: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:54.703522: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:54.703565: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:54.704794: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 19:35:54.704852: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-11 19:35:54.704980: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 19:35:54.705030: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-11 19:35:54.705173: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:54.705940: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:54.706417: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:2/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-11 19:35:54.709274: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 19:35:54.709438: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 19:35:54.711278: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12341
2022-12-11 19:35:54.722230: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:54.722611: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:54.723308: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:54.723927: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:54.724226: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:54.724902: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:54.725447: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:54.725960: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:54.726323: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:54.726871: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:54.727433: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:5/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-11 19:35:54.727711: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:3/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-11 19:35:54.746611: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:54.747245: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:54.748484: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:54.749053: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 19:35:54.749104: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-11 19:35:54.751070: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:54.751695: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:54.752221: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:54.752695: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 19:35:54.752744: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-11 19:35:54.765814: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:54.766463: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:54.767210: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:54.767811: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:54.768319: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:54.768794: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:6/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-11 19:35:54.769670: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:54.770300: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:54.770822: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:54.771413: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:54.772035: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:54.772410: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:54.772718: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 19:35:54.772750: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:7/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-11 19:35:54.772810: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 19:35:54.772886: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 19:35:54.772937: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 19:35:54.773154: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:54.773684: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:54.774149: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 19:35:54.774190: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-11 19:35:54.774778: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12343
2022-12-11 19:35:54.775174: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12345
2022-12-11 19:35:54.791739: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:54.792394: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:54.792910: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:54.793482: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:54.794003: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:35:54.794469: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:4/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-11 19:35:54.802357: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 19:35:54.802542: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 19:35:54.804263: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12342
2022-12-11 19:35:54.813885: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 19:35:54.814048: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 19:35:54.815605: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12346
2022-12-11 19:35:54.818037: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 19:35:54.818174: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 19:35:54.819945: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12347
2022-12-11 19:35:54.839043: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 19:35:54.839237: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 19:35:54.840936: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12344
[HCTR][19:35:56.102][ERROR][RK0][main]: using mock embedding with 882774585 * 128 elements
[HCTR][19:35:56.102][ERROR][RK0][main]: using mock embedding with 882774585 * 128 elements
[HCTR][19:35:56.102][ERROR][RK0][main]: using mock embedding with 882774585 * 128 elements
[HCTR][19:35:56.102][ERROR][RK0][main]: using mock embedding with 882774585 * 128 elements
[HCTR][19:35:56.103][ERROR][RK0][main]: using mock embedding with 882774585 * 128 elements
[HCTR][19:35:56.103][ERROR][RK0][main]: using mock embedding with 882774585 * 128 elements
[HCTR][19:35:56.125][ERROR][RK0][main]: using mock embedding with 882774585 * 128 elements
[HCTR][19:35:56.125][ERROR][RK0][main]: using mock embedding with 882774585 * 128 elements
warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 1it [00:00,  2.65it/s]warmup run: 1it [00:00,  2.56it/s]warmup run: 1it [00:00,  2.53it/s]warmup run: 1it [00:00,  2.52it/s]warmup run: 65it [00:00, 177.55it/s]warmup run: 61it [00:00, 162.13it/s]warmup run: 65it [00:00, 171.42it/s]warmup run: 66it [00:00, 173.84it/s]warmup run: 120it [00:00, 281.11it/s]warmup run: 117it [00:00, 271.35it/s]warmup run: 118it [00:00, 268.58it/s]warmup run: 122it [00:00, 279.10it/s]warmup run: 174it [00:00, 354.48it/s]warmup run: 174it [00:00, 353.98it/s]warmup run: 178it [00:00, 357.34it/s]warmup run: 172it [00:00, 343.23it/s]warmup run: 228it [00:00, 405.93it/s]warmup run: 231it [00:00, 414.15it/s]warmup run: 235it [00:00, 416.24it/s]warmup run: 226it [00:00, 397.75it/s]warmup run: 279it [00:00, 435.86it/s]warmup run: 288it [00:00, 457.72it/s]warmup run: 291it [00:00, 457.39it/s]warmup run: 280it [00:00, 437.76it/s]warmup run: 331it [00:00, 458.66it/s]warmup run: 344it [00:00, 487.30it/s]warmup run: 334it [00:01, 466.99it/s]warmup run: 348it [00:01, 487.67it/s]warmup run: 387it [00:01, 487.08it/s]warmup run: 399it [00:01, 504.78it/s]warmup run: 387it [00:01, 484.33it/s]warmup run: 404it [00:01, 507.39it/s]warmup run: 444it [00:01, 510.50it/s]warmup run: 454it [00:01, 517.15it/s]warmup run: 440it [00:01, 496.38it/s]warmup run: 461it [00:01, 523.01it/s]warmup run: 501it [00:01, 526.14it/s]warmup run: 509it [00:01, 524.87it/s]warmup run: 495it [00:01, 511.23it/s]warmup run: 517it [00:01, 533.75it/s]warmup run: 1it [00:01,  1.39s/it]warmup run: 559it [00:01, 539.62it/s]warmup run: 1it [00:01,  1.39s/it]warmup run: 1it [00:01,  1.39s/it]warmup run: 1it [00:01,  1.39s/it]warmup run: 564it [00:01, 530.07it/s]warmup run: 550it [00:01, 522.20it/s]warmup run: 574it [00:01, 542.11it/s]warmup run: 94it [00:01, 87.49it/s]warmup run: 616it [00:01, 548.54it/s]warmup run: 91it [00:01, 84.58it/s]warmup run: 88it [00:01, 81.54it/s]warmup run: 94it [00:01, 87.03it/s]warmup run: 619it [00:01, 528.00it/s]warmup run: 631it [00:01, 547.95it/s]warmup run: 604it [00:01, 511.77it/s]warmup run: 188it [00:01, 187.65it/s]warmup run: 673it [00:01, 553.41it/s]warmup run: 187it [00:01, 187.43it/s]warmup run: 180it [00:01, 179.80it/s]warmup run: 189it [00:01, 187.91it/s]warmup run: 688it [00:01, 553.05it/s]warmup run: 673it [00:01, 519.72it/s]warmup run: 657it [00:01, 498.13it/s]warmup run: 284it [00:01, 298.46it/s]warmup run: 284it [00:01, 299.87it/s]warmup run: 730it [00:01, 557.59it/s]warmup run: 274it [00:01, 288.78it/s]warmup run: 282it [00:01, 294.04it/s]warmup run: 745it [00:01, 555.44it/s]warmup run: 726it [00:01, 507.47it/s]warmup run: 708it [00:01, 481.99it/s]warmup run: 381it [00:01, 412.75it/s]warmup run: 380it [00:01, 412.24it/s]warmup run: 787it [00:01, 558.96it/s]warmup run: 364it [00:01, 392.32it/s]warmup run: 377it [00:01, 405.04it/s]warmup run: 802it [00:01, 557.93it/s]warmup run: 778it [00:01, 499.87it/s]warmup run: 757it [00:01, 477.66it/s]warmup run: 477it [00:01, 519.70it/s]warmup run: 475it [00:01, 516.71it/s]warmup run: 844it [00:01, 559.55it/s]warmup run: 458it [00:01, 498.41it/s]warmup run: 467it [00:01, 500.90it/s]warmup run: 859it [00:01, 559.68it/s]warmup run: 833it [00:01, 511.60it/s]warmup run: 814it [00:01, 502.67it/s]warmup run: 576it [00:01, 621.33it/s]warmup run: 567it [00:01, 604.54it/s]warmup run: 901it [00:01, 560.59it/s]warmup run: 550it [00:01, 589.57it/s]warmup run: 555it [00:01, 583.31it/s]warmup run: 916it [00:02, 558.50it/s]warmup run: 888it [00:02, 522.28it/s]warmup run: 871it [00:02, 521.05it/s]warmup run: 673it [00:02, 703.92it/s]warmup run: 661it [00:02, 682.82it/s]warmup run: 640it [00:02, 662.79it/s]warmup run: 958it [00:02, 559.85it/s]warmup run: 651it [00:02, 671.32it/s]warmup run: 973it [00:02, 556.64it/s]warmup run: 944it [00:02, 531.57it/s]warmup run: 928it [00:02, 534.42it/s]warmup run: 771it [00:02, 772.44it/s]warmup run: 754it [00:02, 743.46it/s]warmup run: 730it [00:02, 722.26it/s]warmup run: 748it [00:02, 746.50it/s]warmup run: 1015it [00:02, 557.13it/s]warmup run: 1029it [00:02, 548.25it/s]warmup run: 998it [00:02, 523.22it/s]warmup run: 985it [00:02, 543.77it/s]warmup run: 868it [00:02, 824.28it/s]warmup run: 849it [00:02, 797.11it/s]warmup run: 824it [00:02, 778.39it/s]warmup run: 843it [00:02, 800.39it/s]warmup run: 1071it [00:02, 557.10it/s]warmup run: 1084it [00:02, 524.07it/s]warmup run: 1055it [00:02, 535.38it/s]warmup run: 1042it [00:02, 549.76it/s]warmup run: 946it [00:02, 842.22it/s]warmup run: 964it [00:02, 845.33it/s]warmup run: 918it [00:02, 822.11it/s]warmup run: 939it [00:02, 842.55it/s]warmup run: 1127it [00:02, 533.43it/s]warmup run: 1112it [00:02, 543.40it/s]warmup run: 1137it [00:02, 515.32it/s]warmup run: 1098it [00:02, 547.43it/s]warmup run: 1042it [00:02, 874.23it/s]warmup run: 1012it [00:02, 854.94it/s]warmup run: 1061it [00:02, 878.53it/s]warmup run: 1034it [00:02, 872.43it/s]warmup run: 1181it [00:02, 525.55it/s]warmup run: 1190it [00:02, 519.24it/s]warmup run: 1167it [00:02, 539.42it/s]warmup run: 1153it [00:02, 529.86it/s]warmup run: 1138it [00:02, 895.97it/s]warmup run: 1106it [00:02, 877.76it/s]warmup run: 1156it [00:02, 896.51it/s]warmup run: 1130it [00:02, 894.91it/s]warmup run: 1235it [00:02, 527.75it/s]warmup run: 1246it [00:02, 529.41it/s]warmup run: 1222it [00:02, 536.46it/s]warmup run: 1207it [00:02, 523.14it/s]warmup run: 1235it [00:02, 917.05it/s]warmup run: 1201it [00:02, 896.57it/s]warmup run: 1253it [00:02, 915.88it/s]warmup run: 1225it [00:02, 908.73it/s]warmup run: 1290it [00:02, 531.73it/s]warmup run: 1303it [00:02, 539.25it/s]warmup run: 1277it [00:02, 537.34it/s]warmup run: 1260it [00:02, 521.81it/s]warmup run: 1333it [00:02, 933.29it/s]warmup run: 1320it [00:02, 915.81it/s]warmup run: 1344it [00:02, 532.14it/s]warmup run: 1360it [00:02, 546.25it/s]warmup run: 1349it [00:02, 802.37it/s]warmup run: 1331it [00:02, 536.90it/s]warmup run: 1295it [00:02, 776.40it/s]warmup run: 1316it [00:02, 530.65it/s]warmup run: 1398it [00:02, 534.27it/s]warmup run: 1416it [00:02, 549.75it/s]warmup run: 1414it [00:02, 805.12it/s]warmup run: 1429it [00:02, 806.13it/s]warmup run: 1385it [00:02, 535.81it/s]warmup run: 1370it [00:02, 530.21it/s]warmup run: 1378it [00:03, 704.86it/s]warmup run: 1435it [00:03, 712.65it/s]warmup run: 1455it [00:03, 542.41it/s]warmup run: 1472it [00:03, 550.58it/s]warmup run: 1440it [00:03, 538.57it/s]warmup run: 1424it [00:03, 529.64it/s]warmup run: 1515it [00:03, 722.14it/s]warmup run: 1499it [00:03, 715.95it/s]warmup run: 1512it [00:03, 549.38it/s]warmup run: 1454it [00:03, 647.21it/s]warmup run: 1512it [00:03, 663.25it/s]warmup run: 1529it [00:03, 554.64it/s]warmup run: 1495it [00:03, 541.27it/s]warmup run: 1478it [00:03, 528.51it/s]warmup run: 1569it [00:03, 554.44it/s]warmup run: 1592it [00:03, 673.99it/s]warmup run: 1575it [00:03, 661.12it/s]warmup run: 1585it [00:03, 550.10it/s]warmup run: 1550it [00:03, 537.12it/s]warmup run: 1582it [00:03, 633.02it/s]warmup run: 1523it [00:03, 614.39it/s]warmup run: 1531it [00:03, 528.11it/s]warmup run: 1626it [00:03, 556.88it/s]warmup run: 1605it [00:03, 537.61it/s]warmup run: 1641it [00:03, 541.36it/s]warmup run: 1663it [00:03, 641.38it/s]warmup run: 1645it [00:03, 628.05it/s]warmup run: 1584it [00:03, 521.78it/s]warmup run: 1648it [00:03, 614.15it/s]warmup run: 1587it [00:03, 585.65it/s]warmup run: 1683it [00:03, 560.14it/s]warmup run: 1659it [00:03, 529.21it/s]warmup run: 1696it [00:03, 516.86it/s]warmup run: 1730it [00:03, 618.19it/s]warmup run: 1637it [00:03, 517.35it/s]warmup run: 1711it [00:03, 605.88it/s]warmup run: 1711it [00:03, 600.28it/s]warmup run: 1648it [00:03, 569.92it/s]warmup run: 1740it [00:03, 561.40it/s]warmup run: 1712it [00:03, 524.72it/s]warmup run: 1748it [00:03, 504.35it/s]warmup run: 1689it [00:03, 513.63it/s]warmup run: 1794it [00:03, 603.21it/s]warmup run: 1774it [00:03, 589.50it/s]warmup run: 1772it [00:03, 590.94it/s]warmup run: 1706it [00:03, 564.99it/s]warmup run: 1797it [00:03, 563.35it/s]warmup run: 1765it [00:03, 523.13it/s]warmup run: 1799it [00:03, 502.69it/s]warmup run: 1741it [00:03, 507.30it/s]warmup run: 1856it [00:03, 586.07it/s]warmup run: 1832it [00:03, 584.92it/s]warmup run: 1834it [00:03, 567.99it/s]warmup run: 1764it [00:03, 559.04it/s]warmup run: 1854it [00:03, 562.29it/s]warmup run: 1819it [00:03, 525.59it/s]warmup run: 1854it [00:03, 513.89it/s]warmup run: 1792it [00:03, 505.82it/s]warmup run: 1891it [00:03, 580.09it/s]warmup run: 1916it [00:03, 570.49it/s]warmup run: 1821it [00:03, 554.79it/s]warmup run: 1892it [00:03, 557.82it/s]warmup run: 1911it [00:03, 549.30it/s]warmup run: 1872it [00:03, 525.80it/s]warmup run: 1909it [00:03, 522.39it/s]warmup run: 1846it [00:03, 513.36it/s]warmup run: 1950it [00:03, 576.79it/s]warmup run: 1877it [00:03, 556.05it/s]warmup run: 1974it [00:03, 549.39it/s]warmup run: 1949it [00:03, 547.04it/s]warmup run: 1967it [00:03, 540.56it/s]warmup run: 1926it [00:03, 527.33it/s]warmup run: 1962it [00:03, 522.70it/s]warmup run: 1900it [00:04, 518.54it/s]warmup run: 2008it [00:04, 574.45it/s]warmup run: 1933it [00:04, 542.37it/s]warmup run: 2005it [00:04, 548.26it/s]warmup run: 2030it [00:04, 543.67it/s]warmup run: 2022it [00:04, 542.68it/s]warmup run: 1979it [00:04, 524.13it/s]warmup run: 2019it [00:04, 534.16it/s]warmup run: 1953it [00:04, 519.60it/s]warmup run: 2066it [00:04, 571.44it/s]warmup run: 2062it [00:04, 553.50it/s]warmup run: 2079it [00:04, 548.39it/s]warmup run: 1988it [00:04, 527.84it/s]warmup run: 2085it [00:04, 530.48it/s]warmup run: 2034it [00:04, 530.24it/s]warmup run: 2074it [00:04, 537.64it/s]warmup run: 2009it [00:04, 531.33it/s]warmup run: 2124it [00:04, 565.23it/s]warmup run: 2120it [00:04, 558.37it/s]warmup run: 2136it [00:04, 553.57it/s]warmup run: 2041it [00:04, 517.54it/s]warmup run: 2139it [00:04, 521.59it/s]warmup run: 2091it [00:04, 540.51it/s]warmup run: 2130it [00:04, 541.98it/s]warmup run: 2065it [00:04, 539.52it/s]warmup run: 2181it [00:04, 564.91it/s]warmup run: 2178it [00:04, 562.05it/s]warmup run: 2192it [00:04, 553.01it/s]warmup run: 2093it [00:04, 515.65it/s]warmup run: 2193it [00:04, 524.04it/s]warmup run: 2148it [00:04, 546.51it/s]warmup run: 2122it [00:04, 547.79it/s]warmup run: 2185it [00:04, 523.32it/s]warmup run: 2238it [00:04, 565.93it/s]warmup run: 2235it [00:04, 557.61it/s]warmup run: 2249it [00:04, 555.77it/s]warmup run: 2145it [00:04, 513.38it/s]warmup run: 2246it [00:04, 512.23it/s]warmup run: 2204it [00:04, 549.02it/s]warmup run: 2179it [00:04, 554.27it/s]warmup run: 2238it [00:04, 510.48it/s]warmup run: 2295it [00:04, 565.49it/s]warmup run: 2292it [00:04, 561.08it/s]warmup run: 2305it [00:04, 554.45it/s]warmup run: 2199it [00:04, 519.38it/s]warmup run: 2298it [00:04, 512.35it/s]warmup run: 2260it [00:04, 549.92it/s]warmup run: 2236it [00:04, 558.28it/s]warmup run: 2290it [00:04, 502.03it/s]warmup run: 2352it [00:04, 558.51it/s]warmup run: 2349it [00:04, 562.36it/s]warmup run: 2361it [00:04, 554.76it/s]warmup run: 2253it [00:04, 523.80it/s]warmup run: 2351it [00:04, 514.57it/s]warmup run: 2316it [00:04, 551.80it/s]warmup run: 2293it [00:04, 561.55it/s]warmup run: 2341it [00:04, 495.53it/s]warmup run: 2410it [00:04, 561.98it/s]warmup run: 2418it [00:04, 558.15it/s]warmup run: 2406it [00:04, 545.68it/s]warmup run: 2310it [00:04, 534.76it/s]warmup run: 2403it [00:04, 509.36it/s]warmup run: 2372it [00:04, 544.94it/s]warmup run: 2350it [00:04, 560.57it/s]warmup run: 2467it [00:04, 560.29it/s]warmup run: 2391it [00:04, 483.94it/s]warmup run: 2475it [00:04, 553.07it/s]warmup run: 2367it [00:04, 544.32it/s]warmup run: 2461it [00:04, 528.40it/s]warmup run: 2454it [00:04, 497.40it/s]warmup run: 2427it [00:04, 537.23it/s]warmup run: 2407it [00:04, 563.20it/s]warmup run: 2524it [00:04, 562.73it/s]warmup run: 2440it [00:04, 482.94it/s]warmup run: 2532it [00:04, 557.15it/s]warmup run: 2425it [00:04, 552.79it/s]warmup run: 2515it [00:04, 526.87it/s]warmup run: 2506it [00:04, 503.61it/s]warmup run: 2484it [00:05, 545.28it/s]warmup run: 2464it [00:05, 563.90it/s]warmup run: 2581it [00:05, 564.53it/s]warmup run: 2489it [00:05, 482.98it/s]warmup run: 2589it [00:05, 558.39it/s]warmup run: 2483it [00:05, 558.29it/s]warmup run: 2568it [00:05, 518.33it/s]warmup run: 2563it [00:05, 522.30it/s]warmup run: 2541it [00:05, 551.88it/s]warmup run: 2521it [00:05, 563.48it/s]warmup run: 2638it [00:05, 565.43it/s]warmup run: 2646it [00:05, 560.50it/s]warmup run: 2538it [00:05, 458.13it/s]warmup run: 2539it [00:05, 533.56it/s]warmup run: 2620it [00:05, 515.14it/s]warmup run: 2620it [00:05, 533.78it/s]warmup run: 2598it [00:05, 555.50it/s]warmup run: 2578it [00:05, 565.11it/s]warmup run: 2677it [00:05, 529.88it/s]warmup run: 2585it [00:05, 438.32it/s]warmup run: 2593it [00:05, 522.16it/s]warmup run: 2635it [00:05, 550.16it/s]warmup run: 2630it [00:05, 437.84it/s]warmup run: 2646it [00:05, 513.00it/s]warmup run: 2695it [00:05, 359.08it/s]warmup run: 2703it [00:05, 348.80it/s]warmup run: 2674it [00:05, 341.63it/s]warmup run: 2654it [00:05, 352.42it/s]warmup run: 2751it [00:05, 400.23it/s]warmup run: 2731it [00:05, 348.04it/s]warmup run: 2758it [00:05, 389.94it/s]warmup run: 2731it [00:05, 388.46it/s]warmup run: 2710it [00:05, 395.56it/s]warmup run: 2691it [00:05, 346.30it/s]warmup run: 2807it [00:05, 435.84it/s]warmup run: 2788it [00:05, 395.24it/s]warmup run: 2812it [00:05, 422.88it/s]warmup run: 2788it [00:05, 428.93it/s]warmup run: 2698it [00:05, 333.33it/s]warmup run: 2766it [00:05, 432.88it/s]warmup run: 2674it [00:05, 268.57it/s]warmup run: 2741it [00:05, 377.52it/s]warmup run: 2861it [00:05, 460.85it/s]warmup run: 2845it [00:05, 436.13it/s]warmup run: 2869it [00:05, 457.72it/s]warmup run: 2845it [00:05, 462.59it/s]warmup run: 2755it [00:05, 381.77it/s]warmup run: 2823it [00:05, 465.20it/s]warmup run: 2730it [00:05, 325.29it/s]warmup run: 2795it [00:05, 414.20it/s]warmup run: 2918it [00:05, 487.81it/s]warmup run: 2902it [00:05, 468.78it/s]warmup run: 2926it [00:05, 485.46it/s]warmup run: 2902it [00:05, 488.85it/s]warmup run: 2812it [00:05, 423.91it/s]warmup run: 2880it [00:05, 491.44it/s]warmup run: 2785it [00:05, 374.02it/s]warmup run: 2849it [00:05, 443.48it/s]warmup run: 2976it [00:05, 510.55it/s]warmup run: 2959it [00:05, 495.32it/s]warmup run: 2983it [00:05, 507.94it/s]warmup run: 3000it [00:05, 500.25it/s]warmup run: 2957it [00:05, 503.28it/s]warmup run: 2869it [00:05, 458.73it/s]warmup run: 2936it [00:06, 508.22it/s]warmup run: 3000it [00:06, 498.60it/s]warmup run: 2840it [00:06, 414.40it/s]warmup run: 2903it [00:06, 467.87it/s]warmup run: 3000it [00:06, 495.25it/s]warmup run: 3000it [00:06, 492.83it/s]warmup run: 2924it [00:06, 482.02it/s]warmup run: 2992it [00:06, 520.28it/s]warmup run: 2896it [00:06, 451.05it/s]warmup run: 3000it [00:06, 489.75it/s]warmup run: 2960it [00:06, 495.04it/s]warmup run: 2981it [00:06, 505.40it/s]warmup run: 3000it [00:06, 483.09it/s]warmup run: 2953it [00:06, 480.75it/s]warmup run: 3000it [00:06, 480.27it/s]warmup run: 3000it [00:06, 473.86it/s]


warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]



warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   6%|         | 167/3000 [00:00<00:01, 1668.03it/s]warmup should be done:   5%|         | 164/3000 [00:00<00:01, 1637.87it/s]warmup should be done:   6%|         | 167/3000 [00:00<00:01, 1665.98it/s]warmup should be done:   6%|         | 165/3000 [00:00<00:01, 1647.07it/s]warmup should be done:   6%|         | 171/3000 [00:00<00:01, 1705.24it/s]warmup should be done:   6%|         | 168/3000 [00:00<00:01, 1672.94it/s]warmup should be done:   6%|         | 166/3000 [00:00<00:01, 1649.86it/s]warmup should be done:   6%|         | 169/3000 [00:00<00:01, 1681.91it/s]warmup should be done:  11%|         | 331/3000 [00:00<00:01, 1654.34it/s]warmup should be done:  11%|         | 335/3000 [00:00<00:01, 1672.67it/s]warmup should be done:  11%|         | 336/3000 [00:00<00:01, 1676.32it/s]warmup should be done:  11%|         | 334/3000 [00:00<00:01, 1666.68it/s]warmup should be done:  11%|        | 343/3000 [00:00<00:01, 1712.06it/s]warmup should be done:  11%|         | 335/3000 [00:00<00:01, 1675.61it/s]warmup should be done:  11%|        | 339/3000 [00:00<00:01, 1690.47it/s]warmup should be done:  11%|         | 334/3000 [00:00<00:01, 1661.68it/s]warmup should be done:  17%|        | 502/3000 [00:00<00:01, 1669.82it/s]warmup should be done:  17%|        | 505/3000 [00:00<00:01, 1678.47it/s]warmup should be done:  17%|        | 503/3000 [00:00<00:01, 1671.98it/s]warmup should be done:  17%|        | 505/3000 [00:00<00:01, 1683.22it/s]warmup should be done:  17%|        | 497/3000 [00:00<00:01, 1648.84it/s]warmup should be done:  17%|        | 502/3000 [00:00<00:01, 1667.11it/s]warmup should be done:  17%|        | 509/3000 [00:00<00:01, 1683.69it/s]warmup should be done:  17%|        | 515/3000 [00:00<00:01, 1696.47it/s]warmup should be done:  22%|       | 669/3000 [00:00<00:01, 1669.26it/s]warmup should be done:  22%|       | 674/3000 [00:00<00:01, 1685.66it/s]warmup should be done:  22%|       | 673/3000 [00:00<00:01, 1675.94it/s]warmup should be done:  22%|       | 671/3000 [00:00<00:01, 1671.04it/s]warmup should be done:  22%|       | 662/3000 [00:00<00:01, 1647.41it/s]warmup should be done:  22%|       | 670/3000 [00:00<00:01, 1668.97it/s]warmup should be done:  23%|       | 678/3000 [00:00<00:01, 1684.30it/s]warmup should be done:  23%|       | 685/3000 [00:00<00:01, 1690.30it/s]warmup should be done:  28%|       | 836/3000 [00:00<00:01, 1665.78it/s]warmup should be done:  28%|       | 841/3000 [00:00<00:01, 1673.49it/s]warmup should be done:  28%|       | 837/3000 [00:00<00:01, 1668.46it/s]warmup should be done:  28%|       | 827/3000 [00:00<00:01, 1644.38it/s]warmup should be done:  28%|       | 839/3000 [00:00<00:01, 1668.65it/s]warmup should be done:  28%|       | 843/3000 [00:00<00:01, 1667.38it/s]warmup should be done:  28%|       | 847/3000 [00:00<00:01, 1673.90it/s]warmup should be done:  28%|       | 855/3000 [00:00<00:01, 1686.00it/s]warmup should be done:  33%|      | 1003/3000 [00:00<00:01, 1663.77it/s]warmup should be done:  33%|      | 1004/3000 [00:00<00:01, 1667.83it/s]warmup should be done:  34%|      | 1006/3000 [00:00<00:01, 1667.35it/s]warmup should be done:  33%|      | 992/3000 [00:00<00:01, 1641.34it/s]warmup should be done:  34%|      | 1009/3000 [00:00<00:01, 1663.70it/s]warmup should be done:  34%|      | 1010/3000 [00:00<00:01, 1666.19it/s]warmup should be done:  34%|      | 1024/3000 [00:00<00:01, 1682.97it/s]warmup should be done:  34%|      | 1015/3000 [00:00<00:01, 1660.85it/s]warmup should be done:  39%|      | 1170/3000 [00:00<00:01, 1662.27it/s]warmup should be done:  39%|      | 1173/3000 [00:00<00:01, 1667.29it/s]warmup should be done:  39%|      | 1171/3000 [00:00<00:01, 1664.48it/s]warmup should be done:  39%|      | 1157/3000 [00:00<00:01, 1639.32it/s]warmup should be done:  39%|      | 1176/3000 [00:00<00:01, 1663.06it/s]warmup should be done:  39%|      | 1177/3000 [00:00<00:01, 1665.13it/s]warmup should be done:  40%|      | 1193/3000 [00:00<00:01, 1683.30it/s]warmup should be done:  39%|      | 1182/3000 [00:00<00:01, 1639.39it/s]warmup should be done:  45%|     | 1337/3000 [00:00<00:00, 1663.47it/s]warmup should be done:  45%|     | 1340/3000 [00:00<00:00, 1667.03it/s]warmup should be done:  45%|     | 1338/3000 [00:00<00:00, 1665.11it/s]warmup should be done:  44%|     | 1322/3000 [00:00<00:01, 1639.61it/s]warmup should be done:  45%|     | 1343/3000 [00:00<00:00, 1664.10it/s]warmup should be done:  45%|     | 1362/3000 [00:00<00:00, 1682.54it/s]warmup should be done:  45%|     | 1344/3000 [00:00<00:01, 1650.63it/s]warmup should be done:  45%|     | 1346/3000 [00:00<00:01, 1637.55it/s]warmup should be done:  50%|     | 1510/3000 [00:00<00:00, 1676.27it/s]warmup should be done:  50%|     | 1504/3000 [00:00<00:00, 1663.07it/s]warmup should be done:  50%|     | 1505/3000 [00:00<00:00, 1663.40it/s]warmup should be done:  50%|     | 1510/3000 [00:00<00:00, 1665.36it/s]warmup should be done:  50%|     | 1487/3000 [00:00<00:00, 1640.62it/s]warmup should be done:  51%|     | 1531/3000 [00:00<00:00, 1681.66it/s]warmup should be done:  50%|     | 1510/3000 [00:00<00:00, 1650.40it/s]warmup should be done:  51%|     | 1516/3000 [00:00<00:00, 1654.61it/s]warmup should be done:  56%|    | 1680/3000 [00:01<00:00, 1680.41it/s]warmup should be done:  56%|    | 1672/3000 [00:01<00:00, 1665.43it/s]warmup should be done:  56%|    | 1674/3000 [00:01<00:00, 1669.33it/s]warmup should be done:  56%|    | 1679/3000 [00:01<00:00, 1670.36it/s]warmup should be done:  55%|    | 1652/3000 [00:01<00:00, 1639.29it/s]warmup should be done:  57%|    | 1700/3000 [00:01<00:00, 1679.30it/s]warmup should be done:  56%|    | 1676/3000 [00:01<00:00, 1648.45it/s]warmup should be done:  56%|    | 1685/3000 [00:01<00:00, 1663.04it/s]warmup should be done:  61%|   | 1840/3000 [00:01<00:00, 1668.75it/s]warmup should be done:  62%|   | 1850/3000 [00:01<00:00, 1684.28it/s]warmup should be done:  61%|   | 1843/3000 [00:01<00:00, 1673.15it/s]warmup should be done:  62%|   | 1850/3000 [00:01<00:00, 1679.86it/s]warmup should be done:  61%|    | 1817/3000 [00:01<00:00, 1639.74it/s]warmup should be done:  62%|   | 1868/3000 [00:01<00:00, 1675.19it/s]warmup should be done:  61%|   | 1841/3000 [00:01<00:00, 1639.60it/s]warmup should be done:  62%|   | 1852/3000 [00:01<00:00, 1662.73it/s]warmup should be done:  67%|   | 2020/3000 [00:01<00:00, 1687.49it/s]warmup should be done:  67%|   | 2009/3000 [00:01<00:00, 1672.63it/s]warmup should be done:  67%|   | 2012/3000 [00:01<00:00, 1677.76it/s]warmup should be done:  67%|   | 2021/3000 [00:01<00:00, 1687.13it/s]warmup should be done:  66%|   | 1981/3000 [00:01<00:00, 1639.75it/s]warmup should be done:  68%|   | 2036/3000 [00:01<00:00, 1671.36it/s]warmup should be done:  67%|   | 2007/3000 [00:01<00:00, 1643.54it/s]warmup should be done:  67%|   | 2019/3000 [00:01<00:00, 1663.35it/s]warmup should be done:  73%|  | 2190/3000 [00:01<00:00, 1690.49it/s]warmup should be done:  73%|  | 2178/3000 [00:01<00:00, 1675.82it/s]warmup should be done:  73%|  | 2182/3000 [00:01<00:00, 1681.65it/s]warmup should be done:  73%|  | 2192/3000 [00:01<00:00, 1693.23it/s]warmup should be done:  72%|  | 2146/3000 [00:01<00:00, 1640.90it/s]warmup should be done:  73%|  | 2204/3000 [00:01<00:00, 1668.18it/s]warmup should be done:  72%|  | 2173/3000 [00:01<00:00, 1646.24it/s]warmup should be done:  73%|  | 2186/3000 [00:01<00:00, 1665.19it/s]warmup should be done:  79%|  | 2360/3000 [00:01<00:00, 1690.21it/s]warmup should be done:  78%|  | 2346/3000 [00:01<00:00, 1675.08it/s]warmup should be done:  78%|  | 2351/3000 [00:01<00:00, 1680.47it/s]warmup should be done:  79%|  | 2362/3000 [00:01<00:00, 1694.72it/s]warmup should be done:  77%|  | 2311/3000 [00:01<00:00, 1640.95it/s]warmup should be done:  79%|  | 2371/3000 [00:01<00:00, 1663.81it/s]warmup should be done:  78%|  | 2353/3000 [00:01<00:00, 1666.28it/s]warmup should be done:  78%|  | 2338/3000 [00:01<00:00, 1636.32it/s]warmup should be done:  84%| | 2514/3000 [00:01<00:00, 1676.54it/s]warmup should be done:  84%| | 2530/3000 [00:01<00:00, 1691.03it/s]warmup should be done:  84%| | 2520/3000 [00:01<00:00, 1683.08it/s]warmup should be done:  84%| | 2532/3000 [00:01<00:00, 1691.02it/s]warmup should be done:  83%| | 2476/3000 [00:01<00:00, 1640.17it/s]warmup should be done:  85%| | 2538/3000 [00:01<00:00, 1660.39it/s]warmup should be done:  84%| | 2520/3000 [00:01<00:00, 1666.62it/s]warmup should be done:  83%| | 2504/3000 [00:01<00:00, 1640.69it/s]warmup should be done:  89%| | 2683/3000 [00:01<00:00, 1677.67it/s]warmup should be done:  90%| | 2700/3000 [00:01<00:00, 1690.39it/s]warmup should be done:  90%| | 2689/3000 [00:01<00:00, 1680.10it/s]warmup should be done:  88%| | 2641/3000 [00:01<00:00, 1636.71it/s]warmup should be done:  90%| | 2702/3000 [00:01<00:00, 1671.96it/s]warmup should be done:  90%| | 2687/3000 [00:01<00:00, 1667.30it/s]warmup should be done:  90%| | 2705/3000 [00:01<00:00, 1657.39it/s]warmup should be done:  89%| | 2669/3000 [00:01<00:00, 1643.34it/s]warmup should be done:  95%|| 2852/3000 [00:01<00:00, 1678.61it/s]warmup should be done:  96%|| 2870/3000 [00:01<00:00, 1690.25it/s]warmup should be done:  95%|| 2858/3000 [00:01<00:00, 1679.13it/s]warmup should be done:  94%|| 2805/3000 [00:01<00:00, 1634.35it/s]warmup should be done:  96%|| 2873/3000 [00:01<00:00, 1680.83it/s]warmup should be done:  95%|| 2855/3000 [00:01<00:00, 1670.77it/s]warmup should be done:  96%|| 2872/3000 [00:01<00:00, 1659.39it/s]warmup should be done:  94%|| 2834/3000 [00:01<00:00, 1643.73it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1682.18it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1679.13it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1674.47it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1673.90it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1671.35it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1665.34it/s]warmup should be done:  99%|| 2970/3000 [00:01<00:00, 1638.16it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1652.20it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1640.61it/s]




warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   6%|         | 170/3000 [00:00<00:01, 1699.51it/s]warmup should be done:   6%|         | 175/3000 [00:00<00:01, 1748.16it/s]warmup should be done:   6%|         | 171/3000 [00:00<00:01, 1708.44it/s]warmup should be done:   6%|         | 170/3000 [00:00<00:01, 1694.49it/s]warmup should be done:   6%|         | 175/3000 [00:00<00:01, 1744.69it/s]warmup should be done:   6%|         | 171/3000 [00:00<00:01, 1705.33it/s]warmup should be done:   6%|         | 173/3000 [00:00<00:01, 1724.39it/s]warmup should be done:   6%|         | 175/3000 [00:00<00:01, 1740.18it/s]warmup should be done:  12%|        | 350/3000 [00:00<00:01, 1747.92it/s]warmup should be done:  11%|        | 340/3000 [00:00<00:01, 1696.32it/s]warmup should be done:  11%|        | 342/3000 [00:00<00:01, 1706.34it/s]warmup should be done:  11%|        | 344/3000 [00:00<00:01, 1719.23it/s]warmup should be done:  12%|        | 350/3000 [00:00<00:01, 1744.09it/s]warmup should be done:  11%|        | 343/3000 [00:00<00:01, 1709.63it/s]warmup should be done:  12%|        | 346/3000 [00:00<00:01, 1722.67it/s]warmup should be done:  12%|        | 350/3000 [00:00<00:01, 1730.05it/s]warmup should be done:  18%|        | 525/3000 [00:00<00:01, 1747.21it/s]warmup should be done:  17%|        | 513/3000 [00:00<00:01, 1706.86it/s]warmup should be done:  17%|        | 518/3000 [00:00<00:01, 1725.23it/s]warmup should be done:  17%|        | 515/3000 [00:00<00:01, 1712.88it/s]warmup should be done:  17%|        | 519/3000 [00:00<00:01, 1722.03it/s]warmup should be done:  18%|        | 525/3000 [00:00<00:01, 1738.90it/s]warmup should be done:  17%|        | 510/3000 [00:00<00:01, 1678.38it/s]warmup should be done:  17%|        | 524/3000 [00:00<00:01, 1720.39it/s]warmup should be done:  23%|       | 701/3000 [00:00<00:01, 1750.24it/s]warmup should be done:  23%|       | 685/3000 [00:00<00:01, 1709.35it/s]warmup should be done:  23%|       | 692/3000 [00:00<00:01, 1728.96it/s]warmup should be done:  23%|       | 689/3000 [00:00<00:01, 1721.47it/s]warmup should be done:  23%|       | 692/3000 [00:00<00:01, 1721.56it/s]warmup should be done:  23%|       | 701/3000 [00:00<00:01, 1745.23it/s]warmup should be done:  23%|       | 697/3000 [00:00<00:01, 1721.32it/s]warmup should be done:  23%|       | 678/3000 [00:00<00:01, 1666.47it/s]warmup should be done:  29%|       | 857/3000 [00:00<00:01, 1712.61it/s]warmup should be done:  29%|       | 877/3000 [00:00<00:01, 1750.15it/s]warmup should be done:  29%|       | 866/3000 [00:00<00:01, 1731.41it/s]warmup should be done:  29%|       | 864/3000 [00:00<00:01, 1729.96it/s]warmup should be done:  29%|       | 865/3000 [00:00<00:01, 1723.14it/s]warmup should be done:  29%|       | 876/3000 [00:00<00:01, 1745.54it/s]warmup should be done:  29%|       | 870/3000 [00:00<00:01, 1722.61it/s]warmup should be done:  28%|       | 845/3000 [00:00<00:01, 1648.33it/s]warmup should be done:  35%|      | 1040/3000 [00:00<00:01, 1732.88it/s]warmup should be done:  34%|      | 1029/3000 [00:00<00:01, 1710.91it/s]warmup should be done:  35%|      | 1053/3000 [00:00<00:01, 1749.84it/s]warmup should be done:  35%|      | 1039/3000 [00:00<00:01, 1734.37it/s]warmup should be done:  35%|      | 1051/3000 [00:00<00:01, 1745.95it/s]warmup should be done:  35%|      | 1038/3000 [00:00<00:01, 1721.90it/s]warmup should be done:  35%|      | 1043/3000 [00:00<00:01, 1722.59it/s]warmup should be done:  34%|      | 1014/3000 [00:00<00:01, 1660.93it/s]warmup should be done:  40%|      | 1213/3000 [00:00<00:01, 1735.24it/s]warmup should be done:  40%|      | 1214/3000 [00:00<00:01, 1731.10it/s]warmup should be done:  41%|      | 1228/3000 [00:00<00:01, 1746.45it/s]warmup should be done:  41%|      | 1226/3000 [00:00<00:01, 1743.71it/s]warmup should be done:  40%|      | 1201/3000 [00:00<00:01, 1706.86it/s]warmup should be done:  40%|      | 1211/3000 [00:00<00:01, 1719.01it/s]warmup should be done:  41%|      | 1216/3000 [00:00<00:01, 1719.67it/s]warmup should be done:  39%|      | 1183/3000 [00:00<00:01, 1669.61it/s]warmup should be done:  46%|     | 1389/3000 [00:00<00:00, 1736.33it/s]warmup should be done:  46%|     | 1389/3000 [00:00<00:00, 1740.30it/s]warmup should be done:  47%|     | 1404/3000 [00:00<00:00, 1748.07it/s]warmup should be done:  46%|     | 1373/3000 [00:00<00:00, 1710.21it/s]warmup should be done:  47%|     | 1402/3000 [00:00<00:00, 1746.87it/s]warmup should be done:  46%|     | 1384/3000 [00:00<00:00, 1722.36it/s]warmup should be done:  46%|     | 1390/3000 [00:00<00:00, 1723.13it/s]warmup should be done:  45%|     | 1353/3000 [00:00<00:00, 1678.36it/s]warmup should be done:  52%|    | 1564/3000 [00:00<00:00, 1738.85it/s]warmup should be done:  53%|    | 1579/3000 [00:00<00:00, 1747.59it/s]warmup should be done:  53%|    | 1577/3000 [00:00<00:00, 1746.63it/s]warmup should be done:  52%|    | 1564/3000 [00:00<00:00, 1738.29it/s]warmup should be done:  52%|    | 1545/3000 [00:00<00:00, 1708.15it/s]warmup should be done:  52%|    | 1557/3000 [00:00<00:00, 1722.44it/s]warmup should be done:  52%|    | 1563/3000 [00:00<00:00, 1723.19it/s]warmup should be done:  51%|     | 1523/3000 [00:00<00:00, 1682.59it/s]warmup should be done:  58%|    | 1739/3000 [00:01<00:00, 1740.27it/s]warmup should be done:  58%|    | 1754/3000 [00:01<00:00, 1744.49it/s]warmup should be done:  58%|    | 1738/3000 [00:01<00:00, 1736.65it/s]warmup should be done:  57%|    | 1717/3000 [00:01<00:00, 1709.28it/s]warmup should be done:  58%|    | 1730/3000 [00:01<00:00, 1721.18it/s]warmup should be done:  58%|    | 1752/3000 [00:01<00:00, 1737.40it/s]warmup should be done:  58%|    | 1738/3000 [00:01<00:00, 1730.62it/s]warmup should be done:  56%|    | 1693/3000 [00:01<00:00, 1687.68it/s]warmup should be done:  64%|   | 1915/3000 [00:01<00:00, 1743.58it/s]warmup should be done:  64%|   | 1929/3000 [00:01<00:00, 1745.21it/s]warmup should be done:  64%|   | 1913/3000 [00:01<00:00, 1739.02it/s]warmup should be done:  63%|   | 1889/3000 [00:01<00:00, 1711.49it/s]warmup should be done:  63%|   | 1903/3000 [00:01<00:00, 1722.58it/s]warmup should be done:  64%|   | 1926/3000 [00:01<00:00, 1733.09it/s]warmup should be done:  64%|   | 1912/3000 [00:01<00:00, 1732.42it/s]warmup should be done:  62%|   | 1864/3000 [00:01<00:00, 1692.39it/s]warmup should be done:  70%|   | 2105/3000 [00:01<00:00, 1748.42it/s]warmup should be done:  70%|   | 2091/3000 [00:01<00:00, 1745.58it/s]warmup should be done:  70%|   | 2088/3000 [00:01<00:00, 1740.90it/s]warmup should be done:  69%|   | 2076/3000 [00:01<00:00, 1724.21it/s]warmup should be done:  69%|   | 2061/3000 [00:01<00:00, 1711.70it/s]warmup should be done:  70%|   | 2100/3000 [00:01<00:00, 1732.89it/s]warmup should be done:  70%|   | 2086/3000 [00:01<00:00, 1732.63it/s]warmup should be done:  68%|   | 2034/3000 [00:01<00:00, 1694.14it/s]warmup should be done:  76%|  | 2266/3000 [00:01<00:00, 1746.68it/s]warmup should be done:  76%|  | 2281/3000 [00:01<00:00, 1750.02it/s]warmup should be done:  75%|  | 2263/3000 [00:01<00:00, 1742.77it/s]warmup should be done:  75%|  | 2249/3000 [00:01<00:00, 1724.47it/s]warmup should be done:  74%|  | 2233/3000 [00:01<00:00, 1711.11it/s]warmup should be done:  76%|  | 2274/3000 [00:01<00:00, 1734.92it/s]warmup should be done:  75%|  | 2260/3000 [00:01<00:00, 1733.77it/s]warmup should be done:  73%|  | 2204/3000 [00:01<00:00, 1694.08it/s]warmup should be done:  81%| | 2438/3000 [00:01<00:00, 1743.49it/s]warmup should be done:  81%| | 2441/3000 [00:01<00:00, 1744.60it/s]warmup should be done:  82%| | 2457/3000 [00:01<00:00, 1749.09it/s]warmup should be done:  80%|  | 2405/3000 [00:01<00:00, 1710.31it/s]warmup should be done:  82%| | 2448/3000 [00:01<00:00, 1735.13it/s]warmup should be done:  81%|  | 2422/3000 [00:01<00:00, 1712.36it/s]warmup should be done:  81%|  | 2434/3000 [00:01<00:00, 1729.98it/s]warmup should be done:  79%|  | 2374/3000 [00:01<00:00, 1677.65it/s]warmup should be done:  87%| | 2613/3000 [00:01<00:00, 1744.37it/s]warmup should be done:  87%| | 2616/3000 [00:01<00:00, 1737.32it/s]warmup should be done:  86%| | 2577/3000 [00:01<00:00, 1712.34it/s]warmup should be done:  86%| | 2595/3000 [00:01<00:00, 1716.62it/s]warmup should be done:  88%| | 2632/3000 [00:01<00:00, 1733.46it/s]warmup should be done:  87%| | 2622/3000 [00:01<00:00, 1726.66it/s]warmup should be done:  87%| | 2608/3000 [00:01<00:00, 1727.32it/s]warmup should be done:  85%| | 2542/3000 [00:01<00:00, 1672.55it/s]warmup should be done:  93%|| 2788/3000 [00:01<00:00, 1744.83it/s]warmup should be done:  92%|| 2749/3000 [00:01<00:00, 1713.27it/s]warmup should be done:  93%|| 2790/3000 [00:01<00:00, 1735.88it/s]warmup should be done:  92%|| 2768/3000 [00:01<00:00, 1717.79it/s]warmup should be done:  94%|| 2808/3000 [00:01<00:00, 1738.41it/s]warmup should be done:  93%|| 2795/3000 [00:01<00:00, 1725.48it/s]warmup should be done:  93%|| 2781/3000 [00:01<00:00, 1723.47it/s]warmup should be done:  90%| | 2712/3000 [00:01<00:00, 1679.75it/s]warmup should be done:  99%|| 2963/3000 [00:01<00:00, 1744.97it/s]warmup should be done:  97%|| 2921/3000 [00:01<00:00, 1713.40it/s]warmup should be done:  99%|| 2965/3000 [00:01<00:00, 1737.96it/s]warmup should be done:  98%|| 2941/3000 [00:01<00:00, 1721.30it/s]warmup should be done:  99%|| 2984/3000 [00:01<00:00, 1744.04it/s]warmup should be done:  99%|| 2969/3000 [00:01<00:00, 1728.39it/s]warmup should be done:  99%|| 2958/3000 [00:01<00:00, 1736.36it/s]warmup should be done:  96%|| 2882/3000 [00:01<00:00, 1683.96it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1745.55it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1736.91it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1736.13it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1736.06it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1728.81it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1720.74it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1710.63it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1680.43it/s]
WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7fa0351ca310>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7fa0351be070>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7fa03528ad60>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7fa0351ca490>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7fa0351ca0a0>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7fa035288790>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7fa035288e50>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7fa0351ca220>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
2022-12-11 19:37:00.481679: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f9b6a82a700 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 19:37:00.481738: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 19:37:00.491349: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 19:37:00.802912: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f9b7682a0e0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 19:37:00.802974: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 19:37:00.812419: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 19:37:01.030611: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f9b72f941b0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 19:37:01.030671: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 19:37:01.040357: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 19:37:01.210951: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f9b7282a5b0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 19:37:01.211020: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 19:37:01.220554: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 19:37:01.223375: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f9b7682db00 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 19:37:01.223435: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 19:37:01.234013: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 19:37:01.234101: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f9b6a825f20 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 19:37:01.234157: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 19:37:01.235754: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f9b6682de90 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 19:37:01.235798: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 19:37:01.238551: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f9b66825fd0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 19:37:01.238595: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 19:37:01.243068: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 19:37:01.244469: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 19:37:01.248126: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 19:37:03.266294: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 19:37:03.389374: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 19:37:03.584567: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 19:37:03.649187: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 19:37:03.662061: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 19:37:03.662063: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 19:37:03.695845: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 19:37:03.697841: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
[HCTR][19:37:25.596][ERROR][RK0][main]: replica 6 reaches 1000, calling init pre replica
[HCTR][19:37:25.596][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][19:37:25.610][ERROR][RK0][main]: coll ps creation done
[HCTR][19:37:25.610][ERROR][RK0][main]: replica 6 waits for coll ps creation barrier
[HCTR][19:37:25.678][ERROR][RK0][tid #140305642813184]: replica 7 reaches 1000, calling init pre replica
[HCTR][19:37:25.678][ERROR][RK0][tid #140305642813184]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][19:37:25.679][ERROR][RK0][main]: replica 5 reaches 1000, calling init pre replica
[HCTR][19:37:25.679][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][19:37:25.684][ERROR][RK0][tid #140305642813184]: coll ps creation done
[HCTR][19:37:25.684][ERROR][RK0][tid #140305642813184]: replica 7 waits for coll ps creation barrier
[HCTR][19:37:25.693][ERROR][RK0][main]: coll ps creation done
[HCTR][19:37:25.693][ERROR][RK0][main]: replica 5 waits for coll ps creation barrier
[HCTR][19:37:25.704][ERROR][RK0][tid #140306196436736]: replica 4 reaches 1000, calling init pre replica
[HCTR][19:37:25.704][ERROR][RK0][tid #140306196436736]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][19:37:25.712][ERROR][RK0][tid #140306196436736]: coll ps creation done
[HCTR][19:37:25.712][ERROR][RK0][tid #140306196436736]: replica 4 waits for coll ps creation barrier
[HCTR][19:37:25.750][ERROR][RK0][tid #140306062219008]: replica 1 reaches 1000, calling init pre replica
[HCTR][19:37:25.751][ERROR][RK0][tid #140306062219008]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][19:37:25.756][ERROR][RK0][tid #140306062219008]: coll ps creation done
[HCTR][19:37:25.756][ERROR][RK0][tid #140306062219008]: replica 1 waits for coll ps creation barrier
[HCTR][19:37:25.836][ERROR][RK0][main]: replica 3 reaches 1000, calling init pre replica
[HCTR][19:37:25.837][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][19:37:25.846][ERROR][RK0][main]: coll ps creation done
[HCTR][19:37:25.846][ERROR][RK0][main]: replica 3 waits for coll ps creation barrier
[HCTR][19:37:25.867][ERROR][RK0][main]: replica 2 reaches 1000, calling init pre replica
[HCTR][19:37:25.867][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][19:37:25.875][ERROR][RK0][main]: coll ps creation done
[HCTR][19:37:25.875][ERROR][RK0][main]: replica 2 waits for coll ps creation barrier
[HCTR][19:37:25.912][ERROR][RK0][main]: replica 0 reaches 1000, calling init pre replica
[HCTR][19:37:25.912][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][19:37:25.920][ERROR][RK0][main]: coll ps creation done
[HCTR][19:37:25.920][ERROR][RK0][main]: replica 0 waits for coll ps creation barrier
[HCTR][19:37:25.920][ERROR][RK0][main]: replica 0 preparing frequency
[HCTR][19:37:32.954][ERROR][RK0][main]: replica 0 preparing frequency done
[HCTR][19:37:32.988][ERROR][RK0][main]: replica 0 calling init per replica
[HCTR][19:37:32.988][ERROR][RK0][tid #140306062219008]: replica 1 calling init per replica
[HCTR][19:37:32.988][ERROR][RK0][main]: replica 6 calling init per replica
[HCTR][19:37:32.988][ERROR][RK0][main]: replica 3 calling init per replica
[HCTR][19:37:32.988][ERROR][RK0][main]: replica 5 calling init per replica
[HCTR][19:37:32.988][ERROR][RK0][main]: replica 2 calling init per replica
[HCTR][19:37:32.988][ERROR][RK0][tid #140305642813184]: replica 7 calling init per replica
[HCTR][19:37:32.988][ERROR][RK0][tid #140306196436736]: replica 4 calling init per replica
[HCTR][19:37:32.988][ERROR][RK0][tid #140305642813184]: Calling build_v2
[HCTR][19:37:32.988][ERROR][RK0][main]: Calling build_v2
[HCTR][19:37:32.988][ERROR][RK0][tid #140306062219008]: Calling build_v2
[HCTR][19:37:32.988][ERROR][RK0][main]: Calling build_v2
[HCTR][19:37:32.988][ERROR][RK0][main]: Calling build_v2
[HCTR][19:37:32.988][ERROR][RK0][main]: Calling build_v2
[HCTR][19:37:32.988][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][19:37:32.988][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][19:37:32.988][ERROR][RK0][main]: Calling build_v2
[HCTR][19:37:32.988][ERROR][RK0][tid #140306196436736]: Calling build_v2
[HCTR][19:37:32.988][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][19:37:32.988][ERROR][RK0][tid #140305642813184]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][19:37:32.988][ERROR][RK0][tid #140306062219008]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][19:37:32.988][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][19:37:32.988][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][19:37:32.988][ERROR][RK0][tid #140306196436736]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[2022-12-11 19:37:32.993215: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:178[] v100x8, slow pcie
2022-12-11 19:37:32.[9932612022-12-11 19:37:32: .E993299 [: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE: 178/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-11 19:37:32] :.v100x8, slow pcie196993301
] : assigning 0 to cpu[E
[ 2022-12-11 19:37:32/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.:993361178: ] Ev100x8, slow pcie 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-11 19:37:32:.196[993344] 2022-12-11 19:37:32: [assigning 0 to cpu.E2022-12-11 19:37:32
[993402 .: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc993410E:2022-12-11 19:37:32:  178[.E[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] 993400 2022-12-11 19:37:322022-12-11 19:37:32:v100x8, slow pcie[: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc..993442196
E:9934652022-12-11 19:37:32: []  212[: .Eassigning 0 to cpu/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] 2022-12-11 19:37:322022-12-11 19:37:32E993486 
:build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8.. : /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc178
993528993556/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE:] [: [: : 178v100x8, slow pcie2022-12-11 19:37:32E2022-12-11 19:37:32E212/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] 
. .[ ] :v100x8, slow pcie993671/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc9936612022-12-11 19:37:32/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccbuild asymm link desc with 8X Tesla V100-SXM2-32GB out of 8178
: :: .:
] E178[E993727196v100x8, slow pcie ] 2022-12-11 19:37:32[ : ] 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccv100x8, slow pcie.2022-12-11 19:37:32/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccEassigning 0 to cpu:
[993784.: 
2132022-12-11 19:37:32: 993813[212/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] .E: 2022-12-11 19:37:32] [:remote time is 8.68421993866 E.build asymm link desc with 8X Tesla V100-SXM2-32GB out of 82022-12-11 19:37:32196
: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc 993896
.] E:[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: 993934assigning 0 to cpu 196[2022-12-11 19:37:32:E: 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] 2022-12-11 19:37:32.213 E:assigning 0 to cpu.994005] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc 196
994027: remote time is 8.68421[:/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] : E
2022-12-11 19:37:32196:assigning 0 to cpuE [.] [212
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-11 19:37:32994106assigning 0 to cpu2022-12-11 19:37:32] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:.: 
.build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8:214994151E994156[
213] :  : [2022-12-11 19:37:32] cpu time is 97.0588E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[E2022-12-11 19:37:32.remote time is 8.68421
 :2022-12-11 19:37:32 .994222
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc212./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc994256: :] [994257:: E212build asymm link desc with 8X Tesla V100-SXM2-32GB out of 82022-12-11 19:37:32: 214E ] 
.E]  /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccbuild asymm link desc with 8X Tesla V100-SXM2-32GB out of 8994326 cpu time is 97.0588/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:[
: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
:2122022-12-11 19:37:32E:[213] . 2122022-12-11 19:37:32] build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8994401/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] .remote time is 8.68421
: :build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8994440
E214
[:  /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:] [2022-12-11 19:37:32E[213cpu time is 97.05882022-12-11 19:37:32. 2022-12-11 19:37:32] 
.994515/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.remote time is 8.68421994528: :994542
: E213: E [] E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-11 19:37:32remote time is 8.68421 /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:.
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:213994618:[214] : 2132022-12-11 19:37:32] remote time is 8.68421E] .cpu time is 97.0588
 remote time is 8.68421994678
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
: [:E2022-12-11 19:37:32214[ .] 2022-12-11 19:37:32/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc994732cpu time is 97.0588.:: 
994747214E: ]  Ecpu time is 97.0588/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc 
:/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc214:] 214cpu time is 97.0588] 
cpu time is 97.0588
[2022-12-11 19:39:16. 37991: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:118] solver created. now build & solve
[2022-12-11 19:39:16.415172: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:123] solver built. now solve
block 0 storage is 00010001
	access is	0	0	0	0	4	4	4	4	
block 1 storage is 00100010
	access is	1	1	1	1	5	5	5	5	
block 2 storage is 01000100
	access is	2	2	2	2	6	6	6	6	
block 3 storage is 10001000
	access is	3	3	3	3	7	7	7	7	
block 4 storage is 00000000
	access is	8	8	8	8	8	8	8	8	
[2022-12-11 19:39:17.928309: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:127] solver solved
[2022-12-11 19:39:17.928370: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:195] 0 solved master
[2022-12-11 19:39:17.928402: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 0 solved
[2022-12-11 19:39:17.928433: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 0 initing device 0
[2022-12-11 19:39:17.928893: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-11 19:39:17.928937: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 3.29 GB
[2022-12-11 19:39:17.933091: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 3.29 GB
[2022-12-11 19:39:17.937076: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 3.29 GB
[2022-12-11 19:39:18. 60871: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 4 solved
[2022-12-11 19:39:18. 60949: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 4 initing device 4
[2022-12-11 19:39:18. 61357: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-11 19:39:18. 61408: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 3.29 GB
[2022-12-11 19:39:18. 62651: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 3 solved
[2022-12-11 19:39:18. 62710: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 3 initing device 3
[2022-12-11 19:39:18. 62922: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 2 solved
[2022-12-11 19:39:18. 62977: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 2 initing device 2
[2022-12-11 19:39:18. 63014: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 6 solved
[2022-12-11 19:39:18. 63070: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 6 initing device 6[
2022-12-11 19:39:18. 63090: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-11 19:39:18. 63149: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:[19802022-12-11 19:39:18] .eager alloc mem 3.29 GB 63157
: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 7 solved
[2022-12-11 19:39:18. 63224: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 7 initing device 7
[2022-12-11 19:39:18. 63264: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 1 solved
[2022-12-11 19:39:18. 63317: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 1 initing device 1
[2022-12-11 19:39:18. 63358: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-11 19:39:18. 63400: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 3.29 GB
[2022-12-11 19:39:18. 63453: [E2022-12-11 19:39:18 ./hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu 63452:: 1815E]  Building Coll Cache with ... num gpu device is 8/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc
:202] 5 solved
[2022-12-11 19:39:18[.2022-12-11 19:39:18 63518.:  63520E:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu :/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc1980:] 205eager alloc mem 3.29 GB] 
worker 0 thread 5 initing device 5
[2022-12-11 19:39:18. 63607: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-11 19:39:18. 63646: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 3.29 GB
[2022-12-11 19:39:18. 63685: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-11 19:39:18. 63725: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 3.29 GB
[2022-12-11 19:39:18. 63912: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-11 19:39:18. 63950: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 3.29 GB
[2022-12-11 19:39:18. 88004: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 3.29 GB
[2022-12-11 19:39:18. 88085: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 3.29 GB
[2022-12-11 19:39:18. 88185: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 3.29 GB
[2022-12-11 19:39:18. 88275: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 3.29 GB
[2022-12-11 19:39:18. 88364: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 3.29 GB
[2022-12-11 19:39:18. 88479: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 3.29 GB
[2022-12-11 19:39:18. 88530: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 3.29 GB
[2022-12-11 19:39:18.118031: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 3.29 GB
[2022-12-11 19:39:18.118107: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 3.29 GB
[2022-12-11 19:39:18.118194: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 3.29 GB
[2022-12-11 19:39:18.118296: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 3.29 GB
[2022-12-11 19:39:18.118368: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 3.29 GB
[2022-12-11 19:39:18.118525: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 3.29 GB
[2022-12-11 19:39:18.118585: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 3.29 GB
[2022-12-11 19:39:18.611616: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.00 Bytes
[2022-12-11 19:39:18.611849: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1855] using empty feat=27
[2022-12-11 19:39:18.629382: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5
[2022-12-11 19:39:18.629502: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 3531098340
[2022-12-11 19:39:18.634813: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 138.07 MB
[2022-12-11 19:39:18.635707: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:39:18.643015: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:39:18.643388: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 134.70 MB
[2022-12-11 19:39:18.742471: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 19:39:18.747064: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 19:39:18.747156: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 16.88 GB
[[[[[[[2022-12-11 19:39:182022-12-11 19:39:182022-12-11 19:39:182022-12-11 19:39:182022-12-11 19:39:182022-12-11 19:39:182022-12-11 19:39:18.......938282938282938282938282938282938282938282: : : : : : : EEEEEEE       /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:::::::1980198019801980198019801980] ] ] ] ] ] ] eager alloc mem 5.00 Byteseager alloc mem 5.00 Byteseager alloc mem 5.00 Byteseager alloc mem 5.00 Byteseager alloc mem 5.00 Byteseager alloc mem 5.00 Byteseager alloc mem 5.00 Bytes






[[[[2022-12-11 19:39:18[[[2022-12-11 19:39:182022-12-11 19:39:182022-12-11 19:39:18.2022-12-11 19:39:182022-12-11 19:39:182022-12-11 19:39:18...938627...938627938628938629: 938631938631938631: : : W: : : WWW WWW   /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu   /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:::1855:::185518551855] 185518551855] ] ] using empty feat=27] ] ] using empty feat=27using empty feat=27using empty feat=27
using empty feat=27using empty feat=27using empty feat=27





[2022-12-11 19:39:18.957316: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5
[2022-12-11 19:39:18.957418: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 3531098340
[2022-12-11 19:39:18.957653: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5
[2022-12-11 19:39:18.957735: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 3531098340
[2022-12-11 19:39:18.957778: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5
[2022-12-11 19:39:18.957858[: 2022-12-11 19:39:18E. 957853/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc: :E638 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cceager release cuda mem 3531098340:
638] eager release cuda mem 5
[[2022-12-11 19:39:182022-12-11 19:39:18..957935957949: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::638638] ] eager release cuda mem 5eager release cuda mem 3531098340

[2022-12-11 19:39:18.958020: [E2022-12-11 19:39:18 ./hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc958043:: 638E]  eager release cuda mem 5/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc
:638] eager release cuda mem 3531098340
[2022-12-11 19:39:18.958111: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc[:2022-12-11 19:39:18638.] 958114eager release cuda mem 3531098340: 
E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5
[2022-12-11 19:39:18.958201: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 3531098340
[2022-12-11 19:39:18.961837: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 138.07 MB
[2022-12-11 19:39:18.966962: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 138.07 MB
[2022-12-11 19:39:18.971156: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 138.07 MB
[2022-12-11 19:39:18.975048: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 138.07 MB
[2022-12-11 19:39:18.978969: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 138.07 MB
[2022-12-11 19:39:18.982884: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 138.07 MB
[2022-12-11 19:39:18.986797: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 138.07 MB
[2022-12-11 19:39:18.990917: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:39:18.995824: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:39:18.995875: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:39:18.995922: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:39:18.999013: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:39:18.999074: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:39:18.999144: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:39:19.  9397: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:39:19.  9695: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 134.70 MB
[2022-12-11 19:39:19. 10453: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:39:19. 10639: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:39:19. 10684: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:39:19.[ 107292022-12-11 19:39:19: .E 10744 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccW: 638/hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc] :eager release cuda mem 551807943
] WORKER[0] alloc host memory 134.70 MB
[2022-12-11 19:39:19. 10901: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:39:19. 10972: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:39:19. 11158: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 134.70 MB
[2022-12-11 19:39:19. 11447: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 134.70 MB
[2022-12-11 19:39:19. 11604: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 134.70 MB
[2022-12-11 19:39:19. 11774: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 134.70 MB
[2022-12-11 19:39:19. 11845: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 134.70 MB
[2022-12-11 19:39:19.100549: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 19:39:19.101373: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 19:39:19.102408: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 19:39:19.102537: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 19:39:19.103323: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 19:39:19.103698: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 19:39:19.103853: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 19:39:19.105095: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 19:39:19.105144: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 16.88 GB
[2022-12-11 19:39:19.105887: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 19:39:19.105931: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 16.88 GB
[2022-12-11 19:39:19.106904: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 19:39:19.106946: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 16.88 GB
[2022-12-11 19:39:19.107030: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 19:39:19.107073: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 16.88 GB
[2022-12-11 19:39:19.107818: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 19:39:19.107864: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 16.88 GB
[2022-12-11 19:39:19.108195: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 19:39:19.108237: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 16.88 GB
[2022-12-11 19:39:19.108354: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 19:39:19.108398: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 16.88 GB
[[[[[[[[2022-12-11 19:39:222022-12-11 19:39:222022-12-11 19:39:222022-12-11 19:39:222022-12-11 19:39:222022-12-11 19:39:222022-12-11 19:39:222022-12-11 19:39:22........343563343563343564343564343564343572343564343563: : : : : : : : EEEEEEEE        /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::::::::19261926192619261926192619261926] ] ] ] ] ] ] ] Device 3 init p2p of link 2Device 7 init p2p of link 4Device 1 init p2p of link 7Device 6 init p2p of link 0Device 2 init p2p of link 1Device 0 init p2p of link 3Device 4 init p2p of link 5Device 5 init p2p of link 6







[[[[[2022-12-11 19:39:22[[2022-12-11 19:39:222022-12-11 19:39:222022-12-11 19:39:222022-12-11 19:39:22.2022-12-11 19:39:222022-12-11 19:39:22[....344074..2022-12-11 19:39:22344073344073344074344074: 344076344074.: : : : E: : 344098EEEE EE:     /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu ::::1980::/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu1980198019801980] 19801980:] ] ] ] eager alloc mem 5.26 MB] ] 1980eager alloc mem 5.26 MBeager alloc mem 5.26 MBeager alloc mem 5.26 MBeager alloc mem 5.26 MB
eager alloc mem 5.26 MBeager alloc mem 5.26 MB] 





eager alloc mem 5.26 MB
[2022-12-11 19:39:22.353360: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:39:22.[3535032022-12-11 19:39:22: .E353512 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccE[: 2022-12-11 19:39:22638/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc.] :353535eager release cuda mem 5518079638: 
] Eeager release cuda mem 5518079 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:39:22.353606: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:39:22.353688: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:39:22.353746: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:39:22.353858: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:39:22.370415: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 2
[2022-12-11 19:39:22.370564: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:39:22.373100: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 6 init p2p of link 5
[2022-12-11 19:39:22.373249: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:39:22.380811: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:39:22.381377: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:39:22.389299: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 3 init p2p of link 0
[2022-12-11 19:39:22.389462: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:39:22.390123: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 7 init p2p of link 1
[2022-12-11 19:39:22.390287: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:39:22.390505: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 2 init p2p of link 3
[2022-12-11 19:39:22.390668: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:39:22.390683: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 4 init p2p of link 7
[2022-12-11 19:39:22.390839: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:39:22.390980: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 0 init p2p of link 6
[2022-12-11 19:39:22.391031: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 5 init p2p of link 4
[2022-12-11 19:39:22.391150: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:39:22.391194: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:39:22.396188: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:39:22.396709: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:39:22.398127: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:39:22.398270: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:39:22.398375: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:39:22.398701: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:39:22.404828: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 3
[2022-12-11 19:39:22.404959: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:39:22.405483: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 6 init p2p of link 4
[2022-12-11 19:39:22.405603: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:39:22.408979: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 7 init p2p of link 6
[2022-12-11 19:39:22.409106: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:39:22.411497: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 0 init p2p of link 1
[2022-12-11 19:39:22.411623: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:39:22.415962: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:39:22.416482: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:39:22.418140: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:39:22.419140: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:39:22.421921: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 3 init p2p of link 5
[2022-12-11 19:39:22.422057: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:39:22.428330: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 4 init p2p of link 2
[2022-12-11 19:39:22.428363: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 2 init p2p of link 0
[2022-12-11 19:39:22.428462: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:39:22.428497: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:39:22.428772: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 5 init p2p of link 7
[2022-12-11 19:39:22.428901: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:39:22.431861: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:39:22.435209: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 3 init p2p of link 1
[2022-12-11 19:39:22.435327: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:39:22.435555: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:39:22.435776: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:39:22.435867: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:39:22.442216: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:39:22.442701: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 4 init p2p of link 6
[2022-12-11 19:39:22.442821: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:39:22.452019: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:39:22.456701: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 0
[2022-12-11 19:39:22.456827: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:39:22.457809: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 6 init p2p of link 7
[2022-12-11 19:39:22.457930: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:39:22.459687: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 7 init p2p of link 5
[2022-12-11 19:39:22.459813: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:39:22.460607: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 0 init p2p of link 2
[2022-12-11 19:39:22.460729: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:39:22.463722: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:39:22.464795: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:39:22.466604: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:39:22.467526: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:39:22.477914: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 2 init p2p of link 4
[2022-12-11 19:39:22.478041: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:39:22.478505: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 5 init p2p of link 3
[2022-12-11 19:39:22.478626: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:39:22.484538: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:39:22.485147: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:39:22.486049: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 144775028
[2022-12-11 19:39:22.487826: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 35310982 / 882774585 nodes ( 4.00 %~4.00 %) | remote 105932946 / 882774585 nodes ( 12.00 %) | cpu 741530657 / 882774585 nodes ( 84.00 %) | 16.88 GB | 4.42468 secs 
[2022-12-11 19:39:22.489339: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 144775028
[2022-12-11 19:39:22.490249: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 144775028
[2022-12-11 19:39:22.490580: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 35310982 / 882774585 nodes ( 4.00 %~4.00 %) | remote 105932946 / 882774585 nodes ( 12.00 %) | cpu 741530657 / 882774585 nodes ( 84.00 %) | 16.88 GB | 4.42919 secs 
[2022-12-11 19:39:22.491145: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 144775028
[2022-12-11 19:39:22.491336: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 35310982 / 882774585 nodes ( 4.00 %~4.00 %) | remote 105932946 / 882774585 nodes ( 12.00 %) | cpu 741530657 / 882774585 nodes ( 84.00 %) | 16.88 GB | 4.42794 secs 
[2022-12-11 19:39:22.491686: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 35310982 / 882774585 nodes ( 4.00 %~4.00 %) | remote 105932946 / 882774585 nodes ( 12.00 %) | cpu 741530657 / 882774585 nodes ( 84.00 %) | 16.88 GB | 4.42774 secs 
[2022-12-11 19:39:22.502368: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 144775028
[2022-12-11 19:39:22.502831: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 35310982 / 882774585 nodes ( 4.00 %~4.00 %) | remote 105932946 / 882774585 nodes ( 12.00 %) | cpu 741530657 / 882774585 nodes ( 84.00 %) | 16.88 GB | 4.43911 secs 
[2022-12-11 19:39:22.505080: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 144775028
[2022-12-11 19:39:22.505531: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 35310982 / 882774585 nodes ( 4.00 %~4.00 %) | remote 105932946 / 882774585 nodes ( 12.00 %) | cpu 741530657 / 882774585 nodes ( 84.00 %) | 16.88 GB | 4.44202 secs 
[[2022-12-11 19:39:222022-12-11 19:39:22..509463509467: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::638638] ] eager release cuda mem 144775028eager release cuda mem 144775028

[2022-12-11 19:39:22.510050: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 35310982 / 882774585 nodes ( 4.00 %~4.00 %) | remote 105932946 / 882774585 nodes ( 12.00 %) | cpu 741530657 / 882774585 nodes ( 84.00 %) | 16.88 GB | 4.58112 secs 
[2022-12-11 19:39:22.510273: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 35310982 / 882774585 nodes ( 4.00 %~4.00 %) | remote 105932946 / 882774585 nodes ( 12.00 %) | cpu 741530657 / 882774585 nodes ( 84.00 %) | 16.88 GB | 4.44663 secs 
[HCTR][19:39:22.510][ERROR][RK0][main]: replica 0 calling init per replica done, doing barrier
[HCTR][19:39:22.510][ERROR][RK0][tid #140305642813184]: replica 7 calling init per replica done, doing barrier
[HCTR][19:39:22.510][ERROR][RK0][main]: replica 5 calling init per replica done, doing barrier
[HCTR][19:39:22.510][ERROR][RK0][main]: replica 6 calling init per replica done, doing barrier
[HCTR][19:39:22.510][ERROR][RK0][main]: replica 3 calling init per replica done, doing barrier
[HCTR][19:39:22.510][ERROR][RK0][main]: replica 2 calling init per replica done, doing barrier
[HCTR][19:39:22.510][ERROR][RK0][tid #140306196436736]: replica 4 calling init per replica done, doing barrier
[HCTR][19:39:22.510][ERROR][RK0][tid #140306062219008]: replica 1 calling init per replica done, doing barrier
[HCTR][19:39:22.510][ERROR][RK0][main]: replica 2 calling init per replica done, doing barrier done
[HCTR][19:39:22.510][ERROR][RK0][tid #140306196436736]: replica 4 calling init per replica done, doing barrier done
[HCTR][19:39:22.510][ERROR][RK0][main]: replica 0 calling init per replica done, doing barrier done
[HCTR][19:39:22.510][ERROR][RK0][tid #140306062219008]: replica 1 calling init per replica done, doing barrier done
[HCTR][19:39:22.510][ERROR][RK0][tid #140305642813184]: replica 7 calling init per replica done, doing barrier done
[HCTR][19:39:22.510][ERROR][RK0][main]: replica 3 calling init per replica done, doing barrier done
[HCTR][19:39:22.510][ERROR][RK0][main]: replica 6 calling init per replica done, doing barrier done
[HCTR][19:39:22.510][ERROR][RK0][main]: replica 5 calling init per replica done, doing barrier done
[HCTR][19:39:22.510][ERROR][RK0][tid #140306062219008]: init per replica done
[HCTR][19:39:22.510][ERROR][RK0][main]: init per replica done
[HCTR][19:39:22.510][ERROR][RK0][tid #140306196436736]: init per replica done
[HCTR][19:39:22.510][ERROR][RK0][tid #140305642813184]: init per replica done
[HCTR][19:39:22.510][ERROR][RK0][main]: init per replica done
[HCTR][19:39:22.510][ERROR][RK0][main]: init per replica done
[HCTR][19:39:22.510][ERROR][RK0][main]: init per replica done
[HCTR][19:39:22.528][ERROR][RK0][main]: init per replica done
