2022-12-12 07:05:43.979106: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:43.986823: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.000943: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.016338: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.021669: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.026601: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.037430: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.050078: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.088141: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.089269: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.090653: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.092969: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.093554: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.094387: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.095285: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.096054: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.096795: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.097576: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.098306: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.099052: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.099809: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.101133: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.101792: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.102428: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.103397: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.104366: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.105308: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.107161: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.107616: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.108635: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.109627: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.110231: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.111340: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.111747: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.112943: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.113241: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.114855: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.115158: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.117202: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.117271: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.117647: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.119653: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.119758: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.120489: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.122485: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.122582: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.123405: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.123664: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.124809: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.126096: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.126381: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.126812: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.128194: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.128674: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.128870: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.130995: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.131902: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.131965: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.134590: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.135631: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.135902: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.137389: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.137848: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.138573: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.138705: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.141908: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.142969: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.142970: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.144869: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.145592: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.145787: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.145833: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.146605: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.147732: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.148458: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.149087: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.149565: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.150676: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.151147: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.151789: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.152007: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.152224: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.153574: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.154057: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.154862: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.155051: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.155337: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.156846: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.156964: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.157830: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.158288: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.158340: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.160120: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.160747: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.160972: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.161389: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.163148: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.163366: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.163865: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.164309: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 07:05:44.165039: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.165342: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.165897: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.166952: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.167264: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.168722: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.168966: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.170300: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.170470: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.171238: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.171934: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.172177: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.173438: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.173838: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.174056: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.175278: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.175353: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.175735: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.175989: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.177679: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.177738: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.178066: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.178366: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.179980: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.180097: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.180353: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.180711: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.182377: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.182974: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.183521: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.184507: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.185179: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.186004: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.186776: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.188129: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.188279: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.188299: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.189071: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.191031: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.191174: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.191259: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.191948: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.193922: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.193980: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.194157: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.194564: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.194730: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.197217: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.197348: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.197563: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.198139: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.198199: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.200572: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.200851: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.200974: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.201452: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.201612: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.203909: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.204265: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.204455: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.204907: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.205073: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.207261: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.208191: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.208212: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.208369: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.208536: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.210198: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.211885: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 07:05:44.212040: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.212129: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.212290: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.213679: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.214031: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.215581: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.217496: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.217587: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.218345: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.218438: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.221000: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.221556: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 07:05:44.222549: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.223232: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.223542: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.224227: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.224235: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.232132: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.264213: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.265229: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.265992: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.266186: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.267017: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.267201: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.268524: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.269203: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.270317: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.271440: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.271532: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.272390: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.272625: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.273584: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.275164: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.277041: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.277153: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.278994: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.279149: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.280758: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.282596: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.282672: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.283896: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.283988: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.285723: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.288155: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.288240: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.289160: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.289347: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.290844: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.293785: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.293813: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.294707: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.295025: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.298071: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.298663: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.298701: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.300035: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.303024: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.303181: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.303289: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.303941: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 07:05:44.304446: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.307535: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.307558: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.308721: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.308983: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.312230: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.312246: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.314496: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.314552: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.314871: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.319059: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.319213: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.320932: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.321027: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.321277: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.324593: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.324741: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.326039: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.326083: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.326316: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.345024: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.346190: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.346757: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.347207: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.351386: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.352513: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.353640: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.353708: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.359990: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 07:05:44.370739: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.387424: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.388717: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.388779: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.422562: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.422925: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.424131: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.424180: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.428749: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.430021: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.431226: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.434937: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 07:05:44.436670: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.437699: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.442633: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.443728: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.445642: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.448130: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.449903: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.453613: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.457367: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.459439: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.459977: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 07:05:44.465919: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 07:05:44.470516: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.475752: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.477588: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.513397: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.514232: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:44.520028: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:45.669399: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:45.670433: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:45.670969: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:45.671455: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 07:05:45.671522: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-12 07:05:45.688828: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:45.689464: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:45.690052: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:45.690636: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:45.691157: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:45.691831: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-12 07:05:45.738802: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 07:05:45.739009: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 07:05:45.767657: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12340
2022-12-12 07:05:45.779901: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:45.780548: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:45.781158: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:45.781830: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 07:05:45.781890: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-12 07:05:45.800031: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:45.800902: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:45.801426: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:45.802209: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:45.802759: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:45.803260: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:1/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-12 07:05:45.807901: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:45.808508: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:45.809029: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:45.809507: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 07:05:45.809564: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-12 07:05:45.827618: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:45.828472: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:45.828985: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:45.829708: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:45.830225: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:45.830707: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:3/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-12 07:05:45.853832: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:45.854465: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:45.855214: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:45.855693: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 07:05:45.855754: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-12 07:05:45.872916: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:45.873644: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:45.874159: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:45.874945: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:45.875486: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:45.875964: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:2/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-12 07:05:45.877331: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 07:05:45.877497: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 07:05:45.879495: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12343
2022-12-12 07:05:45.885559: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 07:05:45.885714: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 07:05:45.887634: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12341
2022-12-12 07:05:45.904422: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:45.905032: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:45.905981: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:45.906467: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 07:05:45.906527: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-12 07:05:45.923048: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:45.923706: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:45.924213: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:45.924998: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:45.925522: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:45.926396: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:4/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-12 07:05:45.943205: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:45.943895: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:45.944996: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:45.945504: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 07:05:45.945567: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-12 07:05:45.951690: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:45.952115: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:45.952592: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:45.953057: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:45.953626: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:45.954149: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:45.954450: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 07:05:45.954515: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-12 07:05:45.955013: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 07:05:45.955080: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-12 07:05:45.962279: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:45.962911: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:45.963522: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:45.964132: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:45.964713: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:45.965205: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:5/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-12 07:05:45.970932: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 07:05:45.971105: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 07:05:45.971428: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:45.971736: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 07:05:45.971871: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 07:05:45.972034: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:45.972557: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:45.972886: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12342
2022-12-12 07:05:45.973028: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:45.973124: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:45.973619: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12344
2022-12-12 07:05:45.974221: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:45.974254: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:45.975334: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:6/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-12 07:05:45.975336: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:45.975924: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:45.976453: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:05:45.976919: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:7/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-12 07:05:46.010621: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 07:05:46.010819: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 07:05:46.012583: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12345
2022-12-12 07:05:46.021057: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 07:05:46.021232: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 07:05:46.021841: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 07:05:46.021974: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 07:05:46.022817: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12346
2022-12-12 07:05:46.023679: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12347
[HCTR][07:05:47.295][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][07:05:47.295][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][07:05:47.295][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][07:05:47.295][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][07:05:47.295][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][07:05:47.295][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][07:05:47.295][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][07:05:47.296][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 1it [00:01,  1.52s/it]warmup run: 1it [00:01,  1.55s/it]warmup run: 1it [00:01,  1.53s/it]warmup run: 97it [00:01, 83.02it/s]warmup run: 96it [00:01, 80.62it/s]warmup run: 1it [00:01,  1.53s/it]warmup run: 99it [00:01, 84.04it/s]warmup run: 194it [00:01, 179.79it/s]warmup run: 193it [00:01, 176.15it/s]warmup run: 1it [00:01,  1.54s/it]warmup run: 94it [00:01, 79.80it/s]warmup run: 1it [00:01,  1.50s/it]warmup run: 197it [00:01, 180.99it/s]warmup run: 290it [00:01, 284.98it/s]warmup run: 290it [00:01, 281.57it/s]warmup run: 77it [00:01, 65.20it/s]warmup run: 187it [00:01, 171.90it/s]warmup run: 100it [00:01, 86.69it/s]warmup run: 1it [00:01,  1.48s/it]warmup run: 295it [00:01, 287.97it/s]warmup run: 389it [00:01, 398.45it/s]warmup run: 388it [00:01, 392.43it/s]warmup run: 144it [00:01, 130.68it/s]warmup run: 1it [00:01,  1.47s/it]warmup run: 281it [00:01, 274.68it/s]warmup run: 198it [00:01, 185.21it/s]warmup run: 99it [00:01, 86.75it/s]warmup run: 394it [00:01, 400.45it/s]warmup run: 490it [00:02, 512.38it/s]warmup run: 485it [00:02, 498.81it/s]warmup run: 235it [00:01, 234.92it/s]warmup run: 100it [00:01, 88.34it/s]warmup run: 375it [00:01, 380.97it/s]warmup run: 296it [00:01, 293.43it/s]warmup run: 198it [00:01, 187.48it/s]warmup run: 494it [00:02, 511.25it/s]warmup run: 591it [00:02, 617.18it/s]warmup run: 584it [00:02, 600.83it/s]warmup run: 336it [00:01, 359.00it/s]warmup run: 200it [00:01, 190.75it/s]warmup run: 468it [00:02, 482.62it/s]warmup run: 396it [00:01, 408.33it/s]warmup run: 298it [00:01, 299.10it/s]warmup run: 594it [00:02, 612.91it/s]warmup run: 693it [00:02, 709.76it/s]warmup run: 678it [00:02, 675.22it/s]warmup run: 438it [00:02, 481.47it/s]warmup run: 300it [00:01, 302.61it/s]warmup run: 569it [00:02, 592.90it/s]warmup run: 497it [00:02, 520.91it/s]warmup run: 399it [00:01, 415.18it/s]warmup run: 693it [00:02, 699.01it/s]warmup run: 795it [00:02, 786.07it/s]warmup run: 776it [00:02, 748.42it/s]warmup run: 541it [00:02, 595.59it/s]warmup run: 401it [00:01, 418.88it/s]warmup run: 671it [00:02, 690.77it/s]warmup run: 599it [00:02, 625.91it/s]warmup run: 499it [00:01, 525.30it/s]warmup run: 793it [00:02, 771.81it/s]warmup run: 896it [00:02, 844.35it/s]warmup run: 873it [00:02, 805.76it/s]warmup run: 644it [00:02, 694.85it/s]warmup run: 502it [00:01, 531.14it/s]warmup run: 772it [00:02, 769.57it/s]warmup run: 701it [00:02, 716.05it/s]warmup run: 600it [00:02, 627.69it/s]warmup run: 892it [00:02, 827.12it/s]warmup run: 997it [00:02, 887.30it/s]warmup run: 971it [00:02, 851.79it/s]warmup run: 746it [00:02, 775.05it/s]warmup run: 605it [00:02, 636.98it/s]warmup run: 872it [00:02, 829.43it/s]warmup run: 799it [00:02, 781.41it/s]warmup run: 702it [00:02, 718.12it/s]warmup run: 990it [00:02, 868.29it/s]warmup run: 1098it [00:02, 919.86it/s]warmup run: 1069it [00:02, 887.39it/s]warmup run: 849it [00:02, 840.58it/s]warmup run: 708it [00:02, 728.41it/s]warmup run: 974it [00:02, 878.89it/s]warmup run: 899it [00:02, 837.26it/s]warmup run: 804it [00:02, 793.12it/s]warmup run: 1090it [00:02, 902.55it/s]warmup run: 1200it [00:02, 945.83it/s]warmup run: 1167it [00:02, 912.73it/s]warmup run: 952it [00:02, 889.90it/s]warmup run: 810it [00:02, 799.79it/s]warmup run: 1075it [00:02, 914.49it/s]warmup run: 998it [00:02, 875.50it/s]warmup run: 905it [00:02, 849.96it/s]warmup run: 1190it [00:02, 927.83it/s]warmup run: 1301it [00:02, 960.24it/s]warmup run: 1264it [00:02, 926.69it/s]warmup run: 1054it [00:02, 924.85it/s]warmup run: 1177it [00:02, 944.17it/s]warmup run: 910it [00:02, 845.02it/s]warmup run: 1100it [00:02, 914.35it/s]warmup run: 1006it [00:02, 891.87it/s]warmup run: 1289it [00:02, 943.06it/s]warmup run: 1402it [00:02, 973.90it/s]warmup run: 1361it [00:02, 938.96it/s]warmup run: 1156it [00:02, 950.93it/s]warmup run: 1278it [00:02, 962.73it/s]warmup run: 1009it [00:02, 881.72it/s]warmup run: 1202it [00:02, 943.83it/s]warmup run: 1107it [00:02, 923.73it/s]warmup run: 1391it [00:02, 963.04it/s]warmup run: 1505it [00:03, 988.58it/s]warmup run: 1458it [00:03, 947.03it/s]warmup run: 1258it [00:02, 968.73it/s]warmup run: 1381it [00:02, 981.53it/s]warmup run: 1111it [00:02, 919.79it/s]warmup run: 1303it [00:02, 961.07it/s]warmup run: 1208it [00:02, 940.60it/s]warmup run: 1493it [00:03, 978.31it/s]warmup run: 1610it [00:03, 1004.23it/s]warmup run: 1361it [00:02, 984.61it/s]warmup run: 1484it [00:03, 993.89it/s]warmup run: 1213it [00:02, 947.53it/s]warmup run: 1405it [00:02, 975.46it/s]warmup run: 1555it [00:03, 881.55it/s]warmup run: 1308it [00:02, 948.67it/s]warmup run: 1594it [00:03, 986.95it/s]warmup run: 1714it [00:03, 1012.16it/s]warmup run: 1464it [00:03, 996.29it/s]warmup run: 1586it [00:03, 1001.16it/s]warmup run: 1315it [00:02, 967.60it/s]warmup run: 1507it [00:03, 988.05it/s]warmup run: 1654it [00:03, 911.91it/s]warmup run: 1407it [00:02, 958.24it/s]warmup run: 1696it [00:03, 995.13it/s]warmup run: 1817it [00:03, 1015.33it/s]warmup run: 1566it [00:03, 995.90it/s]warmup run: 1417it [00:02, 980.68it/s]warmup run: 1688it [00:03, 996.07it/s] warmup run: 1611it [00:03, 1001.77it/s]warmup run: 1753it [00:03, 926.05it/s]warmup run: 1506it [00:02, 967.20it/s]warmup run: 1797it [00:03, 999.36it/s]warmup run: 1920it [00:03, 1013.40it/s]warmup run: 1668it [00:03, 995.90it/s]warmup run: 1518it [00:02, 987.64it/s]warmup run: 1713it [00:03, 1004.69it/s]warmup run: 1789it [00:03, 984.44it/s]warmup run: 1855it [00:03, 951.17it/s]warmup run: 1605it [00:03, 970.85it/s]warmup run: 1898it [00:03, 1000.45it/s]warmup run: 2022it [00:03, 1005.62it/s]warmup run: 1769it [00:03, 998.88it/s]warmup run: 1620it [00:03, 994.35it/s]warmup run: 1816it [00:03, 1011.45it/s]warmup run: 1957it [00:03, 969.41it/s]warmup run: 1889it [00:03, 978.28it/s]warmup run: 1704it [00:03, 973.88it/s]warmup run: 1999it [00:03, 999.90it/s] warmup run: 2137it [00:03, 1047.29it/s]warmup run: 1871it [00:03, 1004.20it/s]warmup run: 1918it [00:03, 1012.28it/s]warmup run: 1721it [00:03, 988.81it/s]warmup run: 2066it [00:03, 1003.68it/s]warmup run: 1988it [00:03, 974.75it/s]warmup run: 1804it [00:03, 978.78it/s]warmup run: 2119it [00:03, 1058.22it/s]warmup run: 2254it [00:03, 1082.58it/s]warmup run: 1972it [00:03, 1003.55it/s]warmup run: 2023it [00:03, 1023.04it/s]warmup run: 1823it [00:03, 996.44it/s]warmup run: 2182it [00:03, 1049.10it/s]warmup run: 2106it [00:03, 1033.95it/s]warmup run: 1903it [00:03, 980.98it/s]warmup run: 2240it [00:03, 1101.54it/s]warmup run: 2372it [00:03, 1108.79it/s]warmup run: 2086it [00:03, 1044.03it/s]warmup run: 2144it [00:03, 1076.96it/s]warmup run: 1926it [00:03, 1003.85it/s]warmup run: 2299it [00:03, 1084.68it/s]warmup run: 2227it [00:03, 1084.86it/s]warmup run: 2003it [00:03, 985.19it/s]warmup run: 2361it [00:03, 1131.18it/s]warmup run: 2490it [00:03, 1127.50it/s]warmup run: 2208it [00:03, 1094.48it/s]warmup run: 2265it [00:03, 1115.73it/s]warmup run: 2033it [00:03, 1021.86it/s]warmup run: 2416it [00:03, 1109.19it/s]warmup run: 2348it [00:03, 1119.88it/s]warmup run: 2125it [00:03, 1054.52it/s]warmup run: 2482it [00:03, 1152.74it/s]warmup run: 2608it [00:04, 1142.12it/s]warmup run: 2326it [00:03, 1119.74it/s]warmup run: 2386it [00:03, 1141.51it/s]warmup run: 2154it [00:03, 1075.80it/s]warmup run: 2533it [00:04, 1125.68it/s]warmup run: 2469it [00:03, 1144.88it/s]warmup run: 2247it [00:03, 1103.52it/s]warmup run: 2603it [00:04, 1168.10it/s]warmup run: 2726it [00:04, 1151.97it/s]warmup run: 2446it [00:03, 1142.39it/s]warmup run: 2506it [00:03, 1157.15it/s]warmup run: 2275it [00:03, 1113.95it/s]warmup run: 2650it [00:04, 1136.85it/s]warmup run: 2590it [00:04, 1162.62it/s]warmup run: 2368it [00:03, 1133.02it/s]warmup run: 2724it [00:04, 1177.82it/s]warmup run: 2842it [00:04, 1146.16it/s]warmup run: 2565it [00:04, 1153.77it/s]warmup run: 2627it [00:04, 1170.12it/s]warmup run: 2393it [00:03, 1133.34it/s]warmup run: 2766it [00:04, 1141.56it/s]warmup run: 2711it [00:04, 1174.29it/s]warmup run: 2489it [00:03, 1153.34it/s]warmup run: 2844it [00:04, 1182.71it/s]warmup run: 2960it [00:04, 1156.06it/s]warmup run: 2684it [00:04, 1163.68it/s]warmup run: 3000it [00:04, 684.65it/s] warmup run: 2750it [00:04, 1187.43it/s]warmup run: 2507it [00:03, 1124.76it/s]warmup run: 2884it [00:04, 1152.18it/s]warmup run: 2830it [00:04, 1178.24it/s]warmup run: 2611it [00:04, 1171.11it/s]warmup run: 2963it [00:04, 1180.41it/s]warmup run: 2802it [00:04, 1165.82it/s]warmup run: 3000it [00:04, 683.00it/s] warmup run: 2871it [00:04, 1193.69it/s]warmup run: 2626it [00:03, 1142.33it/s]warmup run: 3000it [00:04, 666.89it/s] warmup run: 2950it [00:04, 1183.61it/s]warmup run: 2731it [00:04, 1179.42it/s]warmup run: 3000it [00:04, 680.54it/s] warmup run: 2921it [00:04, 1171.79it/s]warmup run: 2994it [00:04, 1201.85it/s]warmup run: 2745it [00:04, 1154.25it/s]warmup run: 3000it [00:04, 693.36it/s] warmup run: 2849it [00:04, 1179.06it/s]warmup run: 3000it [00:04, 677.48it/s] warmup run: 2864it [00:04, 1162.91it/s]warmup run: 2970it [00:04, 1186.59it/s]warmup run: 3000it [00:04, 692.37it/s] warmup run: 2985it [00:04, 1174.70it/s]warmup run: 3000it [00:04, 695.81it/s] 

warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]




warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   5%|▌         | 164/3000 [00:00<00:01, 1636.70it/s]warmup should be done:   6%|▌         | 165/3000 [00:00<00:01, 1644.24it/s]warmup should be done:   6%|▌         | 165/3000 [00:00<00:01, 1642.27it/s]warmup should be done:   5%|▌         | 164/3000 [00:00<00:01, 1632.09it/s]warmup should be done:   5%|▌         | 163/3000 [00:00<00:01, 1620.37it/s]warmup should be done:   6%|▌         | 166/3000 [00:00<00:01, 1651.32it/s]warmup should be done:   5%|▌         | 164/3000 [00:00<00:01, 1630.31it/s]warmup should be done:   6%|▌         | 167/3000 [00:00<00:01, 1661.11it/s]warmup should be done:  11%|█         | 328/3000 [00:00<00:01, 1637.52it/s]warmup should be done:  11%|█         | 330/3000 [00:00<00:01, 1647.47it/s]warmup should be done:  11%|█         | 329/3000 [00:00<00:01, 1642.27it/s]warmup should be done:  11%|█         | 331/3000 [00:00<00:01, 1651.57it/s]warmup should be done:  11%|█         | 327/3000 [00:00<00:01, 1630.84it/s]warmup should be done:  11%|█         | 333/3000 [00:00<00:01, 1658.80it/s]warmup should be done:  11%|█         | 329/3000 [00:00<00:01, 1638.63it/s]warmup should be done:  11%|█         | 335/3000 [00:00<00:01, 1668.39it/s]warmup should be done:  16%|█▋        | 492/3000 [00:00<00:01, 1633.04it/s]warmup should be done:  16%|█▋        | 493/3000 [00:00<00:01, 1639.18it/s]warmup should be done:  17%|█▋        | 499/3000 [00:00<00:01, 1658.61it/s]warmup should be done:  16%|█▋        | 495/3000 [00:00<00:01, 1643.23it/s]warmup should be done:  16%|█▋        | 494/3000 [00:00<00:01, 1640.14it/s]warmup should be done:  17%|█▋        | 502/3000 [00:00<00:01, 1666.33it/s]warmup should be done:  16%|█▋        | 491/3000 [00:00<00:01, 1629.02it/s]warmup should be done:  17%|█▋        | 497/3000 [00:00<00:01, 1647.04it/s]warmup should be done:  22%|██▏       | 665/3000 [00:00<00:01, 1655.88it/s]warmup should be done:  22%|██▏       | 654/3000 [00:00<00:01, 1627.25it/s]warmup should be done:  22%|██▏       | 669/3000 [00:00<00:01, 1664.79it/s]warmup should be done:  22%|██▏       | 660/3000 [00:00<00:01, 1640.10it/s]warmup should be done:  22%|██▏       | 656/3000 [00:00<00:01, 1628.36it/s]warmup should be done:  22%|██▏       | 659/3000 [00:00<00:01, 1638.52it/s]warmup should be done:  22%|██▏       | 662/3000 [00:00<00:01, 1643.06it/s]warmup should be done:  22%|██▏       | 657/3000 [00:00<00:01, 1629.91it/s]warmup should be done:  27%|██▋       | 817/3000 [00:00<00:01, 1627.36it/s]warmup should be done:  28%|██▊       | 831/3000 [00:00<00:01, 1652.47it/s]warmup should be done:  27%|██▋       | 823/3000 [00:00<00:01, 1637.03it/s]warmup should be done:  27%|██▋       | 819/3000 [00:00<00:01, 1625.23it/s]warmup should be done:  28%|██▊       | 836/3000 [00:00<00:01, 1659.64it/s]warmup should be done:  28%|██▊       | 825/3000 [00:00<00:01, 1637.46it/s]warmup should be done:  28%|██▊       | 827/3000 [00:00<00:01, 1638.77it/s]warmup should be done:  27%|██▋       | 820/3000 [00:00<00:01, 1620.50it/s]warmup should be done:  33%|███▎      | 980/3000 [00:00<00:01, 1625.50it/s]warmup should be done:  33%|███▎      | 987/3000 [00:00<00:01, 1636.21it/s]warmup should be done:  33%|███▎      | 997/3000 [00:00<00:01, 1651.34it/s]warmup should be done:  33%|███▎      | 1002/3000 [00:00<00:01, 1657.75it/s]warmup should be done:  33%|███▎      | 982/3000 [00:00<00:01, 1621.34it/s]warmup should be done:  33%|███▎      | 989/3000 [00:00<00:01, 1633.83it/s]warmup should be done:  33%|███▎      | 991/3000 [00:00<00:01, 1632.37it/s]warmup should be done:  33%|███▎      | 983/3000 [00:00<00:01, 1612.08it/s]warmup should be done:  38%|███▊      | 1143/3000 [00:00<00:01, 1621.29it/s]warmup should be done:  38%|███▊      | 1151/3000 [00:00<00:01, 1631.40it/s]warmup should be done:  39%|███▉      | 1163/3000 [00:00<00:01, 1647.82it/s]warmup should be done:  39%|███▉      | 1168/3000 [00:00<00:01, 1653.92it/s]warmup should be done:  38%|███▊      | 1145/3000 [00:00<00:01, 1619.57it/s]warmup should be done:  38%|███▊      | 1153/3000 [00:00<00:01, 1629.08it/s]warmup should be done:  38%|███▊      | 1155/3000 [00:00<00:01, 1626.61it/s]warmup should be done:  38%|███▊      | 1145/3000 [00:00<00:01, 1604.08it/s]warmup should be done:  44%|████▎     | 1306/3000 [00:00<00:01, 1620.03it/s]warmup should be done:  44%|████▍     | 1334/3000 [00:00<00:01, 1653.47it/s]warmup should be done:  44%|████▍     | 1315/3000 [00:00<00:01, 1629.68it/s]warmup should be done:  44%|████▎     | 1308/3000 [00:00<00:01, 1620.02it/s]warmup should be done:  44%|████▍     | 1316/3000 [00:00<00:01, 1628.35it/s]warmup should be done:  44%|████▍     | 1328/3000 [00:00<00:01, 1636.08it/s]warmup should be done:  44%|████▍     | 1318/3000 [00:00<00:01, 1624.65it/s]warmup should be done:  44%|████▎     | 1306/3000 [00:00<00:01, 1600.31it/s]warmup should be done:  49%|████▉     | 1478/3000 [00:00<00:00, 1627.19it/s]warmup should be done:  50%|█████     | 1500/3000 [00:00<00:00, 1652.12it/s]warmup should be done:  49%|████▉     | 1469/3000 [00:00<00:00, 1616.91it/s]warmup should be done:  49%|████▉     | 1471/3000 [00:00<00:00, 1619.95it/s]warmup should be done:  49%|████▉     | 1479/3000 [00:00<00:00, 1626.13it/s]warmup should be done:  50%|████▉     | 1493/3000 [00:00<00:00, 1638.52it/s]warmup should be done:  49%|████▉     | 1481/3000 [00:00<00:00, 1622.85it/s]warmup should be done:  49%|████▉     | 1467/3000 [00:00<00:00, 1602.04it/s]warmup should be done:  56%|█████▌    | 1666/3000 [00:01<00:00, 1654.33it/s]warmup should be done:  55%|█████▍    | 1641/3000 [00:01<00:00, 1625.32it/s]warmup should be done:  54%|█████▍    | 1631/3000 [00:01<00:00, 1616.07it/s]warmup should be done:  54%|█████▍    | 1633/3000 [00:01<00:00, 1618.17it/s]warmup should be done:  55%|█████▍    | 1642/3000 [00:01<00:00, 1623.57it/s]warmup should be done:  55%|█████▌    | 1658/3000 [00:01<00:00, 1640.06it/s]warmup should be done:  55%|█████▍    | 1644/3000 [00:01<00:00, 1621.75it/s]warmup should be done:  54%|█████▍    | 1629/3000 [00:01<00:00, 1605.28it/s]warmup should be done:  61%|██████    | 1832/3000 [00:01<00:00, 1655.37it/s]warmup should be done:  60%|██████    | 1805/3000 [00:01<00:00, 1626.84it/s]warmup should be done:  60%|█████▉    | 1794/3000 [00:01<00:00, 1617.22it/s]warmup should be done:  60%|█████▉    | 1795/3000 [00:01<00:00, 1613.70it/s]warmup should be done:  60%|██████    | 1805/3000 [00:01<00:00, 1621.27it/s]warmup should be done:  61%|██████    | 1823/3000 [00:01<00:00, 1638.12it/s]warmup should be done:  60%|██████    | 1807/3000 [00:01<00:00, 1621.91it/s]warmup should be done:  60%|█████▉    | 1790/3000 [00:01<00:00, 1605.28it/s]warmup should be done:  67%|██████▋   | 1998/3000 [00:01<00:00, 1655.85it/s]warmup should be done:  66%|██████▌   | 1968/3000 [00:01<00:00, 1627.19it/s]warmup should be done:  65%|██████▌   | 1957/3000 [00:01<00:00, 1618.25it/s]warmup should be done:  66%|██████▌   | 1968/3000 [00:01<00:00, 1621.31it/s]warmup should be done:  65%|██████▌   | 1957/3000 [00:01<00:00, 1609.59it/s]warmup should be done:  66%|██████▌   | 1987/3000 [00:01<00:00, 1635.40it/s]warmup should be done:  66%|██████▌   | 1970/3000 [00:01<00:00, 1622.06it/s]warmup should be done:  65%|██████▌   | 1951/3000 [00:01<00:00, 1605.95it/s]warmup should be done:  72%|███████▏  | 2164/3000 [00:01<00:00, 1655.98it/s]warmup should be done:  71%|███████   | 2132/3000 [00:01<00:00, 1628.25it/s]warmup should be done:  71%|███████   | 2120/3000 [00:01<00:00, 1618.68it/s]warmup should be done:  71%|███████   | 2119/3000 [00:01<00:00, 1611.25it/s]warmup should be done:  71%|███████   | 2131/3000 [00:01<00:00, 1618.73it/s]warmup should be done:  71%|███████   | 2133/3000 [00:01<00:00, 1622.91it/s]warmup should be done:  72%|███████▏  | 2151/3000 [00:01<00:00, 1629.72it/s]warmup should be done:  70%|███████   | 2113/3000 [00:01<00:00, 1607.29it/s]warmup should be done:  78%|███████▊  | 2330/3000 [00:01<00:00, 1656.01it/s]warmup should be done:  76%|███████▋  | 2295/3000 [00:01<00:00, 1626.67it/s]warmup should be done:  76%|███████▌  | 2282/3000 [00:01<00:00, 1617.09it/s]warmup should be done:  76%|███████▌  | 2281/3000 [00:01<00:00, 1611.87it/s]warmup should be done:  76%|███████▋  | 2293/3000 [00:01<00:00, 1616.61it/s]warmup should be done:  77%|███████▋  | 2297/3000 [00:01<00:00, 1626.26it/s]warmup should be done:  76%|███████▌  | 2274/3000 [00:01<00:00, 1607.97it/s]warmup should be done:  77%|███████▋  | 2314/3000 [00:01<00:00, 1625.68it/s]warmup should be done:  83%|████████▎ | 2496/3000 [00:01<00:00, 1649.71it/s]warmup should be done:  82%|████████▏ | 2458/3000 [00:01<00:00, 1624.24it/s]warmup should be done:  81%|████████▏ | 2444/3000 [00:01<00:00, 1615.13it/s]warmup should be done:  81%|████████▏ | 2443/3000 [00:01<00:00, 1609.54it/s]warmup should be done:  82%|████████▏ | 2460/3000 [00:01<00:00, 1626.77it/s]warmup should be done:  82%|████████▏ | 2455/3000 [00:01<00:00, 1614.19it/s]warmup should be done:  81%|████████  | 2435/3000 [00:01<00:00, 1604.89it/s]warmup should be done:  83%|████████▎ | 2477/3000 [00:01<00:00, 1621.05it/s]warmup should be done:  89%|████████▊ | 2661/3000 [00:01<00:00, 1645.99it/s]warmup should be done:  87%|████████▋ | 2621/3000 [00:01<00:00, 1624.85it/s]warmup should be done:  87%|████████▋ | 2606/3000 [00:01<00:00, 1615.31it/s]warmup should be done:  87%|████████▋ | 2618/3000 [00:01<00:00, 1618.76it/s]warmup should be done:  87%|████████▋ | 2624/3000 [00:01<00:00, 1628.55it/s]warmup should be done:  87%|████████▋ | 2605/3000 [00:01<00:00, 1609.97it/s]warmup should be done:  87%|████████▋ | 2597/3000 [00:01<00:00, 1607.64it/s]warmup should be done:  88%|████████▊ | 2640/3000 [00:01<00:00, 1621.64it/s]warmup should be done:  93%|█████████▎| 2784/3000 [00:01<00:00, 1626.28it/s]warmup should be done:  94%|█████████▍| 2826/3000 [00:01<00:00, 1645.26it/s]warmup should be done:  92%|█████████▏| 2768/3000 [00:01<00:00, 1616.53it/s]warmup should be done:  93%|█████████▎| 2781/3000 [00:01<00:00, 1622.00it/s]warmup should be done:  93%|█████████▎| 2788/3000 [00:01<00:00, 1631.27it/s]warmup should be done:  92%|█████████▏| 2767/3000 [00:01<00:00, 1611.06it/s]warmup should be done:  92%|█████████▏| 2759/3000 [00:01<00:00, 1610.33it/s]warmup should be done:  93%|█████████▎| 2803/3000 [00:01<00:00, 1622.18it/s]warmup should be done:  98%|█████████▊| 2949/3000 [00:01<00:00, 1631.92it/s]warmup should be done: 100%|█████████▉| 2992/3000 [00:01<00:00, 1647.58it/s]warmup should be done:  98%|█████████▊| 2932/3000 [00:01<00:00, 1622.72it/s]warmup should be done:  98%|█████████▊| 2946/3000 [00:01<00:00, 1628.21it/s]warmup should be done:  98%|█████████▊| 2955/3000 [00:01<00:00, 1640.36it/s]warmup should be done:  98%|█████████▊| 2932/3000 [00:01<00:00, 1620.14it/s]warmup should be done:  97%|█████████▋| 2921/3000 [00:01<00:00, 1612.14it/s]warmup should be done:  99%|█████████▉| 2968/3000 [00:01<00:00, 1628.62it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1653.50it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1635.91it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1631.23it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1630.29it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1626.79it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1620.36it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1618.00it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1611.04it/s]




warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]

warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   6%|▌         | 168/3000 [00:00<00:01, 1675.85it/s]warmup should be done:   6%|▌         | 169/3000 [00:00<00:01, 1687.29it/s]warmup should be done:   6%|▌         | 167/3000 [00:00<00:01, 1666.66it/s]warmup should be done:   6%|▌         | 167/3000 [00:00<00:01, 1666.05it/s]warmup should be done:   5%|▌         | 164/3000 [00:00<00:01, 1634.44it/s]warmup should be done:   6%|▌         | 166/3000 [00:00<00:01, 1651.10it/s]warmup should be done:   6%|▌         | 171/3000 [00:00<00:01, 1700.78it/s]warmup should be done:   6%|▌         | 168/3000 [00:00<00:01, 1671.63it/s]warmup should be done:  11%|█         | 333/3000 [00:00<00:01, 1661.99it/s]warmup should be done:  11%|█         | 336/3000 [00:00<00:01, 1676.45it/s]warmup should be done:  11%|█         | 336/3000 [00:00<00:01, 1673.59it/s]warmup should be done:  11%|█▏        | 338/3000 [00:00<00:01, 1683.06it/s]warmup should be done:  11%|█         | 334/3000 [00:00<00:01, 1663.26it/s]warmup should be done:  11%|█         | 334/3000 [00:00<00:01, 1663.32it/s]warmup should be done:  11%|█▏        | 343/3000 [00:00<00:01, 1706.20it/s]warmup should be done:  11%|█         | 328/3000 [00:00<00:01, 1624.09it/s]warmup should be done:  17%|█▋        | 500/3000 [00:00<00:01, 1665.42it/s]warmup should be done:  17%|█▋        | 504/3000 [00:00<00:01, 1675.78it/s]warmup should be done:  17%|█▋        | 505/3000 [00:00<00:01, 1679.78it/s]warmup should be done:  17%|█▋        | 501/3000 [00:00<00:01, 1664.62it/s]warmup should be done:  17%|█▋        | 501/3000 [00:00<00:01, 1662.33it/s]warmup should be done:  17%|█▋        | 515/3000 [00:00<00:01, 1709.15it/s]warmup should be done:  17%|█▋        | 507/3000 [00:00<00:01, 1678.67it/s]warmup should be done:  16%|█▋        | 491/3000 [00:00<00:01, 1616.84it/s]warmup should be done:  22%|██▏       | 673/3000 [00:00<00:01, 1679.61it/s]warmup should be done:  22%|██▏       | 667/3000 [00:00<00:01, 1663.13it/s]warmup should be done:  22%|██▏       | 669/3000 [00:00<00:01, 1667.93it/s]warmup should be done:  22%|██▏       | 669/3000 [00:00<00:01, 1667.87it/s]warmup should be done:  23%|██▎       | 687/3000 [00:00<00:01, 1711.42it/s]warmup should be done:  23%|██▎       | 676/3000 [00:00<00:01, 1682.53it/s]warmup should be done:  22%|██▏       | 672/3000 [00:00<00:01, 1666.01it/s]warmup should be done:  22%|██▏       | 653/3000 [00:00<00:01, 1606.20it/s]warmup should be done:  28%|██▊       | 834/3000 [00:00<00:01, 1663.87it/s]warmup should be done:  28%|██▊       | 842/3000 [00:00<00:01, 1680.54it/s]warmup should be done:  28%|██▊       | 837/3000 [00:00<00:01, 1671.12it/s]warmup should be done:  28%|██▊       | 846/3000 [00:00<00:01, 1688.47it/s]warmup should be done:  28%|██▊       | 837/3000 [00:00<00:01, 1668.78it/s]warmup should be done:  28%|██▊       | 840/3000 [00:00<00:01, 1670.14it/s]warmup should be done:  29%|██▊       | 859/3000 [00:00<00:01, 1675.10it/s]warmup should be done:  27%|██▋       | 814/3000 [00:00<00:01, 1587.03it/s]warmup should be done:  33%|███▎      | 1001/3000 [00:00<00:01, 1665.32it/s]warmup should be done:  34%|███▎      | 1011/3000 [00:00<00:01, 1682.36it/s]warmup should be done:  34%|███▎      | 1005/3000 [00:00<00:01, 1673.28it/s]warmup should be done:  34%|███▍      | 1015/3000 [00:00<00:01, 1686.86it/s]warmup should be done:  33%|███▎      | 1004/3000 [00:00<00:01, 1666.64it/s]warmup should be done:  34%|███▎      | 1008/3000 [00:00<00:01, 1669.80it/s]warmup should be done:  32%|███▏      | 973/3000 [00:00<00:01, 1568.83it/s]warmup should be done:  34%|███▍      | 1027/3000 [00:00<00:01, 1646.05it/s]warmup should be done:  39%|███▉      | 1168/3000 [00:00<00:01, 1664.37it/s]warmup should be done:  39%|███▉      | 1173/3000 [00:00<00:01, 1673.25it/s]warmup should be done:  39%|███▉      | 1180/3000 [00:00<00:01, 1681.27it/s]warmup should be done:  39%|███▉      | 1171/3000 [00:00<00:01, 1665.30it/s]warmup should be done:  39%|███▉      | 1184/3000 [00:00<00:01, 1683.43it/s]warmup should be done:  39%|███▉      | 1175/3000 [00:00<00:01, 1668.07it/s]warmup should be done:  38%|███▊      | 1130/3000 [00:00<00:01, 1561.59it/s]warmup should be done:  40%|███▉      | 1192/3000 [00:00<00:01, 1627.36it/s]warmup should be done:  44%|████▍     | 1335/3000 [00:00<00:00, 1665.15it/s]warmup should be done:  45%|████▍     | 1349/3000 [00:00<00:00, 1681.38it/s]warmup should be done:  45%|████▍     | 1341/3000 [00:00<00:00, 1671.45it/s]warmup should be done:  45%|████▍     | 1338/3000 [00:00<00:00, 1665.74it/s]warmup should be done:  45%|████▌     | 1353/3000 [00:00<00:00, 1683.87it/s]warmup should be done:  45%|████▍     | 1343/3000 [00:00<00:00, 1669.81it/s]warmup should be done:  43%|████▎     | 1287/3000 [00:00<00:01, 1550.80it/s]warmup should be done:  45%|████▌     | 1355/3000 [00:00<00:01, 1618.79it/s]warmup should be done:  50%|█████     | 1502/3000 [00:00<00:00, 1665.68it/s]warmup should be done:  50%|█████     | 1505/3000 [00:00<00:00, 1665.13it/s]warmup should be done:  51%|█████     | 1518/3000 [00:00<00:00, 1680.33it/s]warmup should be done:  50%|█████     | 1509/3000 [00:00<00:00, 1669.29it/s]warmup should be done:  51%|█████     | 1522/3000 [00:00<00:00, 1680.75it/s]warmup should be done:  50%|█████     | 1511/3000 [00:00<00:00, 1670.87it/s]warmup should be done:  48%|████▊     | 1449/3000 [00:00<00:00, 1569.42it/s]warmup should be done:  51%|█████     | 1522/3000 [00:00<00:00, 1633.62it/s]warmup should be done:  56%|█████▌    | 1670/3000 [00:01<00:00, 1667.90it/s]warmup should be done:  56%|█████▌    | 1672/3000 [00:01<00:00, 1666.39it/s]warmup should be done:  56%|█████▌    | 1677/3000 [00:01<00:00, 1670.87it/s]warmup should be done:  56%|█████▌    | 1687/3000 [00:01<00:00, 1679.11it/s]warmup should be done:  56%|█████▌    | 1679/3000 [00:01<00:00, 1672.95it/s]warmup should be done:  56%|█████▋    | 1691/3000 [00:01<00:00, 1679.23it/s]warmup should be done:  54%|█████▍    | 1615/3000 [00:01<00:00, 1596.85it/s]warmup should be done:  56%|█████▋    | 1694/3000 [00:01<00:00, 1657.24it/s]warmup should be done:  61%|██████    | 1837/3000 [00:01<00:00, 1668.02it/s]warmup should be done:  61%|██████▏   | 1839/3000 [00:01<00:00, 1667.33it/s]warmup should be done:  62%|██████▏   | 1855/3000 [00:01<00:00, 1678.46it/s]warmup should be done:  62%|██████▏   | 1845/3000 [00:01<00:00, 1672.20it/s]warmup should be done:  62%|██████▏   | 1847/3000 [00:01<00:00, 1673.32it/s]warmup should be done:  62%|██████▏   | 1860/3000 [00:01<00:00, 1679.93it/s]warmup should be done:  59%|█████▉    | 1780/3000 [00:01<00:00, 1612.40it/s]warmup should be done:  62%|██████▏   | 1865/3000 [00:01<00:00, 1671.57it/s]warmup should be done:  67%|██████▋   | 2004/3000 [00:01<00:00, 1665.43it/s]warmup should be done:  67%|██████▋   | 2006/3000 [00:01<00:00, 1667.61it/s]warmup should be done:  67%|██████▋   | 2024/3000 [00:01<00:00, 1679.78it/s]warmup should be done:  67%|██████▋   | 2013/3000 [00:01<00:00, 1671.43it/s]warmup should be done:  68%|██████▊   | 2029/3000 [00:01<00:00, 1680.16it/s]warmup should be done:  67%|██████▋   | 2015/3000 [00:01<00:00, 1666.36it/s]warmup should be done:  65%|██████▍   | 1945/3000 [00:01<00:00, 1620.94it/s]warmup should be done:  68%|██████▊   | 2035/3000 [00:01<00:00, 1677.21it/s]warmup should be done:  72%|███████▏  | 2172/3000 [00:01<00:00, 1669.47it/s]warmup should be done:  72%|███████▏  | 2173/3000 [00:01<00:00, 1667.13it/s]warmup should be done:  73%|███████▎  | 2192/3000 [00:01<00:00, 1678.71it/s]warmup should be done:  73%|███████▎  | 2181/3000 [00:01<00:00, 1668.86it/s]warmup should be done:  73%|███████▎  | 2182/3000 [00:01<00:00, 1666.92it/s]warmup should be done:  73%|███████▎  | 2198/3000 [00:01<00:00, 1678.07it/s]warmup should be done:  70%|███████   | 2111/3000 [00:01<00:00, 1630.00it/s]warmup should be done:  73%|███████▎  | 2204/3000 [00:01<00:00, 1679.15it/s]warmup should be done:  78%|███████▊  | 2341/3000 [00:01<00:00, 1674.07it/s]warmup should be done:  78%|███████▊  | 2341/3000 [00:01<00:00, 1667.98it/s]warmup should be done:  79%|███████▊  | 2361/3000 [00:01<00:00, 1679.47it/s]warmup should be done:  78%|███████▊  | 2349/3000 [00:01<00:00, 1669.86it/s]warmup should be done:  78%|███████▊  | 2350/3000 [00:01<00:00, 1670.15it/s]warmup should be done:  79%|███████▉  | 2366/3000 [00:01<00:00, 1677.70it/s]warmup should be done:  76%|███████▌  | 2277/3000 [00:01<00:00, 1637.27it/s]warmup should be done:  79%|███████▉  | 2373/3000 [00:01<00:00, 1681.73it/s]warmup should be done:  84%|████████▎ | 2509/3000 [00:01<00:00, 1673.85it/s]warmup should be done:  84%|████████▎ | 2509/3000 [00:01<00:00, 1669.66it/s]warmup should be done:  84%|████████▍ | 2530/3000 [00:01<00:00, 1681.91it/s]warmup should be done:  84%|████████▍ | 2517/3000 [00:01<00:00, 1671.67it/s]warmup should be done:  84%|████████▍ | 2519/3000 [00:01<00:00, 1673.76it/s]warmup should be done:  84%|████████▍ | 2535/3000 [00:01<00:00, 1679.34it/s]warmup should be done:  81%|████████▏ | 2443/3000 [00:01<00:00, 1643.48it/s]warmup should be done:  85%|████████▍ | 2543/3000 [00:01<00:00, 1685.65it/s]warmup should be done:  89%|████████▉ | 2677/3000 [00:01<00:00, 1669.92it/s]warmup should be done:  89%|████████▉ | 2677/3000 [00:01<00:00, 1669.85it/s]warmup should be done:  90%|████████▉ | 2699/3000 [00:01<00:00, 1682.60it/s]warmup should be done:  90%|████████▉ | 2685/3000 [00:01<00:00, 1670.68it/s]warmup should be done:  90%|████████▉ | 2687/3000 [00:01<00:00, 1674.44it/s]warmup should be done:  90%|█████████ | 2704/3000 [00:01<00:00, 1679.97it/s]warmup should be done:  87%|████████▋ | 2610/3000 [00:01<00:00, 1648.70it/s]warmup should be done:  90%|█████████ | 2713/3000 [00:01<00:00, 1688.41it/s]warmup should be done:  95%|█████████▍| 2844/3000 [00:01<00:00, 1667.04it/s]warmup should be done:  95%|█████████▍| 2844/3000 [00:01<00:00, 1665.52it/s]warmup should be done:  96%|█████████▌| 2868/3000 [00:01<00:00, 1681.07it/s]warmup should be done:  95%|█████████▌| 2853/3000 [00:01<00:00, 1671.30it/s]warmup should be done:  95%|█████████▌| 2855/3000 [00:01<00:00, 1671.05it/s]warmup should be done:  96%|█████████▌| 2872/3000 [00:01<00:00, 1677.68it/s]warmup should be done:  93%|█████████▎| 2777/3000 [00:01<00:00, 1654.52it/s]warmup should be done:  96%|█████████▌| 2882/3000 [00:01<00:00, 1687.26it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1680.53it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1680.34it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1673.46it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1671.30it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1671.17it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1667.01it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1666.82it/s]warmup should be done:  98%|█████████▊| 2945/3000 [00:01<00:00, 1659.76it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1618.49it/s]
WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f802ebbee80>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f802ebbf730>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f802ebc1d30>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f802e87b190>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f802e88a2b0>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f802e88b190>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f802e87b0d0>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f802e88a0d0>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
2022-12-12 07:07:18.057219: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f7b6e8307c0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 07:07:18.057284: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 07:07:18.066932: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 07:07:18.763912: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f7b6e831350 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 07:07:18.763978: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 07:07:18.767524: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f7b6e831570 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 07:07:18.767585: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 07:07:18.773711: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 07:07:18.776967: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 07:07:18.906354: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f7b6ef97100 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 07:07:18.906410: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 07:07:18.916632: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 07:07:19.031784: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f7b628309f0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 07:07:19.031846: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 07:07:19.032137: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f7b628303e0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 07:07:19.032196: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 07:07:19.039960: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 07:07:19.040260: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 07:07:19.053878: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f7b668383a0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 07:07:19.053938: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 07:07:19.061459: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 07:07:19.066480: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f7b6e8347b0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 07:07:19.066535: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 07:07:19.076314: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 07:07:25.277965: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 07:07:25.803786: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 07:07:25.879174: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 07:07:25.884636: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 07:07:26.044401: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 07:07:26.095245: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 07:07:26.124304: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 07:07:26.125858: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
[HCTR][07:08:15.461][ERROR][RK0][main]: replica 5 reaches 1000, calling init pre replica
[HCTR][07:08:15.461][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][07:08:15.463][ERROR][RK0][tid #140168690374400]: replica 7 reaches 1000, calling init pre replica
[HCTR][07:08:15.464][ERROR][RK0][tid #140168690374400]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][07:08:15.469][ERROR][RK0][tid #140168690374400]: coll ps creation done
[HCTR][07:08:15.469][ERROR][RK0][tid #140168690374400]: replica 7 waits for coll ps creation barrier
[HCTR][07:08:15.470][ERROR][RK0][main]: coll ps creation done
[HCTR][07:08:15.470][ERROR][RK0][main]: replica 5 waits for coll ps creation barrier
[HCTR][07:08:15.805][ERROR][RK0][main]: replica 1 reaches 1000, calling init pre replica
[HCTR][07:08:15.806][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][07:08:15.813][ERROR][RK0][main]: coll ps creation done
[HCTR][07:08:15.813][ERROR][RK0][main]: replica 1 waits for coll ps creation barrier
[HCTR][07:08:15.882][ERROR][RK0][tid #140168631658240]: replica 4 reaches 1000, calling init pre replica
[HCTR][07:08:15.882][ERROR][RK0][tid #140168631658240]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][07:08:15.887][ERROR][RK0][tid #140168631658240]: coll ps creation done
[HCTR][07:08:15.887][ERROR][RK0][tid #140168631658240]: replica 4 waits for coll ps creation barrier
[HCTR][07:08:15.892][ERROR][RK0][tid #140168564549376]: replica 6 reaches 1000, calling init pre replica
[HCTR][07:08:15.892][ERROR][RK0][tid #140168564549376]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][07:08:15.897][ERROR][RK0][tid #140168564549376]: coll ps creation done
[HCTR][07:08:15.897][ERROR][RK0][tid #140168564549376]: replica 6 waits for coll ps creation barrier
[HCTR][07:08:15.924][ERROR][RK0][tid #140168623265536]: replica 3 reaches 1000, calling init pre replica
[HCTR][07:08:15.924][ERROR][RK0][tid #140168623265536]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][07:08:15.930][ERROR][RK0][tid #140168623265536]: coll ps creation done
[HCTR][07:08:15.930][ERROR][RK0][tid #140168623265536]: replica 3 waits for coll ps creation barrier
[HCTR][07:08:15.972][ERROR][RK0][main]: replica 0 reaches 1000, calling init pre replica
[HCTR][07:08:15.972][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][07:08:15.977][ERROR][RK0][main]: coll ps creation done
[HCTR][07:08:15.977][ERROR][RK0][main]: replica 0 waits for coll ps creation barrier
[HCTR][07:08:16.019][ERROR][RK0][main]: replica 2 reaches 1000, calling init pre replica
[HCTR][07:08:16.019][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][07:08:16.026][ERROR][RK0][main]: coll ps creation done
[HCTR][07:08:16.026][ERROR][RK0][main]: replica 2 waits for coll ps creation barrier
[HCTR][07:08:16.026][ERROR][RK0][main]: replica 0 preparing frequency
[HCTR][07:08:16.864][ERROR][RK0][main]: replica 0 preparing frequency done
[HCTR][07:08:16.918][ERROR][RK0][main]: replica 0 calling init per replica
[HCTR][07:08:16.918][ERROR][RK0][main]: replica 1 calling init per replica
[HCTR][07:08:16.918][ERROR][RK0][tid #140168690374400]: replica 7 calling init per replica
[HCTR][07:08:16.918][ERROR][RK0][main]: replica 5 calling init per replica
[HCTR][07:08:16.918][ERROR][RK0][tid #140168631658240]: replica 4 calling init per replica
[HCTR][07:08:16.918][ERROR][RK0][tid #140168623265536]: replica 3 calling init per replica
[HCTR][07:08:16.918][ERROR][RK0][main]: replica 2 calling init per replica
[HCTR][07:08:16.918][ERROR][RK0][tid #140168564549376]: replica 6 calling init per replica
[HCTR][07:08:16.918][ERROR][RK0][main]: Calling build_v2
[HCTR][07:08:16.918][ERROR][RK0][main]: Calling build_v2
[HCTR][07:08:16.918][ERROR][RK0][tid #140168690374400]: Calling build_v2
[HCTR][07:08:16.918][ERROR][RK0][main]: Calling build_v2
[HCTR][07:08:16.918][ERROR][RK0][tid #140168631658240]: Calling build_v2
[HCTR][07:08:16.918][ERROR][RK0][tid #140168623265536]: Calling build_v2
[HCTR][07:08:16.918][ERROR][RK0][main]: Calling build_v2
[HCTR][07:08:16.918][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][07:08:16.918][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][07:08:16.918][ERROR][RK0][tid #140168564549376]: Calling build_v2
[HCTR][07:08:16.918][ERROR][RK0][tid #140168690374400]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][07:08:16.918][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][07:08:16.918][ERROR][RK0][tid #140168631658240]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][07:08:16.918][ERROR][RK0][tid #140168623265536]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][07:08:16.918][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][07:08:16.918][ERROR][RK0][tid #140168564549376]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[[[[[[2022-12-12 07:08:162022-12-12 07:08:162022-12-12 07:08:16.2022-12-12 07:08:16[2022-12-12 07:08:16.2022-12-12 07:08:16.918918[..918919.9189182022-12-12 07:08:16: 918923918918: 918924: .E: : E: E2022-12-12 07:08:16918944 EE E .: /hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc  /hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc /hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc918966E:/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc:/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc::  136::136:136E/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc] 136136] 136]  :using concurrent impl MPS] ] using concurrent impl MPS] using concurrent impl MPS/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc136
using concurrent impl MPSusing concurrent impl MPS
using concurrent impl MPS
:] 


136using concurrent impl MPS] 
using concurrent impl MPS
[2022-12-12 07:08:16.923227: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:178] v100x8, slow pcie
[2022-12-12 07:08:16.923265: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:196[] 2022-12-12 07:08:16assigning 8 to cpu.
923276: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:178] v100x8, slow pcie
[2022-12-12 07:08:16.923319: E [/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[2022-12-12 07:08:16:2022-12-12 07:08:16.196.923324] 923334: assigning 8 to cpu: E
E  /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc::178[212] 2022-12-12 07:08:16] v100x8, slow pcie.build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8
923372[
: 2022-12-12 07:08:16[E[[.2022-12-12 07:08:16 2022-12-12 07:08:162022-12-12 07:08:16923398./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc..: 923407:923418923425E: 178: : [ E] EE2022-12-12 07:08:16/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc v100x8, slow pcie  .:/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc923468212:::: [] 196178213E2022-12-12 07:08:16build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8] ] ]  .
[assigning 8 to cpuv100x8, slow pcieremote time is 8.68421/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc923520[2022-12-12 07:08:16


:: 2022-12-12 07:08:16.178E.[923534[: ] [ 9235692022-12-12 07:08:162022-12-12 07:08:16E[v100x8, slow pcie2022-12-12 07:08:16/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: .. 2022-12-12 07:08:16
.:E923585923588/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.923583[196 : : :923610: 2022-12-12 07:08:16] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccEE178: E.assigning 8 to cpu:  ] E 923661
213/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc v100x8, slow pcie/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: ] ::/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
:Eremote time is 8.68421196214:[178 
[] ] 2122022-12-12 07:08:16] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 07:08:16assigning 8 to cpu[cpu time is 97.0588] .v100x8, slow pcie:196.
2022-12-12 07:08:16
build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8923784
] 923790.
: assigning 8 to cpu[: 923817[E
2022-12-12 07:08:16[: 2022-12-12 07:08:16.E .2022-12-12 07:08:16E923892 /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc923881. : [/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:: 923902/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE2022-12-12 07:08:16:212E: : .196]  E214/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc923947] build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc ] :: assigning 8 to cpu
:/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cccpu time is 97.0588212E
196:
] [ ] 213build asymm link desc with 8X Tesla V100-SXM2-32GB out of 82022-12-12 07:08:16/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccassigning 8 to cpu] 
.:
remote time is 8.68421924058212[
: ] 2022-12-12 07:08:16[Ebuild asymm link desc with 8X Tesla V100-SXM2-32GB out of 8[.2022-12-12 07:08:16 
2022-12-12 07:08:16924112./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.: :924116924127[E[213: : 2022-12-12 07:08:16 2022-12-12 07:08:16] EE./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.remote time is 8.68421  /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc924154:924155
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:: 213: :[212E] E2142022-12-12 07:08:16]  remote time is 8.68421 ] .build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cccpu time is 97.0588924238
::
: [212213E2022-12-12 07:08:16] ] [ .build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8remote time is 8.68421/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 07:08:16924303

:.: 214924327E[] :  [2022-12-12 07:08:16cpu time is 97.0588E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 07:08:16.
 :.924390/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc214924396: :] : E213cpu time is 97.0588E ] 
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccremote time is 8.68421/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:
:214213] ] cpu time is 97.0588[remote time is 8.68421
2022-12-12 07:08:16
.924532: [E2022-12-12 07:08:16 ./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc924572:: 214E]  cpu time is 97.0588/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
:214] cpu time is 97.0588
[2022-12-12 07:09:34.354199: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:118] solver created. now build & solve
[2022-12-12 07:09:34.394218: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 381.47 MB
[2022-12-12 07:09:34.394304: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 381.47 MB
[2022-12-12 07:09:34.395516: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:74] mapping nid to rank...
[2022-12-12 07:09:34.465597: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:91] counting slots...
[2022-12-12 07:09:34.854885: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:105] Final num slot is 49
[2022-12-12 07:09:34.854979: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:109] counting blocks...
[2022-12-12 07:09:41.395875: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:118] Final num block is 1023
[2022-12-12 07:09:41.395978: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:123] counting freq and density...
[2022-12-12 07:09:43.158288: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:154] averaging freq and density...
[2022-12-12 07:09:43.158380: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:155] 1023
[2022-12-12 07:09:43.161146: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:123] solver built. now solve
[2022-12-12 07:09:43.161205: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:354] constructing optimal solver, device=8, stream=1
1023 blocks, 8 devices
[2022-12-12 07:09:43.427544: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:527] Add Var...
[2022-12-12 07:09:43.456051: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:545] Capacity...
[2022-12-12 07:09:43.457469: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:548] Connect CPU...
[2022-12-12 07:09:43.477467: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:550] Connect Access To Storage...
[2022-12-12 07:09:43.990021: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:554] Time...
[2022-12-12 07:12:56. 50656: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:569] Coll Cache init block placement array
[2022-12-12 07:12:56. 60541: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:645] Coll Cache init block placement array done
[2022-12-12 07:12:56. 63583: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:647] Coll Cache model reset done
[2022-12-12 07:12:56.108179: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:127] solver solved
[2022-12-12 07:12:56.108280: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:195] 0 solved master
[2022-12-12 07:12:56.108323: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 0 solved
[2022-12-12 07:12:56.108351: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 0 initing device 0
[2022-12-12 07:12:56.108959: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-12 07:12:56.109011: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 07:12:56.109972: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 07:12:56.110630: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 07:12:56.123677: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 4 solved
[2022-12-12 07:12:56.123752: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 4 initing device 4
[[2022-12-12 07:12:562022-12-12 07:12:56..123886123886: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc::202202] ] 7 solved2 solved

[[2022-12-12 07:12:562022-12-12 07:12:56..123974123975: : EE[  2022-12-12 07:12:56/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc.::123981205205: ] ] Eworker 0 thread 7 initing device 7worker 0 thread 2 initing device 2 

/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 5 solved
[2022-12-12 07:12:56.124069: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 5 initing device 5
[2022-12-12 07:12:56.124201: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-12 07:12:56.124251: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 07:12:56.124448: [E2022-12-12 07:12:56 ./hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu124457:: 1815E]  Building Coll Cache with ... num gpu device is 8/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu
:[18152022-12-12 07:12:56] .Building Coll Cache with ... num gpu device is 8124489
[: 2022-12-12 07:12:56E. 124508/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu: [:E2022-12-12 07:12:561815 .] /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu124525Building Coll Cache with ... num gpu device is 8:: 
1980E]  eager alloc mem 381.47 MB/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu
:1980[] 2022-12-12 07:12:56eager alloc mem 381.47 MB.
124570: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 07:12:56.124993: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 6 solved
[2022-12-12 07:12:56.125052: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 6 initing device 6
[2022-12-12 07:12:56.125473: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-12 07:12:56.125513: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 07:12:56.127467: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 07:12:56.127512: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 07:12:56.127646: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 07:12:56.127703: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 07:12:56.128206: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 07:12:56.128344: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 1 solved
[2022-12-12 07:12:56.128397: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 1 initing device 1
[2022-12-12 07:12:56.128420: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 3 solved
[2022-12-12 07:12:56.128500: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 3 initing device 3
[2022-12-12 07:12:56.128790: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-12 07:12:56.128832: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 07:12:56.128957: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-12 07:12:56.129009: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 07:12:56.131560: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 07:12:56.131669: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 07:12:56.131790: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 07:12:56.131941: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 07:12:56.131996: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 07:12:56.132935: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 07:12:56.133456: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 07:12:56.136241: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 07:12:56.136312: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 07:12:56.188532: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1023.00 Bytes
[2022-12-12 07:12:56.193734: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1023
[2022-12-12 07:12:56.193827: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 07:12:56.194649: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 95.75 MB
[2022-12-12 07:12:56.195316: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 07:12:56.196417: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 07:12:56.196464: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 95.16 MB
[2022-12-12 07:12:56.213151: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1023.00 Bytes
[[2022-12-12 07:12:56[2022-12-12 07:12:56[.2022-12-12 07:12:56213212.2022-12-12 07:12:56.: 213212.213235E: 213236:  E: E/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu E :/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu1980:/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:] 1980:1980eager alloc mem 1023.00 Bytes] 1980] 
eager alloc mem 1023.00 Bytes] eager alloc mem 1023.00 Bytes
eager alloc mem 1023.00 Bytes

[2022-12-12 07:12:56.216975: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1023.00 Bytes
[2022-12-12 07:12:56.219000: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1023.00 Bytes
[2022-12-12 07:12:56.219507: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1023
[2022-12-12 07:12:56.219633: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 07:12:56.219677: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1023
[2022-12-12 07:12:56.219762: [E2022-12-12 07:12:56 ./hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc219757:: 638E]  eager release cuda mem 400000000/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc
:638] eager release cuda mem 1023
[2022-12-12 07:12:56.219827[: 2022-12-12 07:12:56E. 219851/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc: :E638 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cceager release cuda mem 1023:
638] eager release cuda mem 400000000
[2022-12-12 07:12:56.219920[: 2022-12-12 07:12:56E. 219914/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc: :E638 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cceager release cuda mem 400000000:
638] eager release cuda mem 1023
[2022-12-12 07:12:56.220010: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 07:12:56.221134: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 95.75 MB
[2022-12-12 07:12:56.221744: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 95.75 MB
[2022-12-12 07:12:56.222186: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1023
[2022-12-12 07:12:56.222258: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 07:12:56.222343: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 95.75 MB
[2022-12-12 07:12:56.223008: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 95.75 MB
[2022-12-12 07:12:56.223898: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 95.75 MB
[2022-12-12 07:12:56.225570: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 95.75 MB
[2022-12-12 07:12:56.225878: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 07:12:56.225988: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 07:12:56.226080: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 07:12:56.226260: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 07:12:56.226313: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 07:12:56.226451: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 07:12:56.226634: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1023
[2022-12-12 07:12:56.226707: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 07:12:56.226959: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 07:12:56.227006: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 95.26 MB
[2022-12-12 07:12:56.227092: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 07:12:56.227155: W [/hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc2022-12-12 07:12:56:.43227164] : WORKER[0] alloc host memory 95.18 MBE
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 07:12:56.227233: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 95.34 MB
[2022-12-12 07:12:56.227343: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 07:12:56.227390: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:[432022-12-12 07:12:56] .WORKER[0] alloc host memory 95.18 MB227403
: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 07:12:56.227458: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 95.30 MB
[2022-12-12 07:12:56.[2275272022-12-12 07:12:56: .E227536 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE: 1980/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc] :eager alloc mem 95.75 MB638
] eager release cuda mem 625663
[2022-12-12 07:12:56.227634: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 95.33 MB
[2022-12-12 07:12:56.228149: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 07:12:56.229234: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 07:12:56.229281: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 95.24 MB
[2022-12-12 07:12:56.260885: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 07:12:56.261516: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 07:12:56.261560: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 11.90 GB
[2022-12-12 07:12:56.289649: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 07:12:56.289811: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 07:12:56.290291: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 07:12:56.290338: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 11.90 GB
[2022-12-12 07:12:56.290425: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 07:12:56.290469: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 11.92 GB
[2022-12-12 07:12:56.292857: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 07:12:56.293157: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 07:12:56.293232: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 07:12:56.293435: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 07:12:56.293509: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 07:12:56.293575: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 11.91 GB
[2022-12-12 07:12:56.293758: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 07:12:56.293799: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 11.92 GB
[2022-12-12 07:12:56.293838: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 07:12:56.293880: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 11.90 GB
[2022-12-12 07:12:56.294051: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 07:12:56.294094: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 11.92 GB
[2022-12-12 07:12:56.294243: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 07:12:56.294875: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 07:12:56.294917: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 11.91 GB
[[[[[[[[2022-12-12 07:13:002022-12-12 07:13:002022-12-12 07:13:002022-12-12 07:13:002022-12-12 07:13:002022-12-12 07:13:002022-12-12 07:13:002022-12-12 07:13:00........132722132722132721132722132721132720132721132721: : : : : : : : EEEEEEEE        /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::::::::19261926192619261926192619261926] ] ] ] ] ] ] ] Device 3 init p2p of link 2Device 4 init p2p of link 5Device 1 init p2p of link 7Device 0 init p2p of link 3Device 7 init p2p of link 4Device 5 init p2p of link 6Device 6 init p2p of link 0Device 2 init p2p of link 1







[[[[[[[2022-12-12 07:13:00[2022-12-12 07:13:002022-12-12 07:13:002022-12-12 07:13:002022-12-12 07:13:002022-12-12 07:13:002022-12-12 07:13:00.2022-12-12 07:13:00......133334.133333133333133333133333133334133333: 133350: : : : : : E: EEEEEE E      /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::::::1980:198019801980198019801980] 1980] ] ] ] ] ] eager alloc mem 611.00 KB] eager alloc mem 611.00 KBeager alloc mem 611.00 KBeager alloc mem 611.00 KBeager alloc mem 611.00 KBeager alloc mem 611.00 KBeager alloc mem 611.00 KB
eager alloc mem 611.00 KB






[2022-12-12 07:13:00.134436: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663[
2022-12-12 07:13:00.134459: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 07:13:00.134506: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc[[:2022-12-12 07:13:002022-12-12 07:13:00[[638[..2022-12-12 07:13:002022-12-12 07:13:00] 2022-12-12 07:13:00134521134522..eager release cuda mem 625663.: : 134525134526
134526EE: : :   EEE/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc   ::/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc638638:::] ] 638638638eager release cuda mem 625663eager release cuda mem 625663] ] ] 

eager release cuda mem 625663eager release cuda mem 625663eager release cuda mem 625663


[2022-12-12 07:13:00.151067: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 2
[2022-12-12 07:13:00.151225: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 07:13:00.152183: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 07:13:00.152451: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 6 init p2p of link 5
[2022-12-12 07:13:00.152614: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 07:13:00.153547: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 07:13:00.155074: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 5 init p2p of link 4
[2022-12-12 07:13:00.155251: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[[2022-12-12 07:13:002022-12-12 07:13:00..155655155658: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::19261926] ] Device 4 init p2p of link 7Device 2 init p2p of link 3

[2022-12-12 07:13:00.155806: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 0 init p2p of link 6
[2022-12-12 07:13:00.155852: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 07:13:00.155878: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 07:13:00.155946: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:[19262022-12-12 07:13:00] .Device 7 init p2p of link 1155977
: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 07:13:00.156121: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 07:13:00.156162: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 07:13:00.156226: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 3 init p2p of link 0
[2022-12-12 07:13:00.156380: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 07:13:00.156789: E[ 2022-12-12 07:13:00/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc.:156802638: ] Eeager release cuda mem 625663 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 07:13:00.156862: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 07:13:00.156985: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 07:13:00.157280: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 07:13:00.162514: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 3
[2022-12-12 07:13:00.162637: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 07:13:00.163549: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 07:13:00.167146: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 6 init p2p of link 4
[2022-12-12 07:13:00.167272: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 07:13:00.168182: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 07:13:00.173159: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 7 init p2p of link 6
[2022-12-12 07:13:00.173240: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 0 init p2p of link 1
[2022-12-12 07:13:00.173280: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 07:13:00.173374: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 07:13:00.173458: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 5 init p2p of link 7
[2022-12-12 07:13:00.173581: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 07:13:00.174232: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 07:13:00.174321: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 07:13:00.174419: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 4 init p2p of link 2
[2022-12-12 07:13:00.174494: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 07:13:00.174547: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 07:13:00.174613: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 2 init p2p of link 0
[2022-12-12 07:13:00.174730: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 07:13:00.175411: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 07:13:00.175642: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 07:13:00.176020: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 3 init p2p of link 5
[2022-12-12 07:13:00.176145: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 07:13:00.177007: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 07:13:00.183999: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 0
[2022-12-12 07:13:00.184113: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 07:13:00.185022: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 07:13:00.185300: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 6 init p2p of link 7
[2022-12-12 07:13:00.185418: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 07:13:00.186347: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 07:13:00.195502: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 4 init p2p of link 6
[2022-12-12 07:13:00.195629: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 07:13:00.196545: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 07:13:00.196887: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 3 init p2p of link 1
[2022-12-12 07:13:00.197007: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 07:13:00.197921: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 07:13:00.200019: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 5 init p2p of link 3
[2022-12-12 07:13:00.200080: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 7 init p2p of link 5
[2022-12-12 07:13:00.200136: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 07:13:00.200195: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 07:13:00.200218: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 0 init p2p of link 2
[2022-12-12 07:13:00.200342: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 07:13:00.201002: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 07:13:00.201123: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 07:13:00.201289: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 07:13:00.201842: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 2 init p2p of link 4
[2022-12-12 07:13:00.201959: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 07:13:00.202801: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 07:13:00.205977: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 100400000
[2022-12-12 07:13:00.206183: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 100400000
[2022-12-12 07:13:00.208098: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 24951610 / 100000000 nodes ( 24.95 %~25.00 %) | remote 69748390 / 100000000 nodes ( 69.75 %) | cpu 5300000 / 100000000 nodes ( 5.30 %) | 11.90 GB | 4.07927 secs 
[2022-12-12 07:13:00.208616: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 24973036 / 100000000 nodes ( 24.97 %~25.00 %) | remote 69726964 / 100000000 nodes ( 69.73 %) | cpu 5300000 / 100000000 nodes ( 5.30 %) | 11.91 GB | 4.08311 secs 
[2022-12-12 07:13:00.213901: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 100400000
[2022-12-12 07:13:00.214289: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 100400000
[2022-12-12 07:13:00.214589: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 24951903 / 100000000 nodes ( 24.95 %~25.00 %) | remote 69748097 / 100000000 nodes ( 69.75 %) | cpu 5300000 / 100000000 nodes ( 5.30 %) | 11.90 GB | 4.09035 secs 
[2022-12-12 07:13:00.215259: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 24965348 / 100000000 nodes ( 24.97 %~25.00 %) | remote 69734652 / 100000000 nodes ( 69.73 %) | cpu 5300000 / 100000000 nodes ( 5.30 %) | 11.91 GB | 4.08626 secs 
[2022-12-12 07:13:00.218763: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 100400000
[2022-12-12 07:13:00.219177: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 100400000
[2022-12-12 07:13:00.221294: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 24988936 / 100000000 nodes ( 24.99 %~25.00 %) | remote 69711064 / 100000000 nodes ( 69.71 %) | cpu 5300000 / 100000000 nodes ( 5.30 %) | 11.92 GB | 4.09679 secs 
[2022-12-12 07:13:00.221518: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 24991856 / 100000000 nodes ( 24.99 %~25.00 %) | remote 69708144 / 100000000 nodes ( 69.71 %) | cpu 5300000 / 100000000 nodes ( 5.30 %) | 11.92 GB | 4.09696 secs 
[2022-12-12 07:13:00.222507: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 100400000
[2022-12-12 07:13:00.223151: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 100400000
[2022-12-12 07:13:00.272342: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 24982647 / 100000000 nodes ( 24.98 %~25.00 %) | remote 69717353 / 100000000 nodes ( 69.72 %) | cpu 5300000 / 100000000 nodes ( 5.30 %) | 11.92 GB | 4.14782 secs 
[2022-12-12 07:13:00.272580: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 24944863 / 100000000 nodes ( 24.94 %~25.00 %) | remote 69755137 / 100000000 nodes ( 69.76 %) | cpu 5300000 / 100000000 nodes ( 5.30 %) | 11.90 GB | 4.16358 secs 
[2022-12-12 07:13:00.274073: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 20.22 GB
[2022-12-12 07:13:01.685672: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 20.48 GB
[2022-12-12 07:13:01.686171: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 20.48 GB
[2022-12-12 07:13:01.686836: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 20.48 GB
[2022-12-12 07:13:03.129534: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 20.75 GB
[2022-12-12 07:13:03.130082: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 20.75 GB
[2022-12-12 07:13:03.153716: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 20.75 GB
[2022-12-12 07:13:04.316005: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 20.96 GB
[2022-12-12 07:13:04.316138: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 20.96 GB
[2022-12-12 07:13:04.316822: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 20.96 GB
[2022-12-12 07:13:05.568417: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 21.17 GB
[2022-12-12 07:13:05.568559: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 21.17 GB
[2022-12-12 07:13:05.568791: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 21.17 GB
[2022-12-12 07:13:06.622490: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 21.63 GB
[2022-12-12 07:13:06.622916: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 21.63 GB
[2022-12-12 07:13:06.623559: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 21.63 GB
[2022-12-12 07:13:08.192530: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 21.83 GB
[2022-12-12 07:13:08.193073: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 21.83 GB
[HCTR][07:13:09.156][ERROR][RK0][main]: replica 1 calling init per replica done, doing barrier
[HCTR][07:13:09.156][ERROR][RK0][main]: replica 5 calling init per replica done, doing barrier
[HCTR][07:13:09.156][ERROR][RK0][tid #140168623265536]: replica 3 calling init per replica done, doing barrier
[HCTR][07:13:09.156][ERROR][RK0][main]: replica 0 calling init per replica done, doing barrier
[HCTR][07:13:09.156][ERROR][RK0][tid #140168690374400]: replica 7 calling init per replica done, doing barrier
[HCTR][07:13:09.156][ERROR][RK0][tid #140168564549376]: replica 6 calling init per replica done, doing barrier
[HCTR][07:13:09.156][ERROR][RK0][main]: replica 2 calling init per replica done, doing barrier
[HCTR][07:13:09.156][ERROR][RK0][tid #140168631658240]: replica 4 calling init per replica done, doing barrier
[HCTR][07:13:09.156][ERROR][RK0][tid #140168564549376]: replica 6 calling init per replica done, doing barrier done
[HCTR][07:13:09.156][ERROR][RK0][tid #140168690374400]: replica 7 calling init per replica done, doing barrier done
[HCTR][07:13:09.156][ERROR][RK0][main]: replica 1 calling init per replica done, doing barrier done
[HCTR][07:13:09.156][ERROR][RK0][main]: replica 2 calling init per replica done, doing barrier done
[HCTR][07:13:09.156][ERROR][RK0][tid #140168631658240]: replica 4 calling init per replica done, doing barrier done
[HCTR][07:13:09.156][ERROR][RK0][main]: replica 0 calling init per replica done, doing barrier done
[HCTR][07:13:09.156][ERROR][RK0][main]: replica 5 calling init per replica done, doing barrier done
[HCTR][07:13:09.156][ERROR][RK0][tid #140168623265536]: replica 3 calling init per replica done, doing barrier done
[HCTR][07:13:09.156][ERROR][RK0][tid #140168564549376]: init per replica done
[HCTR][07:13:09.156][ERROR][RK0][tid #140168690374400]: init per replica done
[HCTR][07:13:09.156][ERROR][RK0][main]: init per replica done
[HCTR][07:13:09.156][ERROR][RK0][main]: init per replica done
[HCTR][07:13:09.156][ERROR][RK0][tid #140168631658240]: init per replica done
[HCTR][07:13:09.156][ERROR][RK0][main]: init per replica done
[HCTR][07:13:09.156][ERROR][RK0][tid #140168623265536]: init per replica done
[HCTR][07:13:09.159][ERROR][RK0][main]: init per replica done
[HCTR][07:13:09.194][ERROR][RK0][tid #140168690374400]: 7 allocated 3276800 at 0x7f5d30238400
[HCTR][07:13:09.194][ERROR][RK0][tid #140168690374400]: 7 allocated 6553600 at 0x7f5d30558400
[HCTR][07:13:09.194][ERROR][RK0][tid #140168690374400]: 7 allocated 3276800 at 0x7f5d30b98400
[HCTR][07:13:09.194][ERROR][RK0][tid #140168690374400]: 7 allocated 6553600 at 0x7f5d30eb8400
[HCTR][07:13:09.195][ERROR][RK0][main]: 5 allocated 3276800 at 0x7f5c9c238400
[HCTR][07:13:09.195][ERROR][RK0][main]: 5 allocated 6553600 at 0x7f5c9c558400
[HCTR][07:13:09.195][ERROR][RK0][main]: 5 allocated 3276800 at 0x7f5c9cb98400
[HCTR][07:13:09.195][ERROR][RK0][main]: 5 allocated 6553600 at 0x7f5c9ceb8400
[HCTR][07:13:09.195][ERROR][RK0][tid #140168564549376]: 6 allocated 3276800 at 0x7f5c88238400
[HCTR][07:13:09.195][ERROR][RK0][tid #140168564549376]: 6 allocated 6553600 at 0x7f5c88558400
[HCTR][07:13:09.195][ERROR][RK0][tid #140168564549376]: 6 allocated 3276800 at 0x7f5c88b98400
[HCTR][07:13:09.195][ERROR][RK0][tid #140168564549376]: 6 allocated 6553600 at 0x7f5c88eb8400
[HCTR][07:13:09.195][ERROR][RK0][tid #140168698767104]: 2 allocated 3276800 at 0x7f5c30238400
[HCTR][07:13:09.195][ERROR][RK0][tid #140168698767104]: 2 allocated 6553600 at 0x7f5c30558400
[HCTR][07:13:09.195][ERROR][RK0][tid #140168698767104]: 2 allocated 3276800 at 0x7f5c30b98400
[HCTR][07:13:09.195][ERROR][RK0][tid #140168698767104]: 2 allocated 6553600 at 0x7f5c30eb8400
[HCTR][07:13:09.195][ERROR][RK0][main]: 3 allocated 3276800 at 0x7f5d30238400
[HCTR][07:13:09.195][ERROR][RK0][main]: 4 allocated 3276800 at 0x7f5c48238400
[HCTR][07:13:09.195][ERROR][RK0][main]: 3 allocated 6553600 at 0x7f5d30558400
[HCTR][07:13:09.195][ERROR][RK0][main]: 4 allocated 6553600 at 0x7f5c48558400
[HCTR][07:13:09.195][ERROR][RK0][main]: 3 allocated 3276800 at 0x7f5d30b98400
[HCTR][07:13:09.195][ERROR][RK0][main]: 4 allocated 3276800 at 0x7f5c48b98400
[HCTR][07:13:09.195][ERROR][RK0][main]: 3 allocated 6553600 at 0x7f5d30eb8400
[HCTR][07:13:09.195][ERROR][RK0][main]: 4 allocated 6553600 at 0x7f5c48eb8400
[HCTR][07:13:09.195][ERROR][RK0][tid #140168690374400]: 1 allocated 3276800 at 0x7f5d10238400
[HCTR][07:13:09.195][ERROR][RK0][tid #140168690374400]: 1 allocated 6553600 at 0x7f5d10558400
[HCTR][07:13:09.195][ERROR][RK0][tid #140168690374400]: 1 allocated 3276800 at 0x7f5d10b98400
[HCTR][07:13:09.195][ERROR][RK0][tid #140168690374400]: 1 allocated 6553600 at 0x7f5d10eb8400
[HCTR][07:13:09.198][ERROR][RK0][tid #140169093027584]: 0 allocated 3276800 at 0x7f5cb4320000
[HCTR][07:13:09.198][ERROR][RK0][tid #140169093027584]: 0 allocated 6553600 at 0x7f5cb4640000
[HCTR][07:13:09.198][ERROR][RK0][tid #140169093027584]: 0 allocated 3276800 at 0x7f5cb4c80000
[HCTR][07:13:09.198][ERROR][RK0][tid #140169093027584]: 0 allocated 6553600 at 0x7f5cb4fa0000
