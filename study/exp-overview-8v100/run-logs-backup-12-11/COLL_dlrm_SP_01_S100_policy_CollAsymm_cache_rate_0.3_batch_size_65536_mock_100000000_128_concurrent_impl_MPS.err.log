2022-12-12 07:41:13.447357: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.454353: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.460838: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.465703: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.471808: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.482582: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.489454: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.501646: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.553755: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.564030: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.568084: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.568753: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.570221: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.571155: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.572188: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.573576: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.574619: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.575861: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.576605: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.578155: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.579005: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.580637: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.581255: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.582897: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.583371: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.585225: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.585681: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.587679: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.589442: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.591031: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.592402: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.593819: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.597825: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.599976: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.603018: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.604840: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.606119: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.607171: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.608548: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.609527: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.615662: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 07:41:13.618619: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.619950: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.621086: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.622206: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.623222: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.624335: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.625414: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.625668: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.626892: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.627190: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.628790: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.630793: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.632707: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.632718: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.635117: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.635353: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.638093: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.638535: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.639013: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.640760: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.641504: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.642266: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.643701: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.644876: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.645942: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.646972: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.647086: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.648754: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.649882: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.650918: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.651157: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.651883: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.652567: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.653695: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.654491: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.655079: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.655782: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.656405: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.657298: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.657801: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.659054: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.669679: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.672080: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.672237: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.672743: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.672850: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.674318: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.675231: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.676514: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.676709: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.677180: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.677319: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.679410: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.680394: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.714875: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.717019: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.717484: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.717525: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.720059: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.722210: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.722803: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.722831: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.723423: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.723751: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.726181: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.727161: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.727335: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.727378: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.728872: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.729555: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.733215: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.733259: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.733305: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.733639: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.734711: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.737067: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.737110: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.738436: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.739202: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.740700: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.740932: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.742253: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.743591: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.744681: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.744986: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.745992: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.748364: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.748473: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.749264: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.751154: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.751243: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.752321: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.753863: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.754236: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.755032: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.757772: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.758329: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.759177: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.760711: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.760983: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.761762: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.762132: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.764172: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.764458: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.765384: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.767662: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.767898: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.768516: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.770140: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 07:41:13.770477: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.771383: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.771483: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.773045: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.774177: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.774429: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.776098: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.777879: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.778657: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.778964: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.779738: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.780182: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.780728: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.781647: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.782616: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.783350: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.784224: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.784598: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.785218: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.786606: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.787413: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.788011: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.788871: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.789211: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.790317: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.790495: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.791234: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.792282: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.792782: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.794660: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.795401: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.795647: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.797078: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.798350: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.799437: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.801527: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.802421: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.802559: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.803398: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.804463: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.805160: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.806898: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.807581: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.807841: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.808925: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.809679: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.811941: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.812633: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.812901: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.813872: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 07:41:13.813900: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.814401: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.816434: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.817023: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.817295: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.819148: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.821357: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.821902: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.822395: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.822481: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 07:41:13.823623: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.825015: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.826880: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.827451: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.827783: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.828381: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.829187: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.831428: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.831842: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.832111: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.832182: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.832773: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.835184: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.835941: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.836169: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.836549: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.836582: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.839691: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.840600: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.840657: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.840965: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.841217: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.846606: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.847658: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.847748: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.848532: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.852081: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.852534: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.852738: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.854375: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.858046: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.859021: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.859588: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.888902: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.892016: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.893401: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.894338: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.921363: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.925145: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.926096: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.926313: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.926392: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.931911: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.933166: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.933470: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.934224: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.938510: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.938684: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.940502: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.941279: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 07:41:13.944712: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.944929: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.945668: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.950651: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.951900: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.952469: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.953525: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.957864: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.958533: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.958979: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.959639: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.964450: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.965565: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.966526: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:13.969848: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 07:41:13.979112: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:14.000842: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:14.002308: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:14.002951: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 07:41:14.012410: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:14.037439: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:14.038308: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:14.045296: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:14.050353: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 07:41:14.051395: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:14.060398: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:14.065492: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:14.073883: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:14.984573: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:14.985315: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:14.986071: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:14.987079: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 07:41:14.987157: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-12 07:41:15.004720: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:15.005472: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:15.006022: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:15.006830: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:15.007806: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:15.008308: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-12 07:41:15.054803: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 07:41:15.055025: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 07:41:15.104467: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12340
2022-12-12 07:41:15.215633: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:15.216464: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:15.217527: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:15.218015: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 07:41:15.218072: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-12 07:41:15.235476: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:15.236123: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:15.236624: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:15.237615: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:15.238159: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:15.238629: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:1/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-12 07:41:15.266451: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:15.267077: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:15.267615: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:15.268106: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 07:41:15.268158: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-12 07:41:15.273795: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:15.274395: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:15.274925: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:15.275413: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 07:41:15.275468: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-12 07:41:15.285625: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:15.286255: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:15.286771: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:15.287357: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:15.287885: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:15.288582: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:6/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-12 07:41:15.293164: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:15.293794: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:15.294397: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:15.294980: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:15.295512: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:15.295988: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:2/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-12 07:41:15.334054: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 07:41:15.334268: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 07:41:15.336968: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12346
2022-12-12 07:41:15.337336: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 07:41:15.337482: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 07:41:15.339251: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12341
2022-12-12 07:41:15.347053: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:15.347664: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:15.348198: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:15.349038: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 07:41:15.349089: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-12 07:41:15.351888: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:15.352467: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:15.353213: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:15.353834: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 07:41:15.353880: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:15.353920: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-12 07:41:15.354656: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:15.355206: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:15.355681: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 07:41:15.355726: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-12 07:41:15.366381: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:15.367008: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:15.367594: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:15.367606: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:15.368686: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:15.368737: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:15.369740: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:15.369805: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:15.370648: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 07:41:15.370715: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-12 07:41:15.370735: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:5/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-12 07:41:15.371649: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:15.372259: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:15.372316: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:15.373175: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:15.373473: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:15.374022: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:15.374487: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:15.374932: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:15.375405: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:15.375855: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:15.376259: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:4/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-12 07:41:15.376526: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:7/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-12 07:41:15.387756: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:15.388438: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:15.388967: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:15.389567: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:15.390115: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:41:15.390596: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:3/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-12 07:41:15.407069: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 07:41:15.407282: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 07:41:15.409061: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12342
2022-12-12 07:41:15.416111: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 07:41:15.416269: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 07:41:15.418049: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12345
2022-12-12 07:41:15.421283: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 07:41:15.421470: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 07:41:15.421612: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 07:41:15.421763: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 07:41:15.423330: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12344
2022-12-12 07:41:15.423575: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12347
2022-12-12 07:41:15.436527: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 07:41:15.436724: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 07:41:15.438620: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12343
[HCTR][07:41:16.706][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][07:41:16.706][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][07:41:16.706][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][07:41:16.706][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][07:41:16.707][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][07:41:16.707][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][07:41:16.707][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][07:41:16.707][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 1it [00:01,  1.53s/it]warmup run: 1it [00:01,  1.51s/it]warmup run: 90it [00:01, 76.57it/s]warmup run: 1it [00:01,  1.53s/it]warmup run: 1it [00:01,  1.50s/it]warmup run: 1it [00:01,  1.54s/it]warmup run: 97it [00:01, 83.49it/s]warmup run: 1it [00:01,  1.48s/it]warmup run: 181it [00:01, 167.05it/s]warmup run: 98it [00:01, 83.36it/s]warmup run: 1it [00:01,  1.50s/it]warmup run: 97it [00:01, 84.04it/s]warmup run: 100it [00:01, 84.73it/s]warmup run: 197it [00:01, 183.92it/s]warmup run: 1it [00:01,  1.49s/it]warmup run: 98it [00:01, 85.85it/s]warmup run: 272it [00:01, 266.80it/s]warmup run: 197it [00:01, 181.77it/s]warmup run: 87it [00:01, 75.48it/s]warmup run: 194it [00:01, 181.82it/s]warmup run: 187it [00:01, 169.51it/s]warmup run: 296it [00:01, 293.10it/s]warmup run: 97it [00:01, 84.66it/s]warmup run: 195it [00:01, 184.36it/s]warmup run: 364it [00:01, 371.51it/s]warmup run: 295it [00:01, 288.78it/s]warmup run: 165it [00:01, 153.33it/s]warmup run: 292it [00:01, 290.22it/s]warmup run: 270it [00:01, 257.51it/s]warmup run: 396it [00:01, 407.08it/s]warmup run: 193it [00:01, 181.91it/s]warmup run: 290it [00:01, 289.52it/s]warmup run: 456it [00:02, 473.16it/s]warmup run: 391it [00:01, 396.66it/s]warmup run: 261it [00:01, 263.07it/s]warmup run: 392it [00:01, 405.08it/s]warmup run: 367it [00:01, 371.51it/s]warmup run: 497it [00:02, 519.39it/s]warmup run: 291it [00:01, 291.12it/s]warmup run: 383it [00:01, 394.09it/s]warmup run: 550it [00:02, 570.49it/s]warmup run: 489it [00:02, 504.84it/s]warmup run: 357it [00:01, 376.11it/s]warmup run: 490it [00:02, 513.10it/s]warmup run: 465it [00:02, 484.14it/s]warmup run: 596it [00:02, 618.26it/s]warmup run: 385it [00:01, 396.67it/s]warmup run: 476it [00:01, 495.30it/s]warmup run: 643it [00:02, 652.73it/s]warmup run: 588it [00:02, 605.66it/s]warmup run: 452it [00:02, 484.08it/s]warmup run: 583it [00:02, 600.01it/s]warmup run: 565it [00:02, 591.37it/s]warmup run: 693it [00:02, 698.43it/s]warmup run: 482it [00:01, 504.64it/s]warmup run: 570it [00:02, 590.02it/s]warmup run: 735it [00:02, 718.12it/s]warmup run: 687it [00:02, 693.09it/s]warmup run: 548it [00:02, 584.41it/s]warmup run: 681it [00:02, 687.36it/s]warmup run: 665it [00:02, 685.21it/s]warmup run: 580it [00:02, 605.62it/s]warmup run: 789it [00:02, 756.90it/s]warmup run: 664it [00:02, 671.58it/s]warmup run: 829it [00:02, 775.73it/s]warmup run: 786it [00:02, 765.66it/s]warmup run: 645it [00:02, 673.09it/s]warmup run: 781it [00:02, 764.07it/s]warmup run: 765it [00:02, 762.85it/s]warmup run: 679it [00:02, 694.01it/s]warmup run: 884it [00:02, 801.17it/s]warmup run: 760it [00:02, 742.17it/s]warmup run: 929it [00:02, 834.34it/s]warmup run: 886it [00:02, 826.11it/s]warmup run: 743it [00:02, 748.84it/s]warmup run: 880it [00:02, 822.82it/s]warmup run: 867it [00:02, 828.91it/s]warmup run: 777it [00:02, 763.65it/s]warmup run: 980it [00:02, 841.91it/s]warmup run: 858it [00:02, 803.48it/s]warmup run: 1028it [00:02, 877.27it/s]warmup run: 984it [00:02, 866.89it/s]warmup run: 843it [00:02, 813.20it/s]warmup run: 980it [00:02, 870.39it/s]warmup run: 969it [00:02, 878.68it/s]warmup run: 876it [00:02, 821.35it/s]warmup run: 1079it [00:02, 880.51it/s]warmup run: 958it [00:02, 856.02it/s]warmup run: 1128it [00:02, 911.50it/s]warmup run: 1084it [00:02, 902.30it/s]warmup run: 943it [00:02, 863.36it/s]warmup run: 1080it [00:02, 904.26it/s]warmup run: 1070it [00:02, 915.26it/s]warmup run: 976it [00:02, 868.58it/s]warmup run: 1177it [00:02, 907.47it/s]warmup run: 1059it [00:02, 897.77it/s]warmup run: 1227it [00:02, 933.20it/s]warmup run: 1183it [00:02, 925.05it/s]warmup run: 1042it [00:02, 897.51it/s]warmup run: 1178it [00:02, 924.50it/s]warmup run: 1170it [00:02, 937.58it/s]warmup run: 1075it [00:02, 901.84it/s]warmup run: 1275it [00:02, 927.25it/s]warmup run: 1160it [00:02, 927.64it/s]warmup run: 1328it [00:02, 955.27it/s]warmup run: 1282it [00:02, 939.62it/s]warmup run: 1141it [00:02, 922.75it/s]warmup run: 1276it [00:02, 938.12it/s]warmup run: 1271it [00:02, 956.10it/s]warmup run: 1176it [00:02, 930.35it/s]warmup run: 1372it [00:02, 936.07it/s]warmup run: 1260it [00:02, 948.34it/s]warmup run: 1428it [00:03, 967.95it/s]warmup run: 1381it [00:02, 952.51it/s]warmup run: 1241it [00:02, 943.76it/s]warmup run: 1374it [00:02, 948.39it/s]warmup run: 1371it [00:02, 966.96it/s]warmup run: 1278it [00:02, 955.18it/s]warmup run: 1469it [00:03, 926.99it/s]warmup run: 1361it [00:02, 965.08it/s]warmup run: 1528it [00:03, 976.00it/s]warmup run: 1480it [00:03, 951.95it/s]warmup run: 1340it [00:02, 955.17it/s]warmup run: 1473it [00:03, 958.54it/s]warmup run: 1473it [00:03, 980.39it/s]warmup run: 1379it [00:02, 970.96it/s]warmup run: 1564it [00:03, 930.47it/s]warmup run: 1463it [00:02, 979.34it/s]warmup run: 1627it [00:03, 972.33it/s]warmup run: 1581it [00:03, 967.87it/s]warmup run: 1439it [00:03, 949.34it/s]warmup run: 1572it [00:03, 965.76it/s]warmup run: 1574it [00:03, 988.81it/s]warmup run: 1479it [00:03, 959.30it/s]warmup run: 1659it [00:03, 932.07it/s]warmup run: 1564it [00:03, 985.94it/s]warmup run: 1727it [00:03, 978.39it/s]warmup run: 1680it [00:03, 964.24it/s]warmup run: 1538it [00:03, 960.82it/s]warmup run: 1671it [00:03, 971.49it/s]warmup run: 1675it [00:03, 990.93it/s]warmup run: 1579it [00:03, 971.07it/s]warmup run: 1754it [00:03, 935.47it/s]warmup run: 1664it [00:03, 970.43it/s]warmup run: 1828it [00:03, 984.91it/s]warmup run: 1778it [00:03, 948.78it/s]warmup run: 1640it [00:03, 975.67it/s]warmup run: 1770it [00:03, 974.72it/s]warmup run: 1776it [00:03, 993.05it/s]warmup run: 1678it [00:03, 976.17it/s]warmup run: 1856it [00:03, 958.78it/s]warmup run: 1763it [00:03, 958.18it/s]warmup run: 1928it [00:03, 987.44it/s]warmup run: 1741it [00:03, 983.05it/s]warmup run: 1874it [00:03, 936.86it/s]warmup run: 1870it [00:03, 981.40it/s]warmup run: 1877it [00:03, 995.74it/s]warmup run: 1779it [00:03, 984.02it/s]warmup run: 1959it [00:03, 977.89it/s]warmup run: 2030it [00:03, 996.70it/s]warmup run: 1860it [00:03, 951.85it/s]warmup run: 1841it [00:03, 986.28it/s]warmup run: 1969it [00:03, 981.21it/s]warmup run: 1969it [00:03, 925.36it/s]warmup run: 1978it [00:03, 996.98it/s]warmup run: 1879it [00:03, 988.48it/s]warmup run: 2072it [00:03, 1021.73it/s]warmup run: 2146it [00:03, 1043.16it/s]warmup run: 1956it [00:03, 949.94it/s]warmup run: 1941it [00:03, 986.09it/s]warmup run: 2082it [00:03, 1024.06it/s]warmup run: 2072it [00:03, 954.12it/s]warmup run: 2094it [00:03, 1043.22it/s]warmup run: 1980it [00:03, 992.84it/s]warmup run: 2194it [00:03, 1078.26it/s]warmup run: 2261it [00:03, 1072.43it/s]warmup run: 2063it [00:03, 983.75it/s]warmup run: 2046it [00:03, 1002.87it/s]warmup run: 2202it [00:03, 1075.77it/s]warmup run: 2182it [00:03, 995.97it/s]warmup run: 2214it [00:03, 1087.80it/s]warmup run: 2095it [00:03, 1039.00it/s]warmup run: 2316it [00:03, 1118.88it/s]warmup run: 2375it [00:03, 1091.88it/s]warmup run: 2182it [00:03, 1043.28it/s]warmup run: 2161it [00:03, 1046.61it/s]warmup run: 2323it [00:03, 1113.25it/s]warmup run: 2293it [00:03, 1027.33it/s]warmup run: 2334it [00:03, 1120.19it/s]warmup run: 2214it [00:03, 1083.07it/s]warmup run: 2437it [00:03, 1144.26it/s]warmup run: 2489it [00:04, 1105.76it/s]warmup run: 2301it [00:03, 1085.04it/s]warmup run: 2276it [00:03, 1076.49it/s]warmup run: 2444it [00:03, 1140.89it/s]warmup run: 2403it [00:03, 1048.43it/s]warmup run: 2454it [00:03, 1143.45it/s]warmup run: 2333it [00:03, 1114.06it/s]warmup run: 2557it [00:04, 1158.99it/s]warmup run: 2603it [00:04, 1114.73it/s]warmup run: 2420it [00:03, 1113.83it/s]warmup run: 2391it [00:03, 1097.40it/s]warmup run: 2566it [00:04, 1162.00it/s]warmup run: 2514it [00:04, 1064.33it/s]warmup run: 2574it [00:04, 1158.00it/s]warmup run: 2452it [00:03, 1136.40it/s]warmup run: 2674it [00:04, 1161.95it/s]warmup run: 2716it [00:04, 1118.88it/s]warmup run: 2539it [00:04, 1133.81it/s]warmup run: 2507it [00:04, 1114.12it/s]warmup run: 2688it [00:04, 1177.97it/s]warmup run: 2624it [00:04, 1073.00it/s]warmup run: 2693it [00:04, 1164.92it/s]warmup run: 2572it [00:04, 1152.97it/s]warmup run: 2793it [00:04, 1170.31it/s]warmup run: 2830it [00:04, 1124.81it/s]warmup run: 2657it [00:04, 1145.05it/s]warmup run: 2625it [00:04, 1132.70it/s]warmup run: 2809it [00:04, 1184.91it/s]warmup run: 2735it [00:04, 1082.38it/s]warmup run: 2813it [00:04, 1173.71it/s]warmup run: 2690it [00:04, 1159.69it/s]warmup run: 2913it [00:04, 1176.34it/s]warmup run: 2949it [00:04, 1142.00it/s]warmup run: 2776it [00:04, 1155.88it/s]warmup run: 3000it [00:04, 667.29it/s] warmup run: 2744it [00:04, 1147.57it/s]warmup run: 2931it [00:04, 1193.85it/s]warmup run: 2846it [00:04, 1087.94it/s]warmup run: 2933it [00:04, 1180.84it/s]warmup run: 3000it [00:04, 678.49it/s] warmup run: 2808it [00:04, 1163.84it/s]warmup run: 2893it [00:04, 1157.70it/s]warmup run: 3000it [00:04, 684.69it/s] warmup run: 3000it [00:04, 678.44it/s] warmup run: 2862it [00:04, 1156.46it/s]warmup run: 2956it [00:04, 1089.22it/s]warmup run: 2927it [00:04, 1170.44it/s]warmup run: 3000it [00:04, 679.91it/s] warmup run: 3000it [00:04, 663.12it/s] warmup run: 3000it [00:04, 685.45it/s] warmup run: 2980it [00:04, 1162.23it/s]warmup run: 3000it [00:04, 675.41it/s] 
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]





warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   5%|▌         | 161/3000 [00:00<00:01, 1609.33it/s]warmup should be done:   5%|▌         | 152/3000 [00:00<00:01, 1518.01it/s]warmup should be done:   5%|▌         | 162/3000 [00:00<00:01, 1616.94it/s]warmup should be done:   5%|▌         | 161/3000 [00:00<00:01, 1605.10it/s]warmup should be done:   6%|▌         | 165/3000 [00:00<00:01, 1642.20it/s]warmup should be done:   5%|▌         | 160/3000 [00:00<00:01, 1593.75it/s]warmup should be done:   5%|▌         | 164/3000 [00:00<00:01, 1629.82it/s]warmup should be done:   5%|▌         | 160/3000 [00:00<00:01, 1594.10it/s]warmup should be done:  10%|█         | 306/3000 [00:00<00:01, 1530.32it/s]warmup should be done:  11%|█         | 322/3000 [00:00<00:01, 1608.14it/s]warmup should be done:  11%|█         | 328/3000 [00:00<00:01, 1640.81it/s]warmup should be done:  11%|█         | 332/3000 [00:00<00:01, 1656.14it/s]warmup should be done:  11%|█         | 327/3000 [00:00<00:01, 1628.01it/s]warmup should be done:  11%|█         | 325/3000 [00:00<00:01, 1622.15it/s]warmup should be done:  11%|█         | 322/3000 [00:00<00:01, 1584.64it/s]warmup should be done:  11%|█         | 320/3000 [00:00<00:01, 1569.96it/s]warmup should be done:  16%|█▌        | 483/3000 [00:00<00:01, 1604.38it/s]warmup should be done:  15%|█▌        | 460/3000 [00:00<00:01, 1529.77it/s]warmup should be done:  17%|█▋        | 498/3000 [00:00<00:01, 1654.06it/s]warmup should be done:  16%|█▋        | 489/3000 [00:00<00:01, 1626.33it/s]warmup should be done:  16%|█▋        | 493/3000 [00:00<00:01, 1635.37it/s]warmup should be done:  16%|█▋        | 490/3000 [00:00<00:01, 1618.72it/s]warmup should be done:  16%|█▌        | 487/3000 [00:00<00:01, 1612.22it/s]warmup should be done:  16%|█▌        | 479/3000 [00:00<00:01, 1574.90it/s]warmup should be done:  22%|██▏       | 646/3000 [00:00<00:01, 1613.47it/s]warmup should be done:  20%|██        | 613/3000 [00:00<00:01, 1527.30it/s]warmup should be done:  22%|██▏       | 652/3000 [00:00<00:01, 1626.70it/s]warmup should be done:  22%|██▏       | 664/3000 [00:00<00:01, 1650.05it/s]warmup should be done:  22%|██▏       | 657/3000 [00:00<00:01, 1631.02it/s]warmup should be done:  22%|██▏       | 652/3000 [00:00<00:01, 1611.96it/s]warmup should be done:  22%|██▏       | 652/3000 [00:00<00:01, 1626.31it/s]warmup should be done:  21%|██▏       | 639/3000 [00:00<00:01, 1581.35it/s]warmup should be done:  27%|██▋       | 809/3000 [00:00<00:01, 1618.98it/s]warmup should be done:  26%|██▌       | 767/3000 [00:00<00:01, 1529.99it/s]warmup should be done:  27%|██▋       | 815/3000 [00:00<00:01, 1627.81it/s]warmup should be done:  28%|██▊       | 830/3000 [00:00<00:01, 1649.69it/s]warmup should be done:  27%|██▋       | 821/3000 [00:00<00:01, 1628.50it/s]warmup should be done:  27%|██▋       | 814/3000 [00:00<00:01, 1607.54it/s]warmup should be done:  27%|██▋       | 799/3000 [00:00<00:01, 1585.52it/s]warmup should be done:  27%|██▋       | 815/3000 [00:00<00:01, 1469.44it/s]warmup should be done:  32%|███▏      | 971/3000 [00:00<00:01, 1616.47it/s]warmup should be done:  31%|███       | 920/3000 [00:00<00:01, 1527.39it/s]warmup should be done:  33%|███▎      | 978/3000 [00:00<00:01, 1622.67it/s]warmup should be done:  33%|███▎      | 995/3000 [00:00<00:01, 1644.27it/s]warmup should be done:  33%|███▎      | 984/3000 [00:00<00:01, 1624.86it/s]warmup should be done:  32%|███▏      | 959/3000 [00:00<00:01, 1588.15it/s]warmup should be done:  32%|███▎      | 975/3000 [00:00<00:01, 1597.91it/s]warmup should be done:  32%|███▏      | 965/3000 [00:00<00:01, 1365.36it/s]warmup should be done:  38%|███▊      | 1133/3000 [00:00<00:01, 1617.07it/s]warmup should be done:  38%|███▊      | 1141/3000 [00:00<00:01, 1623.47it/s]warmup should be done:  36%|███▌      | 1073/3000 [00:00<00:01, 1523.19it/s]warmup should be done:  39%|███▊      | 1160/3000 [00:00<00:01, 1645.12it/s]warmup should be done:  38%|███▊      | 1147/3000 [00:00<00:01, 1617.86it/s]warmup should be done:  38%|███▊      | 1135/3000 [00:00<00:01, 1595.63it/s]warmup should be done:  37%|███▋      | 1118/3000 [00:00<00:01, 1571.74it/s]warmup should be done:  37%|███▋      | 1116/3000 [00:00<00:01, 1405.90it/s]warmup should be done:  43%|████▎     | 1296/3000 [00:00<00:01, 1620.04it/s]warmup should be done:  43%|████▎     | 1304/3000 [00:00<00:01, 1623.57it/s]warmup should be done:  41%|████      | 1226/3000 [00:00<00:01, 1523.67it/s]warmup should be done:  44%|████▍     | 1325/3000 [00:00<00:01, 1644.83it/s]warmup should be done:  44%|████▎     | 1309/3000 [00:00<00:01, 1617.21it/s]warmup should be done:  43%|████▎     | 1295/3000 [00:00<00:01, 1593.96it/s]warmup should be done:  43%|████▎     | 1276/3000 [00:00<00:01, 1551.48it/s]warmup should be done:  42%|████▏     | 1273/3000 [00:00<00:01, 1452.27it/s]warmup should be done:  49%|████▊     | 1459/3000 [00:00<00:00, 1620.88it/s]warmup should be done:  46%|████▌     | 1379/3000 [00:00<00:01, 1524.03it/s]warmup should be done:  49%|████▉     | 1467/3000 [00:00<00:00, 1621.38it/s]warmup should be done:  50%|████▉     | 1490/3000 [00:00<00:00, 1644.49it/s]warmup should be done:  49%|████▉     | 1471/3000 [00:00<00:00, 1611.68it/s]warmup should be done:  48%|████▊     | 1455/3000 [00:00<00:00, 1592.78it/s]warmup should be done:  48%|████▊     | 1438/3000 [00:00<00:00, 1569.95it/s]warmup should be done:  48%|████▊     | 1436/3000 [00:00<00:01, 1503.03it/s]warmup should be done:  54%|█████▍    | 1622/3000 [00:01<00:00, 1620.72it/s]warmup should be done:  51%|█████     | 1532/3000 [00:01<00:00, 1523.75it/s]warmup should be done:  55%|█████▌    | 1655/3000 [00:01<00:00, 1644.45it/s]warmup should be done:  54%|█████▍    | 1630/3000 [00:01<00:00, 1619.36it/s]warmup should be done:  54%|█████▍    | 1633/3000 [00:01<00:00, 1613.20it/s]warmup should be done:  54%|█████▍    | 1615/3000 [00:01<00:00, 1591.92it/s]warmup should be done:  53%|█████▎    | 1601/3000 [00:01<00:00, 1586.86it/s]warmup should be done:  53%|█████▎    | 1599/3000 [00:01<00:00, 1540.54it/s]warmup should be done:  60%|█████▉    | 1785/3000 [00:01<00:00, 1620.57it/s]warmup should be done:  56%|█████▌    | 1685/3000 [00:01<00:00, 1524.46it/s]warmup should be done:  61%|██████    | 1820/3000 [00:01<00:00, 1645.22it/s]warmup should be done:  60%|█████▉    | 1793/3000 [00:01<00:00, 1620.51it/s]warmup should be done:  60%|█████▉    | 1795/3000 [00:01<00:00, 1614.76it/s]warmup should be done:  59%|█████▉    | 1775/3000 [00:01<00:00, 1591.58it/s]warmup should be done:  59%|█████▉    | 1764/3000 [00:01<00:00, 1597.73it/s]warmup should be done:  59%|█████▊    | 1762/3000 [00:01<00:00, 1566.32it/s]warmup should be done:  65%|██████▍   | 1948/3000 [00:01<00:00, 1617.08it/s]warmup should be done:  61%|██████▏   | 1838/3000 [00:01<00:00, 1523.54it/s]warmup should be done:  66%|██████▌   | 1985/3000 [00:01<00:00, 1644.35it/s]warmup should be done:  65%|██████▌   | 1956/3000 [00:01<00:00, 1620.72it/s]warmup should be done:  65%|██████▌   | 1957/3000 [00:01<00:00, 1615.98it/s]warmup should be done:  64%|██████▍   | 1935/3000 [00:01<00:00, 1591.51it/s]warmup should be done:  64%|██████▍   | 1927/3000 [00:01<00:00, 1605.44it/s]warmup should be done:  64%|██████▍   | 1925/3000 [00:01<00:00, 1584.24it/s]warmup should be done:  70%|███████   | 2110/3000 [00:01<00:00, 1615.18it/s]warmup should be done:  66%|██████▋   | 1991/3000 [00:01<00:00, 1523.02it/s]warmup should be done:  71%|███████   | 2119/3000 [00:01<00:00, 1621.09it/s]warmup should be done:  72%|███████▏  | 2150/3000 [00:01<00:00, 1638.34it/s]warmup should be done:  71%|███████   | 2121/3000 [00:01<00:00, 1620.91it/s]warmup should be done:  70%|██████▉   | 2095/3000 [00:01<00:00, 1583.93it/s]warmup should be done:  70%|██████▉   | 2090/3000 [00:01<00:00, 1611.59it/s]warmup should be done:  70%|██████▉   | 2088/3000 [00:01<00:00, 1596.58it/s]warmup should be done:  71%|███████▏  | 2144/3000 [00:01<00:00, 1522.37it/s]warmup should be done:  76%|███████▌  | 2272/3000 [00:01<00:00, 1612.05it/s]warmup should be done:  76%|███████▌  | 2282/3000 [00:01<00:00, 1618.38it/s]warmup should be done:  76%|███████▌  | 2286/3000 [00:01<00:00, 1627.39it/s]warmup should be done:  77%|███████▋  | 2314/3000 [00:01<00:00, 1630.87it/s]warmup should be done:  75%|███████▌  | 2254/3000 [00:01<00:00, 1569.95it/s]warmup should be done:  75%|███████▌  | 2252/3000 [00:01<00:00, 1611.54it/s]warmup should be done:  75%|███████▌  | 2250/3000 [00:01<00:00, 1602.29it/s]warmup should be done:  81%|████████  | 2434/3000 [00:01<00:00, 1612.88it/s]warmup should be done:  77%|███████▋  | 2297/3000 [00:01<00:00, 1520.70it/s]warmup should be done:  82%|████████▏ | 2445/3000 [00:01<00:00, 1619.99it/s]warmup should be done:  83%|████████▎ | 2479/3000 [00:01<00:00, 1634.49it/s]warmup should be done:  82%|████████▏ | 2450/3000 [00:01<00:00, 1628.78it/s]warmup should be done:  80%|████████  | 2414/3000 [00:01<00:00, 1605.42it/s]warmup should be done:  80%|████████  | 2412/3000 [00:01<00:00, 1457.84it/s]warmup should be done:  80%|████████  | 2412/3000 [00:01<00:00, 1604.91it/s]warmup should be done:  87%|████████▋ | 2596/3000 [00:01<00:00, 1612.94it/s]warmup should be done:  82%|████████▏ | 2450/3000 [00:01<00:00, 1521.53it/s]warmup should be done:  87%|████████▋ | 2608/3000 [00:01<00:00, 1620.48it/s]warmup should be done:  88%|████████▊ | 2644/3000 [00:01<00:00, 1637.18it/s]warmup should be done:  87%|████████▋ | 2615/3000 [00:01<00:00, 1633.17it/s]warmup should be done:  86%|████████▌ | 2575/3000 [00:01<00:00, 1597.53it/s]warmup should be done:  86%|████████▌ | 2571/3000 [00:01<00:00, 1492.75it/s]warmup should be done:  86%|████████▌ | 2574/3000 [00:01<00:00, 1607.12it/s]warmup should be done:  92%|█████████▏| 2759/3000 [00:01<00:00, 1615.83it/s]warmup should be done:  92%|█████████▏| 2771/3000 [00:01<00:00, 1621.71it/s]warmup should be done:  94%|█████████▎| 2809/3000 [00:01<00:00, 1640.39it/s]warmup should be done:  87%|████████▋ | 2603/3000 [00:01<00:00, 1513.39it/s]warmup should be done:  93%|█████████▎| 2780/3000 [00:01<00:00, 1636.87it/s]warmup should be done:  91%|█████████ | 2735/3000 [00:01<00:00, 1590.15it/s]warmup should be done:  91%|█████████ | 2733/3000 [00:01<00:00, 1527.41it/s]warmup should be done:  91%|█████████ | 2737/3000 [00:01<00:00, 1613.21it/s]warmup should be done:  98%|█████████▊| 2925/3000 [00:01<00:00, 1627.49it/s]warmup should be done:  98%|█████████▊| 2935/3000 [00:01<00:00, 1627.10it/s]warmup should be done:  99%|█████████▉| 2975/3000 [00:01<00:00, 1645.78it/s]warmup should be done:  98%|█████████▊| 2947/3000 [00:01<00:00, 1643.79it/s]warmup should be done:  92%|█████████▏| 2755/3000 [00:01<00:00, 1507.17it/s]warmup should be done:  97%|█████████▋| 2896/3000 [00:01<00:00, 1593.24it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1643.43it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1627.54it/s]warmup should be done:  97%|█████████▋| 2897/3000 [00:01<00:00, 1558.07it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1622.65it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1618.43it/s]warmup should be done:  97%|█████████▋| 2902/3000 [00:01<00:00, 1621.68it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1589.69it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1572.80it/s]warmup should be done:  97%|█████████▋| 2907/3000 [00:01<00:00, 1508.57it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1559.81it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1519.72it/s]




warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]


warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   6%|▌         | 169/3000 [00:00<00:01, 1688.52it/s]warmup should be done:   6%|▌         | 166/3000 [00:00<00:01, 1659.46it/s]warmup should be done:   6%|▌         | 167/3000 [00:00<00:01, 1664.98it/s]warmup should be done:   6%|▌         | 167/3000 [00:00<00:01, 1665.07it/s]warmup should be done:   5%|▌         | 154/3000 [00:00<00:01, 1536.77it/s]warmup should be done:   6%|▌         | 169/3000 [00:00<00:01, 1682.37it/s]warmup should be done:   5%|▌         | 154/3000 [00:00<00:01, 1531.52it/s]warmup should be done:   5%|▌         | 162/3000 [00:00<00:01, 1610.84it/s]warmup should be done:  11%|█         | 332/3000 [00:00<00:01, 1658.01it/s]warmup should be done:  11%|█▏        | 338/3000 [00:00<00:01, 1685.77it/s]warmup should be done:  11%|█         | 334/3000 [00:00<00:01, 1666.39it/s]warmup should be done:  11%|█         | 334/3000 [00:00<00:01, 1666.64it/s]warmup should be done:  10%|█         | 314/3000 [00:00<00:01, 1569.06it/s]warmup should be done:  10%|█         | 309/3000 [00:00<00:01, 1537.72it/s]warmup should be done:  11%|█▏        | 338/3000 [00:00<00:01, 1679.69it/s]warmup should be done:  11%|█         | 326/3000 [00:00<00:01, 1622.13it/s]warmup should be done:  17%|█▋        | 498/3000 [00:00<00:01, 1657.39it/s]warmup should be done:  17%|█▋        | 502/3000 [00:00<00:01, 1670.69it/s]warmup should be done:  17%|█▋        | 502/3000 [00:00<00:01, 1670.73it/s]warmup should be done:  16%|█▌        | 465/3000 [00:00<00:01, 1546.81it/s]warmup should be done:  17%|█▋        | 506/3000 [00:00<00:01, 1679.16it/s]warmup should be done:  16%|█▌        | 475/3000 [00:00<00:01, 1584.00it/s]warmup should be done:  16%|█▋        | 489/3000 [00:00<00:01, 1620.79it/s]warmup should be done:  17%|█▋        | 507/3000 [00:00<00:01, 1670.75it/s]warmup should be done:  22%|██▏       | 665/3000 [00:00<00:01, 1661.43it/s]warmup should be done:  22%|██▏       | 670/3000 [00:00<00:01, 1673.39it/s]warmup should be done:  22%|██▏       | 670/3000 [00:00<00:01, 1670.83it/s]warmup should be done:  21%|██        | 621/3000 [00:00<00:01, 1551.49it/s]warmup should be done:  21%|██        | 634/3000 [00:00<00:01, 1584.23it/s]warmup should be done:  22%|██▎       | 675/3000 [00:00<00:01, 1680.13it/s]warmup should be done:  22%|██▏       | 652/3000 [00:00<00:01, 1620.41it/s]warmup should be done:  22%|██▎       | 675/3000 [00:00<00:01, 1664.63it/s]warmup should be done:  28%|██▊       | 833/3000 [00:00<00:01, 1665.74it/s]warmup should be done:  28%|██▊       | 838/3000 [00:00<00:01, 1671.69it/s]warmup should be done:  28%|██▊       | 838/3000 [00:00<00:01, 1670.79it/s]warmup should be done:  26%|██▌       | 779/3000 [00:00<00:01, 1558.58it/s]warmup should be done:  26%|██▋       | 793/3000 [00:00<00:01, 1582.09it/s]warmup should be done:  28%|██▊       | 844/3000 [00:00<00:01, 1677.43it/s]warmup should be done:  27%|██▋       | 815/3000 [00:00<00:01, 1617.72it/s]warmup should be done:  28%|██▊       | 842/3000 [00:00<00:01, 1662.80it/s]warmup should be done:  33%|███▎      | 1000/3000 [00:00<00:01, 1665.54it/s]warmup should be done:  34%|███▎      | 1006/3000 [00:00<00:01, 1673.88it/s]warmup should be done:  31%|███       | 937/3000 [00:00<00:01, 1564.93it/s]warmup should be done:  34%|███▎      | 1006/3000 [00:00<00:01, 1670.72it/s]warmup should be done:  32%|███▏      | 953/3000 [00:00<00:01, 1588.11it/s]warmup should be done:  34%|███▎      | 1012/3000 [00:00<00:01, 1672.65it/s]warmup should be done:  33%|███▎      | 977/3000 [00:00<00:01, 1617.49it/s]warmup should be done:  34%|███▎      | 1009/3000 [00:00<00:01, 1659.39it/s]warmup should be done:  36%|███▋      | 1094/3000 [00:00<00:01, 1565.40it/s]warmup should be done:  39%|███▉      | 1174/3000 [00:00<00:01, 1672.34it/s]warmup should be done:  39%|███▉      | 1167/3000 [00:00<00:01, 1660.62it/s]warmup should be done:  37%|███▋      | 1113/3000 [00:00<00:01, 1590.72it/s]warmup should be done:  39%|███▉      | 1174/3000 [00:00<00:01, 1670.25it/s]warmup should be done:  38%|███▊      | 1140/3000 [00:00<00:01, 1619.47it/s]warmup should be done:  39%|███▉      | 1180/3000 [00:00<00:01, 1666.78it/s]warmup should be done:  39%|███▉      | 1175/3000 [00:00<00:01, 1653.07it/s]warmup should be done:  45%|████▍     | 1342/3000 [00:00<00:00, 1672.17it/s]warmup should be done:  45%|████▍     | 1342/3000 [00:00<00:00, 1670.07it/s]warmup should be done:  44%|████▍     | 1334/3000 [00:00<00:01, 1656.52it/s]warmup should be done:  42%|████▏     | 1251/3000 [00:00<00:01, 1558.31it/s]warmup should be done:  42%|████▏     | 1273/3000 [00:00<00:01, 1585.02it/s]warmup should be done:  43%|████▎     | 1302/3000 [00:00<00:01, 1614.88it/s]warmup should be done:  45%|████▍     | 1348/3000 [00:00<00:00, 1670.02it/s]warmup should be done:  45%|████▍     | 1341/3000 [00:00<00:01, 1652.67it/s]warmup should be done:  50%|█████     | 1510/3000 [00:00<00:00, 1671.61it/s]warmup should be done:  47%|████▋     | 1408/3000 [00:00<00:01, 1559.69it/s]warmup should be done:  50%|█████     | 1510/3000 [00:00<00:00, 1667.04it/s]warmup should be done:  50%|█████     | 1500/3000 [00:00<00:00, 1652.39it/s]warmup should be done:  48%|████▊     | 1434/3000 [00:00<00:00, 1589.69it/s]warmup should be done:  49%|████▉     | 1465/3000 [00:00<00:00, 1617.39it/s]warmup should be done:  51%|█████     | 1517/3000 [00:00<00:00, 1674.74it/s]warmup should be done:  50%|█████     | 1507/3000 [00:00<00:00, 1652.00it/s]warmup should be done:  56%|█████▌    | 1678/3000 [00:01<00:00, 1672.46it/s]warmup should be done:  52%|█████▏    | 1566/3000 [00:01<00:00, 1562.93it/s]warmup should be done:  56%|█████▌    | 1666/3000 [00:01<00:00, 1653.86it/s]warmup should be done:  53%|█████▎    | 1594/3000 [00:01<00:00, 1591.44it/s]warmup should be done:  56%|█████▌    | 1677/3000 [00:01<00:00, 1662.95it/s]warmup should be done:  54%|█████▍    | 1628/3000 [00:01<00:00, 1619.90it/s]warmup should be done:  56%|█████▌    | 1686/3000 [00:01<00:00, 1678.59it/s]warmup should be done:  56%|█████▌    | 1673/3000 [00:01<00:00, 1653.66it/s]warmup should be done:  62%|██████▏   | 1846/3000 [00:01<00:00, 1669.15it/s]warmup should be done:  57%|█████▋    | 1723/3000 [00:01<00:00, 1564.08it/s]warmup should be done:  61%|██████    | 1832/3000 [00:01<00:00, 1654.54it/s]warmup should be done:  61%|██████▏   | 1844/3000 [00:01<00:00, 1662.91it/s]warmup should be done:  60%|█████▉    | 1791/3000 [00:01<00:00, 1621.59it/s]warmup should be done:  62%|██████▏   | 1854/3000 [00:01<00:00, 1670.28it/s]warmup should be done:  61%|██████▏   | 1839/3000 [00:01<00:00, 1652.81it/s]warmup should be done:  58%|█████▊    | 1754/3000 [00:01<00:00, 1434.65it/s]warmup should be done:  63%|██████▎   | 1881/3000 [00:01<00:00, 1567.38it/s]warmup should be done:  67%|██████▋   | 2011/3000 [00:01<00:00, 1665.00it/s]warmup should be done:  67%|██████▋   | 1998/3000 [00:01<00:00, 1651.42it/s]warmup should be done:  67%|██████▋   | 2013/3000 [00:01<00:00, 1662.53it/s]warmup should be done:  65%|██████▌   | 1954/3000 [00:01<00:00, 1622.33it/s]warmup should be done:  67%|██████▋   | 2022/3000 [00:01<00:00, 1667.09it/s]warmup should be done:  67%|██████▋   | 2005/3000 [00:01<00:00, 1649.39it/s]warmup should be done:  64%|██████▍   | 1914/3000 [00:01<00:00, 1480.10it/s]warmup should be done:  73%|███████▎  | 2178/3000 [00:01<00:00, 1666.20it/s]warmup should be done:  68%|██████▊   | 2039/3000 [00:01<00:00, 1568.58it/s]warmup should be done:  72%|███████▏  | 2165/3000 [00:01<00:00, 1654.02it/s]warmup should be done:  71%|███████   | 2118/3000 [00:01<00:00, 1624.74it/s]warmup should be done:  73%|███████▎  | 2180/3000 [00:01<00:00, 1658.29it/s]warmup should be done:  73%|███████▎  | 2189/3000 [00:01<00:00, 1665.45it/s]warmup should be done:  72%|███████▏  | 2170/3000 [00:01<00:00, 1647.71it/s]warmup should be done:  69%|██████▉   | 2079/3000 [00:01<00:00, 1527.13it/s]warmup should be done:  73%|███████▎  | 2196/3000 [00:01<00:00, 1568.36it/s]warmup should be done:  78%|███████▊  | 2346/3000 [00:01<00:00, 1669.36it/s]warmup should be done:  78%|███████▊  | 2332/3000 [00:01<00:00, 1658.13it/s]warmup should be done:  76%|███████▌  | 2282/3000 [00:01<00:00, 1626.61it/s]warmup should be done:  78%|███████▊  | 2346/3000 [00:01<00:00, 1658.32it/s]warmup should be done:  79%|███████▊  | 2358/3000 [00:01<00:00, 1671.78it/s]warmup should be done:  78%|███████▊  | 2336/3000 [00:01<00:00, 1651.22it/s]warmup should be done:  75%|███████▍  | 2243/3000 [00:01<00:00, 1559.73it/s]warmup should be done:  84%|████████▍ | 2514/3000 [00:01<00:00, 1671.69it/s]warmup should be done:  78%|███████▊  | 2354/3000 [00:01<00:00, 1570.15it/s]warmup should be done:  83%|████████▎ | 2500/3000 [00:01<00:00, 1662.06it/s]warmup should be done:  84%|████████▍ | 2513/3000 [00:01<00:00, 1659.60it/s]warmup should be done:  82%|████████▏ | 2445/3000 [00:01<00:00, 1624.14it/s]warmup should be done:  84%|████████▍ | 2527/3000 [00:01<00:00, 1676.31it/s]warmup should be done:  83%|████████▎ | 2503/3000 [00:01<00:00, 1654.44it/s]warmup should be done:  80%|████████  | 2407/3000 [00:01<00:00, 1581.10it/s]warmup should be done:  89%|████████▉ | 2682/3000 [00:01<00:00, 1672.10it/s]warmup should be done:  84%|████████▎ | 2512/3000 [00:01<00:00, 1571.33it/s]warmup should be done:  89%|████████▉ | 2679/3000 [00:01<00:00, 1658.04it/s]warmup should be done:  89%|████████▉ | 2667/3000 [00:01<00:00, 1659.59it/s]warmup should be done:  87%|████████▋ | 2608/3000 [00:01<00:00, 1619.67it/s]warmup should be done:  90%|████████▉ | 2696/3000 [00:01<00:00, 1679.62it/s]warmup should be done:  89%|████████▉ | 2670/3000 [00:01<00:00, 1657.28it/s]warmup should be done:  86%|████████▌ | 2572/3000 [00:01<00:00, 1599.00it/s]warmup should be done:  89%|████████▉ | 2670/3000 [00:01<00:00, 1571.50it/s]warmup should be done:  95%|█████████▌| 2850/3000 [00:01<00:00, 1668.89it/s]warmup should be done:  95%|█████████▍| 2845/3000 [00:01<00:00, 1655.65it/s]warmup should be done:  94%|█████████▍| 2833/3000 [00:01<00:00, 1654.66it/s]warmup should be done:  92%|█████████▏| 2771/3000 [00:01<00:00, 1622.15it/s]warmup should be done:  95%|█████████▌| 2864/3000 [00:01<00:00, 1678.52it/s]warmup should be done:  95%|█████████▍| 2837/3000 [00:01<00:00, 1659.27it/s]warmup should be done:  91%|█████████▏| 2738/3000 [00:01<00:00, 1615.19it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1675.21it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1668.68it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1664.24it/s]warmup should be done:  94%|█████████▍| 2828/3000 [00:01<00:00, 1570.80it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1658.32it/s]warmup should be done:  98%|█████████▊| 2935/3000 [00:01<00:00, 1624.72it/s]warmup should be done: 100%|█████████▉| 2999/3000 [00:01<00:00, 1652.00it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1656.35it/s]warmup should be done:  97%|█████████▋| 2904/3000 [00:01<00:00, 1626.57it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1620.86it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1576.98it/s]warmup should be done: 100%|█████████▉| 2986/3000 [00:01<00:00, 1573.22it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1564.47it/s]2022-12-12 07:42:51.397540: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f7bd0028c80 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 07:42:51.397609: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 07:42:52.436773: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f9b5782b9f0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 07:42:52.436828: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 07:42:52.437175: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f7c1002ccd0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 07:42:52.437233: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 07:42:52.438234: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f7c4002c830 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 07:42:52.438280: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 07:42:52.928004: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f9b5795b700 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 07:42:52.928070: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 07:42:52.953242: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f7b6402cd70 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 07:42:52.953320: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 07:42:52.953730: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f9b53f91210 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 07:42:52.953801: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 07:42:52.982364: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f9b57832fd0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 07:42:52.982458: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 07:42:53.684226: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 07:42:54.687879: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 07:42:54.694442: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 07:42:54.715920: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 07:42:55.219231: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 07:42:55.233972: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 07:42:55.272618: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 07:42:55.346209: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 07:42:56.593870: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 07:42:57.553269: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 07:42:57.595866: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 07:42:57.634851: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 07:42:58.090565: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 07:42:58.158513: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 07:42:58.202995: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 07:42:58.299117: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
[HCTR][07:43:21.278][ERROR][RK0][main]: replica 6 reaches 1000, calling init pre replica
[HCTR][07:43:21.278][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][07:43:21.282][ERROR][RK0][main]: replica 3 reaches 1000, calling init pre replica
[HCTR][07:43:21.282][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][07:43:21.287][ERROR][RK0][main]: coll ps creation done
[HCTR][07:43:21.287][ERROR][RK0][main]: replica 3 waits for coll ps creation barrier
[HCTR][07:43:21.289][ERROR][RK0][main]: coll ps creation done
[HCTR][07:43:21.289][ERROR][RK0][main]: replica 6 waits for coll ps creation barrier
[HCTR][07:43:21.297][ERROR][RK0][tid #140305500202752]: replica 2 reaches 1000, calling init pre replica
[HCTR][07:43:21.298][ERROR][RK0][tid #140305500202752]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][07:43:21.303][ERROR][RK0][main]: replica 7 reaches 1000, calling init pre replica
[HCTR][07:43:21.303][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][07:43:21.303][ERROR][RK0][main]: replica 4 reaches 1000, calling init pre replica
[HCTR][07:43:21.303][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][07:43:21.304][ERROR][RK0][main]: replica 5 reaches 1000, calling init pre replica
[HCTR][07:43:21.304][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][07:43:21.306][ERROR][RK0][tid #140305500202752]: coll ps creation done
[HCTR][07:43:21.306][ERROR][RK0][tid #140305500202752]: replica 2 waits for coll ps creation barrier
[HCTR][07:43:21.308][ERROR][RK0][main]: coll ps creation done
[HCTR][07:43:21.308][ERROR][RK0][main]: replica 5 waits for coll ps creation barrier
[HCTR][07:43:21.308][ERROR][RK0][main]: coll ps creation done
[HCTR][07:43:21.308][ERROR][RK0][main]: replica 4 waits for coll ps creation barrier
[HCTR][07:43:21.308][ERROR][RK0][main]: coll ps creation done
[HCTR][07:43:21.308][ERROR][RK0][main]: replica 7 waits for coll ps creation barrier
[HCTR][07:43:21.368][ERROR][RK0][tid #140305307268864]: replica 1 reaches 1000, calling init pre replica
[HCTR][07:43:21.368][ERROR][RK0][tid #140305307268864]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][07:43:21.374][ERROR][RK0][tid #140305307268864]: coll ps creation done
[HCTR][07:43:21.374][ERROR][RK0][tid #140305307268864]: replica 1 waits for coll ps creation barrier
[HCTR][07:43:21.404][ERROR][RK0][main]: replica 0 reaches 1000, calling init pre replica
[HCTR][07:43:21.404][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][07:43:21.409][ERROR][RK0][main]: coll ps creation done
[HCTR][07:43:21.409][ERROR][RK0][main]: replica 0 waits for coll ps creation barrier
[HCTR][07:43:21.409][ERROR][RK0][main]: replica 0 preparing frequency
[HCTR][07:43:22.254][ERROR][RK0][main]: replica 0 preparing frequency done
[HCTR][07:43:22.314][ERROR][RK0][main]: replica 0 calling init per replica
[HCTR][07:43:22.314][ERROR][RK0][main]: replica 6 calling init per replica
[HCTR][07:43:22.314][ERROR][RK0][main]: replica 3 calling init per replica
[HCTR][07:43:22.314][ERROR][RK0][main]: replica 4 calling init per replica
[HCTR][07:43:22.314][ERROR][RK0][tid #140305500202752]: replica 2 calling init per replica
[HCTR][07:43:22.314][ERROR][RK0][main]: replica 5 calling init per replica
[HCTR][07:43:22.314][ERROR][RK0][main]: replica 7 calling init per replica
[HCTR][07:43:22.314][ERROR][RK0][tid #140305307268864]: replica 1 calling init per replica
[HCTR][07:43:22.314][ERROR][RK0][main]: Calling build_v2
[HCTR][07:43:22.314][ERROR][RK0][main]: Calling build_v2
[HCTR][07:43:22.314][ERROR][RK0][main]: Calling build_v2
[HCTR][07:43:22.315][ERROR][RK0][main]: Calling build_v2
[HCTR][07:43:22.315][ERROR][RK0][tid #140305500202752]: Calling build_v2
[HCTR][07:43:22.315][ERROR][RK0][main]: Calling build_v2
[HCTR][07:43:22.315][ERROR][RK0][main]: Calling build_v2
[HCTR][07:43:22.315][ERROR][RK0][tid #140305307268864]: Calling build_v2
[HCTR][07:43:22.315][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][07:43:22.315][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][07:43:22.315][ERROR][RK0][tid #140305307268864]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][07:43:22.315][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][07:43:22.315][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][07:43:22.315][ERROR][RK0][tid #140305500202752]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][07:43:22.315][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][07:43:22.315][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[[[[[[[2022-12-12 07:43:222022-12-12 07:43:222022-12-12 07:43:222022-12-12 07:43:222022-12-12 07:43:22.2022-12-12 07:43:22.2022-12-12 07:43:22.[..315152.315150.315150315147315147: 315151: 315153: : : E2022-12-12 07:43:22: E: EEE .E E   /hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc315211 /hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc /hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc:: /hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc:/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc:::136E:136:136136136]  136] 136] ] ] using concurrent impl MPS/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc] using concurrent impl MPS] using concurrent impl MPSusing concurrent impl MPSusing concurrent impl MPS
:using concurrent impl MPS
using concurrent impl MPS


136

] using concurrent impl MPS
[2022-12-12 07:43:22.319545: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:178] v100x8, slow pcie
[2022-12-12 07:43:22.319586: E [/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 07:43:22:.196319591] : assigning 8 to cpuE
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:178] v100x8, slow pcie
[[2022-12-12 07:43:222022-12-12 07:43:22..319641319637: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:[:1962022-12-12 07:43:22178] .] assigning 8 to cpu[319672v100x8, slow pcie
2022-12-12 07:43:22: 
.E319683 : [/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE2022-12-12 07:43:22: .[212/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc319714[2022-12-12 07:43:22] :: 2022-12-12 07:43:22.build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8178E.319727
]  319727: v100x8, slow pcie/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: E
:E [[196 [/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 07:43:222022-12-12 07:43:22] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 07:43:22:..assigning 8 to cpu:.212[319776319782
178319789] 2022-12-12 07:43:22: : ] : build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8.EEv100x8, slow pcieE
319821 [ 
 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 07:43:22[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE[:.2022-12-12 07:43:22:2022-12-12 07:43:22: 2022-12-12 07:43:22178319875.213.196/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.] : 319905] 319907] :319915v100x8, slow pcieE: remote time is 8.68421: assigning 8 to cpu178: 
 E
E
] E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[  [v100x8, slow pcie :2022-12-12 07:43:22/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 07:43:22
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc178.::[.:[] 3200592122132022-12-12 07:43:223200711962022-12-12 07:43:22v100x8, slow pcie: ] ] .: ] .
Ebuild asymm link desc with 8X Tesla V100-SXM2-32GB out of 8remote time is 8.68421320120Eassigning 8 to cpu320131 

[:  
: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 07:43:22[E[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE:.2022-12-12 07:43:22 2022-12-12 07:43:22: 196320209./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.214/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] [: 320238:320239] :assigning 8 to cpu2022-12-12 07:43:22E: 212: cpu time is 97.0588196
. E] E
] 320292/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8 assigning 8 to cpu: :/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
E196[:: [] 2022-12-12 07:43:22214213/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 07:43:22assigning 8 to cpu.] ] :.[
320434cpu time is 97.0588remote time is 8.684212123204502022-12-12 07:43:22: 

] : .Ebuild asymm link desc with 8X Tesla V100-SXM2-32GB out of 8E[320487 
 2022-12-12 07:43:22: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.E[[::320542 2022-12-12 07:43:222022-12-12 07:43:22212213: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc..] ] E:320572320577build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8remote time is 8.68421 212: : 

/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] EE:[build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8 [ 2142022-12-12 07:43:22
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 07:43:22/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] .:.:cpu time is 97.0588[320700212320706213
2022-12-12 07:43:22: ] : ] .Ebuild asymm link desc with 8X Tesla V100-SXM2-32GB out of 8Eremote time is 8.68421320749 
 
: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE[::[ 2022-12-12 07:43:222142132022-12-12 07:43:22/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.] ] .:320857cpu time is 97.0588remote time is 8.68421320858213: 

: ] EE[remote time is 8.68421  2022-12-12 07:43:22/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.::[3209412132142022-12-12 07:43:22: ] ] .Eremote time is 8.68421cpu time is 97.0588320979 

: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE:[ 2142022-12-12 07:43:22/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] .:cpu time is 97.0588321048214
: ] Ecpu time is 97.0588 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:214] cpu time is 97.0588
[2022-12-12 07:44:42. 52637: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:118] solver created. now build & solve
[2022-12-12 07:44:42. 92823: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 381.47 MB
[2022-12-12 07:44:42. 92914: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 381.47 MB
[2022-12-12 07:44:42. 93967: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:74] mapping nid to rank...
[2022-12-12 07:44:42.166797: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:91] counting slots...
[2022-12-12 07:44:42.568818: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:105] Final num slot is 49
[2022-12-12 07:44:42.568911: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:109] counting blocks...
[2022-12-12 07:44:50. 79571: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:118] Final num block is 1023
[2022-12-12 07:44:50. 79666: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:123] counting freq and density...
[2022-12-12 07:44:51.834912: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:154] averaging freq and density...
[2022-12-12 07:44:51.835031: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:155] 1023
[2022-12-12 07:44:51.837944: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:123] solver built. now solve
[2022-12-12 07:44:51.838003: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:354] constructing optimal solver, device=8, stream=1
1023 blocks, 8 devices
[2022-12-12 07:44:52. 64615: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:527] Add Var...
[2022-12-12 07:44:52. 93257: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:545] Capacity...
[2022-12-12 07:44:52. 94684: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:548] Connect CPU...
[2022-12-12 07:44:52.114967: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:550] Connect Access To Storage...
[2022-12-12 07:44:52.631379: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:554] Time...
[2022-12-12 07:45:13.924248: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:569] Coll Cache init block placement array
[2022-12-12 07:45:13.933900: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:645] Coll Cache init block placement array done
[2022-12-12 07:45:13.934310: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:647] Coll Cache model reset done
[2022-12-12 07:45:13.981395: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:127] solver solved
[2022-12-12 07:45:13.981502: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:195] 0 solved master
[2022-12-12 07:45:13.981534: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 0 solved
[2022-12-12 07:45:13.981562: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 0 initing device 0
[2022-12-12 07:45:13.982122: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-12 07:45:13.982171: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 07:45:13.985101: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 07:45:13.985881: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 07:45:13.996918: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 2 solved
[2022-12-12 07:45:13.996991: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 2 initing device 2
[2022-12-12 07:45:13.997165: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 5 solved
[2022-12-12 07:45:13.997223: E [/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc2022-12-12 07:45:13:.205997223] : worker 0 thread 5 initing device 5E
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] [6 solved2022-12-12 07:45:13
.997261: E[ 2022-12-12 07:45:13/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc.:997294202: ] E 1 solved/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc
:205] worker 0 thread 6 initing device 6[
2022-12-12 07:45:13.997330: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 1 initing device 1
[2022-12-12 07:45:13.997348: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 7 solved
[2022-12-12 07:45:13.997420: E [/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc2022-12-12 07:45:13:.205997427] : worker 0 thread 7 initing device 7E
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-12 07:45:13.997529: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 07:45:13.997662: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-12 07:45:13.997707: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 07:45:13.997727: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8[
2022-12-12 07:45:13.997751: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815[] 2022-12-12 07:45:13Building Coll Cache with ... num gpu device is 8.
997774: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB[
2022-12-12 07:45:13.997801: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 07:45:13.997912: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-12 07:45:13.997962: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 07:45:13.999869: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 4 solved
[2022-12-12 07:45:13.999924: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 4 initing device 4
[2022-12-12 07:45:14.    70: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 3 solved
[2022-12-12 07:45:14.   132: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 3 initing device 3
[2022-12-12 07:45:14.   341: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-12 07:45:14.   384: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 07:45:14.   572: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-12 07:45:14.   618: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 07:45:14.   786: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 07:45:14.  1311: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 07:45:14.  1369: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 07:45:14.  1428: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 07:45:14.  1970: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 07:45:14.  4829: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 07:45:14.  5067: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 07:45:14.  5159: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 07:45:14.  5213: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 07:45:14.  5719: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 07:45:14.  5773: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 07:45:14.  5836: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 07:45:14.  9171: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 07:45:14.  9465: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 07:45:14. 66426: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1023.00 Bytes
[2022-12-12 07:45:14. 71808: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1023
[2022-12-12 07:45:14. 71925: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 07:45:14. 72853: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 114.82 MB
[2022-12-12 07:45:14. 73716: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 07:45:14. 74851: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 07:45:14. 74899: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 112.28 MB
[2022-12-12 07:45:14. 86198: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1023.00 Bytes
[[[2022-12-12 07:45:142022-12-12 07:45:142022-12-12 07:45:14... 86298 86298 86298: : : EEE   /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:::198019801980] ] ] eager alloc mem 1023.00 Byteseager alloc mem 1023.00 Byteseager alloc mem 1023.00 Bytes


[[2022-12-12 07:45:142022-12-12 07:45:14.. 86666 86680: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::19801980] ] eager alloc mem 1023.00 Byteseager alloc mem 1023.00 Bytes

[2022-12-12 07:45:14. 90121: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1023.00 Bytes
[2022-12-12 07:45:14.103396: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1023
[2022-12-12 07:45:14.103485: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638[] 2022-12-12 07:45:14eager release cuda mem 400000000.
103487: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1023
[[2022-12-12 07:45:142022-12-12 07:45:14..103584103571: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::638638] ] eager release cuda mem 400000000eager release cuda mem 1023

[[2022-12-12 07:45:142022-12-12 07:45:14..103653103671: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::638638] ] eager release cuda mem 400000000eager release cuda mem 1023

[2022-12-12 07:45:14.103753: E[ 2022-12-12 07:45:14/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc.:103748638: ] Eeager release cuda mem 400000000 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1023
[[2022-12-12 07:45:142022-12-12 07:45:14..103845103863: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::638638] ] eager release cuda mem 1023eager release cuda mem 400000000[

2022-12-12 07:45:14.103926: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1023
[2022-12-12 07:45:14.103991: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 07:45:14.104016: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 07:45:14.108333: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 114.82 MB
[2022-12-12 07:45:14.108845: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 114.82 MB
[2022-12-12 07:45:14.109346: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 114.82 MB
[2022-12-12 07:45:14.109874: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 114.82 MB
[2022-12-12 07:45:14.110436: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 114.82 MB
[2022-12-12 07:45:14.110947: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 114.82 MB
[2022-12-12 07:45:14.111479: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 114.82 MB
[2022-12-12 07:45:14.114149: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 07:45:14.114200: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 07:45:14.114243: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 07:45:14.114292: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 07:45:14.114385: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 07:45:14.114429: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 07:45:14.114485: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 07:45:14.115246: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 07:45:14.115297: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc[:2022-12-12 07:45:1443.] 115306WORKER[0] alloc host memory 114.39 MB: [
E2022-12-12 07:45:14 ./hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc115337:: 638E]  eager release cuda mem 625663/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc
:638] eager release cuda mem 625663
[2022-12-12 07:45:14.115387: [E2022-12-12 07:45:14 .[/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc1154002022-12-12 07:45:14:: .638W115409]  : eager release cuda mem 625663/hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.ccW
: 43/hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc] :WORKER[0] alloc host memory 114.36 MB43
] WORKER[0] alloc host memory 114.37 MB[
2022-12-12 07:45:14.115470: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 112.59 MB
[2022-12-12 07:45:14.115497: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[[2022-12-12 07:45:142022-12-12 07:45:14..115546115543: : WE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc[/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:2022-12-12 07:45:14:43.638] 115592] WORKER[0] alloc host memory 113.84 MB: eager release cuda mem 625663
E
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 07:45:14.115678: W [/hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc2022-12-12 07:45:14:.43115690] : WORKER[0] alloc host memory 114.37 MBW
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 114.24 MB
[2022-12-12 07:45:14.149508: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 07:45:14.150146: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 07:45:14.150188: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 14.04 GB
[2022-12-12 07:45:14.188517: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 07:45:14.188923: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 07:45:14.189166: E [/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc2022-12-12 07:45:14:.638189162] : eager release cuda mem 25855E
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 07:45:14.189229: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 14.30 GB
[2022-12-12 07:45:14.189558: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 07:45:14.189603: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 14.30 GB
[2022-12-12 07:45:14.189788: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 07:45:14.189832: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 14.08 GB
[2022-12-12 07:45:14.190752: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 07:45:14.191365: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 07:45:14.191409: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 14.29 GB
[2022-12-12 07:45:14.191693: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 07:45:14.192037: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 07:45:14.192323: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 07:45:14.192366: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 14.23 GB
[2022-12-12 07:45:14.192658: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 07:45:14.192701: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 14.30 GB
[2022-12-12 07:45:14.192937: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 07:45:14.193544: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 07:45:14.193586: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 14.30 GB
[[[[[[[[2022-12-12 07:45:182022-12-12 07:45:182022-12-12 07:45:182022-12-12 07:45:182022-12-12 07:45:182022-12-12 07:45:182022-12-12 07:45:182022-12-12 07:45:18........733748733748733748733749733749733748733748733749: : : : : : : : EEEEEEEE        /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::::::::19261926192619261926192619261926] ] ] ] ] ] ] ] Device 0 init p2p of link 3Device 4 init p2p of link 5Device 1 init p2p of link 7Device 6 init p2p of link 0Device 5 init p2p of link 6Device 7 init p2p of link 4Device 2 init p2p of link 1Device 3 init p2p of link 2







[[[2022-12-12 07:45:18[2022-12-12 07:45:18[2022-12-12 07:45:18.[2022-12-12 07:45:18[.[2022-12-12 07:45:18.7343552022-12-12 07:45:18.2022-12-12 07:45:187343552022-12-12 07:45:18.734355: .734361.: .734363: E734370: 734370E734375: E : E:  : E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE E/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu: /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu : /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu1980/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] :1980:] :1980] eager alloc mem 611.00 KB1980] 1980eager alloc mem 611.00 KB1980] eager alloc mem 611.00 KB
] eager alloc mem 611.00 KB] 
] eager alloc mem 611.00 KB
eager alloc mem 611.00 KB
eager alloc mem 611.00 KBeager alloc mem 611.00 KB



[2022-12-12 07:45:18[.2022-12-12 07:45:18735569[.: 2022-12-12 07:45:18735572E.:  [735583E/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc2022-12-12 07:45:18:  :.E/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc638735599 :[] [[: /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc638[2022-12-12 07:45:18eager release cuda mem 6256632022-12-12 07:45:182022-12-12 07:45:18E:] 2022-12-12 07:45:18.
.. 638eager release cuda mem 625663.735627735629735650/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc] 
735639: : : :eager release cuda mem 625663: EEE638
E   ]  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cceager release cuda mem 625663/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:::
:638638638638] ] ] ] eager release cuda mem 625663eager release cuda mem 625663eager release cuda mem 625663eager release cuda mem 625663



[2022-12-12 07:45:18.756988: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 2
[2022-12-12 07:45:18.757157: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 07:45:18.758106: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 07:45:18.758615: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 6 init p2p of link 5
[2022-12-12 07:45:18.758777: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 07:45:18.759143: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 4 init p2p of link 7
[2022-12-12 07:45:18.759313: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 07:45:18.759478: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 5 init p2p of link 4
[2022-12-12 07:45:18.759637: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 07:45:18.759725: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 07:45:18.760061: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 3 init p2p of link 0
[2022-12-12 07:45:18.760238[: 2022-12-12 07:45:18E. 760246/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc: :E638 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cueager release cuda mem 625663:
1980] eager alloc mem 611.00 KB
[2022-12-12 07:45:18.760346: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 7 init p2p of link 1
[2022-12-12 07:45:18.760514: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 07:45:18.760578: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 07:45:18[.2022-12-12 07:45:18761163.: 761172E:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu :/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu1926:] 1926Device 0 init p2p of link 6] 
Device 2 init p2p of link 3
[2022-12-12 07:45:18.761254: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[[2022-12-12 07:45:182022-12-12 07:45:18..761368761370: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::19801980] ] [eager alloc mem 611.00 KBeager alloc mem 611.00 KB2022-12-12 07:45:18

.761410: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 07:45:18.762221: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 07:45:18.762319: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 07:45:18.775223: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 3
[2022-12-12 07:45:18.775370: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 07:45:18.776277: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 07:45:18.777319: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 0 init p2p of link 1
[2022-12-12 07:45:18.777461: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 07:45:18.778430: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 07:45:18.781078: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 4 init p2p of link 2
[2022-12-12 07:45:18.781215: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 07:45:18.[7816402022-12-12 07:45:18: .E781648 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE: 1926/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu] :Device 6 init p2p of link 41926
] Device 7 init p2p of link 6
[[2022-12-12 07:45:182022-12-12 07:45:18..781830781833: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::19801980] ] eager alloc mem 611.00 KBeager alloc mem 611.00 KB

[2022-12-12 07:45:18.782162: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 07:45:18.782608: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 5 init p2p of link 7
[2022-12-12 07:45:18.782674: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 2 init p2p of link 0
[2022-12-12 07:45:18.782739: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 07:45:18.782798: E[ [2022-12-12 07:45:18/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc2022-12-12 07:45:18.:.782812638782814: ] : Eeager release cuda mem 625663E 
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::1980638] ] eager alloc mem 611.00 KBeager release cuda mem 625663

[2022-12-12 07:45:18.783035: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 3 init p2p of link 5
[2022-12-12 07:45:18.783193: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 07:45:18.783609: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 07:45:18.783825: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 07:45:18.784024: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 07:45:18.791934: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 0
[2022-12-12 07:45:18.792062: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 07:45:18.793023: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 07:45:18.796328: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 0 init p2p of link 2
[2022-12-12 07:45:18.796456: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 07:45:18.797377: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 07:45:18.805255: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 3 init p2p of link 1
[2022-12-12 07:45:18.805375: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 07:45:18.806295: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 07:45:18.810458: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 7 init p2p of link 5
[2022-12-12 07:45:18.810515: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 4 init p2p of link 6
[2022-12-12 07:45:18.810577: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 07:45:18.810638: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 07:45:18.810809: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 5 init p2p of link 3
[2022-12-12 07:45:18.810935: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 07:45:18.811485: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 07:45:18.811553: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 07:45:18.811821: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 07:45:18.812944: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 6 init p2p of link 7
[2022-12-12 07:45:18.813061: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 07:45:18.813930: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 07:45:18.816799: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 2 init p2p of link 4
[2022-12-12 07:45:18.816928: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 07:45:18.817274: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 120400004
[2022-12-12 07:45:18.817760: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 07:45:18.819357: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 120400004
[2022-12-12 07:45:18.820094: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 29948202 / 100000000 nodes ( 29.95 %~30.00 %) | remote 70051798 / 100000000 nodes ( 70.05 %) | cpu 0 / 100000000 nodes ( 0.00 %) | 14.29 GB | 4.8223 secs 
[2022-12-12 07:45:18.821751: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 29432791 / 100000000 nodes ( 29.43 %~30.00 %) | remote 70567209 / 100000000 nodes ( 70.57 %) | cpu 0 / 100000000 nodes ( 0.00 %) | 14.04 GB | 4.83959 secs 
[2022-12-12 07:45:18.823038: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 120400004
[2022-12-12 07:45:18.824091: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 20.68 GB
[2022-12-12 07:45:18.825260: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 29842209 / 100000000 nodes ( 29.84 %~30.00 %) | remote 70157791 / 100000000 nodes ( 70.16 %) | cpu 0 / 100000000 nodes ( 0.00 %) | 14.23 GB | 4.82465 secs 
[2022-12-12 07:45:18.832600: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 120400004
[2022-12-12 07:45:18.833717: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 29981908 / 100000000 nodes ( 29.98 %~30.00 %) | remote 70018092 / 100000000 nodes ( 70.02 %) | cpu 0 / 100000000 nodes ( 0.00 %) | 14.30 GB | 4.8362 secs 
[2022-12-12 07:45:18.834380: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 120400004
[2022-12-12 07:45:18.834745: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 120400004
[2022-12-12 07:45:18.834801: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 120400004
[2022-12-12 07:45:18.835401: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 120400004
[2022-12-12 07:45:18.836733: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 29986077 / 100000000 nodes ( 29.99 %~30.00 %) | remote 70013923 / 100000000 nodes ( 70.01 %) | cpu 0 / 100000000 nodes ( 0.00 %) | 14.30 GB | 4.83878 secs 
[2022-12-12 07:45:18.837012: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 29977646 / 100000000 nodes ( 29.98 %~30.00 %) | remote 70022354 / 100000000 nodes ( 70.02 %) | cpu 0 / 100000000 nodes ( 0.00 %) | 14.30 GB | 4.83931 secs 
[2022-12-12 07:45:18.837353: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 29515632 / 100000000 nodes ( 29.52 %~30.00 %) | remote 70484368 / 100000000 nodes ( 70.48 %) | cpu 0 / 100000000 nodes ( 0.00 %) | 14.08 GB | 4.83698 secs 
[2022-12-12 07:45:18.860704: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 29982110 / 100000000 nodes ( 29.98 %~30.00 %) | remote 70017890 / 100000000 nodes ( 70.02 %) | cpu 0 / 100000000 nodes ( 0.00 %) | 14.30 GB | 4.86294 secs 
[2022-12-12 07:45:20.531457: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 20.94 GB
[2022-12-12 07:45:20.532155: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 20.94 GB
[2022-12-12 07:45:20.532696: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 20.94 GB
[2022-12-12 07:45:22.301377: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 21.21 GB
[2022-12-12 07:45:22.301507: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 21.21 GB
[2022-12-12 07:45:22.302592: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 21.21 GB
[2022-12-12 07:45:24.310850: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 21.42 GB
[2022-12-12 07:45:24.311259: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 21.42 GB
[2022-12-12 07:45:24.311552: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 21.42 GB
[2022-12-12 07:45:26.792512: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 21.63 GB
[2022-12-12 07:45:26.792667: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 21.63 GB
[2022-12-12 07:45:26.793622: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 21.63 GB
[2022-12-12 07:45:29.577845: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 22.09 GB
[2022-12-12 07:45:29.581710: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 22.09 GB
[2022-12-12 07:45:29.582497: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 22.09 GB
[2022-12-12 07:45:32.785208: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 22.29 GB
[2022-12-12 07:45:32.785476: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 22.29 GB
[HCTR][07:45:32.875][ERROR][RK0][main]: replica 7 calling init per replica done, doing barrier
[HCTR][07:45:32.875][ERROR][RK0][tid #140305500202752]: replica 2 calling init per replica done, doing barrier
[HCTR][07:45:32.875][ERROR][RK0][main]: replica 0 calling init per replica done, doing barrier
[HCTR][07:45:32.875][ERROR][RK0][main]: replica 6 calling init per replica done, doing barrier
[HCTR][07:45:32.875][ERROR][RK0][main]: replica 3 calling init per replica done, doing barrier
[HCTR][07:45:32.875][ERROR][RK0][main]: replica 5 calling init per replica done, doing barrier
[HCTR][07:45:32.875][ERROR][RK0][tid #140305307268864]: replica 1 calling init per replica done, doing barrier
[HCTR][07:45:32.875][ERROR][RK0][main]: replica 4 calling init per replica done, doing barrier
[HCTR][07:45:32.875][ERROR][RK0][main]: replica 6 calling init per replica done, doing barrier done
[HCTR][07:45:32.875][ERROR][RK0][main]: replica 7 calling init per replica done, doing barrier done
[HCTR][07:45:32.875][ERROR][RK0][main]: replica 4 calling init per replica done, doing barrier done
[HCTR][07:45:32.875][ERROR][RK0][main]: replica 3 calling init per replica done, doing barrier done
[HCTR][07:45:32.875][ERROR][RK0][tid #140305500202752]: replica 2 calling init per replica done, doing barrier done
[HCTR][07:45:32.875][ERROR][RK0][main]: replica 5 calling init per replica done, doing barrier done
[HCTR][07:45:32.875][ERROR][RK0][tid #140305307268864]: replica 1 calling init per replica done, doing barrier done
[HCTR][07:45:32.875][ERROR][RK0][main]: replica 0 calling init per replica done, doing barrier done
[HCTR][07:45:32.876][ERROR][RK0][main]: init per replica done
[HCTR][07:45:32.876][ERROR][RK0][tid #140305500202752]: init per replica done
[HCTR][07:45:32.876][ERROR][RK0][main]: init per replica done
[HCTR][07:45:32.876][ERROR][RK0][main]: init per replica done
[HCTR][07:45:32.876][ERROR][RK0][main]: init per replica done
[HCTR][07:45:32.876][ERROR][RK0][main]: init per replica done
[HCTR][07:45:32.876][ERROR][RK0][tid #140305307268864]: init per replica done
[HCTR][07:45:32.879][ERROR][RK0][main]: init per replica done
[HCTR][07:45:32.882][ERROR][RK0][main]: 2 allocated 3276800 at 0x7f84df920000
[HCTR][07:45:32.882][ERROR][RK0][main]: 6 allocated 3276800 at 0x7f84df920000
[HCTR][07:45:32.882][ERROR][RK0][main]: 2 allocated 6553600 at 0x7f9d4f000000
[HCTR][07:45:32.882][ERROR][RK0][main]: 2 allocated 3276800 at 0x7f9d4f640000
[HCTR][07:45:32.882][ERROR][RK0][main]: 6 allocated 6553600 at 0x7f9d4f000000
[HCTR][07:45:32.882][ERROR][RK0][main]: 2 allocated 6553600 at 0x7f9d4f960000
[HCTR][07:45:32.882][ERROR][RK0][main]: 6 allocated 3276800 at 0x7f9d4f640000
[HCTR][07:45:32.882][ERROR][RK0][main]: 6 allocated 6553600 at 0x7f9d4f960000
[HCTR][07:45:32.882][ERROR][RK0][tid #140305374377728]: 3 allocated 3276800 at 0x7f84df920000
[HCTR][07:45:32.882][ERROR][RK0][tid #140305307268864]: 1 allocated 3276800 at 0x7f84df920000
[HCTR][07:45:32.882][ERROR][RK0][tid #140305374377728]: 3 allocated 6553600 at 0x7f9d4f000000
[HCTR][07:45:32.882][ERROR][RK0][tid #140305307268864]: 1 allocated 6553600 at 0x7f9d4f000000
[HCTR][07:45:32.882][ERROR][RK0][tid #140305374377728]: 3 allocated 3276800 at 0x7f9d4f640000
[HCTR][07:45:32.882][ERROR][RK0][tid #140305365985024]: 4 allocated 3276800 at 0x7f84e3920000
[HCTR][07:45:32.882][ERROR][RK0][tid #140305307268864]: 1 allocated 3276800 at 0x7f9d4f640000
[HCTR][07:45:32.882][ERROR][RK0][tid #140305374377728]: 3 allocated 6553600 at 0x7f9d4f960000
[HCTR][07:45:32.882][ERROR][RK0][tid #140305365985024]: 4 allocated 6553600 at 0x7f9d4f000000
[HCTR][07:45:32.882][ERROR][RK0][tid #140305307268864]: 1 allocated 6553600 at 0x7f9d4f960000
[HCTR][07:45:32.882][ERROR][RK0][tid #140305365985024]: 4 allocated 3276800 at 0x7f9d4f640000
[HCTR][07:45:32.882][ERROR][RK0][tid #140305365985024]: 4 allocated 6553600 at 0x7f9d4f960000
[HCTR][07:45:32.882][ERROR][RK0][main]: 7 allocated 3276800 at 0x7f84df920000
[HCTR][07:45:32.882][ERROR][RK0][main]: 7 allocated 6553600 at 0x7f9d4f000000
[HCTR][07:45:32.882][ERROR][RK0][main]: 7 allocated 3276800 at 0x7f9d4f640000
[HCTR][07:45:32.882][ERROR][RK0][main]: 7 allocated 6553600 at 0x7f9d4f960000
[HCTR][07:45:32.882][ERROR][RK0][tid #140305374377728]: 5 allocated 3276800 at 0x7f84d7920000
[HCTR][07:45:32.882][ERROR][RK0][tid #140305374377728]: 5 allocated 6553600 at 0x7f9d4f000000
[HCTR][07:45:32.882][ERROR][RK0][tid #140305374377728]: 5 allocated 3276800 at 0x7f9d4f640000
[HCTR][07:45:32.882][ERROR][RK0][tid #140305374377728]: 5 allocated 6553600 at 0x7f9d4f960000
[HCTR][07:45:32.885][ERROR][RK0][tid #140305374377728]: 0 allocated 3276800 at 0x7f9d50f20000
[HCTR][07:45:32.885][ERROR][RK0][tid #140305374377728]: 0 allocated 6553600 at 0x7f9d51400000
[HCTR][07:45:32.885][ERROR][RK0][tid #140305374377728]: 0 allocated 3276800 at 0x7f9d5210e800
[HCTR][07:45:32.885][ERROR][RK0][tid #140305374377728]: 0 allocated 6553600 at 0x7f9d5242e800








