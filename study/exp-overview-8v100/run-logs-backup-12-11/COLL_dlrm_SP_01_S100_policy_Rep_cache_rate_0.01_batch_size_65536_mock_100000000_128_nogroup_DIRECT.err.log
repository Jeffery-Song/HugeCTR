2022-12-11 20:21:56.933385: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:56.947583: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:56.954114: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:56.958076: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:56.969860: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:56.975855: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:56.994431: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.000937: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.035558: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.036845: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.037892: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.038942: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.040026: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.041084: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.042164: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.043221: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.048960: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.049940: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.050822: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.051772: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.052712: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.053638: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.054594: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.055548: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.056748: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.058263: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.058951: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.059390: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.060544: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.060672: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.062034: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.062096: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.063301: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.063580: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.064938: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.065363: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.066897: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.067637: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.068344: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.069877: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.069954: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.070793: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.070997: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.072062: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.072335: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.073635: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.073903: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.074669: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.076557: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.076773: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.077248: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.077456: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.078909: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.079148: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.079742: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.080115: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.081399: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.081749: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.082189: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.082934: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.084314: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.084780: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.084972: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.085993: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.087379: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.087413: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.087621: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.087635: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.088948: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.089146: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.090830: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.091195: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.091729: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.092161: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.092376: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.093700: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.094928: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.094950: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.095245: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 20:21:57.095313: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.096286: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.097317: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.097719: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.097873: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.098759: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.099526: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.100399: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.101016: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.101389: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.102103: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.102696: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.102933: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.103807: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.104411: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.104502: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.105124: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.105538: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.106301: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.107036: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.107428: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.107868: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.108592: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.109327: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.110106: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.111192: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.112343: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.113406: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.114476: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.116356: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.117542: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.118127: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.118948: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.119852: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.123186: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.123300: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.124662: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.124756: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.136524: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.137895: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.144133: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.150293: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.152813: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.159293: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.159755: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.163335: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.163873: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.163920: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.163953: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.163999: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.164375: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.164594: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.168111: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.168423: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.168514: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.168602: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.168652: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.169259: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.173202: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.173466: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.173614: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.173663: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.173660: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 20:21:57.173708: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.174029: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.176905: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.177210: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.177273: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.177425: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.177446: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.177744: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.181095: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.181344: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.181406: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.181426: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.181654: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.181914: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.183357: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.185389: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.185724: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.185949: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.186011: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.186308: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.186674: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.187990: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.190017: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.190392: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.190526: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.190680: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.191058: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.191207: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.192802: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.194606: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.194957: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.195042: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.195167: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.195517: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.195684: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.199198: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.199555: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.199638: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.200112: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.200249: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.200461: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.203842: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.204070: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.204202: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.204579: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.204672: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.204898: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.208075: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.208236: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.208358: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.208630: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.208723: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.209181: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.212537: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.212680: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.212805: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.213048: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.213135: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.213379: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.217098: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.217212: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.217387: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.217731: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.217777: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.218894: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.221510: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.221556: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.221714: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.221943: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.222078: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.223614: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.225864: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.225909: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.226053: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.226238: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.226411: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.228086: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.231080: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.231310: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.231506: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.231715: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.232084: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.232225: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.235103: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.235554: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.236153: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.236285: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.236698: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.236815: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.239333: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.239707: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.239993: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.240216: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.240779: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.245265: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.248444: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.249031: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.249242: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.249363: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.249930: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.250132: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.252473: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.253421: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.253593: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.253828: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.254216: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.254600: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.285130: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.285528: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.285796: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.286072: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.286779: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.289358: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.289934: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.290484: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.290832: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 20:21:57.295626: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.296802: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.299681: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.300256: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.300262: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.300298: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.300448: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.301506: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.305426: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.307812: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.307948: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 20:21:57.308714: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 20:21:57.308730: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 20:21:57.308761: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 20:21:57.309176: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 20:21:57.317836: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.318426: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.318768: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.319003: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.319054: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.321889: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.323116: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.323752: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.323892: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.323931: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.326627: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.327816: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.328359: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.328617: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:57.328653: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:58.416643: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:58.417507: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:58.418056: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:58.418778: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 20:21:58.418833: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-11 20:21:58.436974: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:58.438005: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:58.438664: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:58.440103: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:58.440923: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:58.443790: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-11 20:21:58.489319: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 20:21:58.489519: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 20:21:58.543802: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12340
2022-12-11 20:21:58.553708: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:58.555087: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:58.556233: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:58.557257: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 20:21:58.557307: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-11 20:21:58.574555: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:58.575756: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:58.576870: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:58.578415: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:58.579635: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:58.580896: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:1/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-11 20:21:58.667240: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:58.668627: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:58.669698: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:58.670763: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 20:21:58.670829: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-11 20:21:58.673769: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 20:21:58.673946: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 20:21:58.675661: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12341
2022-12-11 20:21:58.689411: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:58.690888: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:58.691849: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:58.692953: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:58.694476: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:58.695625: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:2/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-11 20:21:58.718930: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:58.720132: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:58.721199: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:58.722328: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 20:21:58.722381: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-11 20:21:58.732498: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:58.733125: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:58.733849: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:58.734397: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 20:21:58.734451: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-11 20:21:58.739774: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:58.740391: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:58.740907: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:58.741481: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:58.741623: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:58.742574: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:58.742652: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:58.743508: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:4/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-11 20:21:58.743642: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:58.744231: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 20:21:58.744300: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-11 20:21:58.745134: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:58.745734: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:58.746260: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:58.746764: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 20:21:58.746812: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-11 20:21:58.752397: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:58.753071: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:58.753685: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:58.753721: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:58.754832: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:58.754863: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:58.755896: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:58.755921: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:58.756845: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 20:21:58.756887: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:5/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-11 20:21:58.756893: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-11 20:21:58.762885: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:58.763550: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:58.764110: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:58.764200: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:58.765997: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:58.766049: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:58.768218: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:58.768284: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:58.770311: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:7/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-11 20:21:58.770354: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:58.771889: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:58.772815: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:6/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-11 20:21:58.773683: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:58.774897: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:58.776033: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:58.777240: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:58.778383: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:21:58.779529: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:3/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-11 20:21:58.785381: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 20:21:58.785594: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 20:21:58.786565: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12342
2022-12-11 20:21:58.789320: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 20:21:58.789493: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 20:21:58.790369: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12344
2022-12-11 20:21:58.801323: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 20:21:58.801503: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 20:21:58.803486: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12345
2022-12-11 20:21:58.816673: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 20:21:58.816862: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 20:21:58.817941: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12347
2022-12-11 20:21:58.818225: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 20:21:58.818371: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 20:21:58.820319: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12346
2022-12-11 20:21:58.824318: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 20:21:58.824493: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 20:21:58.826200: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12343
[HCTR][20:22:00.087][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][20:22:00.088][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][20:22:00.088][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][20:22:00.088][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][20:22:00.088][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][20:22:00.089][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][20:22:00.089][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][20:22:00.089][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 1it [00:01,  1.56s/it]warmup run: 1it [00:01,  1.55s/it]warmup run: 1it [00:01,  1.56s/it]warmup run: 1it [00:01,  1.54s/it]warmup run: 97it [00:01, 81.35it/s]warmup run: 1it [00:01,  1.54s/it]warmup run: 1it [00:01,  1.54s/it]warmup run: 99it [00:01, 83.26it/s]warmup run: 100it [00:01, 83.56it/s]warmup run: 1it [00:01,  1.51s/it]warmup run: 1it [00:01,  1.50s/it]warmup run: 195it [00:01, 177.75it/s]warmup run: 99it [00:01, 83.72it/s]warmup run: 96it [00:01, 81.01it/s]warmup run: 100it [00:01, 84.84it/s]warmup run: 199it [00:01, 181.76it/s]warmup run: 197it [00:01, 178.20it/s]warmup run: 96it [00:01, 82.78it/s]warmup run: 98it [00:01, 84.73it/s]warmup run: 293it [00:01, 284.17it/s]warmup run: 199it [00:01, 182.59it/s]warmup run: 193it [00:01, 176.72it/s]warmup run: 201it [00:01, 185.10it/s]warmup run: 300it [00:01, 291.92it/s]warmup run: 296it [00:01, 285.58it/s]warmup run: 193it [00:01, 180.32it/s]warmup run: 196it [00:01, 183.34it/s]warmup run: 391it [00:01, 394.60it/s]warmup run: 299it [00:01, 291.62it/s]warmup run: 287it [00:01, 278.25it/s]warmup run: 301it [00:01, 294.25it/s]warmup run: 401it [00:01, 406.43it/s]warmup run: 396it [00:01, 398.76it/s]warmup run: 291it [00:01, 288.70it/s]warmup run: 295it [00:01, 292.81it/s]warmup run: 489it [00:02, 502.66it/s]warmup run: 399it [00:01, 404.63it/s]warmup run: 380it [00:01, 382.06it/s]warmup run: 402it [00:01, 408.97it/s]warmup run: 500it [00:02, 514.07it/s]warmup run: 495it [00:02, 507.56it/s]warmup run: 394it [00:01, 405.91it/s]warmup run: 390it [00:01, 402.04it/s]warmup run: 498it [00:02, 513.43it/s]warmup run: 474it [00:02, 485.22it/s]warmup run: 504it [00:02, 522.32it/s]warmup run: 581it [00:02, 575.22it/s]warmup run: 601it [00:02, 617.83it/s]warmup run: 594it [00:02, 607.06it/s]warmup run: 493it [00:02, 515.51it/s]warmup run: 491it [00:02, 515.38it/s]warmup run: 599it [00:02, 617.21it/s]warmup run: 570it [00:02, 584.07it/s]warmup run: 605it [00:02, 624.11it/s]warmup run: 670it [00:02, 641.11it/s]warmup run: 701it [00:02, 705.60it/s]warmup run: 691it [00:02, 689.77it/s]warmup run: 592it [00:02, 615.65it/s]warmup run: 593it [00:02, 621.09it/s]warmup run: 700it [00:02, 707.73it/s]warmup run: 668it [00:02, 674.15it/s]warmup run: 703it [00:02, 705.58it/s]warmup run: 769it [00:02, 724.58it/s]warmup run: 801it [00:02, 778.15it/s]warmup run: 787it [00:02, 750.49it/s]warmup run: 692it [00:02, 703.82it/s]warmup run: 694it [00:02, 710.97it/s]warmup run: 801it [00:02, 781.83it/s]warmup run: 767it [00:02, 750.74it/s]warmup run: 800it [00:02, 754.40it/s]warmup run: 869it [00:02, 793.55it/s]warmup run: 901it [00:02, 834.00it/s]warmup run: 882it [00:02, 801.80it/s]warmup run: 793it [00:02, 779.10it/s]warmup run: 796it [00:02, 787.21it/s]warmup run: 901it [00:02, 838.01it/s]warmup run: 869it [00:02, 819.47it/s]warmup run: 895it [00:02, 803.92it/s]warmup run: 967it [00:02, 843.04it/s]warmup run: 1000it [00:02, 873.48it/s]warmup run: 977it [00:02, 838.54it/s]warmup run: 892it [00:02, 833.69it/s]warmup run: 895it [00:02, 840.21it/s]warmup run: 1001it [00:02, 880.36it/s]warmup run: 971it [00:02, 873.12it/s]warmup run: 990it [00:02, 841.53it/s]warmup run: 1066it [00:02, 882.24it/s]warmup run: 1099it [00:02, 896.47it/s]warmup run: 1075it [00:02, 877.69it/s]warmup run: 990it [00:02, 868.81it/s]warmup run: 994it [00:02, 874.05it/s]warmup run: 1100it [00:02, 910.55it/s]warmup run: 1070it [00:02, 895.63it/s]warmup run: 1085it [00:02, 869.68it/s]warmup run: 1163it [00:02, 906.59it/s]warmup run: 1197it [00:02, 905.73it/s]warmup run: 1171it [00:02, 898.96it/s]warmup run: 1092it [00:02, 902.38it/s]warmup run: 1088it [00:02, 829.37it/s]warmup run: 1199it [00:02, 932.07it/s]warmup run: 1168it [00:02, 910.90it/s]warmup run: 1260it [00:02, 924.30it/s]warmup run: 1180it [00:02, 888.88it/s]warmup run: 1294it [00:02, 921.71it/s]warmup run: 1269it [00:02, 922.20it/s]warmup run: 1191it [00:02, 925.85it/s]warmup run: 1299it [00:02, 950.71it/s]warmup run: 1179it [00:02, 843.97it/s]warmup run: 1265it [00:02, 919.57it/s]warmup run: 1357it [00:02, 937.52it/s]warmup run: 1278it [00:02, 912.76it/s]warmup run: 1391it [00:02, 933.01it/s]warmup run: 1368it [00:02, 940.23it/s]warmup run: 1292it [00:02, 948.51it/s]warmup run: 1400it [00:02, 967.14it/s]warmup run: 1276it [00:02, 876.27it/s]warmup run: 1362it [00:02, 932.94it/s]warmup run: 1457it [00:03, 954.21it/s]warmup run: 1378it [00:02, 937.65it/s]warmup run: 1489it [00:03, 945.69it/s]warmup run: 1466it [00:03, 950.30it/s]warmup run: 1393it [00:02, 966.11it/s]warmup run: 1501it [00:03, 979.18it/s]warmup run: 1375it [00:02, 907.97it/s]warmup run: 1461it [00:03, 947.89it/s]warmup run: 1478it [00:03, 954.31it/s]warmup run: 1555it [00:03, 955.07it/s]warmup run: 1587it [00:03, 955.12it/s]warmup run: 1564it [00:03, 950.77it/s]warmup run: 1493it [00:03, 975.51it/s]warmup run: 1602it [00:03, 986.59it/s]warmup run: 1475it [00:03, 931.97it/s]warmup run: 1561it [00:03, 963.10it/s]warmup run: 1578it [00:03, 965.20it/s]warmup run: 1652it [00:03, 953.79it/s]warmup run: 1684it [00:03, 946.34it/s]warmup run: 1661it [00:03, 946.79it/s]warmup run: 1593it [00:03, 981.84it/s]warmup run: 1703it [00:03, 989.45it/s]warmup run: 1576it [00:03, 953.55it/s]warmup run: 1661it [00:03, 973.90it/s]warmup run: 1676it [00:03, 961.85it/s]warmup run: 1749it [00:03, 954.91it/s]warmup run: 1780it [00:03, 942.79it/s]warmup run: 1757it [00:03, 946.75it/s]warmup run: 1694it [00:03, 987.78it/s]warmup run: 1804it [00:03, 994.62it/s]warmup run: 1676it [00:03, 966.30it/s]warmup run: 1760it [00:03, 970.21it/s]warmup run: 1777it [00:03, 974.06it/s]warmup run: 1847it [00:03, 960.43it/s]warmup run: 1875it [00:03, 941.16it/s]warmup run: 1854it [00:03, 951.88it/s]warmup run: 1794it [00:03, 990.45it/s]warmup run: 1905it [00:03, 998.87it/s]warmup run: 1774it [00:03, 969.03it/s]warmup run: 1858it [00:03, 967.66it/s]warmup run: 1877it [00:03, 980.04it/s]warmup run: 1946it [00:03, 966.87it/s]warmup run: 1970it [00:03, 942.45it/s]warmup run: 1953it [00:03, 960.90it/s]warmup run: 1894it [00:03, 992.89it/s]warmup run: 2006it [00:03, 1001.42it/s]warmup run: 1872it [00:03, 968.36it/s]warmup run: 1959it [00:03, 977.52it/s]warmup run: 1977it [00:03, 984.96it/s]warmup run: 2049it [00:03, 985.28it/s]warmup run: 2080it [00:03, 987.13it/s]warmup run: 1995it [00:03, 996.34it/s]warmup run: 2060it [00:03, 991.43it/s]warmup run: 2124it [00:03, 1053.57it/s]warmup run: 1972it [00:03, 977.45it/s]warmup run: 2073it [00:03, 1025.03it/s]warmup run: 2091it [00:03, 1029.47it/s]warmup run: 2167it [00:03, 1042.14it/s]warmup run: 2200it [00:03, 1048.68it/s]warmup run: 2179it [00:03, 1050.41it/s]warmup run: 2112it [00:03, 1048.05it/s]warmup run: 2240it [00:03, 1082.85it/s]warmup run: 2085it [00:03, 1022.42it/s]warmup run: 2196it [00:03, 1083.82it/s]warmup run: 2285it [00:03, 1080.90it/s]warmup run: 2210it [00:03, 1074.71it/s]warmup run: 2321it [00:03, 1094.28it/s]warmup run: 2229it [00:03, 1083.64it/s]warmup run: 2299it [00:03, 1093.65it/s]warmup run: 2357it [00:03, 1108.72it/s]warmup run: 2204it [00:03, 1072.19it/s]warmup run: 2319it [00:03, 1126.01it/s]warmup run: 2402it [00:03, 1106.03it/s]warmup run: 2329it [00:03, 1107.46it/s]warmup run: 2442it [00:03, 1128.38it/s]warmup run: 2349it [00:03, 1117.83it/s]warmup run: 2419it [00:03, 1123.95it/s]warmup run: 2474it [00:03, 1125.50it/s]warmup run: 2324it [00:03, 1107.76it/s]warmup run: 2442it [00:03, 1156.06it/s]warmup run: 2448it [00:03, 1131.63it/s]warmup run: 2519it [00:04, 1124.04it/s]warmup run: 2562it [00:04, 1148.03it/s]warmup run: 2469it [00:03, 1141.98it/s]warmup run: 2539it [00:04, 1146.26it/s]warmup run: 2591it [00:04, 1137.66it/s]warmup run: 2444it [00:03, 1133.59it/s]warmup run: 2566it [00:04, 1178.31it/s]warmup run: 2633it [00:04, 1128.45it/s]warmup run: 2567it [00:04, 1147.97it/s]warmup run: 2681it [00:04, 1157.96it/s]warmup run: 2658it [00:04, 1158.35it/s]warmup run: 2590it [00:04, 1159.98it/s]warmup run: 2708it [00:04, 1144.39it/s]warmup run: 2564it [00:04, 1152.23it/s]warmup run: 2688it [00:04, 1190.02it/s]warmup run: 2749it [00:04, 1137.55it/s]warmup run: 2685it [00:04, 1155.96it/s]warmup run: 2801it [00:04, 1169.46it/s]warmup run: 2778it [00:04, 1170.33it/s]warmup run: 2710it [00:04, 1169.57it/s]warmup run: 2825it [00:04, 1151.50it/s]warmup run: 2684it [00:04, 1165.85it/s]warmup run: 2811it [00:04, 1201.48it/s]warmup run: 2804it [00:04, 1165.68it/s]warmup run: 2866it [00:04, 1144.74it/s]warmup run: 2921it [00:04, 1177.60it/s]warmup run: 2898it [00:04, 1178.09it/s]warmup run: 2830it [00:04, 1178.53it/s]warmup run: 2942it [00:04, 1155.54it/s]warmup run: 2803it [00:04, 1170.32it/s]warmup run: 3000it [00:04, 673.79it/s] warmup run: 2934it [00:04, 1207.54it/s]warmup run: 2922it [00:04, 1169.58it/s]warmup run: 2983it [00:04, 1150.01it/s]warmup run: 3000it [00:04, 665.39it/s] warmup run: 3000it [00:04, 680.45it/s] warmup run: 3000it [00:04, 670.25it/s] warmup run: 2948it [00:04, 1169.48it/s]warmup run: 3000it [00:04, 676.70it/s] warmup run: 3000it [00:04, 676.64it/s] warmup run: 2921it [00:04, 1159.88it/s]warmup run: 3000it [00:04, 685.59it/s] warmup run: 3000it [00:04, 676.71it/s] 



warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]


warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   5%|         | 159/3000 [00:00<00:01, 1589.80it/s]warmup should be done:   5%|         | 163/3000 [00:00<00:01, 1627.27it/s]warmup should be done:   5%|         | 164/3000 [00:00<00:01, 1635.99it/s]warmup should be done:   6%|         | 165/3000 [00:00<00:01, 1644.89it/s]warmup should be done:   5%|         | 156/3000 [00:00<00:01, 1554.70it/s]warmup should be done:   6%|         | 165/3000 [00:00<00:01, 1643.45it/s]warmup should be done:   6%|         | 165/3000 [00:00<00:01, 1639.86it/s]warmup should be done:   6%|         | 167/3000 [00:00<00:01, 1659.29it/s]warmup should be done:  11%|         | 320/3000 [00:00<00:01, 1600.84it/s]warmup should be done:  11%|         | 318/3000 [00:00<00:01, 1590.42it/s]warmup should be done:  11%|         | 328/3000 [00:00<00:01, 1635.76it/s]warmup should be done:  11%|         | 331/3000 [00:00<00:01, 1650.19it/s]warmup should be done:  11%|         | 330/3000 [00:00<00:01, 1644.78it/s]warmup should be done:  11%|         | 330/3000 [00:00<00:01, 1641.16it/s]warmup should be done:  11%|         | 331/3000 [00:00<00:01, 1647.20it/s]warmup should be done:  11%|         | 335/3000 [00:00<00:01, 1667.77it/s]warmup should be done:  16%|        | 481/3000 [00:00<00:01, 1597.26it/s]warmup should be done:  16%|        | 495/3000 [00:00<00:01, 1641.89it/s]warmup should be done:  16%|        | 492/3000 [00:00<00:01, 1629.47it/s]warmup should be done:  17%|        | 502/3000 [00:00<00:01, 1662.70it/s]warmup should be done:  17%|        | 497/3000 [00:00<00:01, 1641.58it/s]warmup should be done:  16%|        | 495/3000 [00:00<00:01, 1632.73it/s]warmup should be done:  16%|        | 478/3000 [00:00<00:01, 1576.88it/s]warmup should be done:  17%|        | 496/3000 [00:00<00:01, 1629.90it/s]warmup should be done:  21%|       | 641/3000 [00:00<00:01, 1592.00it/s]warmup should be done:  22%|       | 660/3000 [00:00<00:01, 1641.01it/s]warmup should be done:  22%|       | 655/3000 [00:00<00:01, 1626.22it/s]warmup should be done:  22%|       | 669/3000 [00:00<00:01, 1661.86it/s]warmup should be done:  22%|       | 662/3000 [00:00<00:01, 1638.31it/s]warmup should be done:  22%|       | 659/3000 [00:00<00:01, 1630.03it/s]warmup should be done:  21%|        | 637/3000 [00:00<00:01, 1578.36it/s]warmup should be done:  22%|       | 661/3000 [00:00<00:01, 1634.89it/s]warmup should be done:  27%|       | 818/3000 [00:00<00:01, 1626.30it/s]warmup should be done:  27%|       | 801/3000 [00:00<00:01, 1588.61it/s]warmup should be done:  28%|       | 836/3000 [00:00<00:01, 1658.90it/s]warmup should be done:  28%|       | 825/3000 [00:00<00:01, 1635.59it/s]warmup should be done:  27%|       | 796/3000 [00:00<00:01, 1580.33it/s]warmup should be done:  28%|       | 826/3000 [00:00<00:01, 1632.58it/s]warmup should be done:  28%|       | 825/3000 [00:00<00:01, 1633.35it/s]warmup should be done:  27%|       | 823/3000 [00:00<00:01, 1623.03it/s]warmup should be done:  33%|      | 981/3000 [00:00<00:01, 1622.09it/s]warmup should be done:  32%|      | 960/3000 [00:00<00:01, 1586.70it/s]warmup should be done:  33%|      | 1002/3000 [00:00<00:01, 1648.70it/s]warmup should be done:  33%|      | 989/3000 [00:00<00:01, 1625.99it/s]warmup should be done:  33%|      | 989/3000 [00:00<00:01, 1627.17it/s]warmup should be done:  33%|      | 990/3000 [00:00<00:01, 1624.62it/s]warmup should be done:  32%|      | 955/3000 [00:00<00:01, 1563.32it/s]warmup should be done:  33%|      | 986/3000 [00:00<00:01, 1611.20it/s]warmup should be done:  38%|      | 1144/3000 [00:00<00:01, 1623.49it/s]warmup should be done:  37%|      | 1119/3000 [00:00<00:01, 1581.42it/s]warmup should be done:  38%|      | 1152/3000 [00:00<00:01, 1626.67it/s]warmup should be done:  38%|      | 1154/3000 [00:00<00:01, 1633.09it/s]warmup should be done:  38%|      | 1153/3000 [00:00<00:01, 1624.74it/s]warmup should be done:  39%|      | 1167/3000 [00:00<00:01, 1637.18it/s]warmup should be done:  38%|      | 1148/3000 [00:00<00:01, 1608.90it/s]warmup should be done:  37%|      | 1112/3000 [00:00<00:01, 1551.48it/s]warmup should be done:  44%|     | 1307/3000 [00:00<00:01, 1622.78it/s]warmup should be done:  43%|     | 1278/3000 [00:00<00:01, 1583.55it/s]warmup should be done:  44%|     | 1319/3000 [00:00<00:01, 1637.20it/s]warmup should be done:  44%|     | 1316/3000 [00:00<00:01, 1623.53it/s]warmup should be done:  44%|     | 1315/3000 [00:00<00:01, 1609.90it/s]warmup should be done:  44%|     | 1331/3000 [00:00<00:01, 1628.45it/s]warmup should be done:  44%|     | 1309/3000 [00:00<00:01, 1604.36it/s]warmup should be done:  42%|     | 1269/3000 [00:00<00:01, 1556.80it/s]warmup should be done:  49%|     | 1470/3000 [00:00<00:00, 1623.19it/s]warmup should be done:  48%|     | 1441/3000 [00:00<00:00, 1596.06it/s]warmup should be done:  49%|     | 1483/3000 [00:00<00:00, 1637.93it/s]warmup should be done:  49%|     | 1479/3000 [00:00<00:00, 1622.15it/s]warmup should be done:  49%|     | 1477/3000 [00:00<00:00, 1607.99it/s]warmup should be done:  50%|     | 1494/3000 [00:00<00:00, 1621.02it/s]warmup should be done:  48%|     | 1426/3000 [00:00<00:01, 1559.23it/s]warmup should be done:  49%|     | 1470/3000 [00:00<00:00, 1591.74it/s]warmup should be done:  53%|    | 1604/3000 [00:01<00:00, 1605.91it/s]warmup should be done:  54%|    | 1633/3000 [00:01<00:00, 1618.25it/s]warmup should be done:  55%|    | 1648/3000 [00:01<00:00, 1639.72it/s]warmup should be done:  55%|    | 1643/3000 [00:01<00:00, 1625.30it/s]warmup should be done:  55%|    | 1638/3000 [00:01<00:00, 1603.44it/s]warmup should be done:  55%|    | 1657/3000 [00:01<00:00, 1617.15it/s]warmup should be done:  53%|    | 1582/3000 [00:01<00:00, 1550.37it/s]warmup should be done:  54%|    | 1630/3000 [00:01<00:00, 1583.80it/s]warmup should be done:  59%|    | 1767/3000 [00:01<00:00, 1612.14it/s]warmup should be done:  60%|    | 1795/3000 [00:01<00:00, 1613.58it/s]warmup should be done:  60%|    | 1813/3000 [00:01<00:00, 1641.93it/s]warmup should be done:  60%|    | 1809/3000 [00:01<00:00, 1634.66it/s]warmup should be done:  60%|    | 1799/3000 [00:01<00:00, 1599.61it/s]warmup should be done:  61%|    | 1819/3000 [00:01<00:00, 1615.91it/s]warmup should be done:  58%|    | 1740/3000 [00:01<00:00, 1559.29it/s]warmup should be done:  60%|    | 1789/3000 [00:01<00:00, 1580.88it/s]warmup should be done:  64%|   | 1930/3000 [00:01<00:00, 1615.38it/s]warmup should be done:  66%|   | 1978/3000 [00:01<00:00, 1642.43it/s]warmup should be done:  65%|   | 1957/3000 [00:01<00:00, 1610.42it/s]warmup should be done:  66%|   | 1976/3000 [00:01<00:00, 1642.56it/s]warmup should be done:  66%|   | 1983/3000 [00:01<00:00, 1622.73it/s]warmup should be done:  65%|   | 1959/3000 [00:01<00:00, 1595.88it/s]warmup should be done:  63%|   | 1899/3000 [00:01<00:00, 1566.73it/s]warmup should be done:  65%|   | 1948/3000 [00:01<00:00, 1577.83it/s]warmup should be done:  70%|   | 2093/3000 [00:01<00:00, 1617.33it/s]warmup should be done:  71%|  | 2143/3000 [00:01<00:00, 1644.35it/s]warmup should be done:  71%|  | 2142/3000 [00:01<00:00, 1645.87it/s]warmup should be done:  71%|   | 2119/3000 [00:01<00:00, 1601.55it/s]warmup should be done:  72%|  | 2147/3000 [00:01<00:00, 1627.34it/s]warmup should be done:  71%|   | 2120/3000 [00:01<00:00, 1598.00it/s]warmup should be done:  69%|   | 2056/3000 [00:01<00:00, 1564.71it/s]warmup should be done:  70%|   | 2106/3000 [00:01<00:00, 1575.46it/s]warmup should be done:  75%|  | 2256/3000 [00:01<00:00, 1619.37it/s]warmup should be done:  77%|  | 2308/3000 [00:01<00:00, 1643.51it/s]warmup should be done:  77%|  | 2307/3000 [00:01<00:00, 1640.53it/s]warmup should be done:  76%|  | 2280/3000 [00:01<00:00, 1594.40it/s]warmup should be done:  77%|  | 2310/3000 [00:01<00:00, 1627.90it/s]warmup should be done:  76%|  | 2280/3000 [00:01<00:00, 1596.73it/s]warmup should be done:  74%|  | 2215/3000 [00:01<00:00, 1571.27it/s]warmup should be done:  75%|  | 2264/3000 [00:01<00:00, 1571.61it/s]warmup should be done:  81%|  | 2419/3000 [00:01<00:00, 1619.64it/s]warmup should be done:  82%| | 2473/3000 [00:01<00:00, 1644.44it/s]warmup should be done:  82%| | 2472/3000 [00:01<00:00, 1639.34it/s]warmup should be done:  81%| | 2440/3000 [00:01<00:00, 1593.06it/s]warmup should be done:  82%| | 2474/3000 [00:01<00:00, 1628.83it/s]warmup should be done:  81%| | 2441/3000 [00:01<00:00, 1600.25it/s]warmup should be done:  79%|  | 2373/3000 [00:01<00:00, 1566.04it/s]warmup should be done:  81%|  | 2422/3000 [00:01<00:00, 1573.17it/s]warmup should be done:  86%| | 2582/3000 [00:01<00:00, 1621.77it/s]warmup should be done:  88%| | 2638/3000 [00:01<00:00, 1645.32it/s]warmup should be done:  88%| | 2636/3000 [00:01<00:00, 1638.30it/s]warmup should be done:  87%| | 2600/3000 [00:01<00:00, 1591.50it/s]warmup should be done:  88%| | 2638/3000 [00:01<00:00, 1630.29it/s]warmup should be done:  87%| | 2602/3000 [00:01<00:00, 1601.36it/s]warmup should be done:  84%| | 2535/3000 [00:01<00:00, 1579.94it/s]warmup should be done:  86%| | 2580/3000 [00:01<00:00, 1573.62it/s]warmup should be done:  92%|| 2745/3000 [00:01<00:00, 1623.25it/s]warmup should be done:  93%|| 2803/3000 [00:01<00:00, 1645.16it/s]warmup should be done:  93%|| 2800/3000 [00:01<00:00, 1636.90it/s]warmup should be done:  93%|| 2802/3000 [00:01<00:00, 1631.51it/s]warmup should be done:  92%|| 2760/3000 [00:01<00:00, 1590.37it/s]warmup should be done:  92%|| 2763/3000 [00:01<00:00, 1602.77it/s]warmup should be done:  90%| | 2696/3000 [00:01<00:00, 1587.80it/s]warmup should be done:  91%|| 2738/3000 [00:01<00:00, 1573.41it/s]warmup should be done:  97%|| 2909/3000 [00:01<00:00, 1626.86it/s]warmup should be done:  99%|| 2965/3000 [00:01<00:00, 1640.21it/s]warmup should be done:  99%|| 2968/3000 [00:01<00:00, 1626.45it/s]warmup should be done:  99%|| 2967/3000 [00:01<00:00, 1636.57it/s]warmup should be done:  98%|| 2926/3000 [00:01<00:00, 1610.42it/s]warmup should be done:  97%|| 2921/3000 [00:01<00:00, 1595.07it/s]warmup should be done:  95%|| 2855/3000 [00:01<00:00, 1582.06it/s]warmup should be done:  97%|| 2898/3000 [00:01<00:00, 1578.74it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1636.31it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1635.64it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1635.01it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1612.21it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1609.15it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1608.75it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1591.61it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1572.77it/s]



warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]

warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   6%|         | 168/3000 [00:00<00:01, 1677.63it/s]warmup should be done:   5%|         | 163/3000 [00:00<00:01, 1628.13it/s]warmup should be done:   6%|         | 167/3000 [00:00<00:01, 1665.11it/s]warmup should be done:   6%|         | 170/3000 [00:00<00:01, 1695.20it/s]warmup should be done:   6%|         | 166/3000 [00:00<00:01, 1651.44it/s]warmup should be done:   5%|         | 162/3000 [00:00<00:01, 1612.98it/s]warmup should be done:   6%|         | 166/3000 [00:00<00:01, 1651.35it/s]warmup should be done:   5%|         | 162/3000 [00:00<00:01, 1610.47it/s]warmup should be done:  11%|         | 327/3000 [00:00<00:01, 1634.99it/s]warmup should be done:  11%|         | 336/3000 [00:00<00:01, 1677.84it/s]warmup should be done:  11%|         | 335/3000 [00:00<00:01, 1673.63it/s]warmup should be done:  11%|         | 325/3000 [00:00<00:01, 1621.24it/s]warmup should be done:  11%|        | 342/3000 [00:00<00:01, 1703.90it/s]warmup should be done:  11%|         | 326/3000 [00:00<00:01, 1623.54it/s]warmup should be done:  11%|         | 332/3000 [00:00<00:01, 1651.87it/s]warmup should be done:  11%|         | 332/3000 [00:00<00:01, 1623.05it/s]warmup should be done:  16%|        | 492/3000 [00:00<00:01, 1640.44it/s]warmup should be done:  17%|        | 503/3000 [00:00<00:01, 1676.32it/s]warmup should be done:  17%|        | 505/3000 [00:00<00:01, 1678.83it/s]warmup should be done:  16%|        | 488/3000 [00:00<00:01, 1622.67it/s]warmup should be done:  16%|        | 489/3000 [00:00<00:01, 1625.63it/s]warmup should be done:  17%|        | 514/3000 [00:00<00:01, 1709.51it/s]warmup should be done:  17%|        | 499/3000 [00:00<00:01, 1656.17it/s]warmup should be done:  17%|        | 498/3000 [00:00<00:01, 1637.90it/s]warmup should be done:  22%|       | 671/3000 [00:00<00:01, 1676.74it/s]warmup should be done:  22%|       | 657/3000 [00:00<00:01, 1641.01it/s]warmup should be done:  22%|       | 674/3000 [00:00<00:01, 1681.01it/s]warmup should be done:  22%|       | 666/3000 [00:00<00:01, 1659.93it/s]warmup should be done:  22%|       | 651/3000 [00:00<00:01, 1620.93it/s]warmup should be done:  22%|       | 652/3000 [00:00<00:01, 1622.62it/s]warmup should be done:  23%|       | 685/3000 [00:00<00:01, 1704.57it/s]warmup should be done:  22%|       | 662/3000 [00:00<00:01, 1638.30it/s]warmup should be done:  28%|       | 839/3000 [00:00<00:01, 1675.65it/s]warmup should be done:  27%|       | 822/3000 [00:00<00:01, 1639.83it/s]warmup should be done:  28%|       | 843/3000 [00:00<00:01, 1681.85it/s]warmup should be done:  28%|       | 833/3000 [00:00<00:01, 1661.85it/s]warmup should be done:  27%|       | 814/3000 [00:00<00:01, 1621.96it/s]warmup should be done:  27%|       | 815/3000 [00:00<00:01, 1621.83it/s]warmup should be done:  29%|       | 856/3000 [00:00<00:01, 1698.88it/s]warmup should be done:  28%|       | 826/3000 [00:00<00:01, 1624.33it/s]warmup should be done:  34%|      | 1007/3000 [00:00<00:01, 1675.67it/s]warmup should be done:  33%|      | 989/3000 [00:00<00:01, 1648.48it/s]warmup should be done:  33%|      | 978/3000 [00:00<00:01, 1624.32it/s]warmup should be done:  34%|      | 1012/3000 [00:00<00:01, 1678.95it/s]warmup should be done:  33%|      | 977/3000 [00:00<00:01, 1621.09it/s]warmup should be done:  33%|      | 1000/3000 [00:00<00:01, 1657.59it/s]warmup should be done:  34%|      | 1026/3000 [00:00<00:01, 1694.51it/s]warmup should be done:  33%|      | 989/3000 [00:00<00:01, 1615.28it/s]warmup should be done:  39%|      | 1156/3000 [00:00<00:01, 1652.67it/s]warmup should be done:  39%|      | 1175/3000 [00:00<00:01, 1667.94it/s]warmup should be done:  38%|      | 1141/3000 [00:00<00:01, 1624.24it/s]warmup should be done:  39%|      | 1180/3000 [00:00<00:01, 1675.35it/s]warmup should be done:  38%|      | 1141/3000 [00:00<00:01, 1624.80it/s]warmup should be done:  39%|      | 1167/3000 [00:00<00:01, 1657.05it/s]warmup should be done:  40%|      | 1196/3000 [00:00<00:01, 1691.79it/s]warmup should be done:  38%|      | 1151/3000 [00:00<00:01, 1604.99it/s]warmup should be done:  44%|     | 1322/3000 [00:00<00:01, 1653.63it/s]warmup should be done:  43%|     | 1304/3000 [00:00<00:01, 1625.58it/s]warmup should be done:  45%|     | 1342/3000 [00:00<00:00, 1663.64it/s]warmup should be done:  45%|     | 1349/3000 [00:00<00:00, 1676.97it/s]warmup should be done:  44%|     | 1333/3000 [00:00<00:01, 1657.07it/s]warmup should be done:  43%|     | 1304/3000 [00:00<00:01, 1616.46it/s]warmup should be done:  46%|     | 1366/3000 [00:00<00:00, 1693.07it/s]warmup should be done:  44%|     | 1312/3000 [00:00<00:01, 1596.92it/s]warmup should be done:  50%|     | 1489/3000 [00:00<00:00, 1656.67it/s]warmup should be done:  49%|     | 1469/3000 [00:00<00:00, 1630.10it/s]warmup should be done:  51%|     | 1518/3000 [00:00<00:00, 1677.90it/s]warmup should be done:  50%|     | 1509/3000 [00:00<00:00, 1661.48it/s]warmup should be done:  50%|     | 1499/3000 [00:00<00:00, 1656.86it/s]warmup should be done:  49%|     | 1467/3000 [00:00<00:00, 1619.99it/s]warmup should be done:  51%|     | 1536/3000 [00:00<00:00, 1691.37it/s]warmup should be done:  49%|     | 1477/3000 [00:00<00:00, 1611.31it/s]warmup should be done:  55%|    | 1656/3000 [00:01<00:00, 1659.27it/s]warmup should be done:  54%|    | 1634/3000 [00:01<00:00, 1633.35it/s]warmup should be done:  56%|    | 1687/3000 [00:01<00:00, 1679.09it/s]warmup should be done:  54%|    | 1630/3000 [00:01<00:00, 1622.61it/s]warmup should be done:  56%|    | 1666/3000 [00:01<00:00, 1659.08it/s]warmup should be done:  56%|    | 1676/3000 [00:01<00:00, 1659.25it/s]warmup should be done:  57%|    | 1706/3000 [00:01<00:00, 1687.37it/s]warmup should be done:  55%|    | 1639/3000 [00:01<00:00, 1587.15it/s]warmup should be done:  61%|    | 1823/3000 [00:01<00:00, 1660.81it/s]warmup should be done:  60%|    | 1798/3000 [00:01<00:00, 1634.21it/s]warmup should be done:  62%|   | 1855/3000 [00:01<00:00, 1678.77it/s]warmup should be done:  61%|    | 1832/3000 [00:01<00:00, 1656.04it/s]warmup should be done:  60%|    | 1793/3000 [00:01<00:00, 1620.97it/s]warmup should be done:  61%|   | 1842/3000 [00:01<00:00, 1656.00it/s]warmup should be done:  62%|   | 1875/3000 [00:01<00:00, 1685.24it/s]warmup should be done:  60%|    | 1798/3000 [00:01<00:00, 1551.09it/s]warmup should be done:  66%|   | 1990/3000 [00:01<00:00, 1660.07it/s]warmup should be done:  65%|   | 1964/3000 [00:01<00:00, 1639.19it/s]warmup should be done:  67%|   | 2023/3000 [00:01<00:00, 1677.00it/s]warmup should be done:  67%|   | 2008/3000 [00:01<00:00, 1654.19it/s]warmup should be done:  65%|   | 1956/3000 [00:01<00:00, 1617.57it/s]warmup should be done:  67%|   | 1998/3000 [00:01<00:00, 1647.83it/s]warmup should be done:  68%|   | 2045/3000 [00:01<00:00, 1687.97it/s]warmup should be done:  65%|   | 1960/3000 [00:01<00:00, 1570.70it/s]warmup should be done:  72%|  | 2157/3000 [00:01<00:00, 1660.47it/s]warmup should be done:  71%|   | 2130/3000 [00:01<00:00, 1644.29it/s]warmup should be done:  73%|  | 2191/3000 [00:01<00:00, 1676.01it/s]warmup should be done:  71%|   | 2118/3000 [00:01<00:00, 1617.25it/s]warmup should be done:  72%|  | 2174/3000 [00:01<00:00, 1653.55it/s]warmup should be done:  74%|  | 2217/3000 [00:01<00:00, 1696.67it/s]warmup should be done:  72%|  | 2163/3000 [00:01<00:00, 1614.22it/s]warmup should be done:  71%|   | 2118/3000 [00:01<00:00, 1573.24it/s]warmup should be done:  77%|  | 2324/3000 [00:01<00:00, 1662.47it/s]warmup should be done:  79%|  | 2359/3000 [00:01<00:00, 1675.76it/s]warmup should be done:  76%|  | 2295/3000 [00:01<00:00, 1643.20it/s]warmup should be done:  78%|  | 2340/3000 [00:01<00:00, 1655.14it/s]warmup should be done:  76%|  | 2281/3000 [00:01<00:00, 1619.58it/s]warmup should be done:  80%|  | 2389/3000 [00:01<00:00, 1702.11it/s]warmup should be done:  78%|  | 2334/3000 [00:01<00:00, 1642.13it/s]warmup should be done:  76%|  | 2283/3000 [00:01<00:00, 1594.11it/s]warmup should be done:  83%| | 2491/3000 [00:01<00:00, 1664.19it/s]warmup should be done:  84%| | 2528/3000 [00:01<00:00, 1677.05it/s]warmup should be done:  82%| | 2460/3000 [00:01<00:00, 1641.63it/s]warmup should be done:  84%| | 2507/3000 [00:01<00:00, 1657.36it/s]warmup should be done:  81%| | 2443/3000 [00:01<00:00, 1617.74it/s]warmup should be done:  85%| | 2562/3000 [00:01<00:00, 1707.89it/s]warmup should be done:  84%| | 2506/3000 [00:01<00:00, 1663.11it/s]warmup should be done:  82%| | 2447/3000 [00:01<00:00, 1606.62it/s]warmup should be done:  89%| | 2658/3000 [00:01<00:00, 1662.34it/s]warmup should be done:  90%| | 2696/3000 [00:01<00:00, 1676.85it/s]warmup should be done:  88%| | 2625/3000 [00:01<00:00, 1638.93it/s]warmup should be done:  87%| | 2605/3000 [00:01<00:00, 1616.97it/s]warmup should be done:  89%| | 2673/3000 [00:01<00:00, 1656.03it/s]warmup should be done:  91%| | 2735/3000 [00:01<00:00, 1712.91it/s]warmup should be done:  89%| | 2677/3000 [00:01<00:00, 1675.46it/s]warmup should be done:  87%| | 2608/3000 [00:01<00:00, 1603.17it/s]warmup should be done:  94%|| 2825/3000 [00:01<00:00, 1661.73it/s]warmup should be done:  96%|| 2865/3000 [00:01<00:00, 1677.86it/s]warmup should be done:  95%|| 2839/3000 [00:01<00:00, 1655.98it/s]warmup should be done:  93%|| 2790/3000 [00:01<00:00, 1639.98it/s]warmup should be done:  92%|| 2768/3000 [00:01<00:00, 1619.02it/s]warmup should be done:  97%|| 2908/3000 [00:01<00:00, 1715.30it/s]warmup should be done:  95%|| 2848/3000 [00:01<00:00, 1683.52it/s]warmup should be done:  92%|| 2774/3000 [00:01<00:00, 1617.82it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1701.26it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1678.04it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1662.46it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1661.90it/s]warmup should be done: 100%|| 2992/3000 [00:01<00:00, 1663.18it/s]warmup should be done:  98%|| 2932/3000 [00:01<00:00, 1625.25it/s]warmup should be done:  98%|| 2955/3000 [00:01<00:00, 1641.77it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1655.82it/s]warmup should be done:  98%|| 2939/3000 [00:01<00:00, 1625.08it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1633.47it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1621.15it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1606.89it/s]2022-12-11 20:23:34.885373: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fea37796ba0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 20:23:34.885435: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 20:23:35.887975: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fce5002a490 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 20:23:35.888042: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 20:23:36.316943: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fea2f82cda0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 20:23:36.317008: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 20:23:36.317303: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fea37830e40 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 20:23:36.317360: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 20:23:36.512144: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fea33830db0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 20:23:36.512214: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 20:23:36.523023: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fea37830b70 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 20:23:36.523088: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 20:23:36.572694: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fea3b833ef0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 20:23:36.572768: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 20:23:36.579111: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fea3f833d80 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 20:23:36.579187: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 20:23:37.069931: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 20:23:38.208387: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 20:23:38.616236: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 20:23:38.616234: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 20:23:38.861541: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 20:23:38.862460: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 20:23:38.893301: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 20:23:38.913150: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 20:23:39.914966: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 20:23:41.108331: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 20:23:41.507791: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 20:23:41.541368: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 20:23:41.796841: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 20:23:41.800848: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 20:23:41.801042: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 20:23:41.803979: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
[HCTR][20:24:25.202][ERROR][RK0][tid #140644324464384]: replica 5 reaches 1000, calling init pre replica
[HCTR][20:24:25.202][ERROR][RK0][tid #140644324464384]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][20:24:25.208][ERROR][RK0][tid #140644324464384]: coll ps creation done
[HCTR][20:24:25.208][ERROR][RK0][tid #140644324464384]: replica 5 waits for coll ps creation barrier
[HCTR][20:24:25.221][ERROR][RK0][main]: replica 4 reaches 1000, calling init pre replica
[HCTR][20:24:25.221][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][20:24:25.222][ERROR][RK0][tid #140644257355520]: replica 3 reaches 1000, calling init pre replica
[HCTR][20:24:25.222][ERROR][RK0][tid #140644257355520]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][20:24:25.227][ERROR][RK0][tid #140644257355520]: coll ps creation done
[HCTR][20:24:25.227][ERROR][RK0][tid #140644257355520]: replica 3 waits for coll ps creation barrier
[HCTR][20:24:25.229][ERROR][RK0][main]: coll ps creation done
[HCTR][20:24:25.229][ERROR][RK0][main]: replica 4 waits for coll ps creation barrier
[HCTR][20:24:25.241][ERROR][RK0][tid #140644441896704]: replica 2 reaches 1000, calling init pre replica
[HCTR][20:24:25.241][ERROR][RK0][tid #140644248962816]: replica 1 reaches 1000, calling init pre replica
[HCTR][20:24:25.241][ERROR][RK0][tid #140644441896704]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][20:24:25.241][ERROR][RK0][tid #140644248962816]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][20:24:25.246][ERROR][RK0][tid #140644248962816]: coll ps creation done
[HCTR][20:24:25.246][ERROR][RK0][tid #140644248962816]: replica 1 waits for coll ps creation barrier
[HCTR][20:24:25.246][ERROR][RK0][tid #140644441896704]: coll ps creation done
[HCTR][20:24:25.246][ERROR][RK0][tid #140644441896704]: replica 2 waits for coll ps creation barrier
[HCTR][20:24:25.260][ERROR][RK0][tid #140644441896704]: replica 0 reaches 1000, calling init pre replica
[HCTR][20:24:25.260][ERROR][RK0][tid #140644441896704]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][20:24:25.265][ERROR][RK0][tid #140644441896704]: coll ps creation done
[HCTR][20:24:25.265][ERROR][RK0][tid #140644441896704]: replica 0 waits for coll ps creation barrier
[HCTR][20:24:25.359][ERROR][RK0][tid #140644324464384]: replica 6 reaches 1000, calling init pre replica
[HCTR][20:24:25.359][ERROR][RK0][tid #140644324464384]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][20:24:25.366][ERROR][RK0][tid #140644324464384]: coll ps creation done
[HCTR][20:24:25.366][ERROR][RK0][tid #140644324464384]: replica 6 waits for coll ps creation barrier
[HCTR][20:24:25.374][ERROR][RK0][main]: replica 7 reaches 1000, calling init pre replica
[HCTR][20:24:25.374][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][20:24:25.381][ERROR][RK0][main]: coll ps creation done
[HCTR][20:24:25.381][ERROR][RK0][main]: replica 7 waits for coll ps creation barrier
[HCTR][20:24:25.381][ERROR][RK0][tid #140644441896704]: replica 0 preparing frequency
[HCTR][20:24:26.232][ERROR][RK0][tid #140644441896704]: replica 0 preparing frequency done
[HCTR][20:24:26.269][ERROR][RK0][tid #140644441896704]: replica 0 calling init per replica
[HCTR][20:24:26.269][ERROR][RK0][main]: replica 7 calling init per replica
[HCTR][20:24:26.269][ERROR][RK0][tid #140644441896704]: Calling build_v2
[HCTR][20:24:26.269][ERROR][RK0][tid #140644441896704]: replica 2 calling init per replica
[HCTR][20:24:26.269][ERROR][RK0][main]: replica 4 calling init per replica
[HCTR][20:24:26.269][ERROR][RK0][tid #140644324464384]: replica 5 calling init per replica
[HCTR][20:24:26.269][ERROR][RK0][tid #140644248962816]: replica 1 calling init per replica
[HCTR][20:24:26.269][ERROR][RK0][tid #140644324464384]: replica 6 calling init per replica
[HCTR][20:24:26.269][ERROR][RK0][tid #140644257355520]: replica 3 calling init per replica
[HCTR][20:24:26.269][ERROR][RK0][main]: Calling build_v2
[HCTR][20:24:26.269][ERROR][RK0][tid #140644441896704]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][20:24:26.269][ERROR][RK0][tid #140644441896704]: Calling build_v2
[HCTR][20:24:26.269][ERROR][RK0][main]: Calling build_v2
[HCTR][20:24:26.269][ERROR][RK0][tid #140644324464384]: Calling build_v2
[HCTR][20:24:26.269][ERROR][RK0][tid #140644248962816]: Calling build_v2
[HCTR][20:24:26.269][ERROR][RK0][tid #140644324464384]: Calling build_v2
[HCTR][20:24:26.269][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][20:24:26.269][ERROR][RK0][tid #140644257355520]: Calling build_v2
[HCTR][20:24:26.269][ERROR][RK0][tid #140644441896704]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][20:24:26.269][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][20:24:26.269][ERROR][RK0][tid #140644324464384]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][20:24:26.269][ERROR][RK0][tid #140644248962816]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][20:24:26.269][ERROR][RK0][tid #140644324464384]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][20:24:26.269][ERROR][RK0][tid #140644257355520]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[2022-12-11 20:24:26.273488: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:[178] v100x8, slow pcie
2022-12-11 20:24:26.273531[: 2022-12-11 20:24:26E. 273570/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: :E[178 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccv100x8, slow pcie:
1962022-12-11 20:24:26] .assigning 0 to cpu[273575
2022-12-11 20:24:26: .E273616 [: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE: 178/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] 2022-12-11 20:24:26:v100x8, slow pcie.196[
273621] 2022-12-11 20:24:26: assigning 0 to cpu.E[
273661 2022-12-11 20:24:26: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.E:273677[ 178: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] E:v100x8, slow pcie [212
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-11 20:24:26] :.build asymm link desc with 8X Tesla V100-SXM2-32GB out of 81962737262022-12-11 20:24:26[
] : .2022-12-11 20:24:26[assigning 0 to cpuE273682.[
2022-12-11 20:24:26 : 2737422022-12-11 20:24:26[./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE: .273733: [E2022-12-11 20:24:26273776: [212/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-11 20:24:26 2022-12-11 20:24:26.: E] :./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.273774E build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8178273820:273817:  /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
] : 196: E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:v100x8, slow pcieE] E[ :178
 assigning 0 to cpu 2022-12-11 20:24:26/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc213] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
.[:] v100x8, slow pcie::2739572022-12-11 20:24:26178remote time is 8.68421
212178: [.] [
] ] E2022-12-11 20:24:26274018v100x8, slow pcie2022-12-11 20:24:26build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8[v100x8, slow pcie .: 
.
2022-12-11 20:24:26
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc274097E274085.[:[: [ : 2741272022-12-11 20:24:262132022-12-11 20:24:26E2022-12-11 20:24:26/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE: .] . .: E274184remote time is 8.68421274193/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc274196196/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc : 
: :: ] :/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccEE196[Eassigning 0 to cpu212:  ] 2022-12-11 20:24:26 
] 214/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccassigning 0 to cpu./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccbuild asymm link desc with 8X Tesla V100-SXM2-32GB out of 8] ::
274327:
cpu time is 97.0588196213: 196
[] ] [E] 2022-12-11 20:24:26[assigning 0 to cpuremote time is 8.684212022-12-11 20:24:26 assigning 0 to cpu.2022-12-11 20:24:26

./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
274454.274459:[: 274475: 2142022-12-11 20:24:26: EE] .[E[  cpu time is 97.05882745352022-12-11 20:24:26 2022-12-11 20:24:26/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
: ./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.::E274570:274575212213 : 212: ] ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE] Eremote time is 8.68421build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8: build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8 

214/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] [::cpu time is 97.05882022-12-11 20:24:26[212212[
.2022-12-11 20:24:26] ] 2022-12-11 20:24:26274731.build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8.: 274742

274752E: :  E[[E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc 2022-12-11 20:24:262022-12-11 20:24:26 :/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc../hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc214:274831274831:] 213: : 213cpu time is 97.0588] EE] 
remote time is 8.68421  remote time is 8.68421
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
::213213[] [] 2022-12-11 20:24:26remote time is 8.684212022-12-11 20:24:26remote time is 8.68421.
.
274966274971[: [: 2022-12-11 20:24:26E2022-12-11 20:24:26E. . 275021/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc275027/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: E:: : 214E214/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc]  ] :cpu time is 97.0588/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
cpu time is 97.0588214:
] 214cpu time is 97.0588] 
cpu time is 97.0588
[2022-12-11 20:25:44.246987: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:118] solver created. now build & solve
[2022-12-11 20:25:44.287033: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:123] solver built. now solve
[2022-12-11 20:25:44.287093: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:919] num_cached_nodes = 999999
[2022-12-11 20:25:44.405025: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:127] solver solved
[2022-12-11 20:25:44.405112: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:195] 0 solved master
[2022-12-11 20:25:44.405145: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 0 solved
[2022-12-11 20:25:44.405176: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 0 initing device 0
[2022-12-11 20:25:44.405611: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 20:25:44.406513: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 20:25:44.407245: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 20:25:44.420427: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 2 solved
[2022-12-11 20:25:44.420501: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 2 initing device 2
[2022-12-11 20:25:44.420627: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:[2022022-12-11 20:25:44] .5 solved420658
: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc[:2022-12-11 20:25:44202.] 420703: 1 solvedE
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205[] 2022-12-11 20:25:44worker 0 thread 5 initing device 5.
420735: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 1 initing device 1
[2022-12-11 20:25:44.420922: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 20:25:44.421141: E [/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu2022-12-11 20:25:44:.1980421156] : eager alloc mem 381.47 MBE
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 20:25:44.421932: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 6 solved
[2022-12-11 20:25:44.421984: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 6 initing device 6
[2022-12-11 20:25:44[.[2022-12-11 20:25:444219962022-12-11 20:25:44.: .422022E422034:  : E/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.ccE : /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc202/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:] :2024 solved202] 
] 7 solved3 solved
[
2022-12-11 20:25:44[.2022-12-11 20:25:44422175[.: 2022-12-11 20:25:44422186E.:  422192E/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:  :E/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc205 :] /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc205worker 0 thread 4 initing device 4:] 
205worker 0 thread 7 initing device 7] 
worker 0 thread 3 initing device 3
[2022-12-11 20:25:44.422381: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 20:25:44.422687: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[[2022-12-11 20:25:442022-12-11 20:25:44..422713422713: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::19801980] ] eager alloc mem 381.47 MBeager alloc mem 381.47 MB

[2022-12-11 20:25:44.425197: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 20:25:44.425266: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 20:25:44.425587: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 20:25:44.426833: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 20:25:44.427408: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 20:25:44.427520: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 20:25:44.427577: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 20:25:44.429760: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 20:25:44.429818: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 20:25:44.430195: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 20:25:44.431305: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 20:25:44.431840: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 20:25:44.431939: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 20:25:44.432004: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 20:25:44.485247: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 2.00 Bytes
[2022-12-11 20:25:44.485632: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1024.00 Bytes
[2022-12-11 20:25:44.491223: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-11 20:25:44.491324: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 2
[2022-12-11 20:25:44.491372: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-11 20:25:44.492348: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 4.20 MB
[2022-12-11 20:25:44.492900: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 20:25:44.493871: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 20:25:44.493964: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 20:25:44.494649: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 20:25:44.494692: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 488.28 MB
[[[[[2022-12-11 20:25:442022-12-11 20:25:442022-12-11 20:25:442022-12-11 20:25:442022-12-11 20:25:44.....515270515270515272515270515277: : : : : EEEEE     /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:::::19801980198019801980] ] ] ] ] eager alloc mem 2.00 Byteseager alloc mem 2.00 Byteseager alloc mem 2.00 Byteseager alloc mem 2.00 Byteseager alloc mem 2.00 Bytes




[[2022-12-11 20:25:44[2022-12-11 20:25:44[.2022-12-11 20:25:44[.2022-12-11 20:25:44515717.2022-12-11 20:25:44515717.: 515722.: 515725E: 515727E:  E:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu E/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu :/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu :/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu1980:/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu1980:] 1980:] 1980eager alloc mem 1024.00 Bytes] 1980eager alloc mem 1024.00 Bytes] 
eager alloc mem 1024.00 Bytes] 
eager alloc mem 1024.00 Bytes
eager alloc mem 1024.00 Bytes

[[2022-12-11 20:25:442022-12-11 20:25:44..518611518620: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::19801980] ] eager alloc mem 2.00 Byteseager alloc mem 2.00 Bytes

[2022-12-11 20:25:44.[5189702022-12-11 20:25:44: .E518974 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE: 1980/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu] :eager alloc mem 1024.00 Bytes1980
] eager alloc mem 1024.00 Bytes
[2022-12-11 20:25:44.521798: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-11 20:25:44.521876: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638[] 2022-12-11 20:25:44eager release cuda mem 2.
521876: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-11 20:25:44.521931: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc[[:2022-12-11 20:25:442022-12-11 20:25:44638..] 521932521950eager release cuda mem 400000000: : 
EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::638638] ] eager release cuda mem 1024eager release cuda mem 2

[2022-12-11 20:25:44[.2022-12-11 20:25:44522030.: 522033E:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc :[/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc6382022-12-11 20:25:44:] .638eager release cuda mem 2522046] 
: eager release cuda mem 400000000E
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] [eager release cuda mem 1024[2022-12-11 20:25:44
2022-12-11 20:25:44..522091522105: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:[:6382022-12-11 20:25:44638] .] eager release cuda mem 1024522139eager release cuda mem 400000000
: 
E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 2
[2022-12-11 20:25:44.522196: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:[6382022-12-11 20:25:44] .eager release cuda mem 2522213
: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-11 20:25:44.522246: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-11 20:25:44.523472: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 4.20 MB
[2022-12-11 20:25:44.524079: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-11 20:25:44.524144: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 2
[2022-12-11 20:25:44.524165: E[ 2022-12-11 20:25:44/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu.:5241861980: ] Eeager alloc mem 4.20 MB 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-11 20:25:44.524242: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-11 20:25:44.524305: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 2
[2022-12-11 20:25:44.524346: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-11 20:25:44.524676: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 4.20 MB
[2022-12-11 20:25:44.525270: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 4.20 MB
[2022-12-11 20:25:44.525794: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 4.20 MB
[2022-12-11 20:25:44.527212: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 4.20 MB
[2022-12-11 20:25:44.527746: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 4.20 MB
[2022-12-11 20:25:44.527913: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 20:25:44.528117: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 20:25:44.528310: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 20:25:44.528362: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 20:25:44.528399: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 20:25:44.528636: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 20:25:44.528693: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 20:25:44.528853: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 20:25:44.528938: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 20:25:44.529057: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 20:25:44.529142: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 20:25:44.529234: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 20:25:44.529305[: 2022-12-11 20:25:44E. 529315/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc: :E638 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu[eager release cuda mem 625663:2022-12-11 20:25:44
1980.] 529339eager alloc mem 25.25 KB: 
E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 20:25:44.529420: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 20:25:44.529448: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 20:25:44.529572: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 20:25:44.[5296182022-12-11 20:25:44: .E529622 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc[E:2022-12-11 20:25:44 638./hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc] 529656:eager release cuda mem 25855: 638
E]  eager release cuda mem 625663/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu
:1980] [eager alloc mem 25.25 KB2022-12-11 20:25:44
.529730: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 488.28 MB
[2022-12-11 20:25:44.529783: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 20:25:44.529813: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 20:25:44.529855: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 488.28 MB
[2022-12-11 20:25:44.530026: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 20:25:44.530067: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 488.28 MB
[2022-12-11 20:25:44.530096: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855[
2022-12-11 20:25:44.530117: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638[] 2022-12-11 20:25:44eager release cuda mem 25855.
530139: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 488.28 MB
[2022-12-11 20:25:44.530166: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 488.28 MB
[2022-12-11 20:25:44.530391: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 20:25:44.530435: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 488.28 MB
[2022-12-11 20:25:44.530459: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 20:25:44.530500: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 488.28 MB
[2022-12-11 20:25:44.631608: [E[[2022-12-11 20:25:44[ [2022-12-11 20:25:44[2022-12-11 20:25:44.2022-12-11 20:25:44/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu2022-12-11 20:25:44.2022-12-11 20:25:44.631620.:.[631623.631626: 63162919806316352022-12-11 20:25:44: 631645: E: ] : .E: E Eeager alloc mem 611.00 KBE631693 E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu 
 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE:/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980:: 1980:1980] 19801980/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu] 1980] eager alloc mem 611.00 KB] ] :eager alloc mem 611.00 KB] eager alloc mem 611.00 KB
eager alloc mem 611.00 KBeager alloc mem 611.00 KB1980
eager alloc mem 611.00 KB


] 
eager alloc mem 611.00 KB
[2022-12-11 20:25:44.632652: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 20:25:44.632684: E [/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc2022-12-11 20:25:44:.638632697] [[: eager release cuda mem 6256632022-12-11 20:25:442022-12-11 20:25:44[E[
.[.[2022-12-11 20:25:44 2022-12-11 20:25:446327282022-12-11 20:25:446327282022-12-11 20:25:44./hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc.: .: .632745:632749E[632759E632762: 638:  2022-12-11 20:25:44:  : E] E/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu.E/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccE eager release cuda mem 625663 :632827 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc
/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc1980: /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc638/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::] E:] :638638eager alloc mem 611.00 KB 638eager release cuda mem 625663638] ] 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu] 
] eager release cuda mem 625663[eager release cuda mem 625663:eager release cuda mem 625663eager release cuda mem 625663
2022-12-11 20:25:44
1980

.] 632982eager alloc mem 611.00 KB: 
E[ 2022-12-11 20:25:44/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu.:[63305519802022-12-11 20:25:44[: [[] .2022-12-11 20:25:44E2022-12-11 20:25:442022-12-11 20:25:44eager alloc mem 611.00 KB633071. ..
: 633084/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu633084633087E: :: :  E1980EE/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu ]   :/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cueager alloc mem 611.00 KB/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu1980:
::] 198019801980eager alloc mem 611.00 KB] ] ] 
eager alloc mem 611.00 KBeager alloc mem 611.00 KBeager alloc mem 611.00 KB


[2022-12-11 20:25:44.633783: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638[] 2022-12-11 20:25:44eager release cuda mem 625663.
633800: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 20:25:44.633867: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] [eager alloc mem 611.00 KB2022-12-11 20:25:44
[.2022-12-11 20:25:44633886.: 633894E:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu :/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc1980:] 638eager alloc mem 611.00 KB] 
eager release cuda mem 625663
[2022-12-11 20:25:44.633950: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 20:25:44.633981: E[ 2022-12-11 20:25:44/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc.:633996[638[: 2022-12-11 20:25:44[] [2022-12-11 20:25:44E.2022-12-11 20:25:44eager release cuda mem 6256632022-12-11 20:25:44. 634015.
.634024/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu: 634026634028: :E: : E1980 EE ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc  [/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cceager alloc mem 611.00 KB:/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu2022-12-11 20:25:44:
638::.638] 6381980634124] eager release cuda mem 625663] ] : eager release cuda mem 625663
eager release cuda mem 625663eager alloc mem 611.00 KBE


 /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 20:25:44.634238: [E[2022-12-11 20:25:44 2022-12-11 20:25:44./hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu.634247:634250: 1980: E] E eager alloc mem 611.00 KB /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu
/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::19801980] ] eager alloc mem 611.00 KBeager alloc mem 611.00 KB

[2022-12-11 20:25:44.634628: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 20:25:44.634694: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 20:25:44.634747: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 20:25:44.634817: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 20:25:44.634917: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 20:25:44.634965[: 2022-12-11 20:25:44E .[/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc6349752022-12-11 20:25:44:: .638E634984]  : eager release cuda mem 625663/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccE
: 638/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu] :eager release cuda mem 6256631980
] eager alloc mem 611.00 KB
[2022-12-11 20:25:44.635092: E [/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc2022-12-11 20:25:44:[[.6382022-12-11 20:25:442022-12-11 20:25:44635105] ..: [eager release cuda mem 625663635111635112E2022-12-11 20:25:44
: :  .EE/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu635143  :: /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc1980E::]  638638eager alloc mem 611.00 KB/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu] [] 
:eager release cuda mem 6256632022-12-11 20:25:44eager release cuda mem 6256631980
.
] 635211eager alloc mem 611.00 KB: 
E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 20:25:44[.2022-12-11 20:25:44635282.: 635286E:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu :/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu1980:] 1980eager alloc mem 611.00 KB] 
eager alloc mem 611.00 KB
[2022-12-11 20:25:44.635456: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 20:25:44.635528: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 20:25:44.635640: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 20:25:44.635710: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 20:25:44.635846: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 20:25:44.635915: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 20:25:44.636001: E[ 2022-12-11 20:25:44/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc.:636015638: ] Eeager release cuda mem 625663 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 20:25:44.636064: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 20:25:44.636096[: 2022-12-11 20:25:44E. 636102/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu: :E1980 [] /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu[[2022-12-11 20:25:44eager alloc mem 611.00 KB:2022-12-11 20:25:442022-12-11 20:25:44.
1980..636126] 636130636131: eager alloc mem 611.00 KB: : E
EE   /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:::6381980638] ] ] eager release cuda mem 625663eager alloc mem 611.00 KBeager release cuda mem 625663


[2022-12-11 20:25:44.[6362602022-12-11 20:25:44: .E636264 : [/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE2022-12-11 20:25:44: .1980/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu636279] :: eager alloc mem 611.00 KB1980E
]  eager alloc mem 611.00 KB/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc
:638] eager release cuda mem 625663
[2022-12-11 20:25:44.636381: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 20:25:44.636533: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 20:25:44.636602: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 20:25:44.636690: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 20:25:44.636758: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 20:25:44.636913: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638[] 2022-12-11 20:25:44eager release cuda mem 625663.
636932: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 20:25:44.636990: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:[19802022-12-11 20:25:44] .eager alloc mem 611.00 KB[637010
2022-12-11 20:25:44: .E637019 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE: 1980/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc] :eager alloc mem 611.00 KB638
] eager release cuda mem 625663
[2022-12-11 20:25:44.637108: [E2022-12-11 20:25:44 ./hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu637115:[: 1980[2022-12-11 20:25:44E] 2022-12-11 20:25:44. eager alloc mem 611.00 KB.637128/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc
637133: :: E638E ]  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cceager release cuda mem 625663/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:
:638638] ] eager release cuda mem 625663eager release cuda mem 625663

[2022-12-11 20:25:44.637230: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu[:[2022-12-11 20:25:4419802022-12-11 20:25:44.] .637244eager alloc mem 611.00 KB637247: 
: EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::19801980] ] eager alloc mem 611.00 KBeager alloc mem 611.00 KB

[2022-12-11 20:25:44.637423: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 20:25:44.637490: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 20:25:44.637529: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 20:25:44.637596: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 20:25:44.637784: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 20:25:44.637820: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 20:25:44.637855: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 20:25:44.637887: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 20:25:44.637959: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 20:25:44.[6380252022-12-11 20:25:44: .E638030 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccE: 638/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu] :eager release cuda mem 6256631980
] eager alloc mem 611.00 KB
[2022-12-11 20:25:44.[6380732022-12-11 20:25:44: .E638079 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccE: [638/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc2022-12-11 20:25:44] :.eager release cuda mem 625663638638101
] : eager release cuda mem 4399996E
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 20:25:44.638167: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 20:25:44.638198: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 20:25:44.638310: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 20:25:44.638348: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 4399996[
2022-12-11 20:25:44.638367: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 20:25:44.638407: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 4399996
[2022-12-11 20:25:44.638632: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[[2022-12-11 20:25:442022-12-11 20:25:44..638667638670: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::638638] ] eager release cuda mem 625663eager release cuda mem 4399996

[2022-12-11 20:25:44.638726: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 4399996
[2022-12-11 20:25:44.638776: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 999999 / 100000000 nodes ( 1.00 %~1.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 99000001 / 100000000 nodes ( 99.00 %) | 488.28 MB | 0.217628 secs 
[2022-12-11 20:25:44.638872: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 20:25:44.638907: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 4399996
[2022-12-11 20:25:44.638989: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 20:25:44.639023[: 2022-12-11 20:25:44E. 639030/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc: :E638 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cceager release cuda mem 625663:
638] eager release cuda mem 4399996
[2022-12-11 20:25:44.639075: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 4399996
[2022-12-11 20:25:44.639965: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 999999 / 100000000 nodes ( 1.00 %~1.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 99000001 / 100000000 nodes ( 99.00 %) | 488.28 MB | 0.217268 secs 
[2022-12-11 20:25:44.640438: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 999999 / 100000000 nodes ( 1.00 %~1.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 99000001 / 100000000 nodes ( 99.00 %) | 488.28 MB | 0.217744 secs 
[2022-12-11 20:25:44.640982: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 999999 / 100000000 nodes ( 1.00 %~1.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 99000001 / 100000000 nodes ( 99.00 %) | 488.28 MB | 0.218305 secs 
[2022-12-11 20:25:44.641387: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 999999 / 100000000 nodes ( 1.00 %~1.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 99000001 / 100000000 nodes ( 99.00 %) | 488.28 MB | 0.235791 secs 
[2022-12-11 20:25:44.641969: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 999999 / 100000000 nodes ( 1.00 %~1.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 99000001 / 100000000 nodes ( 99.00 %) | 488.28 MB | 0.219599 secs 
[2022-12-11 20:25:44.642386: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 999999 / 100000000 nodes ( 1.00 %~1.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 99000001 / 100000000 nodes ( 99.00 %) | 488.28 MB | 0.221257 secs 
[2022-12-11 20:25:44.642780: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 999999 / 100000000 nodes ( 1.00 %~1.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 99000001 / 100000000 nodes ( 99.00 %) | 488.28 MB | 0.221871 secs 
[HCTR][20:25:44.642][ERROR][RK0][tid #140644324464384]: replica 5 calling init per replica done, doing barrier
[HCTR][20:25:44.642][ERROR][RK0][main]: replica 7 calling init per replica done, doing barrier
[HCTR][20:25:44.642][ERROR][RK0][tid #140644257355520]: replica 3 calling init per replica done, doing barrier
[HCTR][20:25:44.642][ERROR][RK0][main]: replica 4 calling init per replica done, doing barrier
[HCTR][20:25:44.642][ERROR][RK0][tid #140644324464384]: replica 6 calling init per replica done, doing barrier
[HCTR][20:25:44.642][ERROR][RK0][tid #140644441896704]: replica 0 calling init per replica done, doing barrier
[HCTR][20:25:44.642][ERROR][RK0][tid #140644248962816]: replica 1 calling init per replica done, doing barrier
[HCTR][20:25:44.642][ERROR][RK0][tid #140644441896704]: replica 2 calling init per replica done, doing barrier
[HCTR][20:25:44.642][ERROR][RK0][tid #140644441896704]: replica 2 calling init per replica done, doing barrier done
[HCTR][20:25:44.642][ERROR][RK0][tid #140644248962816]: replica 1 calling init per replica done, doing barrier done
[HCTR][20:25:44.642][ERROR][RK0][main]: replica 4 calling init per replica done, doing barrier done
[HCTR][20:25:44.642][ERROR][RK0][tid #140644441896704]: replica 0 calling init per replica done, doing barrier done
[HCTR][20:25:44.642][ERROR][RK0][main]: replica 7 calling init per replica done, doing barrier done
[HCTR][20:25:44.642][ERROR][RK0][tid #140644248962816]: init per replica done
[HCTR][20:25:44.642][ERROR][RK0][tid #140644257355520]: replica 3 calling init per replica done, doing barrier done
[HCTR][20:25:44.642][ERROR][RK0][tid #140644324464384]: replica 6 calling init per replica done, doing barrier done
[HCTR][20:25:44.642][ERROR][RK0][tid #140644324464384]: replica 5 calling init per replica done, doing barrier done
[HCTR][20:25:44.642][ERROR][RK0][tid #140644441896704]: init per replica done
[HCTR][20:25:44.642][ERROR][RK0][main]: init per replica done
[HCTR][20:25:44.642][ERROR][RK0][main]: init per replica done
[HCTR][20:25:44.642][ERROR][RK0][tid #140644257355520]: init per replica done
[HCTR][20:25:44.642][ERROR][RK0][tid #140644324464384]: init per replica done
[HCTR][20:25:44.643][ERROR][RK0][tid #140644324464384]: init per replica done
[HCTR][20:25:44.645][ERROR][RK0][tid #140644441896704]: init per replica done








