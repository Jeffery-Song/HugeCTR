2022-12-11 21:47:00.846230: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:00.853853: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:00.860168: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:00.867059: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:00.870738: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:00.882136: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:00.889846: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:00.903762: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:00.955576: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:00.958803: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:00.962050: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:00.963998: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:00.964661: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:00.965126: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:00.966307: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:00.966426: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:00.967822: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:00.967883: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:00.969041: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:00.969320: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:00.970775: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:00.970979: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:00.972992: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:00.973227: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:00.974400: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:00.974868: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:00.975772: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:00.976527: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:00.977221: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:00.978675: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:00.979748: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:00.980694: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:00.982411: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:00.983655: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:00.984714: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:00.985642: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:00.986557: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:00.987648: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:00.988702: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:00.989732: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:00.995070: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 21:47:00.995851: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:00.997560: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:00.999724: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:00.999775: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:01.001598: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:01.001658: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:01.001885: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:01.003861: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:01.003905: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:01.004132: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:01.005603: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:01.006137: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:01.006273: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:01.006531: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:01.008157: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:01.009040: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:01.009206: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:01.009498: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:01.011052: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:01.012340: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:01.012531: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:01.012851: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:01.014047: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:01.015797: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:01.016010: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:01.017236: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:01.018653: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:01.018840: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:01.019963: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:01.021458: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:01.022603: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:01.023599: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:01.024921: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:01.025726: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:01.027476: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:01.028125: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:01.030033: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:01.030358: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:01.030360: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:01.032552: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:01.032681: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:01.033058: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:01.035187: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:01.035633: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:01.036075: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:01.036591: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:01.037833: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:01.039666: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:01.040013: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:01.040875: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:01.041786: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:01.041864: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:01.043065: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:01.061155: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:01.066768: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:01.069050: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:01.078683: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:01.078701: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:01.080003: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:01.080644: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:01.082240: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:01.082413: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:01.082870: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:01.082955: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:01.083445: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:01.084584: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:01.086003: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:01.087587: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:01.087626: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:01.087706: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:01.087851: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:01.088556: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:01.090118: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:01.093378: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:01.093421: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:01.093463: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:01.093612: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:01.094453: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:01.094960: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:01.097221: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:01.097378: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:01.097534: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:01.097684: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:01.098857: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:01.098906: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:01.101337: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:01.101561: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:01.101657: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:01.101840: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:01.103201: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:01.105338: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:01.105488: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:01.105628: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:01.105781: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:01.106874: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:01.108760: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:01.108887: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:01.109166: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:01.109900: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:01.109919: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:01.111906: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:01.111949: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:01.112223: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:01.113361: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:01.113361: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:01.114951: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:01.115046: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:01.115829: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:01.116988: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:01.117108: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:01.118939: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:01.119024: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:01.119320: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:01.120447: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:01.120610: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:01.122097: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:01.122276: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:01.122525: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:01.124357: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:01.124633: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:01.126026: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:01.126211: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:01.126462: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:01.127408: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:01.127925: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:01.129435: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:01.129565: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:01.129971: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:01.130462: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:01.131436: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:01.132569: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:01.132745: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:01.133216: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:01.133852: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:01.134661: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:01.135697: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:01.135865: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:01.136620: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:01.136628: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:01.136989: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:01.140188: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:01.140246: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:01.140287: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:01.141511: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:01.141659: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:01.142486: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 21:47:01.142677: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:01.142788: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:01.143434: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:01.145312: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:01.145321: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:01.146282: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:01.146366: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:01.146801: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:01.148628: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:01.148817: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:01.149011: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:01.149967: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:01.150118: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:01.150534: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:01.152925: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:01.153126: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:01.153225: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:01.153298: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:01.154372: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:01.154436: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:01.154973: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:01.157633: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:01.157841: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:01.158024: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:01.158121: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:01.158808: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:01.158896: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:01.159580: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:01.162143: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:01.162390: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:01.162573: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:01.162580: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:01.163435: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:01.163605: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:01.164640: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:01.166695: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:01.167224: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:01.172766: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:01.173518: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:01.173717: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:01.174952: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:01.177116: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:01.177406: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:01.177521: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:01.178597: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:01.181334: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:01.182221: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:01.182899: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 21:47:01.183047: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 21:47:01.183858: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:01.184370: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:01.184922: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 21:47:01.185453: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 21:47:01.186216: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:01.187037: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:01.188704: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:01.189759: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:01.191198: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:01.192292: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:01.193827: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:01.193965: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:01.194761: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:01.194876: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:01.196552: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:01.196960: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:01.197172: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:01.197345: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:01.199086: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:01.199906: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:01.201220: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:01.201365: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:01.201543: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:01.201666: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:01.203278: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:01.209488: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:01.210343: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:01.211535: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:01.240347: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:01.242831: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:01.245530: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:01.248808: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:01.251416: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:01.283495: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:01.285851: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:01.289968: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:01.292952: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:01.297253: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:01.300212: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:01.303708: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:01.305843: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:01.310675: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:01.312417: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 21:47:01.315212: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:01.320097: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:01.322725: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:01.327873: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 21:47:01.330589: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:01.335597: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:01.337938: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:01.423827: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:01.457636: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:02.315500: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:02.316332: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:02.316960: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:02.317436: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 21:47:02.317488: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-11 21:47:02.335403: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:02.336033: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:02.336550: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:02.337556: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:02.338259: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:02.338952: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-11 21:47:02.384692: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 21:47:02.384874: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 21:47:02.424512: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12340
2022-12-11 21:47:02.525652: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:02.526292: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:02.527243: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:02.527752: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 21:47:02.527811: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-11 21:47:02.545702: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:02.546560: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:02.547079: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:02.547932: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:02.548486: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:02.549165: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:1/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-11 21:47:02.584596: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:02.585226: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:02.585752: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:02.586333: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 21:47:02.586389: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-11 21:47:02.593081: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:02.593696: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:02.594215: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:02.594673: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 21:47:02.594725: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-11 21:47:02.596994: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:02.597962: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:02.598612: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:02.599072: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 21:47:02.599122: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-11 21:47:02.602847: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:02.603524: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:02.603690: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:02.604507: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:02.604992: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:02.605620: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:02.605951: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:02.606623: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:02.606798: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 21:47:02.606870: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-11 21:47:02.607459: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:2/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-11 21:47:02.611650: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:02.612243: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:02.612830: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:02.613460: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:02.613968: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:02.614435: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:3/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-11 21:47:02.617293: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:02.618020: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:02.618876: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:02.619517: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:02.620034: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:02.620508: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:5/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-11 21:47:02.625008: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:02.625801: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:02.626332: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:02.626903: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:02.627449: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:02.627925: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:4/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-11 21:47:02.633771: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 21:47:02.633940: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 21:47:02.635789: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12341
2022-12-11 21:47:02.636274: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:02.636873: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:02.637463: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:02.637955: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 21:47:02.638006: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-11 21:47:02.646235: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:02.646842: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:02.647389: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:02.647858: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 21:47:02.647907: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-11 21:47:02.655346: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:02.656433: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:02.656993: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:02.657585: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:02.658095: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:02.658404: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 21:47:02.658564: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 21:47:02.658574: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:6/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-11 21:47:02.660352: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12343
2022-12-11 21:47:02.663378: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 21:47:02.663535: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 21:47:02.665313: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12345
2022-12-11 21:47:02.665647: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:02.666280: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:02.666788: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:02.667384: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:02.667906: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:47:02.668381: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:7/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-11 21:47:02.672175: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 21:47:02.672341: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 21:47:02.674168: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12344
2022-12-11 21:47:02.686676: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 21:47:02.686825: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 21:47:02.688618: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12342
2022-12-11 21:47:02.703522: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 21:47:02.703701: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 21:47:02.705410: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12346
2022-12-11 21:47:02.712452: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 21:47:02.712604: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 21:47:02.714273: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12347
[HCTR][21:47:03.981][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][21:47:03.981][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][21:47:03.981][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][21:47:03.981][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][21:47:03.982][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][21:47:03.982][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][21:47:03.982][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][21:47:03.982][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 1it [00:01,  1.55s/it]warmup run: 96it [00:01, 80.66it/s]warmup run: 1it [00:01,  1.60s/it]warmup run: 193it [00:01, 176.08it/s]warmup run: 95it [00:01, 77.57it/s]warmup run: 1it [00:01,  1.51s/it]warmup run: 1it [00:01,  1.53s/it]warmup run: 1it [00:01,  1.51s/it]warmup run: 1it [00:01,  1.53s/it]warmup run: 290it [00:01, 281.51it/s]warmup run: 1it [00:01,  1.49s/it]warmup run: 190it [00:01, 168.90it/s]warmup run: 1it [00:01,  1.47s/it]warmup run: 93it [00:01, 79.93it/s]warmup run: 92it [00:01, 78.44it/s]warmup run: 91it [00:01, 78.40it/s]warmup run: 96it [00:01, 81.48it/s]warmup run: 385it [00:01, 388.15it/s]warmup run: 100it [00:01, 87.00it/s]warmup run: 284it [00:01, 269.08it/s]warmup run: 90it [00:01, 79.29it/s]warmup run: 195it [00:01, 182.83it/s]warmup run: 181it [00:01, 168.68it/s]warmup run: 185it [00:01, 170.98it/s]warmup run: 480it [00:02, 492.30it/s]warmup run: 192it [00:01, 176.64it/s]warmup run: 201it [00:01, 189.08it/s]warmup run: 378it [00:02, 373.69it/s]warmup run: 181it [00:01, 172.36it/s]warmup run: 297it [00:01, 296.05it/s]warmup run: 271it [00:01, 267.86it/s]warmup run: 278it [00:01, 272.90it/s]warmup run: 577it [00:02, 591.39it/s]warmup run: 290it [00:01, 284.07it/s]warmup run: 302it [00:01, 301.01it/s]warmup run: 472it [00:02, 476.96it/s]warmup run: 271it [00:01, 272.64it/s]warmup run: 400it [00:01, 414.41it/s]warmup run: 361it [00:01, 370.43it/s]warmup run: 370it [00:01, 377.03it/s]warmup run: 672it [00:02, 673.28it/s]warmup run: 388it [00:01, 395.65it/s]warmup run: 403it [00:01, 416.09it/s]warmup run: 567it [00:02, 574.60it/s]warmup run: 361it [00:01, 375.65it/s]warmup run: 503it [00:02, 529.59it/s]warmup run: 462it [00:02, 478.57it/s]warmup run: 452it [00:02, 471.37it/s]warmup run: 769it [00:02, 745.12it/s]warmup run: 485it [00:02, 501.86it/s]warmup run: 501it [00:01, 522.43it/s]warmup run: 662it [00:02, 659.52it/s]warmup run: 452it [00:01, 476.35it/s]warmup run: 607it [00:02, 637.10it/s]warmup run: 555it [00:02, 573.61it/s]warmup run: 543it [00:02, 563.89it/s]warmup run: 583it [00:02, 602.05it/s]warmup run: 867it [00:02, 804.52it/s]warmup run: 601it [00:02, 623.53it/s]warmup run: 758it [00:02, 731.76it/s]warmup run: 543it [00:02, 567.92it/s]warmup run: 712it [00:02, 731.65it/s]warmup run: 649it [00:02, 657.18it/s]warmup run: 635it [00:02, 645.60it/s]warmup run: 964it [00:02, 848.99it/s]warmup run: 681it [00:02, 688.26it/s]warmup run: 701it [00:02, 709.70it/s]warmup run: 853it [00:02, 787.42it/s]warmup run: 636it [00:02, 650.83it/s]warmup run: 816it [00:02, 806.73it/s]warmup run: 742it [00:02, 724.13it/s]warmup run: 727it [00:02, 712.49it/s]warmup run: 779it [00:02, 759.64it/s]warmup run: 1061it [00:02, 880.60it/s]warmup run: 801it [00:02, 781.33it/s]warmup run: 947it [00:02, 827.86it/s]warmup run: 728it [00:02, 716.61it/s]warmup run: 920it [00:02, 865.76it/s]warmup run: 836it [00:02, 779.10it/s]warmup run: 818it [00:02, 763.86it/s]warmup run: 877it [00:02, 815.99it/s]warmup run: 1158it [00:02, 904.89it/s]warmup run: 901it [00:02, 837.64it/s]warmup run: 1041it [00:02, 855.90it/s]warmup run: 820it [00:02, 768.50it/s]warmup run: 1023it [00:02, 910.22it/s]warmup run: 929it [00:02, 820.01it/s]warmup run: 908it [00:02, 800.23it/s]warmup run: 974it [00:02, 857.21it/s]warmup run: 1256it [00:02, 926.14it/s]warmup run: 1000it [00:02, 878.84it/s]warmup run: 1135it [00:02, 875.20it/s]warmup run: 911it [00:02, 806.46it/s]warmup run: 1126it [00:02, 942.05it/s]warmup run: 1023it [00:02, 851.14it/s]warmup run: 1000it [00:02, 831.33it/s]warmup run: 1354it [00:02, 941.47it/s]warmup run: 1071it [00:02, 882.66it/s]warmup run: 1100it [00:02, 910.27it/s]warmup run: 1229it [00:02, 893.41it/s]warmup run: 1002it [00:02, 833.16it/s]warmup run: 1231it [00:02, 970.17it/s]warmup run: 1091it [00:02, 852.99it/s]warmup run: 1118it [00:02, 877.49it/s]warmup run: 1452it [00:03, 951.83it/s]warmup run: 1168it [00:02, 906.68it/s]warmup run: 1201it [00:02, 936.38it/s]warmup run: 1093it [00:02, 854.44it/s]warmup run: 1323it [00:03, 900.24it/s]warmup run: 1335it [00:02, 988.94it/s]warmup run: 1182it [00:02, 868.90it/s]warmup run: 1211it [00:02, 891.13it/s]warmup run: 1550it [00:03, 956.22it/s]warmup run: 1267it [00:02, 928.03it/s]warmup run: 1301it [00:02, 954.19it/s]warmup run: 1185it [00:02, 872.10it/s]warmup run: 1416it [00:03, 905.40it/s]warmup run: 1439it [00:02, 1001.16it/s]warmup run: 1274it [00:02, 883.49it/s]warmup run: 1305it [00:02, 904.93it/s]warmup run: 1648it [00:03, 961.19it/s]warmup run: 1367it [00:02, 946.35it/s]warmup run: 1402it [00:02, 967.62it/s]warmup run: 1276it [00:02, 881.82it/s]warmup run: 1509it [00:03, 907.69it/s]warmup run: 1542it [00:03, 1002.69it/s]warmup run: 1369it [00:03, 901.24it/s]warmup run: 1400it [00:03, 916.36it/s]warmup run: 1746it [00:03, 964.47it/s]warmup run: 1465it [00:03, 953.36it/s]warmup run: 1502it [00:03, 974.50it/s]warmup run: 1368it [00:02, 892.09it/s]warmup run: 1602it [00:03, 909.51it/s]warmup run: 1645it [00:03, 1005.63it/s]warmup run: 1465it [00:03, 917.17it/s]warmup run: 1496it [00:03, 926.59it/s]warmup run: 1844it [00:03, 963.93it/s]warmup run: 1563it [00:03, 956.12it/s]warmup run: 1460it [00:03, 897.71it/s]warmup run: 1694it [00:03, 911.78it/s]warmup run: 1602it [00:03, 912.43it/s]warmup run: 1747it [00:03, 1007.21it/s]warmup run: 1591it [00:03, 933.29it/s]warmup run: 1558it [00:03, 916.85it/s]warmup run: 1941it [00:03, 963.76it/s]warmup run: 1660it [00:03, 958.12it/s]warmup run: 1551it [00:03, 900.18it/s]warmup run: 1786it [00:03, 913.85it/s]warmup run: 1704it [00:03, 942.64it/s]warmup run: 1850it [00:03, 1011.56it/s]warmup run: 1686it [00:03, 936.68it/s]warmup run: 1654it [00:03, 928.92it/s]warmup run: 2043it [00:03, 979.89it/s]warmup run: 1757it [00:03, 955.42it/s]warmup run: 1642it [00:03, 902.39it/s]warmup run: 1880it [00:03, 919.34it/s]warmup run: 1807it [00:03, 964.94it/s]warmup run: 1953it [00:03, 1014.39it/s]warmup run: 1781it [00:03, 939.52it/s]warmup run: 1750it [00:03, 936.87it/s]warmup run: 2160it [00:03, 1034.95it/s]warmup run: 1855it [00:03, 960.19it/s]warmup run: 1734it [00:03, 906.66it/s]warmup run: 1973it [00:03, 921.08it/s]warmup run: 1909it [00:03, 978.08it/s]warmup run: 2065it [00:03, 1043.89it/s]warmup run: 1845it [00:03, 940.42it/s]warmup run: 1879it [00:03, 949.18it/s]warmup run: 2277it [00:03, 1073.38it/s]warmup run: 1953it [00:03, 964.11it/s]warmup run: 1826it [00:03, 909.98it/s]warmup run: 2086it [00:03, 981.46it/s]warmup run: 2013it [00:03, 994.65it/s]warmup run: 2187it [00:03, 1095.59it/s]warmup run: 1941it [00:03, 944.52it/s]warmup run: 1978it [00:03, 960.59it/s]warmup run: 2395it [00:03, 1104.89it/s]warmup run: 2060it [00:03, 995.41it/s]warmup run: 1918it [00:03, 911.59it/s]warmup run: 2207it [00:03, 1047.79it/s]warmup run: 2133it [00:03, 1053.93it/s]warmup run: 2309it [00:03, 1131.62it/s]warmup run: 2091it [00:03, 1010.93it/s]warmup run: 2045it [00:03, 971.79it/s]warmup run: 2513it [00:04, 1127.28it/s]warmup run: 2179it [00:03, 1050.82it/s]warmup run: 2012it [00:03, 919.52it/s]warmup run: 2329it [00:04, 1096.64it/s]warmup run: 2253it [00:03, 1095.84it/s]warmup run: 2431it [00:03, 1156.57it/s]warmup run: 2209it [00:03, 1059.81it/s]warmup run: 2166it [00:03, 1040.99it/s]warmup run: 2630it [00:04, 1138.96it/s]warmup run: 2298it [00:03, 1090.66it/s]warmup run: 2130it [00:03, 995.42it/s]warmup run: 2451it [00:04, 1130.90it/s]warmup run: 2373it [00:03, 1125.34it/s]warmup run: 2553it [00:03, 1173.68it/s]warmup run: 2327it [00:03, 1093.96it/s]warmup run: 2287it [00:03, 1089.49it/s]warmup run: 2748it [00:04, 1150.29it/s]warmup run: 2417it [00:03, 1119.87it/s]warmup run: 2248it [00:03, 1049.06it/s]warmup run: 2573it [00:04, 1155.29it/s]warmup run: 2492it [00:03, 1141.75it/s]warmup run: 2674it [00:04, 1182.18it/s]warmup run: 2445it [00:04, 1118.80it/s]warmup run: 2408it [00:04, 1123.25it/s]warmup run: 2866it [00:04, 1158.49it/s]warmup run: 2536it [00:04, 1140.39it/s]warmup run: 2366it [00:03, 1086.60it/s]warmup run: 2693it [00:04, 1168.37it/s]warmup run: 2607it [00:04, 1133.62it/s]warmup run: 2796it [00:04, 1191.99it/s]warmup run: 2563it [00:04, 1135.30it/s]warmup run: 2528it [00:04, 1146.01it/s]warmup run: 2983it [00:04, 1161.27it/s]warmup run: 2654it [00:04, 1151.16it/s]warmup run: 3000it [00:04, 668.57it/s] warmup run: 2483it [00:04, 1109.32it/s]warmup run: 2814it [00:04, 1180.24it/s]warmup run: 2727it [00:04, 1153.21it/s]warmup run: 2917it [00:04, 1196.46it/s]warmup run: 2679it [00:04, 1141.68it/s]warmup run: 2647it [00:04, 1156.69it/s]warmup run: 2773it [00:04, 1161.30it/s]warmup run: 2599it [00:04, 1124.28it/s]warmup run: 2936it [00:04, 1189.68it/s]warmup run: 2849it [00:04, 1172.16it/s]warmup run: 3000it [00:04, 696.48it/s] warmup run: 3000it [00:04, 654.85it/s] warmup run: 2797it [00:04, 1151.52it/s]warmup run: 2765it [00:04, 1161.98it/s]warmup run: 2892it [00:04, 1167.73it/s]warmup run: 2716it [00:04, 1136.37it/s]warmup run: 2972it [00:04, 1186.66it/s]warmup run: 3000it [00:04, 687.04it/s] warmup run: 2918it [00:04, 1168.13it/s]warmup run: 2884it [00:04, 1167.53it/s]warmup run: 3000it [00:04, 673.74it/s] warmup run: 2831it [00:04, 1140.41it/s]warmup run: 3000it [00:04, 664.41it/s] warmup run: 3000it [00:04, 662.53it/s] warmup run: 2949it [00:04, 1150.46it/s]warmup run: 3000it [00:04, 660.93it/s] 


warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]

warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   5%|▌         | 162/3000 [00:00<00:01, 1617.26it/s]warmup should be done:   5%|▌         | 158/3000 [00:00<00:01, 1578.09it/s]warmup should be done:   6%|▌         | 165/3000 [00:00<00:01, 1648.11it/s]warmup should be done:   5%|▌         | 160/3000 [00:00<00:01, 1594.34it/s]warmup should be done:   5%|▌         | 164/3000 [00:00<00:01, 1634.42it/s]warmup should be done:   5%|▌         | 163/3000 [00:00<00:01, 1624.45it/s]warmup should be done:   6%|▌         | 165/3000 [00:00<00:01, 1641.49it/s]warmup should be done:   5%|▌         | 160/3000 [00:00<00:01, 1590.75it/s]warmup should be done:  11%|█         | 332/3000 [00:00<00:01, 1659.02it/s]warmup should be done:  11%|█         | 325/3000 [00:00<00:01, 1620.57it/s]warmup should be done:  11%|█         | 321/3000 [00:00<00:01, 1600.53it/s]warmup should be done:  11%|█         | 330/3000 [00:00<00:01, 1645.37it/s]warmup should be done:  11%|█         | 319/3000 [00:00<00:01, 1592.32it/s]warmup should be done:  11%|█         | 329/3000 [00:00<00:01, 1640.23it/s]warmup should be done:  11%|█         | 328/3000 [00:00<00:01, 1636.43it/s]warmup should be done:  11%|█         | 323/3000 [00:00<00:01, 1609.27it/s]warmup should be done:  17%|█▋        | 498/3000 [00:00<00:01, 1656.71it/s]warmup should be done:  16%|█▋        | 492/3000 [00:00<00:01, 1635.78it/s]warmup should be done:  16%|█▌        | 479/3000 [00:00<00:01, 1591.32it/s]warmup should be done:  16%|█▋        | 495/3000 [00:00<00:01, 1641.52it/s]warmup should be done:  16%|█▌        | 485/3000 [00:00<00:01, 1610.33it/s]warmup should be done:  16%|█▌        | 482/3000 [00:00<00:01, 1595.77it/s]warmup should be done:  16%|█▋        | 488/3000 [00:00<00:01, 1613.02it/s]warmup should be done:  16%|█▋        | 494/3000 [00:00<00:01, 1634.00it/s]warmup should be done:  22%|██▏       | 664/3000 [00:00<00:01, 1654.81it/s]warmup should be done:  22%|██▏       | 656/3000 [00:00<00:01, 1634.45it/s]warmup should be done:  21%|██▏       | 639/3000 [00:00<00:01, 1588.68it/s]warmup should be done:  22%|██▏       | 660/3000 [00:00<00:01, 1639.45it/s]warmup should be done:  21%|██▏       | 642/3000 [00:00<00:01, 1593.27it/s]warmup should be done:  22%|██▏       | 647/3000 [00:00<00:01, 1609.27it/s]warmup should be done:  22%|██▏       | 650/3000 [00:00<00:01, 1610.00it/s]warmup should be done:  22%|██▏       | 658/3000 [00:00<00:01, 1630.03it/s]warmup should be done:  28%|██▊       | 830/3000 [00:00<00:01, 1652.98it/s]warmup should be done:  27%|██▋       | 820/3000 [00:00<00:01, 1633.38it/s]warmup should be done:  27%|██▋       | 808/3000 [00:00<00:01, 1609.36it/s]warmup should be done:  27%|██▋       | 824/3000 [00:00<00:01, 1636.31it/s]warmup should be done:  27%|██▋       | 799/3000 [00:00<00:01, 1589.37it/s]warmup should be done:  27%|██▋       | 802/3000 [00:00<00:01, 1590.12it/s]warmup should be done:  27%|██▋       | 812/3000 [00:00<00:01, 1608.20it/s]warmup should be done:  27%|██▋       | 822/3000 [00:00<00:01, 1626.90it/s]warmup should be done:  32%|███▏      | 958/3000 [00:00<00:01, 1589.23it/s]warmup should be done:  33%|███▎      | 996/3000 [00:00<00:01, 1648.48it/s]warmup should be done:  33%|███▎      | 984/3000 [00:00<00:01, 1628.14it/s]warmup should be done:  32%|███▏      | 969/3000 [00:00<00:01, 1602.68it/s]warmup should be done:  33%|███▎      | 988/3000 [00:00<00:01, 1631.33it/s]warmup should be done:  32%|███▏      | 962/3000 [00:00<00:01, 1589.27it/s]warmup should be done:  33%|███▎      | 985/3000 [00:00<00:01, 1626.62it/s]warmup should be done:  32%|███▏      | 973/3000 [00:00<00:01, 1602.07it/s]warmup should be done:  38%|███▊      | 1147/3000 [00:00<00:01, 1627.90it/s]warmup should be done:  39%|███▊      | 1162/3000 [00:00<00:01, 1649.08it/s]warmup should be done:  37%|███▋      | 1117/3000 [00:00<00:01, 1584.06it/s]warmup should be done:  38%|███▊      | 1152/3000 [00:00<00:01, 1631.99it/s]warmup should be done:  38%|███▊      | 1130/3000 [00:00<00:01, 1601.77it/s]warmup should be done:  38%|███▊      | 1151/3000 [00:00<00:01, 1636.13it/s]warmup should be done:  37%|███▋      | 1121/3000 [00:00<00:01, 1585.97it/s]warmup should be done:  38%|███▊      | 1134/3000 [00:00<00:01, 1603.03it/s]warmup should be done:  44%|████▎     | 1310/3000 [00:00<00:01, 1627.69it/s]warmup should be done:  44%|████▍     | 1327/3000 [00:00<00:01, 1648.08it/s]warmup should be done:  43%|████▎     | 1276/3000 [00:00<00:01, 1584.31it/s]warmup should be done:  43%|████▎     | 1291/3000 [00:00<00:01, 1601.79it/s]warmup should be done:  44%|████▍     | 1316/3000 [00:00<00:01, 1630.34it/s]warmup should be done:  43%|████▎     | 1280/3000 [00:00<00:01, 1585.78it/s]warmup should be done:  43%|████▎     | 1295/3000 [00:00<00:01, 1602.99it/s]warmup should be done:  44%|████▍     | 1315/3000 [00:00<00:01, 1630.64it/s]warmup should be done:  49%|████▉     | 1473/3000 [00:00<00:00, 1627.81it/s]warmup should be done:  50%|████▉     | 1492/3000 [00:00<00:00, 1647.81it/s]warmup should be done:  48%|████▊     | 1435/3000 [00:00<00:00, 1584.33it/s]warmup should be done:  48%|████▊     | 1439/3000 [00:00<00:00, 1586.13it/s]warmup should be done:  48%|████▊     | 1452/3000 [00:00<00:00, 1601.08it/s]warmup should be done:  49%|████▉     | 1480/3000 [00:00<00:00, 1627.48it/s]warmup should be done:  49%|████▊     | 1456/3000 [00:00<00:00, 1602.36it/s]warmup should be done:  49%|████▉     | 1479/3000 [00:00<00:00, 1627.34it/s]warmup should be done:  55%|█████▍    | 1636/3000 [00:01<00:00, 1627.57it/s]warmup should be done:  55%|█████▌    | 1657/3000 [00:01<00:00, 1648.11it/s]warmup should be done:  53%|█████▎    | 1594/3000 [00:01<00:00, 1584.78it/s]warmup should be done:  53%|█████▎    | 1598/3000 [00:01<00:00, 1585.35it/s]warmup should be done:  55%|█████▍    | 1643/3000 [00:01<00:00, 1625.76it/s]warmup should be done:  54%|█████▍    | 1617/3000 [00:01<00:00, 1602.55it/s]warmup should be done:  54%|█████▍    | 1613/3000 [00:01<00:00, 1596.40it/s]warmup should be done:  55%|█████▍    | 1642/3000 [00:01<00:00, 1624.75it/s]warmup should be done:  60%|██████    | 1800/3000 [00:01<00:00, 1628.20it/s]warmup should be done:  58%|█████▊    | 1753/3000 [00:01<00:00, 1585.36it/s]warmup should be done:  61%|██████    | 1822/3000 [00:01<00:00, 1643.11it/s]warmup should be done:  59%|█████▊    | 1757/3000 [00:01<00:00, 1584.25it/s]warmup should be done:  60%|██████    | 1806/3000 [00:01<00:00, 1625.87it/s]warmup should be done:  59%|█████▉    | 1778/3000 [00:01<00:00, 1598.66it/s]warmup should be done:  59%|█████▉    | 1773/3000 [00:01<00:00, 1591.86it/s]warmup should be done:  60%|██████    | 1805/3000 [00:01<00:00, 1624.78it/s]warmup should be done:  65%|██████▌   | 1963/3000 [00:01<00:00, 1627.78it/s]warmup should be done:  66%|██████▌   | 1987/3000 [00:01<00:00, 1644.42it/s]warmup should be done:  64%|██████▎   | 1912/3000 [00:01<00:00, 1584.22it/s]warmup should be done:  64%|██████▍   | 1916/3000 [00:01<00:00, 1583.91it/s]warmup should be done:  66%|██████▌   | 1969/3000 [00:01<00:00, 1625.30it/s]warmup should be done:  66%|██████▌   | 1968/3000 [00:01<00:00, 1624.09it/s]warmup should be done:  65%|██████▍   | 1938/3000 [00:01<00:00, 1594.06it/s]warmup should be done:  64%|██████▍   | 1933/3000 [00:01<00:00, 1589.48it/s]warmup should be done:  71%|███████   | 2126/3000 [00:01<00:00, 1626.56it/s]warmup should be done:  72%|███████▏  | 2152/3000 [00:01<00:00, 1644.13it/s]warmup should be done:  69%|██████▉   | 2071/3000 [00:01<00:00, 1581.77it/s]warmup should be done:  69%|██████▉   | 2075/3000 [00:01<00:00, 1583.09it/s]warmup should be done:  71%|███████   | 2132/3000 [00:01<00:00, 1626.28it/s]warmup should be done:  71%|███████   | 2131/3000 [00:01<00:00, 1625.34it/s]warmup should be done:  70%|██████▉   | 2093/3000 [00:01<00:00, 1591.72it/s]warmup should be done:  70%|██████▉   | 2098/3000 [00:01<00:00, 1589.75it/s]warmup should be done:  76%|███████▋  | 2289/3000 [00:01<00:00, 1623.59it/s]warmup should be done:  77%|███████▋  | 2317/3000 [00:01<00:00, 1642.35it/s]warmup should be done:  74%|███████▍  | 2234/3000 [00:01<00:00, 1584.24it/s]warmup should be done:  76%|███████▋  | 2295/3000 [00:01<00:00, 1625.20it/s]warmup should be done:  74%|███████▍  | 2230/3000 [00:01<00:00, 1575.38it/s]warmup should be done:  76%|███████▋  | 2294/3000 [00:01<00:00, 1622.94it/s]warmup should be done:  75%|███████▌  | 2253/3000 [00:01<00:00, 1593.20it/s]warmup should be done:  75%|███████▌  | 2257/3000 [00:01<00:00, 1586.79it/s]warmup should be done:  82%|████████▏ | 2452/3000 [00:01<00:00, 1624.88it/s]warmup should be done:  83%|████████▎ | 2482/3000 [00:01<00:00, 1643.62it/s]warmup should be done:  82%|████████▏ | 2458/3000 [00:01<00:00, 1624.80it/s]warmup should be done:  80%|███████▉  | 2388/3000 [00:01<00:00, 1575.02it/s]warmup should be done:  80%|███████▉  | 2393/3000 [00:01<00:00, 1580.21it/s]warmup should be done:  80%|████████  | 2414/3000 [00:01<00:00, 1596.57it/s]warmup should be done:  82%|████████▏ | 2457/3000 [00:01<00:00, 1623.05it/s]warmup should be done:  81%|████████  | 2416/3000 [00:01<00:00, 1586.90it/s]warmup should be done:  87%|████████▋ | 2615/3000 [00:01<00:00, 1626.31it/s]warmup should be done:  88%|████████▊ | 2647/3000 [00:01<00:00, 1644.70it/s]warmup should be done:  87%|████████▋ | 2621/3000 [00:01<00:00, 1625.36it/s]warmup should be done:  85%|████████▌ | 2552/3000 [00:01<00:00, 1581.60it/s]warmup should be done:  85%|████████▍ | 2547/3000 [00:01<00:00, 1577.44it/s]warmup should be done:  87%|████████▋ | 2620/3000 [00:01<00:00, 1623.84it/s]warmup should be done:  86%|████████▌ | 2575/3000 [00:01<00:00, 1597.63it/s]warmup should be done:  86%|████████▌ | 2575/3000 [00:01<00:00, 1586.82it/s]warmup should be done:  93%|█████████▎| 2778/3000 [00:01<00:00, 1626.43it/s]warmup should be done:  94%|█████████▎| 2812/3000 [00:01<00:00, 1645.94it/s]warmup should be done:  93%|█████████▎| 2785/3000 [00:01<00:00, 1627.33it/s]warmup should be done:  90%|█████████ | 2711/3000 [00:01<00:00, 1581.99it/s]warmup should be done:  90%|█████████ | 2706/3000 [00:01<00:00, 1578.62it/s]warmup should be done:  93%|█████████▎| 2784/3000 [00:01<00:00, 1625.86it/s]warmup should be done:  91%|█████████ | 2735/3000 [00:01<00:00, 1595.37it/s]warmup should be done:  91%|█████████ | 2734/3000 [00:01<00:00, 1587.55it/s]warmup should be done:  98%|█████████▊| 2943/3000 [00:01<00:00, 1630.87it/s]warmup should be done:  99%|█████████▉| 2979/3000 [00:01<00:00, 1650.94it/s]warmup should be done:  98%|█████████▊| 2950/3000 [00:01<00:00, 1632.50it/s]warmup should be done:  96%|█████████▌| 2866/3000 [00:01<00:00, 1583.22it/s]warmup should be done:  96%|█████████▌| 2871/3000 [00:01<00:00, 1584.60it/s]warmup should be done:  98%|█████████▊| 2949/3000 [00:01<00:00, 1631.69it/s]warmup should be done:  97%|█████████▋| 2896/3000 [00:01<00:00, 1599.01it/s]warmup should be done:  96%|█████████▋| 2895/3000 [00:01<00:00, 1591.82it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1647.99it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1629.98it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1628.69it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1628.32it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1599.26it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1597.52it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1586.05it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1583.70it/s]




warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]

warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   5%|▌         | 164/3000 [00:00<00:01, 1637.72it/s]warmup should be done:   6%|▌         | 169/3000 [00:00<00:01, 1684.51it/s]warmup should be done:   5%|▌         | 163/3000 [00:00<00:01, 1623.10it/s]warmup should be done:   5%|▌         | 163/3000 [00:00<00:01, 1621.18it/s]warmup should be done:   6%|▌         | 168/3000 [00:00<00:01, 1669.83it/s]warmup should be done:   5%|▌         | 163/3000 [00:00<00:01, 1621.65it/s]warmup should be done:   6%|▌         | 168/3000 [00:00<00:01, 1670.33it/s]warmup should be done:   6%|▌         | 167/3000 [00:00<00:01, 1660.61it/s]warmup should be done:  11%|█         | 329/3000 [00:00<00:01, 1644.43it/s]warmup should be done:  11%|█▏        | 338/3000 [00:00<00:01, 1686.30it/s]warmup should be done:  11%|█         | 336/3000 [00:00<00:01, 1675.31it/s]warmup should be done:  11%|█         | 327/3000 [00:00<00:01, 1628.72it/s]warmup should be done:  11%|█         | 327/3000 [00:00<00:01, 1627.94it/s]warmup should be done:  11%|█         | 336/3000 [00:00<00:01, 1671.62it/s]warmup should be done:  11%|█         | 335/3000 [00:00<00:01, 1666.06it/s]warmup should be done:  11%|█         | 326/3000 [00:00<00:01, 1618.21it/s]warmup should be done:  16%|█▋        | 495/3000 [00:00<00:01, 1649.58it/s]warmup should be done:  17%|█▋        | 504/3000 [00:00<00:01, 1676.54it/s]warmup should be done:  17%|█▋        | 508/3000 [00:00<00:01, 1688.38it/s]warmup should be done:  16%|█▋        | 491/3000 [00:00<00:01, 1632.33it/s]warmup should be done:  16%|█▋        | 491/3000 [00:00<00:01, 1632.53it/s]warmup should be done:  17%|█▋        | 503/3000 [00:00<00:01, 1671.55it/s]warmup should be done:  17%|█▋        | 504/3000 [00:00<00:01, 1671.65it/s]warmup should be done:  16%|█▋        | 490/3000 [00:00<00:01, 1622.78it/s]warmup should be done:  22%|██▏       | 660/3000 [00:00<00:01, 1648.35it/s]warmup should be done:  22%|██▏       | 655/3000 [00:00<00:01, 1635.30it/s]warmup should be done:  22%|██▏       | 672/3000 [00:00<00:01, 1675.12it/s]warmup should be done:  23%|██▎       | 678/3000 [00:00<00:01, 1691.70it/s]warmup should be done:  22%|██▏       | 671/3000 [00:00<00:01, 1673.82it/s]warmup should be done:  22%|██▏       | 672/3000 [00:00<00:01, 1674.45it/s]warmup should be done:  22%|██▏       | 655/3000 [00:00<00:01, 1630.05it/s]warmup should be done:  22%|██▏       | 653/3000 [00:00<00:01, 1622.50it/s]warmup should be done:  28%|██▊       | 827/3000 [00:00<00:01, 1653.61it/s]warmup should be done:  28%|██▊       | 840/3000 [00:00<00:01, 1674.67it/s]warmup should be done:  27%|██▋       | 821/3000 [00:00<00:01, 1641.52it/s]warmup should be done:  28%|██▊       | 848/3000 [00:00<00:01, 1690.31it/s]warmup should be done:  28%|██▊       | 841/3000 [00:00<00:01, 1677.44it/s]warmup should be done:  28%|██▊       | 839/3000 [00:00<00:01, 1670.65it/s]warmup should be done:  27%|██▋       | 816/3000 [00:00<00:01, 1624.95it/s]warmup should be done:  27%|██▋       | 819/3000 [00:00<00:01, 1625.47it/s]warmup should be done:  33%|███▎      | 994/3000 [00:00<00:01, 1659.11it/s]warmup should be done:  34%|███▎      | 1008/3000 [00:00<00:01, 1675.99it/s]warmup should be done:  33%|███▎      | 988/3000 [00:00<00:01, 1648.05it/s]warmup should be done:  34%|███▎      | 1009/3000 [00:00<00:01, 1677.11it/s]warmup should be done:  33%|███▎      | 979/3000 [00:00<00:01, 1626.42it/s]warmup should be done:  34%|███▎      | 1007/3000 [00:00<00:01, 1671.34it/s]warmup should be done:  33%|███▎      | 983/3000 [00:00<00:01, 1628.83it/s]warmup should be done:  34%|███▍      | 1018/3000 [00:00<00:01, 1672.65it/s]warmup should be done:  39%|███▊      | 1162/3000 [00:00<00:01, 1663.46it/s]warmup should be done:  39%|███▉      | 1176/3000 [00:00<00:01, 1674.77it/s]warmup should be done:  38%|███▊      | 1155/3000 [00:00<00:01, 1654.15it/s]warmup should be done:  39%|███▉      | 1177/3000 [00:00<00:01, 1674.87it/s]warmup should be done:  38%|███▊      | 1143/3000 [00:00<00:01, 1628.87it/s]warmup should be done:  39%|███▉      | 1175/3000 [00:00<00:01, 1671.43it/s]warmup should be done:  38%|███▊      | 1147/3000 [00:00<00:01, 1630.42it/s]warmup should be done:  40%|███▉      | 1186/3000 [00:00<00:01, 1652.52it/s]warmup should be done:  44%|████▍     | 1329/3000 [00:00<00:01, 1664.47it/s]warmup should be done:  44%|████▍     | 1323/3000 [00:00<00:01, 1661.24it/s]warmup should be done:  45%|████▍     | 1344/3000 [00:00<00:00, 1670.33it/s]warmup should be done:  45%|████▍     | 1346/3000 [00:00<00:00, 1676.70it/s]warmup should be done:  45%|████▍     | 1343/3000 [00:00<00:00, 1673.46it/s]warmup should be done:  44%|████▎     | 1306/3000 [00:00<00:01, 1627.04it/s]warmup should be done:  44%|████▎     | 1311/3000 [00:00<00:01, 1628.19it/s]warmup should be done:  45%|████▌     | 1352/3000 [00:00<00:01, 1635.28it/s]warmup should be done:  50%|████▉     | 1490/3000 [00:00<00:00, 1657.54it/s]warmup should be done:  49%|████▉     | 1470/3000 [00:00<00:00, 1629.17it/s]warmup should be done:  50%|█████     | 1511/3000 [00:00<00:00, 1671.53it/s]warmup should be done:  50%|█████     | 1514/3000 [00:00<00:00, 1668.46it/s]warmup should be done:  50%|█████     | 1512/3000 [00:00<00:00, 1663.27it/s]warmup should be done:  49%|████▉     | 1475/3000 [00:00<00:00, 1629.29it/s]warmup should be done:  50%|████▉     | 1496/3000 [00:00<00:00, 1645.04it/s]warmup should be done:  51%|█████     | 1516/3000 [00:00<00:00, 1623.41it/s]warmup should be done:  55%|█████▌    | 1656/3000 [00:01<00:00, 1657.08it/s]warmup should be done:  54%|█████▍    | 1634/3000 [00:01<00:00, 1631.60it/s]warmup should be done:  56%|█████▌    | 1679/3000 [00:01<00:00, 1673.41it/s]warmup should be done:  56%|█████▌    | 1679/3000 [00:01<00:00, 1663.56it/s]warmup should be done:  56%|█████▌    | 1683/3000 [00:01<00:00, 1672.10it/s]warmup should be done:  55%|█████▍    | 1639/3000 [00:01<00:00, 1630.47it/s]warmup should be done:  55%|█████▌    | 1662/3000 [00:01<00:00, 1648.38it/s]warmup should be done:  56%|█████▌    | 1679/3000 [00:01<00:00, 1624.92it/s]warmup should be done:  62%|██████▏   | 1847/3000 [00:01<00:00, 1675.21it/s]warmup should be done:  61%|██████    | 1822/3000 [00:01<00:00, 1655.30it/s]warmup should be done:  60%|█████▉    | 1798/3000 [00:01<00:00, 1631.45it/s]warmup should be done:  62%|██████▏   | 1846/3000 [00:01<00:00, 1661.98it/s]warmup should be done:  62%|██████▏   | 1852/3000 [00:01<00:00, 1674.72it/s]warmup should be done:  61%|██████    | 1828/3000 [00:01<00:00, 1651.02it/s]warmup should be done:  60%|██████    | 1803/3000 [00:01<00:00, 1630.43it/s]warmup should be done:  62%|██████▏   | 1848/3000 [00:01<00:00, 1642.86it/s]warmup should be done:  67%|██████▋   | 2015/3000 [00:01<00:00, 1674.70it/s]warmup should be done:  66%|██████▋   | 1988/3000 [00:01<00:00, 1653.40it/s]warmup should be done:  65%|██████▌   | 1962/3000 [00:01<00:00, 1630.11it/s]warmup should be done:  67%|██████▋   | 2014/3000 [00:01<00:00, 1665.76it/s]warmup should be done:  67%|██████▋   | 2020/3000 [00:01<00:00, 1675.01it/s]warmup should be done:  66%|██████▌   | 1967/3000 [00:01<00:00, 1628.41it/s]warmup should be done:  66%|██████▋   | 1994/3000 [00:01<00:00, 1645.80it/s]warmup should be done:  67%|██████▋   | 2017/3000 [00:01<00:00, 1656.24it/s]warmup should be done:  73%|███████▎  | 2183/3000 [00:01<00:00, 1674.53it/s]warmup should be done:  72%|███████▏  | 2154/3000 [00:01<00:00, 1653.78it/s]warmup should be done:  71%|███████   | 2126/3000 [00:01<00:00, 1631.06it/s]warmup should be done:  73%|███████▎  | 2188/3000 [00:01<00:00, 1675.28it/s]warmup should be done:  73%|███████▎  | 2182/3000 [00:01<00:00, 1668.03it/s]warmup should be done:  71%|███████   | 2131/3000 [00:01<00:00, 1629.24it/s]warmup should be done:  72%|███████▏  | 2159/3000 [00:01<00:00, 1643.39it/s]warmup should be done:  73%|███████▎  | 2186/3000 [00:01<00:00, 1664.24it/s]warmup should be done:  78%|███████▊  | 2351/3000 [00:01<00:00, 1674.83it/s]warmup should be done:  77%|███████▋  | 2320/3000 [00:01<00:00, 1654.13it/s]warmup should be done:  76%|███████▋  | 2290/3000 [00:01<00:00, 1628.80it/s]warmup should be done:  79%|███████▊  | 2356/3000 [00:01<00:00, 1674.64it/s]warmup should be done:  78%|███████▊  | 2349/3000 [00:01<00:00, 1667.24it/s]warmup should be done:  76%|███████▋  | 2295/3000 [00:01<00:00, 1629.99it/s]warmup should be done:  77%|███████▋  | 2324/3000 [00:01<00:00, 1642.33it/s]warmup should be done:  78%|███████▊  | 2353/3000 [00:01<00:00, 1660.25it/s]warmup should be done:  84%|████████▍ | 2519/3000 [00:01<00:00, 1674.62it/s]warmup should be done:  83%|████████▎ | 2486/3000 [00:01<00:00, 1655.61it/s]warmup should be done:  82%|████████▏ | 2454/3000 [00:01<00:00, 1629.91it/s]warmup should be done:  84%|████████▍ | 2517/3000 [00:01<00:00, 1669.43it/s]warmup should be done:  84%|████████▍ | 2525/3000 [00:01<00:00, 1676.46it/s]warmup should be done:  82%|████████▏ | 2458/3000 [00:01<00:00, 1629.26it/s]warmup should be done:  83%|████████▎ | 2489/3000 [00:01<00:00, 1642.56it/s]warmup should be done:  84%|████████▍ | 2520/3000 [00:01<00:00, 1655.74it/s]warmup should be done:  90%|████████▉ | 2687/3000 [00:01<00:00, 1673.85it/s]warmup should be done:  88%|████████▊ | 2652/3000 [00:01<00:00, 1654.78it/s]warmup should be done:  89%|████████▉ | 2684/3000 [00:01<00:00, 1669.11it/s]warmup should be done:  90%|████████▉ | 2693/3000 [00:01<00:00, 1676.00it/s]warmup should be done:  87%|████████▋ | 2621/3000 [00:01<00:00, 1625.96it/s]warmup should be done:  88%|████████▊ | 2654/3000 [00:01<00:00, 1638.95it/s]warmup should be done:  87%|████████▋ | 2617/3000 [00:01<00:00, 1609.38it/s]warmup should be done:  90%|████████▉ | 2689/3000 [00:01<00:00, 1663.11it/s]warmup should be done:  94%|█████████▍| 2818/3000 [00:01<00:00, 1655.63it/s]warmup should be done:  95%|█████████▌| 2855/3000 [00:01<00:00, 1672.74it/s]warmup should be done:  95%|█████████▌| 2852/3000 [00:01<00:00, 1670.79it/s]warmup should be done:  95%|█████████▌| 2861/3000 [00:01<00:00, 1674.76it/s]warmup should be done:  93%|█████████▎| 2785/3000 [00:01<00:00, 1628.66it/s]warmup should be done:  94%|█████████▍| 2818/3000 [00:01<00:00, 1637.73it/s]warmup should be done:  93%|█████████▎| 2781/3000 [00:01<00:00, 1616.30it/s]warmup should be done:  95%|█████████▌| 2858/3000 [00:01<00:00, 1670.25it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1674.52it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1672.86it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1670.25it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1662.17it/s]warmup should be done:  99%|█████████▉| 2984/3000 [00:01<00:00, 1654.41it/s]warmup should be done:  98%|█████████▊| 2949/3000 [00:01<00:00, 1630.96it/s]warmup should be done:  99%|█████████▉| 2983/3000 [00:01<00:00, 1639.31it/s]warmup should be done:  98%|█████████▊| 2945/3000 [00:01<00:00, 1620.56it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1651.03it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1646.22it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1628.92it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1624.43it/s]2022-12-11 21:48:39.192770: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7ee86c02a490 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 21:48:39.192835: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 21:48:39.213851: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f03e782ff60 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 21:48:39.213906: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 21:48:39.246106: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7ee7e402d5a0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 21:48:39.246164: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 21:48:39.253813: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f03d3f92780 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 21:48:39.253862: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 21:48:39.443980: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f03db830a40 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 21:48:39.444043: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 21:48:39.623001: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f03e38330b0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 21:48:39.623075: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 21:48:39.636456: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f03e382baa0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 21:48:39.636519: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 21:48:39.667044: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7ee82c02f770 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 21:48:39.667147: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 21:48:41.483079: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 21:48:41.540460: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 21:48:41.543234: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 21:48:41.607336: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 21:48:41.672795: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 21:48:41.891533: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 21:48:41.947207: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 21:48:42.000204: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 21:48:44.426300: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 21:48:44.486621: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 21:48:44.516061: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 21:48:44.585077: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 21:48:44.610369: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 21:48:44.841313: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 21:48:44.899333: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 21:48:44.937980: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
[HCTR][21:49:24.789][ERROR][RK0][main]: replica 7 reaches 1000, calling init pre replica
[HCTR][21:49:24.789][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][21:49:24.794][ERROR][RK0][main]: coll ps creation done
[HCTR][21:49:24.794][ERROR][RK0][main]: replica 7 waits for coll ps creation barrier
[HCTR][21:49:24.807][ERROR][RK0][tid #139654821050112]: replica 6 reaches 1000, calling init pre replica
[HCTR][21:49:24.807][ERROR][RK0][tid #139654821050112]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][21:49:24.812][ERROR][RK0][tid #139654821050112]: coll ps creation done
[HCTR][21:49:24.812][ERROR][RK0][tid #139654821050112]: replica 6 waits for coll ps creation barrier
[HCTR][21:49:24.824][ERROR][RK0][tid #139654753941248]: replica 2 reaches 1000, calling init pre replica
[HCTR][21:49:24.824][ERROR][RK0][tid #139654753941248]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][21:49:24.829][ERROR][RK0][tid #139654753941248]: coll ps creation done
[HCTR][21:49:24.829][ERROR][RK0][tid #139654753941248]: replica 2 waits for coll ps creation barrier
[HCTR][21:49:24.843][ERROR][RK0][main]: replica 3 reaches 1000, calling init pre replica
[HCTR][21:49:24.843][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][21:49:24.847][ERROR][RK0][main]: coll ps creation done
[HCTR][21:49:24.847][ERROR][RK0][main]: replica 3 waits for coll ps creation barrier
[HCTR][21:49:24.908][ERROR][RK0][main]: replica 1 reaches 1000, calling init pre replica
[HCTR][21:49:24.908][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][21:49:24.913][ERROR][RK0][main]: coll ps creation done
[HCTR][21:49:24.913][ERROR][RK0][main]: replica 1 waits for coll ps creation barrier
[HCTR][21:49:24.924][ERROR][RK0][tid #139654812657408]: replica 0 reaches 1000, calling init pre replica
[HCTR][21:49:24.924][ERROR][RK0][tid #139654812657408]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][21:49:24.932][ERROR][RK0][tid #139654812657408]: coll ps creation done
[HCTR][21:49:24.932][ERROR][RK0][tid #139654812657408]: replica 0 waits for coll ps creation barrier
[HCTR][21:49:24.954][ERROR][RK0][tid #139655282419456]: replica 5 reaches 1000, calling init pre replica
[HCTR][21:49:24.954][ERROR][RK0][tid #139655282419456]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][21:49:24.959][ERROR][RK0][tid #139655282419456]: coll ps creation done
[HCTR][21:49:24.959][ERROR][RK0][tid #139655282419456]: replica 5 waits for coll ps creation barrier
[HCTR][21:49:24.968][ERROR][RK0][main]: replica 4 reaches 1000, calling init pre replica
[HCTR][21:49:24.968][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][21:49:24.974][ERROR][RK0][main]: coll ps creation done
[HCTR][21:49:24.974][ERROR][RK0][main]: replica 4 waits for coll ps creation barrier
[HCTR][21:49:24.974][ERROR][RK0][tid #139654812657408]: replica 0 preparing frequency
[HCTR][21:49:25.842][ERROR][RK0][tid #139654812657408]: replica 0 preparing frequency done
[HCTR][21:49:25.874][ERROR][RK0][tid #139654812657408]: replica 0 calling init per replica
[HCTR][21:49:25.874][ERROR][RK0][tid #139654753941248]: replica 2 calling init per replica
[HCTR][21:49:25.874][ERROR][RK0][main]: replica 4 calling init per replica
[HCTR][21:49:25.874][ERROR][RK0][tid #139655282419456]: replica 5 calling init per replica
[HCTR][21:49:25.874][ERROR][RK0][main]: replica 7 calling init per replica
[HCTR][21:49:25.874][ERROR][RK0][main]: replica 3 calling init per replica
[HCTR][21:49:25.874][ERROR][RK0][main]: replica 1 calling init per replica
[HCTR][21:49:25.874][ERROR][RK0][tid #139654821050112]: replica 6 calling init per replica
[HCTR][21:49:25.874][ERROR][RK0][tid #139654812657408]: Calling build_v2
[HCTR][21:49:25.874][ERROR][RK0][tid #139654753941248]: Calling build_v2
[HCTR][21:49:25.874][ERROR][RK0][main]: Calling build_v2
[HCTR][21:49:25.874][ERROR][RK0][tid #139655282419456]: Calling build_v2
[HCTR][21:49:25.874][ERROR][RK0][main]: Calling build_v2
[HCTR][21:49:25.874][ERROR][RK0][main]: Calling build_v2
[HCTR][21:49:25.874][ERROR][RK0][main]: Calling build_v2
[HCTR][21:49:25.874][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][21:49:25.874][ERROR][RK0][tid #139654812657408]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][21:49:25.874][ERROR][RK0][tid #139654821050112]: Calling build_v2
[HCTR][21:49:25.874][ERROR][RK0][tid #139654753941248]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][21:49:25.874][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][21:49:25.874][ERROR][RK0][tid #139655282419456]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][21:49:25.874][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][21:49:25.874][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][21:49:25.874][ERROR][RK0][tid #139654821050112]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[[[[[[[2022-12-11 21:49:25[2022-12-11 21:49:252022-12-11 21:49:252022-12-11 21:49:25.2022-12-11 21:49:252022-12-11 21:49:25..2022-12-11 21:49:25.2022-12-11 21:49:25874427..874421874421.874421.: 874440874445: : 874432: 874445E: : EE: E:  EE  E E/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc  /hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc /hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc :/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc::/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc:/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc136::136136:136:] 136136] ] 136] 136using concurrent impl MPS] ] using concurrent impl MPSusing concurrent impl MPS] using concurrent impl MPS] 
using concurrent impl MPSusing concurrent impl MPS

using concurrent impl MPS
using concurrent impl MPS



[2022-12-11 21:49:25.878651: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:178] v100x8, slow pcie
[2022-12-11 21:49:25.878691: E [/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-11 21:49:25:.196878697] : assigning 8 to cpuE
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:178] v100x8, slow pcie
[[2022-12-11 21:49:252022-12-11 21:49:25..878739878746: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:[:1782022-12-11 21:49:25196] .] v100x8, slow pcie[878773assigning 8 to cpu
2022-12-11 21:49:25: 
.E[878785 2022-12-11 21:49:25: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.E:878808 212: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] [E:build asymm link desc with 8X Tesla V100-SXM2-32GB out of 82022-12-11 21:49:25 178
./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] [878828:v100x8, slow pcie2022-12-11 21:49:25: 196
.E[] 878856 2022-12-11 21:49:25assigning 8 to cpu: [[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.
E2022-12-11 21:49:252022-12-11 21:49:25:878876 ..178: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc878884[878880] E:: 2022-12-11 21:49:25: v100x8, slow pcie 212[E.E
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] [2022-12-11 21:49:25 878924 :build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8[2022-12-11 21:49:25./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc213
2022-12-11 21:49:25.878956:E:] .878967: [196 178remote time is 8.68421878992: E2022-12-11 21:49:25] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] 
: E .assigning 8 to cpu:v100x8, slow pcieE[ /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc879051
178
 2022-12-11 21:49:25/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[:: ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.:2022-12-11 21:49:25212Ev100x8, slow pcie:[879114178.]  
1962022-12-11 21:49:25: ] 879169build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] .[Ev100x8, slow pcie: 
:assigning 8 to cpu8791902022-12-11 21:49:25 
E213
: .[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc ] [E8792392022-12-11 21:49:25:/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[remote time is 8.684212022-12-11 21:49:25 : .214:2022-12-11 21:49:25
./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE879285] 196.879297[: : cpu time is 97.0588] 879317: 2022-12-11 21:49:25212/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE
assigning 8 to cpu: E.] : 
E 879391build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8196/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: 
] :/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:Eassigning 8 to cpu213:[196 [
] 2122022-12-11 21:49:25] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-11 21:49:25remote time is 8.68421] .assigning 8 to cpu:.
build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8879538
[214879543
: [2022-12-11 21:49:25] : E2022-12-11 21:49:25.[cpu time is 97.0588E .879616[2022-12-11 21:49:25
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc879615: 2022-12-11 21:49:25./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:: E.879644:213E 879660: 212]  /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: E] remote time is 8.68421/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:E build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8
:214[ /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
212] 2022-12-11 21:49:25/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:[] cpu time is 97.0588.:2132022-12-11 21:49:25build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8
879823212] .
: ] remote time is 8.68421879857[Ebuild asymm link desc with 8X Tesla V100-SXM2-32GB out of 8
: 2022-12-11 21:49:25 [
E./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-11 21:49:25 879920:.[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: 2148799432022-12-11 21:49:25:E] : .213 cpu time is 97.0588E879963] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
 : remote time is 8.68421:/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE
213:[ ] 2142022-12-11 21:49:25/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccremote time is 8.68421] .:
cpu time is 97.0588880057213
: [] E2022-12-11 21:49:25remote time is 8.68421 .
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc880098:: 214[E] 2022-12-11 21:49:25 cpu time is 97.0588./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
880129:: 214E]  cpu time is 97.0588/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
:214] cpu time is 97.0588
[2022-12-11 21:50:42.649448: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:118] solver created. now build & solve
[2022-12-11 21:50:42.689336: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 381.47 MB
[2022-12-11 21:50:42.689424: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 381.47 MB
[2022-12-11 21:50:42.690599: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:74] mapping nid to rank...
[2022-12-11 21:50:42.761850: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:91] counting slots...
[2022-12-11 21:50:43.147550: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:105] Final num slot is 49
[2022-12-11 21:50:43.147645: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:109] counting blocks...
[2022-12-11 21:50:49.984950: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:118] Final num block is 1024
[2022-12-11 21:50:49.985060: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:123] counting freq and density...
[2022-12-11 21:50:51.724567: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:154] averaging freq and density...
[2022-12-11 21:50:51.724662: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:155] 1024
[2022-12-11 21:50:51.727615: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:123] solver built. now solve
[2022-12-11 21:50:51.727675: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:354] constructing optimal solver, device=8, stream=1
1024 blocks, 8 devices
[2022-12-11 21:50:52.102891: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:527] Add Var...
[2022-12-11 21:50:52.159000: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:545] Capacity...
[2022-12-11 21:50:52.161631: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:548] Connect CPU...
[2022-12-11 21:50:52.200935: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:550] Connect Access To Storage...
[2022-12-11 21:50:52.765552: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:554] Time...
[2022-12-11 21:51:08. 18012: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:569] Coll Cache init block placement array
[2022-12-11 21:51:08. 25644: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:645] Coll Cache init block placement array done
[2022-12-11 21:51:08. 26676: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:647] Coll Cache model reset done
[2022-12-11 21:51:08. 72197: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:127] solver solved
[2022-12-11 21:51:08. 72292: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:195] 0 solved master
[2022-12-11 21:51:08. 72325: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 0 solved
[2022-12-11 21:51:08. 72354: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 0 initing device 0
[2022-12-11 21:51:08. 72863: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-11 21:51:08. 72913: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 21:51:08. 73812: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 21:51:08. 74467: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 21:51:08. 87913: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 5 solved
[2022-12-11 21:51:08. 87994: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 5 initing device 5
[2022-12-11 21:51:08. 88146: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc[:2022-12-11 21:51:08202.]  88181: 6 solvedE
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202[] 2022-12-11 21:51:084 solved.
 88228: E [/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc2022-12-11 21:51:08:.205 88246] : worker 0 thread 6 initing device 6E
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 4 initing device 4
[2022-12-11 21:51:08. 88291: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 7 solved
[2022-12-11 21:51:08. 88348: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 7 initing device 7
[2022-12-11 21:51:08. 88452: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[[2022-12-11 21:51:082022-12-11 21:51:08.. 88486 88502: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::2021980] ] eager alloc mem 381.47 MB2 solved

[2022-12-11 21:51:08. 88572[: E[2022-12-11 21:51:08 2022-12-11 21:51:08./hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc. 88559: 88566: 205: E] E worker 0 thread 2 initing device 2 /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc
/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc::[202[2022022-12-11 21:51:08] 2022-12-11 21:51:081 solved] ..
3 solved 88693 88692[
: : 2022-12-11 21:51:08EE[.  2022-12-11 21:51:08 88753/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu[/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu.: :2022-12-11 21:51:08: 88771E1815.1815:  ]  88789] E/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.ccBuilding Coll Cache with ... num gpu device is 8: Building Coll Cache with ... num gpu device is 8 :
E
/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc205 :] [/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu[205worker 0 thread 1 initing device 12022-12-11 21:51:08:2022-12-11 21:51:08] 
.1815.worker 0 thread 3 initing device 3 88900]  88897
: Building Coll Cache with ... num gpu device is 8: E
E  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::19801980] [] eager alloc mem 381.47 MB2022-12-11 21:51:08eager alloc mem 381.47 MB
.
 88980: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 21:51:08. 89090: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-11 21:51:08. 89132: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 21:51:08. 89358: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8[
2022-12-11 21:51:08. 89384: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-11 21:51:08. 89411: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] [eager alloc mem 381.47 MB2022-12-11 21:51:08
. 89431: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 21:51:08. 92955: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 21:51:08. 93263: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 21:51:08. 93317: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 21:51:08. 93378: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 21:51:08. 93431: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 21:51:08. 93943: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 21:51:08. 93998: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 21:51:08. 97358: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 21:51:08. 97532: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 21:51:08. 97584: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 21:51:08. 97633: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 21:51:08. 97691: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 21:51:08. 97758: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 21:51:08. 97807: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 21:51:08.152162: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1024.00 Bytes
[2022-12-11 21:51:08.157530: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-11 21:51:08.157668: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-11 21:51:08.158528: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 8.01 MB
[2022-12-11 21:51:08.159148: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:51:08.160154: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:51:08.162098: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 21:51:08.162838: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 21:51:08.162884: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 980.11 MB
[2022-12-11 21:51:08.[[[[[177582[2022-12-11 21:51:082022-12-11 21:51:082022-12-11 21:51:082022-12-11 21:51:082022-12-11 21:51:08: 2022-12-11 21:51:08.....E.177615177615177615177616177615 177640: : : : : /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu: EEEEE:E     1980 /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu] /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:::::eager alloc mem 1024.00 Bytes:19801980198019801980
1980] ] ] ] ] ] eager alloc mem 1024.00 Byteseager alloc mem 1024.00 Byteseager alloc mem 1024.00 Byteseager alloc mem 1024.00 Byteseager alloc mem 1024.00 Byteseager alloc mem 1024.00 Bytes





[2022-12-11 21:51:08.184396: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-11 21:51:08.184478: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-11 21:51:08.184499: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-11 21:51:08.184576: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638[] 2022-12-11 21:51:08eager release cuda mem 400000000.
184579: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-11 21:51:08.184653: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638[] 2022-12-11 21:51:08eager release cuda mem 1024.
184691: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-11 21:51:08.184743: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:[6382022-12-11 21:51:08] .eager release cuda mem 400000000184746
: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-11 21:51:08.184837: [E2022-12-11 21:51:08 ./hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc184833:: 638E]  eager release cuda mem 400000000/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc
:638] eager release cuda mem 1024
[2022-12-11 21:51:08.184927: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] [eager release cuda mem 4000000002022-12-11 21:51:08
.184930: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-11 21:51:08.185033: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-11 21:51:08.185320: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 8.01 MB
[2022-12-11 21:51:08.186523: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 8.01 MB
[2022-12-11 21:51:08.187339: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 8.01 MB
[2022-12-11 21:51:08.187917: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 8.01 MB
[2022-12-11 21:51:08.188487: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 8.01 MB
[2022-12-11 21:51:08.189001: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 8.01 MB
[2022-12-11 21:51:08.189535: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 8.01 MB
[2022-12-11 21:51:08.190131: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:51:08.190619: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:51:08.190930: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB[
2022-12-11 21:51:08.190960: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:51:08.191002: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:51:08.191046: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[[2022-12-11 21:51:082022-12-11 21:51:08..191100191096: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::6381980] ] eager release cuda mem 625663eager alloc mem 611.00 KB

[2022-12-11 21:51:08.191588: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:51:08.191909: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:51:08.191935: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:51:08.191968: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:51:08.192012: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:51:08.192179: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:51:08.197806: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 21:51:08.197921: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 21:51:08.198315: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 21:51:08.198520: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 21:51:08.198564: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 981.10 MB
[[2022-12-11 21:51:082022-12-11 21:51:08..198614198630: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::1980638] ] eager alloc mem 25.25 KBeager release cuda mem 25855

[2022-12-11 21:51:08.198731: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 981.36 MB
[2022-12-11 21:51:08.198885: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB[
2022-12-11 21:51:08.198922: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 21:51:08.198983: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 980.24 MB
[2022-12-11 21:51:08.199031: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 21:51:08.199274: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855[
2022-12-11 21:51:08.199286: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980[] 2022-12-11 21:51:08eager alloc mem 25.25 KB.
199330: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 980.96 MB
[2022-12-11 21:51:08.199629: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 21:51:08.199676: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 980.61 MB
[2022-12-11 21:51:08.199753: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 21:51:08.199799: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 980.69 MB
[2022-12-11 21:51:08.200028: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 21:51:08.200073: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 980.86 MB
[[[[[[[[2022-12-11 21:51:082022-12-11 21:51:082022-12-11 21:51:082022-12-11 21:51:082022-12-11 21:51:082022-12-11 21:51:082022-12-11 21:51:082022-12-11 21:51:08........494641494642494641494641494642494641494641494641: : : : : : : : EEEEEEEE        /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::::::::19261926192619261926192619261926] ] ] Device 5 init p2p of link 6] ] ] ] ] Device 4 init p2p of link 5Device 6 init p2p of link 0
Device 2 init p2p of link 1Device 0 init p2p of link 3Device 7 init p2p of link 4Device 1 init p2p of link 7Device 3 init p2p of link 2






[2022-12-11 21:51:08.495099: [E2022-12-11 21:51:08 [.[/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu[2022-12-11 21:51:084951112022-12-11 21:51:08:2022-12-11 21:51:08.[: .1980[.4951162022-12-11 21:51:08E495118[] 2022-12-11 21:51:08495120: . : 2022-12-11 21:51:08eager alloc mem 611.00 KB.: E495139/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE.
495156E : : 495156:  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE1980/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu: E/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu: ] :E :1980/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cueager alloc mem 611.00 KB1980 /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu1980] :
] /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:] eager alloc mem 611.00 KB1980eager alloc mem 611.00 KB:1980eager alloc mem 611.00 KB
] 
1980] 
eager alloc mem 611.00 KB] eager alloc mem 611.00 KB
eager alloc mem 611.00 KB

[2022-12-11 21:51:08.496028: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:51:08.496093: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:51:08.[4961352022-12-11 21:51:08: .E[496140 2022-12-11 21:51:08: /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc.[E[[:4961502022-12-11 21:51:08 2022-12-11 21:51:082022-12-11 21:51:08638: ./hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc..] E496161:496165496167eager release cuda mem 625663 : 638: : 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccE] EE: eager release cuda mem 625663  638/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc
/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc] :::eager release cuda mem 625663638638638
] ] ] eager release cuda mem 625663eager release cuda mem 625663eager release cuda mem 625663


[2022-12-11 21:51:08.509252: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 5 init p2p of link 4
[[2022-12-11 21:51:082022-12-11 21:51:08..509378509393: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::19261980] ] Device 4 init p2p of link 7eager alloc mem 611.00 KB

[2022-12-11 21:51:08[.2022-12-11 21:51:08509536.: 509530E:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu :/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu1980:] 1926eager alloc mem 611.00 KB] 
Device 6 init p2p of link 5
[2022-12-11 21:51:08.509707: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:51:08.509829: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 7 init p2p of link 1
[2022-12-11 21:51:08.509979: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:51:08.510102: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 2 init p2p of link 3
[2022-12-11 21:51:08.510202: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:51:08.510238: E[ 2022-12-11 21:51:08/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu.:5102391980: ] Eeager alloc mem 611.00 KB 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 0 init p2p of link 6
[2022-12-11 21:51:08.510324: E [/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu2022-12-11 21:51:08:.1926510356] : Device 1 init p2p of link 2E
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:51:08.510412: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:51:08.510497: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663[
2022-12-11 21:51:08.510517: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:51:08.510766: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:51:08.510789: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 3 init p2p of link 0
[2022-12-11 21:51:08.510953: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:51:08.511085: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:51:08.511198: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:51:08.511315: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:51:08.511748: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:51:08.527891: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 5 init p2p of link 7
[2022-12-11 21:51:08.528016: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:51:08.528285: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 4 init p2p of link 2
[2022-12-11 21:51:08.528398: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:51:08.528604: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 6 init p2p of link 4
[2022-12-11 21:51:08.528679: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 7 init p2p of link 6
[2022-12-11 21:51:08.528731: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:51:08.528798: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:51:08.528823: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:51:08.528983: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 2 init p2p of link 0
[2022-12-11 21:51:08.529097: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu[:2022-12-11 21:51:081980.] 529100eager alloc mem 611.00 KB: 
E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 0 init p2p of link 1
[2022-12-11 21:51:08.529176: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:51:08.529233: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:51:08.529464: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 3
[2022-12-11 21:51:08.529528: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:51:08.529578: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:51:08.529606: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:51:08.529635: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 3 init p2p of link 5
[2022-12-11 21:51:08.529770: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:51:08.529904: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:51:08.530028: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:51:08.530399: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:51:08.530550: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:51:08.543620: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 4 init p2p of link 6
[2022-12-11 21:51:08.543656: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 5 init p2p of link 3
[2022-12-11 21:51:08.543732: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:51:08.543771: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:51:08.544525: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:51:08.544552: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:51:08.544597: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 6 init p2p of link 7
[2022-12-11 21:51:08.544709: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:51:08.544847: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 7 init p2p of link 5
[2022-12-11 21:51:08.544971: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:51:08.545311: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 2 init p2p of link 4
[2022-12-11 21:51:08.545424: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[[2022-12-11 21:51:082022-12-11 21:51:08..545473545486: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::1926638] ] Device 0 init p2p of link 2eager release cuda mem 625663

[2022-12-11 21:51:08.545534: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 3 init p2p of link 1
[2022-12-11 21:51:08.545622: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:51:08.545674: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:51:08.545755: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:51:08.545862: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 0
[2022-12-11 21:51:08.545989: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:51:08.546196: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:51:08.546411: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:51:08.546467: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:51:08.546775: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:51:08.559628: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 8399996
[2022-12-11 21:51:08.559667: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 8399996
[2022-12-11 21:51:08.560199: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 8399996
[2022-12-11 21:51:08.560490: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 8399996
[2022-12-11 21:51:08.560686: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 8399996
[2022-12-11 21:51:08.561335: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 8399996
[2022-12-11 21:51:08.561686: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 8399996
[2022-12-11 21:51:08.561831: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 1999822 / 100000000 nodes ( 2.00 %~2.00 %) | remote 5991121 / 100000000 nodes ( 5.99 %) | cpu 92009057 / 100000000 nodes ( 92.01 %) | 981.36 MB | 0.472941 secs 
[2022-12-11 21:51:08.561964: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 1998796 / 100000000 nodes ( 2.00 %~2.00 %) | remote 5992147 / 100000000 nodes ( 5.99 %) | cpu 92009057 / 100000000 nodes ( 92.01 %) | 980.86 MB | 0.473476 secs 
[2022-12-11 21:51:08.562047: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 1998453 / 100000000 nodes ( 2.00 %~2.00 %) | remote 5992490 / 100000000 nodes ( 5.99 %) | cpu 92009057 / 100000000 nodes ( 92.01 %) | 980.69 MB | 0.473154 secs 
[2022-12-11 21:51:08.562084: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 8399996
[2022-12-11 21:51:08.562456: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 1999289 / 100000000 nodes ( 2.00 %~2.00 %) | remote 5991654 / 100000000 nodes ( 5.99 %) | cpu 92009057 / 100000000 nodes ( 92.01 %) | 981.10 MB | 0.473331 secs 
[2022-12-11 21:51:08.562542: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 1998290 / 100000000 nodes ( 2.00 %~2.00 %) | remote 5992653 / 100000000 nodes ( 5.99 %) | cpu 92009057 / 100000000 nodes ( 92.01 %) | 980.61 MB | 0.47357 secs 
[2022-12-11 21:51:08.563313: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 1997267 / 100000000 nodes ( 2.00 %~2.00 %) | remote 5993676 / 100000000 nodes ( 5.99 %) | cpu 92009057 / 100000000 nodes ( 92.01 %) | 980.11 MB | 0.490412 secs 
[2022-12-11 21:51:08.563535: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 1997526 / 100000000 nodes ( 2.00 %~2.00 %) | remote 5993417 / 100000000 nodes ( 5.99 %) | cpu 92009057 / 100000000 nodes ( 92.01 %) | 980.24 MB | 0.474112 secs 
[2022-12-11 21:51:08.564185: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 1999011 / 100000000 nodes ( 2.00 %~2.00 %) | remote 5991932 / 100000000 nodes ( 5.99 %) | cpu 92009057 / 100000000 nodes ( 92.01 %) | 980.96 MB | 0.474787 secs 
[2022-12-11 21:51:08.565342: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 7.60 GB
[2022-12-11 21:51:09.841799: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 7.86 GB
[2022-12-11 21:51:09.842585: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 7.86 GB
[2022-12-11 21:51:09.843197: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 7.86 GB
[2022-12-11 21:51:11.204197: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 8.12 GB
[2022-12-11 21:51:11.204529: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 8.12 GB
[2022-12-11 21:51:11.205676: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 8.12 GB
[2022-12-11 21:51:12.290076: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 8.34 GB
[2022-12-11 21:51:12.290428: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 8.34 GB
[2022-12-11 21:51:12.290799: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 8.34 GB
[2022-12-11 21:51:13.311184: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 8.55 GB
[2022-12-11 21:51:13.311903: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 8.55 GB
[2022-12-11 21:51:13.312851: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 8.55 GB
[2022-12-11 21:51:14.892126: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 9.01 GB
[2022-12-11 21:51:14.893422: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 9.01 GB
[2022-12-11 21:51:14.894918: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 9.01 GB
[2022-12-11 21:51:16.518933: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 9.21 GB
[2022-12-11 21:51:16.519297: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 9.21 GB
[HCTR][21:51:17.667][ERROR][RK0][tid #139654753941248]: replica 2 calling init per replica done, doing barrier
[HCTR][21:51:17.667][ERROR][RK0][tid #139654821050112]: replica 6 calling init per replica done, doing barrier
[HCTR][21:51:17.667][ERROR][RK0][tid #139654812657408]: replica 0 calling init per replica done, doing barrier
[HCTR][21:51:17.667][ERROR][RK0][main]: replica 7 calling init per replica done, doing barrier
[HCTR][21:51:17.667][ERROR][RK0][main]: replica 4 calling init per replica done, doing barrier
[HCTR][21:51:17.667][ERROR][RK0][tid #139655282419456]: replica 5 calling init per replica done, doing barrier
[HCTR][21:51:17.667][ERROR][RK0][main]: replica 1 calling init per replica done, doing barrier
[HCTR][21:51:17.667][ERROR][RK0][main]: replica 3 calling init per replica done, doing barrier
[HCTR][21:51:17.668][ERROR][RK0][tid #139654812657408]: replica 0 calling init per replica done, doing barrier done
[HCTR][21:51:17.668][ERROR][RK0][main]: replica 4 calling init per replica done, doing barrier done
[HCTR][21:51:17.668][ERROR][RK0][tid #139655282419456]: replica 5 calling init per replica done, doing barrier done
[HCTR][21:51:17.668][ERROR][RK0][main]: replica 3 calling init per replica done, doing barrier done
[HCTR][21:51:17.668][ERROR][RK0][main]: replica 1 calling init per replica done, doing barrier done
[HCTR][21:51:17.668][ERROR][RK0][tid #139654821050112]: replica 6 calling init per replica done, doing barrier done
[HCTR][21:51:17.668][ERROR][RK0][main]: replica 7 calling init per replica done, doing barrier done
[HCTR][21:51:17.668][ERROR][RK0][tid #139654753941248]: replica 2 calling init per replica done, doing barrier done
[HCTR][21:51:17.668][ERROR][RK0][main]: init per replica done
[HCTR][21:51:17.668][ERROR][RK0][tid #139655282419456]: init per replica done
[HCTR][21:51:17.668][ERROR][RK0][main]: init per replica done
[HCTR][21:51:17.668][ERROR][RK0][tid #139654821050112]: init per replica done
[HCTR][21:51:17.668][ERROR][RK0][main]: init per replica done
[HCTR][21:51:17.668][ERROR][RK0][main]: init per replica done
[HCTR][21:51:17.668][ERROR][RK0][tid #139654753941248]: init per replica done
[HCTR][21:51:17.671][ERROR][RK0][tid #139654812657408]: init per replica done
[HCTR][21:51:17.674][ERROR][RK0][tid #139655282419456]: 5 allocated 3276800 at 0x7f05c1f20000
[HCTR][21:51:17.674][ERROR][RK0][main]: 1 allocated 3276800 at 0x7f05cff20000
[HCTR][21:51:17.674][ERROR][RK0][tid #139655282419456]: 5 allocated 6553600 at 0x7f05c2400000
[HCTR][21:51:17.674][ERROR][RK0][main]: 3 allocated 3276800 at 0x7f05cff20000
[HCTR][21:51:17.674][ERROR][RK0][main]: 1 allocated 6553600 at 0x7f05d0400000
[HCTR][21:51:17.674][ERROR][RK0][tid #139655282419456]: 5 allocated 3276800 at 0x7f05c2a40000
[HCTR][21:51:17.674][ERROR][RK0][main]: 3 allocated 6553600 at 0x7f05d0400000
[HCTR][21:51:17.674][ERROR][RK0][main]: 1 allocated 3276800 at 0x7f05d0a40000
[HCTR][21:51:17.674][ERROR][RK0][tid #139655282419456]: 5 allocated 6553600 at 0x7f05c2d60000
[HCTR][21:51:17.674][ERROR][RK0][main]: 3 allocated 3276800 at 0x7f05d0a40000
[HCTR][21:51:17.674][ERROR][RK0][main]: 1 allocated 6553600 at 0x7f05d0d60000
[HCTR][21:51:17.674][ERROR][RK0][main]: 3 allocated 6553600 at 0x7f05d0d60000
[HCTR][21:51:17.674][ERROR][RK0][main]: 2 allocated 3276800 at 0x7f05c9f20000
[HCTR][21:51:17.674][ERROR][RK0][tid #139655081092864]: 7 allocated 3276800 at 0x7f05cff20000
[HCTR][21:51:17.674][ERROR][RK0][main]: 2 allocated 6553600 at 0x7f05ca400000
[HCTR][21:51:17.674][ERROR][RK0][main]: 2 allocated 3276800 at 0x7f05caa40000
[HCTR][21:51:17.674][ERROR][RK0][tid #139655081092864]: 7 allocated 6553600 at 0x7f05d0400000
[HCTR][21:51:17.674][ERROR][RK0][main]: 2 allocated 6553600 at 0x7f05cad60000
[HCTR][21:51:17.674][ERROR][RK0][tid #139654821050112]: 6 allocated 3276800 at 0x7f05cff20000
[HCTR][21:51:17.674][ERROR][RK0][tid #139655081092864]: 7 allocated 3276800 at 0x7f05d0a40000
[HCTR][21:51:17.674][ERROR][RK0][tid #139654821050112]: 6 allocated 6553600 at 0x7f05d0400000
[HCTR][21:51:17.674][ERROR][RK0][tid #139655081092864]: 7 allocated 6553600 at 0x7f05d0d60000
[HCTR][21:51:17.674][ERROR][RK0][tid #139654821050112]: 6 allocated 3276800 at 0x7f05d0a40000
[HCTR][21:51:17.674][ERROR][RK0][tid #139654821050112]: 6 allocated 6553600 at 0x7f05d0d60000
[HCTR][21:51:17.674][ERROR][RK0][tid #139654879766272]: 4 allocated 3276800 at 0x7f05c1f20000
[HCTR][21:51:17.674][ERROR][RK0][tid #139654879766272]: 4 allocated 6553600 at 0x7f05c2400000
[HCTR][21:51:17.674][ERROR][RK0][tid #139654879766272]: 4 allocated 3276800 at 0x7f05c2a40000
[HCTR][21:51:17.674][ERROR][RK0][tid #139654879766272]: 4 allocated 6553600 at 0x7f05c2d60000
[HCTR][21:51:17.678][ERROR][RK0][main]: 0 allocated 3276800 at 0x7f05d2520000
[HCTR][21:51:17.678][ERROR][RK0][main]: 0 allocated 6553600 at 0x7f05d2a00000
[HCTR][21:51:17.678][ERROR][RK0][main]: 0 allocated 3276800 at 0x7f05d370e800
[HCTR][21:51:17.678][ERROR][RK0][main]: 0 allocated 6553600 at 0x7f05d3a2e800








