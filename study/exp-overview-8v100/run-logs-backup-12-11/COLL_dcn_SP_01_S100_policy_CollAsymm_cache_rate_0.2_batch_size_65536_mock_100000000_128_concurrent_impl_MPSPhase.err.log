2022-12-12 05:58:33.305487: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.319506: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.327410: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.334904: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.338164: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.345097: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.355068: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.362200: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.418883: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.420338: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.420654: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.421618: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.422468: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.423044: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.424353: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.424483: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.426184: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.426230: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.427823: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.427874: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.429543: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.429639: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.431298: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.431589: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.432706: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.433302: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.434227: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.435419: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.436501: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.437542: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.438572: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.439678: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.441463: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.442781: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.443930: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.445004: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.446045: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.447061: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.448301: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.449945: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.450729: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.452593: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.454694: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.454769: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.455730: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 05:58:33.456805: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.456809: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.456924: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.459231: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.459331: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.459442: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.462146: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.462309: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.462350: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.462829: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.464819: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.465053: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.465116: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.465258: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.465965: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.468691: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.468845: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.469150: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.469312: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.470184: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.472373: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.472598: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.473083: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.474061: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.475372: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.475666: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.476187: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.477137: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.478566: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.479375: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.480209: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.480699: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.481604: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.482629: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.483266: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.483576: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.484576: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.485694: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.486137: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.486254: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.487489: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.488687: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.488895: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.490062: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.491004: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.491105: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.492275: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.493119: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.493345: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.494557: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.495231: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.495597: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.497123: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.497316: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.498206: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.498638: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.508666: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.510677: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.511294: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.512459: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.517696: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.523582: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.526300: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.533489: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.540938: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.554265: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.558494: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.559377: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.559848: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.559941: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.559995: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.560035: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.560070: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.563920: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.564592: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.564784: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.564876: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.564933: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.564975: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.565025: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.569406: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.571204: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.571344: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.571383: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.571470: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.571517: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.571565: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.575989: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.576009: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.576039: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.576121: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.576177: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.576221: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.576270: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.581191: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.581323: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.581396: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.581424: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.581451: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.581495: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.581595: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.587072: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.587163: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.587203: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.587243: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.587325: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.587372: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.587439: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.592464: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.592464: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.592541: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.592637: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.592720: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.592759: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.592814: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.597348: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.597455: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.597485: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.597636: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.597641: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.597677: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.597725: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.602136: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.602255: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.602539: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.602633: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.602705: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.602761: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.602819: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.607507: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.607599: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.607837: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.607891: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.607974: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.607984: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.608086: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.611979: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.612119: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.612249: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.612303: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.612392: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.612443: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.612491: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.617022: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.617196: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.617295: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.617349: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.617491: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.617538: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.621305: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.621492: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.621531: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.621585: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.621634: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.621736: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.621920: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 05:58:33.625187: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.625433: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.625469: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.625510: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.625556: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.625665: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.629675: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.629779: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.629883: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.629957: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.629975: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.630322: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.631026: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.635490: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.635534: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.635667: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.635804: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.635890: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.635923: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.636003: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.639885: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.640132: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.640354: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.640506: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.640709: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.640742: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.640882: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.644456: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.644906: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.645152: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.645251: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.645372: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.645776: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.649074: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.649736: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.650080: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.650185: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.650301: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.650480: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.654180: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.654732: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.655051: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.655156: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.655329: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.655518: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.658864: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.659415: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.660034: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.660180: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.660257: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.660354: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.663911: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.664974: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.665026: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.665057: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.665098: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.665261: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.668715: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.669525: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.669664: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.669840: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.669886: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.669916: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.677752: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 05:58:33.678236: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 05:58:33.678327: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 05:58:33.678339: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 05:58:33.678342: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 05:58:33.678353: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 05:58:33.687766: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.688132: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.688340: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.688339: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.688430: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.688465: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.696164: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.696259: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.696373: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.696396: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.696421: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.696472: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.700894: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.700990: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.701076: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.701356: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.701388: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:33.701446: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:34.841349: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:34.841977: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:34.842496: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:34.843184: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 05:58:34.843242: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-12 05:58:34.861884: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:34.862526: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:34.863463: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:34.864277: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:34.864811: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:34.865283: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-12 05:58:34.911419: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 05:58:34.911623: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 05:58:34.967864: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12340
2022-12-12 05:58:35.056075: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:35.056688: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:35.057413: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:35.058114: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 05:58:35.058169: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-12 05:58:35.076338: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:35.076992: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:35.077733: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:35.078320: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:35.079060: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:35.079537: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:1/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-12 05:58:35.141967: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:35.142822: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:35.143422: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:35.143910: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 05:58:35.143964: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-12 05:58:35.157250: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 05:58:35.157447: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 05:58:35.159115: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12341
2022-12-12 05:58:35.159813: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:35.160422: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:35.161427: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:35.161431: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:35.162885: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 05:58:35.162938: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-12 05:58:35.163015: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:35.163759: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:35.164353: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:35.164874: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:35.165337: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:2/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-12 05:58:35.165838: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:35.166401: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:35.166774: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:35.167276: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:35.167762: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:35.168311: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 05:58:35.168374: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-12 05:58:35.168593: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:35.169180: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 05:58:35.169228: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-12 05:58:35.174456: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:35.175051: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:35.175581: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:35.176175: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 05:58:35.176246: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:35.176261: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-12 05:58:35.177095: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:35.177624: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:35.178121: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 05:58:35.178169: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-12 05:58:35.180875: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:35.181494: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:35.182012: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:35.182580: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:35.183104: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:35.183587: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:4/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-12 05:58:35.186399: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:35.186717: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:35.187244: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:35.187785: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:35.188217: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:35.188813: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:35.189393: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:35.189592: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:35.190308: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:35.190493: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:35.191166: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:5/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-12 05:58:35.191315: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:6/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-12 05:58:35.195319: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:35.195945: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:35.196074: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:35.196776: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:35.197166: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:35.197812: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:35.198106: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:35.198681: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:35.199150: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:35.199490: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:7/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-12 05:58:35.199866: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:58:35.200339: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:3/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-12 05:58:35.229326: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 05:58:35.229506: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 05:58:35.231553: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12344
2022-12-12 05:58:35.236032: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 05:58:35.236216: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 05:58:35.236773: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 05:58:35.236917: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 05:58:35.238064: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12345
2022-12-12 05:58:35.238743: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12346
2022-12-12 05:58:35.245437: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 05:58:35.245597: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 05:58:35.245626: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 05:58:35.245773: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 05:58:35.247460: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12347
2022-12-12 05:58:35.247621: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12343
2022-12-12 05:58:35.259699: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 05:58:35.259883: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 05:58:35.261709: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12342
[HCTR][05:58:36.484][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][05:58:36.489][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][05:58:36.508][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][05:58:36.523][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][05:58:36.523][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][05:58:36.523][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][05:58:36.523][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][05:58:36.523][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 1it [00:01,  1.53s/it]warmup run: 1it [00:01,  1.52s/it]warmup run: 1it [00:01,  1.52s/it]warmup run: 103it [00:01, 87.79it/s]warmup run: 101it [00:01, 86.31it/s]warmup run: 1it [00:01,  1.50s/it]warmup run: 1it [00:01,  1.50s/it]warmup run: 1it [00:01,  1.51s/it]warmup run: 1it [00:01,  1.50s/it]warmup run: 98it [00:01, 84.11it/s]warmup run: 206it [00:01, 190.33it/s]warmup run: 1it [00:01,  1.48s/it]warmup run: 201it [00:01, 185.82it/s]warmup run: 98it [00:01, 84.82it/s]warmup run: 100it [00:01, 86.61it/s]warmup run: 101it [00:01, 86.95it/s]warmup run: 98it [00:01, 85.00it/s]warmup run: 192it [00:01, 177.84it/s]warmup run: 310it [00:01, 304.52it/s]warmup run: 99it [00:01, 87.02it/s]warmup run: 303it [00:01, 297.89it/s]warmup run: 198it [00:01, 185.69it/s]warmup run: 199it [00:01, 186.18it/s]warmup run: 202it [00:01, 188.14it/s]warmup run: 198it [00:01, 185.98it/s]warmup run: 287it [00:01, 282.11it/s]warmup run: 411it [00:01, 417.75it/s]warmup run: 198it [00:01, 187.91it/s]warmup run: 405it [00:01, 413.98it/s]warmup run: 299it [00:01, 297.79it/s]warmup run: 298it [00:01, 295.33it/s]warmup run: 304it [00:01, 300.53it/s]warmup run: 293it [00:01, 289.20it/s]warmup run: 382it [00:01, 390.09it/s]warmup run: 509it [00:02, 522.64it/s]warmup run: 298it [00:01, 299.52it/s]warmup run: 507it [00:02, 527.19it/s]warmup run: 400it [00:01, 413.36it/s]warmup run: 397it [00:01, 407.68it/s]warmup run: 405it [00:01, 415.30it/s]warmup run: 391it [00:01, 401.23it/s]warmup run: 476it [00:02, 493.25it/s]warmup run: 611it [00:02, 626.41it/s]warmup run: 399it [00:01, 415.77it/s]warmup run: 611it [00:02, 634.27it/s]warmup run: 501it [00:02, 525.15it/s]warmup run: 498it [00:02, 520.44it/s]warmup run: 506it [00:02, 526.37it/s]warmup run: 489it [00:02, 510.13it/s]warmup run: 571it [00:02, 588.73it/s]warmup run: 708it [00:02, 705.27it/s]warmup run: 502it [00:01, 531.59it/s]warmup run: 714it [00:02, 725.03it/s]warmup run: 604it [00:02, 631.83it/s]warmup run: 602it [00:02, 629.87it/s]warmup run: 608it [00:02, 630.07it/s]warmup run: 588it [00:02, 611.68it/s]warmup run: 666it [00:02, 672.00it/s]warmup run: 805it [00:02, 763.03it/s]warmup run: 604it [00:02, 634.59it/s]warmup run: 816it [00:02, 796.19it/s]warmup run: 708it [00:02, 726.32it/s]warmup run: 704it [00:02, 719.12it/s]warmup run: 710it [00:02, 719.88it/s]warmup run: 687it [00:02, 698.29it/s]warmup run: 766it [00:02, 753.16it/s]warmup run: 904it [00:02, 820.49it/s]warmup run: 703it [00:02, 716.97it/s]warmup run: 916it [00:02, 849.25it/s]warmup run: 810it [00:02, 799.27it/s]warmup run: 804it [00:02, 787.90it/s]warmup run: 811it [00:02, 790.81it/s]warmup run: 784it [00:02, 764.65it/s]warmup run: 863it [00:02, 809.89it/s]warmup run: 1001it [00:02, 860.72it/s]warmup run: 801it [00:02, 780.71it/s]warmup run: 1018it [00:02, 895.52it/s]warmup run: 913it [00:02, 857.71it/s]warmup run: 903it [00:02, 839.38it/s]warmup run: 911it [00:02, 840.30it/s]warmup run: 883it [00:02, 822.75it/s]warmup run: 965it [00:02, 866.37it/s]warmup run: 1100it [00:02, 894.80it/s]warmup run: 900it [00:02, 834.61it/s]warmup run: 1015it [00:02, 901.85it/s]warmup run: 1119it [00:02, 907.40it/s]warmup run: 1002it [00:02, 873.29it/s]warmup run: 1010it [00:02, 857.00it/s]warmup run: 982it [00:02, 867.74it/s]warmup run: 1068it [00:02, 911.33it/s]warmup run: 1199it [00:02, 919.17it/s]warmup run: 1003it [00:02, 885.61it/s]warmup run: 1119it [00:02, 938.34it/s]warmup run: 1219it [00:02, 932.29it/s]warmup run: 1100it [00:02, 898.31it/s]warmup run: 1080it [00:02, 894.88it/s]warmup run: 1106it [00:02, 868.23it/s]warmup run: 1171it [00:02, 945.04it/s]warmup run: 1297it [00:02, 935.47it/s]warmup run: 1105it [00:02, 921.26it/s]warmup run: 1222it [00:02, 964.43it/s]warmup run: 1319it [00:02, 950.72it/s]warmup run: 1198it [00:02, 913.78it/s]warmup run: 1181it [00:02, 927.33it/s]warmup run: 1201it [00:02, 872.85it/s]warmup run: 1273it [00:02, 966.08it/s]warmup run: 1396it [00:02, 951.02it/s]warmup run: 1205it [00:02, 941.25it/s]warmup run: 1325it [00:02, 979.54it/s]warmup run: 1419it [00:02, 964.31it/s]warmup run: 1295it [00:02, 918.22it/s]warmup run: 1281it [00:02, 945.78it/s]warmup run: 1294it [00:02, 870.12it/s]warmup run: 1375it [00:02, 979.91it/s]warmup run: 1496it [00:03, 964.34it/s]warmup run: 1305it [00:02, 954.67it/s]warmup run: 1429it [00:02, 995.86it/s]warmup run: 1519it [00:03, 973.12it/s]warmup run: 1391it [00:02, 923.74it/s]warmup run: 1382it [00:02, 963.70it/s]warmup run: 1386it [00:02, 882.67it/s]warmup run: 1476it [00:03, 982.36it/s]warmup run: 1595it [00:03, 971.28it/s]warmup run: 1407it [00:02, 972.58it/s]warmup run: 1532it [00:03, 1004.48it/s]warmup run: 1619it [00:03, 976.49it/s]warmup run: 1489it [00:03, 938.21it/s]warmup run: 1484it [00:03, 978.27it/s]warmup run: 1485it [00:03, 911.75it/s]warmup run: 1577it [00:03, 986.42it/s]warmup run: 1694it [00:03, 975.87it/s]warmup run: 1510it [00:02, 987.72it/s]warmup run: 1635it [00:03, 1006.80it/s]warmup run: 1720it [00:03, 984.74it/s]warmup run: 1589it [00:03, 955.47it/s]warmup run: 1586it [00:03, 989.84it/s]warmup run: 1584it [00:03, 932.16it/s]warmup run: 1678it [00:03, 989.67it/s]warmup run: 1793it [00:03, 978.93it/s]warmup run: 1613it [00:03, 999.22it/s]warmup run: 1738it [00:03, 1008.66it/s]warmup run: 1820it [00:03, 989.07it/s]warmup run: 1689it [00:03, 967.75it/s]warmup run: 1688it [00:03, 996.66it/s]warmup run: 1683it [00:03, 948.25it/s]warmup run: 1778it [00:03, 990.73it/s]warmup run: 1893it [00:03, 983.00it/s]warmup run: 1717it [00:03, 1008.48it/s]warmup run: 1841it [00:03, 1012.39it/s]warmup run: 1920it [00:03, 992.18it/s]warmup run: 1787it [00:03, 970.25it/s]warmup run: 1790it [00:03, 1002.51it/s]warmup run: 1782it [00:03, 959.15it/s]warmup run: 1878it [00:03, 987.53it/s]warmup run: 1992it [00:03, 984.06it/s]warmup run: 1820it [00:03, 1014.26it/s]warmup run: 1943it [00:03, 1014.39it/s]warmup run: 2024it [00:03, 1005.91it/s]warmup run: 1885it [00:03, 972.27it/s]warmup run: 1892it [00:03, 1007.52it/s]warmup run: 1881it [00:03, 966.22it/s]warmup run: 1978it [00:03, 990.15it/s]warmup run: 2111it [00:03, 1043.16it/s]warmup run: 1924it [00:03, 1020.30it/s]warmup run: 2053it [00:03, 1037.68it/s]warmup run: 2146it [00:03, 1069.79it/s]warmup run: 1983it [00:03, 963.43it/s]warmup run: 1995it [00:03, 1012.12it/s]warmup run: 1979it [00:03, 961.68it/s]warmup run: 2094it [00:03, 1038.85it/s]warmup run: 2232it [00:03, 1091.85it/s]warmup run: 2027it [00:03, 1018.20it/s]warmup run: 2174it [00:03, 1088.31it/s]warmup run: 2269it [00:03, 1115.48it/s]warmup run: 2098it [00:03, 1017.58it/s]warmup run: 2113it [00:03, 1061.86it/s]warmup run: 2093it [00:03, 1013.27it/s]warmup run: 2217it [00:03, 1095.72it/s]warmup run: 2353it [00:03, 1125.89it/s]warmup run: 2141it [00:03, 1053.09it/s]warmup run: 2295it [00:03, 1123.95it/s]warmup run: 2393it [00:03, 1150.69it/s]warmup run: 2219it [00:03, 1072.29it/s]warmup run: 2233it [00:03, 1101.59it/s]warmup run: 2213it [00:03, 1067.64it/s]warmup run: 2341it [00:03, 1136.08it/s]warmup run: 2474it [00:03, 1149.94it/s]warmup run: 2255it [00:03, 1076.61it/s]warmup run: 2416it [00:03, 1148.88it/s]warmup run: 2517it [00:03, 1175.15it/s]warmup run: 2340it [00:03, 1110.86it/s]warmup run: 2353it [00:03, 1129.97it/s]warmup run: 2333it [00:03, 1106.01it/s]warmup run: 2465it [00:03, 1164.59it/s]warmup run: 2592it [00:04, 1157.80it/s]warmup run: 2369it [00:03, 1093.79it/s]warmup run: 2537it [00:03, 1166.21it/s]warmup run: 2641it [00:04, 1192.94it/s]warmup run: 2460it [00:03, 1137.03it/s]warmup run: 2473it [00:03, 1150.38it/s]warmup run: 2453it [00:03, 1133.38it/s]warmup run: 2589it [00:04, 1184.30it/s]warmup run: 2714it [00:04, 1173.50it/s]warmup run: 2483it [00:03, 1105.95it/s]warmup run: 2658it [00:04, 1179.00it/s]warmup run: 2764it [00:04, 1201.80it/s]warmup run: 2580it [00:04, 1154.52it/s]warmup run: 2593it [00:04, 1165.02it/s]warmup run: 2574it [00:04, 1154.09it/s]warmup run: 2713it [00:04, 1198.55it/s]warmup run: 2834it [00:04, 1180.65it/s]warmup run: 2596it [00:03, 1112.61it/s]warmup run: 2778it [00:04, 1184.20it/s]warmup run: 2888it [00:04, 1211.68it/s]warmup run: 2701it [00:04, 1168.20it/s]warmup run: 2714it [00:04, 1177.28it/s]warmup run: 2695it [00:04, 1168.80it/s]warmup run: 2835it [00:04, 1203.83it/s]warmup run: 2955it [00:04, 1189.18it/s]warmup run: 2708it [00:04, 1111.33it/s]warmup run: 2899it [00:04, 1190.77it/s]warmup run: 3000it [00:04, 691.06it/s] warmup run: 3000it [00:04, 683.75it/s] warmup run: 2819it [00:04, 1170.10it/s]warmup run: 2833it [00:04, 1179.84it/s]warmup run: 2814it [00:04, 1172.44it/s]warmup run: 2958it [00:04, 1210.45it/s]warmup run: 3000it [00:04, 696.88it/s] warmup run: 2820it [00:04, 1106.82it/s]warmup run: 3000it [00:04, 685.98it/s] warmup run: 2939it [00:04, 1176.34it/s]warmup run: 2952it [00:04, 1180.61it/s]warmup run: 2933it [00:04, 1175.23it/s]warmup run: 3000it [00:04, 683.52it/s] warmup run: 3000it [00:04, 687.91it/s] warmup run: 2931it [00:04, 1106.09it/s]warmup run: 3000it [00:04, 678.07it/s] warmup run: 3000it [00:04, 687.18it/s] 


warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]

warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   5%|▌         | 162/3000 [00:00<00:01, 1617.70it/s]warmup should be done:   5%|▌         | 155/3000 [00:00<00:01, 1543.62it/s]warmup should be done:   6%|▌         | 165/3000 [00:00<00:01, 1646.41it/s]warmup should be done:   6%|▌         | 166/3000 [00:00<00:01, 1653.82it/s]warmup should be done:   5%|▌         | 164/3000 [00:00<00:01, 1636.49it/s]warmup should be done:   5%|▌         | 162/3000 [00:00<00:01, 1614.04it/s]warmup should be done:   5%|▌         | 164/3000 [00:00<00:01, 1630.40it/s]warmup should be done:   6%|▌         | 169/3000 [00:00<00:01, 1681.61it/s]warmup should be done:  11%|█         | 330/3000 [00:00<00:01, 1649.94it/s]warmup should be done:  11%|█         | 326/3000 [00:00<00:01, 1629.01it/s]warmup should be done:  11%|█         | 326/3000 [00:00<00:01, 1626.15it/s]warmup should be done:  11%|█         | 330/3000 [00:00<00:01, 1646.14it/s]warmup should be done:  11%|█▏        | 339/3000 [00:00<00:01, 1691.05it/s]warmup should be done:  11%|█         | 334/3000 [00:00<00:01, 1665.76it/s]warmup should be done:  11%|█         | 333/3000 [00:00<00:01, 1660.32it/s]warmup should be done:  10%|█         | 310/3000 [00:00<00:01, 1526.25it/s]warmup should be done:  16%|█▋        | 489/3000 [00:00<00:01, 1626.23it/s]warmup should be done:  17%|█▋        | 496/3000 [00:00<00:01, 1650.33it/s]warmup should be done:  16%|█▋        | 495/3000 [00:00<00:01, 1643.81it/s]warmup should be done:  17%|█▋        | 501/3000 [00:00<00:01, 1663.42it/s]warmup should be done:  16%|█▋        | 489/3000 [00:00<00:01, 1621.63it/s]warmup should be done:  17%|█▋        | 509/3000 [00:00<00:01, 1687.82it/s]warmup should be done:  17%|█▋        | 500/3000 [00:00<00:01, 1655.65it/s]warmup should be done:  15%|█▌        | 464/3000 [00:00<00:01, 1531.17it/s]warmup should be done:  22%|██▏       | 662/3000 [00:00<00:01, 1650.80it/s]warmup should be done:  22%|██▏       | 652/3000 [00:00<00:01, 1623.53it/s]warmup should be done:  22%|██▏       | 662/3000 [00:00<00:01, 1651.04it/s]warmup should be done:  23%|██▎       | 678/3000 [00:00<00:01, 1687.75it/s]warmup should be done:  22%|██▏       | 652/3000 [00:00<00:01, 1618.18it/s]warmup should be done:  22%|██▏       | 666/3000 [00:00<00:01, 1653.65it/s]warmup should be done:  22%|██▏       | 668/3000 [00:00<00:01, 1656.60it/s]warmup should be done:  21%|██        | 618/3000 [00:00<00:01, 1529.11it/s]warmup should be done:  28%|██▊       | 828/3000 [00:00<00:01, 1651.69it/s]warmup should be done:  28%|██▊       | 847/3000 [00:00<00:01, 1685.00it/s]warmup should be done:  27%|██▋       | 815/3000 [00:00<00:01, 1619.80it/s]warmup should be done:  28%|██▊       | 828/3000 [00:00<00:01, 1646.39it/s]warmup should be done:  27%|██▋       | 814/3000 [00:00<00:01, 1615.12it/s]warmup should be done:  28%|██▊       | 832/3000 [00:00<00:01, 1648.64it/s]warmup should be done:  28%|██▊       | 834/3000 [00:00<00:01, 1649.57it/s]warmup should be done:  26%|██▌       | 772/3000 [00:00<00:01, 1530.55it/s]warmup should be done:  34%|███▍      | 1016/3000 [00:00<00:01, 1683.14it/s]warmup should be done:  33%|███▎      | 993/3000 [00:00<00:01, 1644.90it/s]warmup should be done:  33%|███▎      | 994/3000 [00:00<00:01, 1645.15it/s]warmup should be done:  33%|███▎      | 977/3000 [00:00<00:01, 1612.68it/s]warmup should be done:  33%|███▎      | 976/3000 [00:00<00:01, 1610.96it/s]warmup should be done:  33%|███▎      | 997/3000 [00:00<00:01, 1646.21it/s]warmup should be done:  33%|███▎      | 999/3000 [00:00<00:01, 1647.42it/s]warmup should be done:  31%|███       | 926/3000 [00:00<00:01, 1528.53it/s]warmup should be done:  39%|███▊      | 1158/3000 [00:00<00:01, 1645.54it/s]warmup should be done:  40%|███▉      | 1185/3000 [00:00<00:01, 1679.30it/s]warmup should be done:  39%|███▉      | 1163/3000 [00:00<00:01, 1648.08it/s]warmup should be done:  39%|███▊      | 1159/3000 [00:00<00:01, 1638.04it/s]warmup should be done:  38%|███▊      | 1138/3000 [00:00<00:01, 1606.88it/s]warmup should be done:  38%|███▊      | 1139/3000 [00:00<00:01, 1605.96it/s]warmup should be done:  39%|███▉      | 1164/3000 [00:00<00:01, 1641.73it/s]warmup should be done:  36%|███▌      | 1079/3000 [00:00<00:01, 1522.72it/s]warmup should be done:  44%|████▍     | 1323/3000 [00:00<00:01, 1646.22it/s]warmup should be done:  45%|████▌     | 1354/3000 [00:00<00:00, 1680.39it/s]warmup should be done:  44%|████▍     | 1329/3000 [00:00<00:01, 1651.10it/s]warmup should be done:  43%|████▎     | 1299/3000 [00:00<00:01, 1607.02it/s]warmup should be done:  44%|████▍     | 1323/3000 [00:00<00:01, 1636.57it/s]warmup should be done:  44%|████▍     | 1330/3000 [00:00<00:01, 1646.09it/s]warmup should be done:  43%|████▎     | 1300/3000 [00:00<00:01, 1604.41it/s]warmup should be done:  41%|████      | 1232/3000 [00:00<00:01, 1508.36it/s]warmup should be done:  50%|████▉     | 1488/3000 [00:00<00:00, 1646.15it/s]warmup should be done:  50%|████▉     | 1495/3000 [00:00<00:00, 1653.34it/s]warmup should be done:  51%|█████     | 1523/3000 [00:00<00:00, 1679.63it/s]warmup should be done:  49%|████▊     | 1460/3000 [00:00<00:00, 1606.99it/s]warmup should be done:  50%|████▉     | 1496/3000 [00:00<00:00, 1649.64it/s]warmup should be done:  50%|████▉     | 1487/3000 [00:00<00:00, 1635.99it/s]warmup should be done:  49%|████▊     | 1461/3000 [00:00<00:00, 1602.43it/s]warmup should be done:  46%|████▌     | 1384/3000 [00:00<00:01, 1511.84it/s]warmup should be done:  55%|█████▌    | 1653/3000 [00:01<00:00, 1645.70it/s]warmup should be done:  55%|█████▌    | 1661/3000 [00:01<00:00, 1654.52it/s]warmup should be done:  56%|█████▋    | 1691/3000 [00:01<00:00, 1679.22it/s]warmup should be done:  54%|█████▍    | 1621/3000 [00:01<00:00, 1606.74it/s]warmup should be done:  55%|█████▌    | 1661/3000 [00:01<00:00, 1649.51it/s]warmup should be done:  55%|█████▌    | 1653/3000 [00:01<00:00, 1641.60it/s]warmup should be done:  54%|█████▍    | 1624/3000 [00:01<00:00, 1610.49it/s]warmup should be done:  51%|█████     | 1537/3000 [00:01<00:00, 1515.04it/s]warmup should be done:  61%|██████    | 1818/3000 [00:01<00:00, 1645.77it/s]warmup should be done:  61%|██████    | 1827/3000 [00:01<00:00, 1655.42it/s]warmup should be done:  62%|██████▏   | 1859/3000 [00:01<00:00, 1678.77it/s]warmup should be done:  59%|█████▉    | 1782/3000 [00:01<00:00, 1606.73it/s]warmup should be done:  61%|██████    | 1819/3000 [00:01<00:00, 1646.74it/s]warmup should be done:  60%|█████▉    | 1789/3000 [00:01<00:00, 1619.94it/s]warmup should be done:  61%|██████    | 1826/3000 [00:01<00:00, 1641.85it/s]warmup should be done:  56%|█████▋    | 1690/3000 [00:01<00:00, 1516.81it/s]warmup should be done:  66%|██████▌   | 1983/3000 [00:01<00:00, 1646.30it/s]warmup should be done:  68%|██████▊   | 2027/3000 [00:01<00:00, 1679.06it/s]warmup should be done:  66%|██████▋   | 1993/3000 [00:01<00:00, 1656.71it/s]warmup should be done:  65%|██████▍   | 1943/3000 [00:01<00:00, 1606.61it/s]warmup should be done:  66%|██████▌   | 1985/3000 [00:01<00:00, 1649.40it/s]warmup should be done:  65%|██████▌   | 1954/3000 [00:01<00:00, 1626.50it/s]warmup should be done:  66%|██████▋   | 1991/3000 [00:01<00:00, 1637.47it/s]warmup should be done:  61%|██████▏   | 1842/3000 [00:01<00:00, 1505.32it/s]warmup should be done:  72%|███████▏  | 2148/3000 [00:01<00:00, 1647.13it/s]warmup should be done:  73%|███████▎  | 2196/3000 [00:01<00:00, 1680.31it/s]warmup should be done:  72%|███████▏  | 2160/3000 [00:01<00:00, 1657.74it/s]warmup should be done:  70%|███████   | 2104/3000 [00:01<00:00, 1607.36it/s]warmup should be done:  72%|███████▏  | 2152/3000 [00:01<00:00, 1653.15it/s]warmup should be done:  71%|███████   | 2121/3000 [00:01<00:00, 1637.50it/s]warmup should be done:  72%|███████▏  | 2155/3000 [00:01<00:00, 1635.39it/s]warmup should be done:  66%|██████▋   | 1994/3000 [00:01<00:00, 1508.98it/s]warmup should be done:  77%|███████▋  | 2313/3000 [00:01<00:00, 1647.05it/s]warmup should be done:  78%|███████▊  | 2328/3000 [00:01<00:00, 1661.58it/s]warmup should be done:  76%|███████▌  | 2265/3000 [00:01<00:00, 1606.72it/s]warmup should be done:  79%|███████▉  | 2365/3000 [00:01<00:00, 1677.23it/s]warmup should be done:  77%|███████▋  | 2319/3000 [00:01<00:00, 1655.96it/s]warmup should be done:  76%|███████▋  | 2288/3000 [00:01<00:00, 1645.57it/s]warmup should be done:  77%|███████▋  | 2319/3000 [00:01<00:00, 1629.86it/s]warmup should be done:  72%|███████▏  | 2145/3000 [00:01<00:00, 1509.06it/s]warmup should be done:  83%|████████▎ | 2478/3000 [00:01<00:00, 1644.14it/s]warmup should be done:  83%|████████▎ | 2495/3000 [00:01<00:00, 1662.48it/s]warmup should be done:  81%|████████  | 2428/3000 [00:01<00:00, 1612.27it/s]warmup should be done:  84%|████████▍ | 2534/3000 [00:01<00:00, 1678.09it/s]warmup should be done:  83%|████████▎ | 2485/3000 [00:01<00:00, 1655.15it/s]warmup should be done:  82%|████████▏ | 2453/3000 [00:01<00:00, 1646.36it/s]warmup should be done:  83%|████████▎ | 2482/3000 [00:01<00:00, 1624.42it/s]warmup should be done:  77%|███████▋  | 2297/3000 [00:01<00:00, 1511.08it/s]warmup should be done:  88%|████████▊ | 2643/3000 [00:01<00:00, 1644.33it/s]warmup should be done:  86%|████████▋ | 2591/3000 [00:01<00:00, 1616.57it/s]warmup should be done:  89%|████████▊ | 2662/3000 [00:01<00:00, 1661.83it/s]warmup should be done:  90%|█████████ | 2702/3000 [00:01<00:00, 1677.98it/s]warmup should be done:  88%|████████▊ | 2652/3000 [00:01<00:00, 1657.92it/s]warmup should be done:  87%|████████▋ | 2618/3000 [00:01<00:00, 1641.87it/s]warmup should be done:  88%|████████▊ | 2645/3000 [00:01<00:00, 1624.47it/s]warmup should be done:  82%|████████▏ | 2449/3000 [00:01<00:00, 1510.62it/s]warmup should be done:  94%|█████████▎| 2808/3000 [00:01<00:00, 1644.93it/s]warmup should be done:  92%|█████████▏| 2754/3000 [00:01<00:00, 1619.17it/s]warmup should be done:  94%|█████████▍| 2829/3000 [00:01<00:00, 1662.92it/s]warmup should be done:  96%|█████████▌| 2871/3000 [00:01<00:00, 1680.43it/s]warmup should be done:  94%|█████████▍| 2818/3000 [00:01<00:00, 1646.43it/s]warmup should be done:  93%|█████████▎| 2783/3000 [00:01<00:00, 1636.46it/s]warmup should be done:  94%|█████████▎| 2808/3000 [00:01<00:00, 1625.45it/s]warmup should be done:  87%|████████▋ | 2602/3000 [00:01<00:00, 1514.05it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1681.36it/s]warmup should be done:  99%|█████████▉| 2974/3000 [00:01<00:00, 1649.32it/s]warmup should be done:  97%|█████████▋| 2919/3000 [00:01<00:00, 1627.41it/s]warmup should be done: 100%|█████████▉| 2998/3000 [00:01<00:00, 1669.72it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1657.93it/s]warmup should be done: 100%|█████████▉| 2987/3000 [00:01<00:00, 1657.30it/s]warmup should be done:  98%|█████████▊| 2949/3000 [00:01<00:00, 1640.53it/s]warmup should be done:  99%|█████████▉| 2973/3000 [00:01<00:00, 1630.25it/s]warmup should be done:  92%|█████████▏| 2755/3000 [00:01<00:00, 1516.74it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1648.62it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1646.50it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1639.05it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1627.04it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1614.68it/s]warmup should be done:  97%|█████████▋| 2910/3000 [00:01<00:00, 1524.40it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1518.51it/s]




warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]


warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   5%|▌         | 157/3000 [00:00<00:01, 1569.85it/s]warmup should be done:   5%|▌         | 161/3000 [00:00<00:01, 1608.42it/s]warmup should be done:   6%|▌         | 172/3000 [00:00<00:01, 1713.17it/s]warmup should be done:   6%|▌         | 167/3000 [00:00<00:01, 1663.49it/s]warmup should be done:   6%|▌         | 169/3000 [00:00<00:01, 1682.96it/s]warmup should be done:   6%|▌         | 172/3000 [00:00<00:01, 1710.52it/s]warmup should be done:   6%|▌         | 168/3000 [00:00<00:01, 1671.47it/s]warmup should be done:   6%|▌         | 172/3000 [00:00<00:01, 1710.38it/s]warmup should be done:  10%|█         | 314/3000 [00:00<00:01, 1566.90it/s]warmup should be done:  11%|█         | 328/3000 [00:00<00:01, 1642.58it/s]warmup should be done:  11%|█▏        | 338/3000 [00:00<00:01, 1684.92it/s]warmup should be done:  12%|█▏        | 345/3000 [00:00<00:01, 1720.88it/s]warmup should be done:  11%|█         | 335/3000 [00:00<00:01, 1667.64it/s]warmup should be done:  11%|█▏        | 344/3000 [00:00<00:01, 1711.60it/s]warmup should be done:  11%|█         | 336/3000 [00:00<00:01, 1670.88it/s]warmup should be done:  11%|█▏        | 344/3000 [00:00<00:01, 1707.08it/s]warmup should be done:  16%|█▋        | 495/3000 [00:00<00:01, 1654.56it/s]warmup should be done:  16%|█▌        | 472/3000 [00:00<00:01, 1570.25it/s]warmup should be done:  17%|█▋        | 518/3000 [00:00<00:01, 1724.87it/s]warmup should be done:  17%|█▋        | 508/3000 [00:00<00:01, 1687.39it/s]warmup should be done:  17%|█▋        | 503/3000 [00:00<00:01, 1669.90it/s]warmup should be done:  17%|█▋        | 516/3000 [00:00<00:01, 1711.28it/s]warmup should be done:  17%|█▋        | 504/3000 [00:00<00:01, 1667.17it/s]warmup should be done:  17%|█▋        | 515/3000 [00:00<00:01, 1695.07it/s]warmup should be done:  21%|██        | 630/3000 [00:00<00:01, 1573.28it/s]warmup should be done:  23%|██▎       | 691/3000 [00:00<00:01, 1726.03it/s]warmup should be done:  22%|██▏       | 663/3000 [00:00<00:01, 1661.96it/s]warmup should be done:  23%|██▎       | 677/3000 [00:00<00:01, 1687.82it/s]warmup should be done:  22%|██▏       | 671/3000 [00:00<00:01, 1672.06it/s]warmup should be done:  23%|██▎       | 688/3000 [00:00<00:01, 1712.62it/s]warmup should be done:  22%|██▏       | 672/3000 [00:00<00:01, 1672.03it/s]warmup should be done:  23%|██▎       | 687/3000 [00:00<00:01, 1703.82it/s]warmup should be done:  28%|██▊       | 830/3000 [00:00<00:01, 1664.22it/s]warmup should be done:  29%|██▉       | 864/3000 [00:00<00:01, 1725.43it/s]warmup should be done:  28%|██▊       | 847/3000 [00:00<00:01, 1690.49it/s]warmup should be done:  28%|██▊       | 839/3000 [00:00<00:01, 1674.51it/s]warmup should be done:  26%|██▋       | 788/3000 [00:00<00:01, 1570.09it/s]warmup should be done:  28%|██▊       | 840/3000 [00:00<00:01, 1674.07it/s]warmup should be done:  29%|██▊       | 859/3000 [00:00<00:01, 1708.27it/s]warmup should be done:  29%|██▊       | 860/3000 [00:00<00:01, 1665.82it/s]warmup should be done:  33%|███▎      | 997/3000 [00:00<00:01, 1665.05it/s]warmup should be done:  35%|███▍      | 1037/3000 [00:00<00:01, 1723.91it/s]warmup should be done:  34%|███▎      | 1007/3000 [00:00<00:01, 1672.67it/s]warmup should be done:  32%|███▏      | 946/3000 [00:00<00:01, 1569.43it/s]warmup should be done:  34%|███▍      | 1017/3000 [00:00<00:01, 1687.29it/s]warmup should be done:  34%|███▎      | 1008/3000 [00:00<00:01, 1672.59it/s]warmup should be done:  34%|███▍      | 1030/3000 [00:00<00:01, 1708.27it/s]warmup should be done:  34%|███▍      | 1032/3000 [00:00<00:01, 1681.80it/s]warmup should be done:  39%|███▉      | 1164/3000 [00:00<00:01, 1664.82it/s]warmup should be done:  40%|████      | 1210/3000 [00:00<00:01, 1724.99it/s]warmup should be done:  37%|███▋      | 1104/3000 [00:00<00:01, 1571.57it/s]warmup should be done:  39%|███▉      | 1175/3000 [00:00<00:01, 1670.50it/s]warmup should be done:  40%|███▉      | 1186/3000 [00:00<00:01, 1684.19it/s]warmup should be done:  39%|███▉      | 1176/3000 [00:00<00:01, 1671.39it/s]warmup should be done:  40%|████      | 1201/3000 [00:00<00:01, 1705.78it/s]warmup should be done:  40%|████      | 1203/3000 [00:00<00:01, 1687.81it/s]warmup should be done:  46%|████▌     | 1384/3000 [00:00<00:00, 1727.79it/s]warmup should be done:  44%|████▍     | 1331/3000 [00:00<00:01, 1660.25it/s]warmup should be done:  45%|████▌     | 1356/3000 [00:00<00:00, 1688.54it/s]warmup should be done:  45%|████▍     | 1343/3000 [00:00<00:00, 1671.62it/s]warmup should be done:  45%|████▍     | 1345/3000 [00:00<00:00, 1674.61it/s]warmup should be done:  42%|████▏     | 1262/3000 [00:00<00:01, 1565.52it/s]warmup should be done:  46%|████▌     | 1372/3000 [00:00<00:00, 1704.70it/s]warmup should be done:  46%|████▌     | 1376/3000 [00:00<00:00, 1699.40it/s]warmup should be done:  51%|█████     | 1526/3000 [00:00<00:00, 1689.90it/s]warmup should be done:  50%|████▉     | 1498/3000 [00:00<00:00, 1656.37it/s]warmup should be done:  50%|█████     | 1511/3000 [00:00<00:00, 1672.03it/s]warmup should be done:  52%|█████▏    | 1557/3000 [00:00<00:00, 1717.05it/s]warmup should be done:  50%|█████     | 1513/3000 [00:00<00:00, 1674.96it/s]warmup should be done:  47%|████▋     | 1420/3000 [00:00<00:01, 1569.84it/s]warmup should be done:  51%|█████▏    | 1543/3000 [00:00<00:00, 1704.70it/s]warmup should be done:  52%|█████▏    | 1548/3000 [00:00<00:00, 1704.20it/s]warmup should be done:  56%|█████▌    | 1679/3000 [00:01<00:00, 1673.50it/s]warmup should be done:  56%|█████▌    | 1665/3000 [00:01<00:00, 1658.50it/s]warmup should be done:  56%|█████▋    | 1695/3000 [00:01<00:00, 1687.23it/s]warmup should be done:  58%|█████▊    | 1729/3000 [00:01<00:00, 1715.70it/s]warmup should be done:  56%|█████▌    | 1682/3000 [00:01<00:00, 1676.15it/s]warmup should be done:  53%|█████▎    | 1578/3000 [00:01<00:00, 1570.17it/s]warmup should be done:  57%|█████▋    | 1714/3000 [00:01<00:00, 1701.03it/s]warmup should be done:  57%|█████▋    | 1720/3000 [00:01<00:00, 1708.85it/s]warmup should be done:  62%|██████▏   | 1847/3000 [00:01<00:00, 1674.29it/s]warmup should be done:  61%|██████    | 1832/3000 [00:01<00:00, 1660.81it/s]warmup should be done:  62%|██████▏   | 1865/3000 [00:01<00:00, 1690.08it/s]warmup should be done:  63%|██████▎   | 1902/3000 [00:01<00:00, 1719.29it/s]warmup should be done:  62%|██████▏   | 1850/3000 [00:01<00:00, 1676.19it/s]warmup should be done:  58%|█████▊    | 1736/3000 [00:01<00:00, 1568.19it/s]warmup should be done:  63%|██████▎   | 1885/3000 [00:01<00:00, 1700.09it/s]warmup should be done:  63%|██████▎   | 1893/3000 [00:01<00:00, 1713.96it/s]warmup should be done:  68%|██████▊   | 2035/3000 [00:01<00:00, 1692.79it/s]warmup should be done:  67%|██████▋   | 2015/3000 [00:01<00:00, 1674.28it/s]warmup should be done:  69%|██████▉   | 2076/3000 [00:01<00:00, 1722.80it/s]warmup should be done:  67%|██████▋   | 2018/3000 [00:01<00:00, 1676.09it/s]warmup should be done:  67%|██████▋   | 1999/3000 [00:01<00:00, 1657.72it/s]warmup should be done:  63%|██████▎   | 1894/3000 [00:01<00:00, 1570.30it/s]warmup should be done:  69%|██████▊   | 2056/3000 [00:01<00:00, 1699.90it/s]warmup should be done:  69%|██████▉   | 2066/3000 [00:01<00:00, 1716.19it/s]warmup should be done:  74%|███████▎  | 2205/3000 [00:01<00:00, 1692.02it/s]warmup should be done:  73%|███████▎  | 2183/3000 [00:01<00:00, 1672.54it/s]warmup should be done:  75%|███████▍  | 2249/3000 [00:01<00:00, 1723.65it/s]warmup should be done:  73%|███████▎  | 2186/3000 [00:01<00:00, 1673.66it/s]warmup should be done:  72%|███████▏  | 2165/3000 [00:01<00:00, 1656.19it/s]warmup should be done:  68%|██████▊   | 2052/3000 [00:01<00:00, 1571.24it/s]warmup should be done:  74%|███████▍  | 2226/3000 [00:01<00:00, 1695.83it/s]warmup should be done:  75%|███████▍  | 2238/3000 [00:01<00:00, 1715.41it/s]warmup should be done:  79%|███████▉  | 2375/3000 [00:01<00:00, 1692.73it/s]warmup should be done:  78%|███████▊  | 2351/3000 [00:01<00:00, 1672.92it/s]warmup should be done:  81%|████████  | 2422/3000 [00:01<00:00, 1723.68it/s]warmup should be done:  78%|███████▊  | 2354/3000 [00:01<00:00, 1675.40it/s]warmup should be done:  74%|███████▎  | 2210/3000 [00:01<00:00, 1567.65it/s]warmup should be done:  80%|███████▉  | 2396/3000 [00:01<00:00, 1694.53it/s]warmup should be done:  78%|███████▊  | 2331/3000 [00:01<00:00, 1640.63it/s]warmup should be done:  80%|████████  | 2410/3000 [00:01<00:00, 1713.82it/s]warmup should be done:  86%|████████▋ | 2595/3000 [00:01<00:00, 1724.58it/s]warmup should be done:  84%|████████▍ | 2519/3000 [00:01<00:00, 1670.85it/s]warmup should be done:  84%|████████▍ | 2523/3000 [00:01<00:00, 1676.88it/s]warmup should be done:  85%|████████▍ | 2545/3000 [00:01<00:00, 1688.98it/s]warmup should be done:  79%|███████▉  | 2367/3000 [00:01<00:00, 1567.26it/s]warmup should be done:  86%|████████▌ | 2566/3000 [00:01<00:00, 1694.03it/s]warmup should be done:  83%|████████▎ | 2496/3000 [00:01<00:00, 1638.31it/s]warmup should be done:  86%|████████▌ | 2582/3000 [00:01<00:00, 1714.86it/s]warmup should be done:  90%|█████████ | 2715/3000 [00:01<00:00, 1692.15it/s]warmup should be done:  90%|████████▉ | 2691/3000 [00:01<00:00, 1675.87it/s]warmup should be done:  92%|█████████▏| 2768/3000 [00:01<00:00, 1718.69it/s]warmup should be done:  90%|████████▉ | 2687/3000 [00:01<00:00, 1668.07it/s]warmup should be done:  84%|████████▍ | 2524/3000 [00:01<00:00, 1566.21it/s]warmup should be done:  91%|█████████ | 2737/3000 [00:01<00:00, 1696.76it/s]warmup should be done:  89%|████████▊ | 2662/3000 [00:01<00:00, 1642.22it/s]warmup should be done:  92%|█████████▏| 2755/3000 [00:01<00:00, 1717.09it/s]warmup should be done:  95%|█████████▌| 2859/3000 [00:01<00:00, 1674.69it/s]warmup should be done:  96%|█████████▌| 2885/3000 [00:01<00:00, 1689.83it/s]warmup should be done:  95%|█████████▌| 2854/3000 [00:01<00:00, 1667.83it/s]warmup should be done:  98%|█████████▊| 2940/3000 [00:01<00:00, 1717.25it/s]warmup should be done:  89%|████████▉ | 2681/3000 [00:01<00:00, 1565.22it/s]warmup should be done:  97%|█████████▋| 2907/3000 [00:01<00:00, 1693.32it/s]warmup should be done:  94%|█████████▍| 2828/3000 [00:01<00:00, 1645.33it/s]warmup should be done:  98%|█████████▊| 2928/3000 [00:01<00:00, 1718.57it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1720.81it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1707.90it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1699.14it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1689.23it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1674.50it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1671.24it/s]warmup should be done:  95%|█████████▍| 2838/3000 [00:01<00:00, 1564.46it/s]warmup should be done: 100%|█████████▉| 2996/3000 [00:01<00:00, 1653.12it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1653.12it/s]warmup should be done: 100%|█████████▉| 2996/3000 [00:01<00:00, 1568.15it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1568.32it/s]
WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7fad1d2b4bb0>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7fad1c7ac070>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7fad1c7a1310>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7fad1c79f280>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7fad1c7ad1c0>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7fad1c79d0a0>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7fad1c79d100>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7fad1d2b1b80>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
2022-12-12 06:00:07.076557: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fa85682ff50 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 06:00:07.076621: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 06:00:07.080392: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fa84e833b90 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 06:00:07.080447: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 06:00:07.085306: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 06:00:07.089100: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 06:00:07.208063: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fa856833bc0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 06:00:07.208121: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 06:00:07.217275: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 06:00:07.441983: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fa85682bb20 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 06:00:07.442050: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 06:00:07.450061: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 06:00:07.562593: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fa85ef92810 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 06:00:07.562660: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 06:00:07.572395: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 06:00:07.725847: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fa84e82bab0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 06:00:07.725910: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 06:00:07.734665: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 06:00:07.734743: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fa84b02d1a0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 06:00:07.734808: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 06:00:07.744358: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 06:00:07.748285: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fa856833c50 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 06:00:07.748339: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 06:00:07.757277: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 06:00:14.207897: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 06:00:14.208272: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 06:00:14.223001: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 06:00:14.283840: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 06:00:14.485984: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 06:00:14.505283: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 06:00:14.547743: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 06:00:14.612652: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
[HCTR][06:01:04.800][ERROR][RK0][main]: replica 5 reaches 1000, calling init pre replica
[HCTR][06:01:04.800][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][06:01:04.810][ERROR][RK0][main]: coll ps creation done
[HCTR][06:01:04.810][ERROR][RK0][main]: replica 5 waits for coll ps creation barrier
[HCTR][06:01:05.115][ERROR][RK0][main]: replica 3 reaches 1000, calling init pre replica
[HCTR][06:01:05.115][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][06:01:05.120][ERROR][RK0][main]: coll ps creation done
[HCTR][06:01:05.120][ERROR][RK0][main]: replica 3 waits for coll ps creation barrier
[HCTR][06:01:05.162][ERROR][RK0][tid #140361259276032]: replica 7 reaches 1000, calling init pre replica
[HCTR][06:01:05.162][ERROR][RK0][tid #140361259276032]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][06:01:05.163][ERROR][RK0][tid #140361125058304]: replica 6 reaches 1000, calling init pre replica
[HCTR][06:01:05.163][ERROR][RK0][tid #140361125058304]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][06:01:05.167][ERROR][RK0][tid #140361259276032]: coll ps creation done
[HCTR][06:01:05.167][ERROR][RK0][tid #140361259276032]: replica 7 waits for coll ps creation barrier
[HCTR][06:01:05.169][ERROR][RK0][tid #140361125058304]: coll ps creation done
[HCTR][06:01:05.169][ERROR][RK0][tid #140361125058304]: replica 6 waits for coll ps creation barrier
[HCTR][06:01:05.184][ERROR][RK0][tid #140361393493760]: replica 4 reaches 1000, calling init pre replica
[HCTR][06:01:05.184][ERROR][RK0][tid #140361393493760]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][06:01:05.189][ERROR][RK0][tid #140361393493760]: coll ps creation done
[HCTR][06:01:05.189][ERROR][RK0][tid #140361393493760]: replica 4 waits for coll ps creation barrier
[HCTR][06:01:05.248][ERROR][RK0][tid #140361536104192]: replica 1 reaches 1000, calling init pre replica
[HCTR][06:01:05.249][ERROR][RK0][tid #140361536104192]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][06:01:05.253][ERROR][RK0][tid #140361536104192]: coll ps creation done
[HCTR][06:01:05.253][ERROR][RK0][tid #140361536104192]: replica 1 waits for coll ps creation barrier
[HCTR][06:01:05.265][ERROR][RK0][main]: replica 2 reaches 1000, calling init pre replica
[HCTR][06:01:05.266][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][06:01:05.273][ERROR][RK0][main]: coll ps creation done
[HCTR][06:01:05.273][ERROR][RK0][main]: replica 2 waits for coll ps creation barrier
[HCTR][06:01:05.377][ERROR][RK0][tid #140362878281472]: replica 0 reaches 1000, calling init pre replica
[HCTR][06:01:05.377][ERROR][RK0][tid #140362878281472]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][06:01:05.384][ERROR][RK0][tid #140362878281472]: coll ps creation done
[HCTR][06:01:05.384][ERROR][RK0][tid #140362878281472]: replica 0 waits for coll ps creation barrier
[HCTR][06:01:05.384][ERROR][RK0][tid #140362878281472]: replica 0 preparing frequency
[HCTR][06:01:06.219][ERROR][RK0][tid #140362878281472]: replica 0 preparing frequency done
[HCTR][06:01:06.271][ERROR][RK0][tid #140362878281472]: replica 0 calling init per replica
[HCTR][06:01:06.271][ERROR][RK0][main]: replica 2 calling init per replica
[HCTR][06:01:06.271][ERROR][RK0][tid #140361125058304]: replica 6 calling init per replica
[HCTR][06:01:06.271][ERROR][RK0][tid #140361536104192]: replica 1 calling init per replica
[HCTR][06:01:06.271][ERROR][RK0][main]: replica 3 calling init per replica
[HCTR][06:01:06.271][ERROR][RK0][tid #140361259276032]: replica 7 calling init per replica
[HCTR][06:01:06.271][ERROR][RK0][main]: replica 5 calling init per replica
[HCTR][06:01:06.271][ERROR][RK0][tid #140361393493760]: replica 4 calling init per replica
[HCTR][06:01:06.271][ERROR][RK0][tid #140362878281472]: Calling build_v2
[HCTR][06:01:06.271][ERROR][RK0][main]: Calling build_v2
[HCTR][06:01:06.271][ERROR][RK0][tid #140361125058304]: Calling build_v2
[HCTR][06:01:06.271][ERROR][RK0][tid #140361536104192]: Calling build_v2
[HCTR][06:01:06.271][ERROR][RK0][main]: Calling build_v2
[HCTR][06:01:06.271][ERROR][RK0][tid #140361259276032]: Calling build_v2
[HCTR][06:01:06.271][ERROR][RK0][main]: Calling build_v2
[HCTR][06:01:06.271][ERROR][RK0][tid #140362878281472]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][06:01:06.271][ERROR][RK0][tid #140361393493760]: Calling build_v2
[HCTR][06:01:06.271][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][06:01:06.271][ERROR][RK0][tid #140361125058304]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][06:01:06.271][ERROR][RK0][tid #140361536104192]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][06:01:06.271][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][06:01:06.271][ERROR][RK0][tid #140361259276032]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][06:01:06.271][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][06:01:06.271][ERROR][RK0][tid #140361393493760]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[[[[[[2022-12-12 06:01:062022-12-12 06:01:06[[2022-12-12 06:01:06.2022-12-12 06:01:062022-12-12 06:01:06..2717412022-12-12 06:01:06.2022-12-12 06:01:06.2717412022-12-12 06:01:06271741: .271744.271742: .: E271760: 271760: E271764E : E: E :  /hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.ccE E /hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.ccE/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc: /hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc /hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc: :136/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc:/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc:136/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc136] :136:136] :] using concurrent impl MPSPhase136] 136] using concurrent impl MPSPhase136using concurrent impl MPSPhase
] using concurrent impl MPSPhase] using concurrent impl MPSPhase
] 
using concurrent impl MPSPhase
using concurrent impl MPSPhase
using concurrent impl MPSPhase


[2022-12-12 06:01:06.275917: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:178] v100x8, slow pcie
[2022-12-12 06:01:06.[2759642022-12-12 06:01:06: .E275964 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE: 196/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] :assigning 8 to cpu178
] v100x8, slow pcie[
2022-12-12 06:01:06.276008: [E2022-12-12 06:01:06 ./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc276025:: 178E]  [v100x8, slow pcie/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 06:01:06
[:.[2022-12-12 06:01:061962760482022-12-12 06:01:06.] : .276052assigning 8 to cpuE276079: 
[ : E2022-12-12 06:01:06/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE .: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc276097212/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:: ] :178Ebuild asymm link desc with 8X Tesla V100-SXM2-32GB out of 8196[] [ 
] 2022-12-12 06:01:06v100x8, slow pcie2022-12-12 06:01:06/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccassigning 8 to cpu.
.[:
2761422761512022-12-12 06:01:06178[: : .] 2022-12-12 06:01:06[EE276184v100x8, slow pcie.2022-12-12 06:01:06  : [[
276189./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE2022-12-12 06:01:062022-12-12 06:01:06: [276193:: ..E2022-12-12 06:01:06: 178212/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc276235276223 .E] ] :: : /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc276258 v100x8, slow pciebuild asymm link desc with 8X Tesla V100-SXM2-32GB out of 8213EE:: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc

]   196E:remote time is 8.68421[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[]  178
2022-12-12 06:01:06::2022-12-12 06:01:06assigning 8 to cpu/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] .[178212.
:v100x8, slow pcie2763982022-12-12 06:01:06] ] 276407196
: .v100x8, slow pciebuild asymm link desc with 8X Tesla V100-SXM2-32GB out of 8: ] E[276451
[
Eassigning 8 to cpu 2022-12-12 06:01:06: 2022-12-12 06:01:06 [
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.[E./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 06:01:06:2765232022-12-12 06:01:06 276529:.[196: ./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: 2132765572022-12-12 06:01:06] E276566:E] : .assigning 8 to cpu : 214 remote time is 8.68421E276627
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
 : : cpu time is 97.0588:/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[[E196/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
212:2022-12-12 06:01:062022-12-12 06:01:06 ] :] 196../hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccassigning 8 to cpu213build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8] 276736276736:
] 
assigning 8 to cpu: : 212remote time is 8.68421
EE] [
  build asymm link desc with 8X Tesla V100-SXM2-32GB out of 82022-12-12 06:01:06/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[
[.::2022-12-12 06:01:062022-12-12 06:01:06[276858212[214..2022-12-12 06:01:06: ] 2022-12-12 06:01:06] 276878276882.Ebuild asymm link desc with 8X Tesla V100-SXM2-32GB out of 8.cpu time is 97.0588: : 276900 
276907
EE: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:  [ E:E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 06:01:06/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc 213 :.:/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc212277008214:remote time is 8.68421:] : ] 212
213build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8Ecpu time is 97.0588] ] [
 
build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8remote time is 8.684212022-12-12 06:01:06/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc

.[:2771332022-12-12 06:01:06[213[: .2022-12-12 06:01:06] 2022-12-12 06:01:06E277184.remote time is 8.68421. : 277201
277205/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE: : [: EE2022-12-12 06:01:06214/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc  .] :/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc277282cpu time is 97.0588213::: 
] 214213Eremote time is 8.68421] ]  
cpu time is 97.0588remote time is 8.68421/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
[
:2022-12-12 06:01:06214[.] 2022-12-12 06:01:06277429cpu time is 97.0588.: 
277451E:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc :/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc214:] 214cpu time is 97.0588] 
cpu time is 97.0588
[2022-12-12 06:02:23.379812: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:118] solver created. now build & solve
[2022-12-12 06:02:23.419869: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 381.47 MB
[2022-12-12 06:02:23.419958: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 381.47 MB
[2022-12-12 06:02:23.421042: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:74] mapping nid to rank...
[2022-12-12 06:02:23.492954: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:91] counting slots...
[2022-12-12 06:02:23.897119: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:105] Final num slot is 49
[2022-12-12 06:02:23.897216: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:109] counting blocks...
[2022-12-12 06:02:31.858537: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:118] Final num block is 1024
[2022-12-12 06:02:31.858631: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:123] counting freq and density...
[2022-12-12 06:02:33.598034: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:154] averaging freq and density...
[2022-12-12 06:02:33.598135: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:155] 1024
[2022-12-12 06:02:33.600834: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:123] solver built. now solve
[2022-12-12 06:02:33.600894: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:354] constructing optimal solver, device=8, stream=1
1024 blocks, 8 devices
[2022-12-12 06:02:33.843009: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:527] Add Var...
[2022-12-12 06:02:33.871768: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:545] Capacity...
[2022-12-12 06:02:33.873204: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:548] Connect CPU...
[2022-12-12 06:02:33.894523: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:550] Connect Access To Storage...
[2022-12-12 06:02:34.410775: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:554] Time...
[2022-12-12 06:02:34.412995: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:516] dst 0, total sm is 80
[2022-12-12 06:02:34.415968: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:516] dst 1, total sm is 80
[2022-12-12 06:02:34.418847: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:516] dst 2, total sm is 80
[2022-12-12 06:02:34.421691: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:516] dst 3, total sm is 80
[2022-12-12 06:02:34.424560: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:516] dst 4, total sm is 80
[2022-12-12 06:02:34.427400: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:516] dst 5, total sm is 80
[2022-12-12 06:02:34.430225: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:516] dst 6, total sm is 80
[2022-12-12 06:02:34.433060: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:516] dst 7, total sm is 80
[2022-12-12 06:05:55.593828: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:569] Coll Cache init block placement array
[2022-12-12 06:05:55.602820: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:645] Coll Cache init block placement array done
[2022-12-12 06:05:55.662051: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:647] Coll Cache model reset done
[2022-12-12 06:05:55.709494: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:127] solver solved
[2022-12-12 06:05:55.709596: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:195] 0 solved master
[2022-12-12 06:05:55.709629: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 0 solved
[2022-12-12 06:05:55.709661: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 0 initing device 0
[2022-12-12 06:05:55.710355: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-12 06:05:55.710409: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 06:05:55.711324: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 06:05:55.711984: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 06:05:55.724993: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 3 solved
[2022-12-12 06:05:55.725073: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 3 initing device 3
[2022-12-12 06:05:55.725345: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 4 solved
[2022-12-12 06:05:55.725407: E [/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc2022-12-12 06:05:55:.205725409] : worker 0 thread 4 initing device 4E
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 2 solved
[[2022-12-12 06:05:552022-12-12 06:05:55..725476725460: : [EE 2022-12-12 06:05:55 [/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc./hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc2022-12-12 06:05:55:725488:.205: 202725516] E] : worker 0 thread 2 initing device 2 7 solvedE
/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc
 :/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu202:[] 18152022-12-12 06:05:55] 1 solved.Building Coll Cache with ... num gpu device is 8
725583
: E[ 2022-12-12 06:05:55/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc.:725612205: ] Eworker 0 thread 7 initing device 7[ 
2022-12-12 06:05:55/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc.:725636205: ] Eworker 0 thread 1 initing device 1 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 06:05:55.725738: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 5 solved
[2022-12-12 06:05:55.725792: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 5 initing device 5
[2022-12-12 06:05:55.725848: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[[2022-12-12 06:05:552022-12-12 06:05:55..725859725894: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::[20219802022-12-12 06:05:55] ] .6 solvedeager alloc mem 381.47 MB725959

: E [/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu2022-12-12 06:05:55:.1815726012[] : 2022-12-12 06:05:55Building Coll Cache with ... num gpu device is 8E.
 726034[/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc: 2022-12-12 06:05:55:E[.205 2022-12-12 06:05:55726069] /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu.: worker 0 thread 6 initing device 6:726089E
1815:  ] E/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuBuilding Coll Cache with ... num gpu device is 8 :
/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu1815:] 1980Building Coll Cache with ... num gpu device is 8] 
eager alloc mem 381.47 MB
[2022-12-12 06:05:55.726187: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu[:2022-12-12 06:05:551980.] [726202eager alloc mem 381.47 MB2022-12-12 06:05:55: 
.E726209 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE: 1980/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu] :eager alloc mem 381.47 MB1815
] Building Coll Cache with ... num gpu device is 8
[2022-12-12 06:05:55.726281: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 06:05:55.726564: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-12 06:05:55.726619: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 06:05:55.729918: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 06:05:55.730287: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 06:05:55.730348: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 06:05:55.730398: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 06:05:55.730895: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 06:05:55.730943: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 06:05:55.731026: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 06:05:55.734279: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 06:05:55.734522: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 06:05:55.734576: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 06:05:55.734627: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 06:05:55.734682: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 06:05:55.735173: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 06:05:55.735242: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 06:05:55.791588: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1024.00 Bytes
[2022-12-12 06:05:55.796854: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-12 06:05:55.796958: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 06:05:55.797752: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 76.68 MB
[2022-12-12 06:05:55.798380: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:05:55.799446: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:05:55.799494: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 76.13 MB
[2022-12-12 06:05:55.815024: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1024.00 Bytes
[[[[[[2022-12-12 06:05:552022-12-12 06:05:552022-12-12 06:05:552022-12-12 06:05:552022-12-12 06:05:552022-12-12 06:05:55...815178...815178815207: 815209815210815209: : E: : : EE EEE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu   /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::1980:::19801980] 198019801980] ] eager alloc mem 1024.00 Bytes] ] ] eager alloc mem 1024.00 Byteseager alloc mem 1024.00 Bytes
eager alloc mem 1024.00 Byteseager alloc mem 1024.00 Byteseager alloc mem 1024.00 Bytes




[2022-12-12 06:05:55.820521: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-12 06:05:55.820599: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 06:05:55.821376: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 76.68 MB
[2022-12-12 06:05:55.821739: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-12 06:05:55.821834: E[ 2022-12-12 06:05:55/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc.:821828638: ] Eeager release cuda mem 400000000 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-12 06:05:55.821898: [E2022-12-12 06:05:55 ./hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc821926:: 638E]  eager release cuda mem 1024/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc
:638] eager release cuda mem 400000000
[[2022-12-12 06:05:552022-12-12 06:05:55..821979821996: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::638638] ] eager release cuda mem 1024eager release cuda mem 400000000

[2022-12-12 06:05:55[.2022-12-12 06:05:55822063.: 822083E:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc :/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc638:] 638eager release cuda mem 1024] 
eager release cuda mem 400000000
[[2022-12-12 06:05:552022-12-12 06:05:55..822142822161: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::638638] ] eager release cuda mem 1024eager release cuda mem 400000000

[2022-12-12 06:05:55.822240: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 06:05:55.831843: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:05:55.832706: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 76.68 MB
[2022-12-12 06:05:55.832899: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:05:55.832946: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 76.17 MB
[2022-12-12 06:05:55.833225: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 76.68 MB
[2022-12-12 06:05:55.833736: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 76.68 MB
[2022-12-12 06:05:55.834263: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 76.68 MB
[2022-12-12 06:05:55.834787: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 76.68 MB
[2022-12-12 06:05:55.835321: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 76.68 MB
[2022-12-12 06:05:55.836958: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:05:55.837009: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:05:55.837060: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB[
2022-12-12 06:05:55.837086: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:05:55.837151: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:05:55.837189: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:05:55.838005: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:05:55.838050: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 76.23 MB
[2022-12-12 06:05:55.838074: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:05:55.838118: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 76.25 MB[
2022-12-12 06:05:55[.2022-12-12 06:05:55838135.: 838140E:  [E/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc2022-12-12 06:05:55 :./hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc638838161:] : 638eager release cuda mem 625663E] 
 eager release cuda mem 625663/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc
:638] eager release cuda mem 625663
[2022-12-12 06:05:55.838218[: 2022-12-12 06:05:55W. [838226/hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc2022-12-12 06:05:55: :.W43838235 [] : /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc2022-12-12 06:05:55WORKER[0] alloc host memory 76.25 MBW:.
 43838253/hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc] : :WORKER[0] alloc host memory 76.27 MBE43
 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccWORKER[0] alloc host memory 76.18 MB:
638] eager release cuda mem 625663
[2022-12-12 06:05:55.838339: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 76.18 MB
[2022-12-12 06:05:55.849861: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 06:05:55.850480: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 06:05:55.850524: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 9.52 GB
[2022-12-12 06:05:55.885235: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 06:05:55.885848: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 06:05:55.885889: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 9.53 GB
[2022-12-12 06:05:55.887106: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 06:05:55.887472: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 06:05:55.887745: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 06:05:55.887788: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 9.53 GB
[2022-12-12 06:05:55.888104: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 06:05:55.888147: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 9.54 GB
[2022-12-12 06:05:55.888272: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 06:05:55.888415: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 06:05:55.888461: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 06:05:55.888874: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 06:05:55.888886: E[ 2022-12-12 06:05:55/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu.:8889151980: ] Eeager alloc mem 25.25 KB 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 9.53 GB
[2022-12-12 06:05:55.889021: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 06:05:55.[8890632022-12-12 06:05:55: .E889064 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE: 1980/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc] :eager alloc mem 9.54 GB638
] eager release cuda mem 25855
[2022-12-12 06:05:55.889127: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 9.53 GB
[2022-12-12 06:05:55.889502: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 06:05:55.889545: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 9.54 GB
[[[[[[[[2022-12-12 06:05:592022-12-12 06:05:592022-12-12 06:05:592022-12-12 06:05:592022-12-12 06:05:592022-12-12 06:05:592022-12-12 06:05:592022-12-12 06:05:59........100150100151100150100150100150100151100151100150: : : : : : : : EEEEEEEE        /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::::::::19261926192619261926192619261926] ] ] Device 2 init p2p of link 1] ] ] ] ] Device 3 init p2p of link 2Device 5 init p2p of link 6
Device 1 init p2p of link 7Device 4 init p2p of link 5Device 0 init p2p of link 3Device 6 init p2p of link 0Device 7 init p2p of link 4






[[[2022-12-12 06:05:592022-12-12 06:05:592022-12-12 06:05:59[...[2022-12-12 06:05:591006771006791006812022-12-12 06:05:59.[: : : [.100688[2022-12-12 06:05:59EEE2022-12-12 06:05:59100695: 2022-12-12 06:05:59.   .: E.100704/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu100711E 100718: ::::  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu: E198019801980E/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:E ] ] ]  :1980 /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cueager alloc mem 611.00 KBeager alloc mem 611.00 KBeager alloc mem 611.00 KB/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu1980] /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:


:] eager alloc mem 611.00 KB:19801980eager alloc mem 611.00 KB
1980] ] 
] eager alloc mem 611.00 KBeager alloc mem 611.00 KBeager alloc mem 611.00 KB


[2022-12-12 06:05:59.101810: [E[2022-12-12 06:05:59 2022-12-12 06:05:59./hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc.101821:[101822: 638[2022-12-12 06:05:59: E] 2022-12-12 06:05:59.[E eager release cuda mem 625663.1018442022-12-12 06:05:59[ /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc
[101853: .2022-12-12 06:05:59/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:2022-12-12 06:05:59: E101886.:638.E : 101892638] 101899 /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccE: ] eager release cuda mem 625663: /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc: Eeager release cuda mem 625663
E:638/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc 
 638] :/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc] eager release cuda mem 625663638::eager release cuda mem 625663
] 638638
eager release cuda mem 625663] ] 
eager release cuda mem 625663eager release cuda mem 625663

[2022-12-12 06:05:59.119710: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 6 init p2p of link 5
[2022-12-12 06:05:59.119877: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:05:59.119904: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 2
[2022-12-12 06:05:59.120062: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:05:59.120276: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 5 init p2p of link 4
[2022-12-12 06:05:59.120424: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:05:59.120780: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:05:59.120981: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:05:59.121310: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu[:2022-12-12 06:05:591926.] 121338Device 2 init p2p of link 3: 
E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:05:59.121472: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:05:59.122181: E [/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu2022-12-12 06:05:59:.1926122197] : Device 3 init p2p of link 0E
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 0 init p2p of link 6
[[[2022-12-12 06:05:592022-12-12 06:05:59[2022-12-12 06:05:59..2022-12-12 06:05:59.122366122379.122384: : 122390: EE: E  [E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu2022-12-12 06:05:59 /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::./hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:19261980122440:1980] ] : 638] Device 4 init p2p of link 7eager alloc mem 611.00 KBE] eager alloc mem 611.00 KB

 eager release cuda mem 625663
/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu
:1926] Device 7 init p2p of link 1
[2022-12-12 06:05:59.122694: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:05:59.122718: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:05:59.123420: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] [eager release cuda mem 6256632022-12-12 06:05:59
.123440: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:05:59.123569: E [/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc2022-12-12 06:05:59:.638123583] : eager release cuda mem 625663E
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:05:59.138766: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 0 init p2p of link 1
[2022-12-12 06:05:59.138879[: 2022-12-12 06:05:59E. 138900/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu: :E1926 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuDevice 6 init p2p of link 4:
1980] eager alloc mem 611.00 KB
[2022-12-12 06:05:59.138997: [E2022-12-12 06:05:59[ .2022-12-12 06:05:59/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu139019.:: 1390171926E: ]  EDevice 1 init p2p of link 3/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu 
:/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu1980:] 1926eager alloc mem 611.00 KB] 
Device 7 init p2p of link 6
[2022-12-12 06:05:59.139193: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:05:59.139222: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:05:59.139711: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 5 init p2p of link 7
[2022-12-12 06:05:59.[1398352022-12-12 06:05:59: .E139840 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE: 1980/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc] :eager alloc mem 611.00 KB638
] eager release cuda mem 625663
[2022-12-12 06:05:59.139991: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:05:59.140073: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:05:59.140115[: 2022-12-12 06:05:59E. 140134/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu: :E1926 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccDevice 2 init p2p of link 0:
638] eager release cuda mem 625663
[2022-12-12 06:05:59.140276: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:05:59.140546: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 3 init p2p of link 5
[2022-12-12 06:05:59.140661: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:05:59.140768: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:05:59.141034: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 4 init p2p of link 2
[2022-12-12 06:05:59.[1411562022-12-12 06:05:59: .E141162 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccE: 638/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu] :eager release cuda mem 6256631980
] eager alloc mem 611.00 KB
[2022-12-12 06:05:59.141499: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:05:59.142014: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:05:59.163333: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 4 init p2p of link 6
[2022-12-12 06:05:59.163454: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:05:59.163964: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 3 init p2p of link 1
[2022-12-12 06:05:59.164076: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:05:59.164350: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:05:59.164965: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:05:59.165301: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 0 init p2p of link 2
[2022-12-12 06:05:59.165427: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:05:59.165492: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 0
[2022-12-12 06:05:59.165607: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:05:59.165711: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 6 init p2p of link 7
[2022-12-12 06:05:59.165832: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:05:59.166323: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:05:59.166484: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:05:59.166617: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 7 init p2p of link 5
[2022-12-12 06:05:59.166722: E[ 2022-12-12 06:05:59/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc.:166732638: ] Eeager release cuda mem 625663 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:05:59.167275: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 2 init p2p of link 4
[2022-12-12 06:05:59.167391: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:05:59.167647: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:05:59.167781: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 5 init p2p of link 3
[2022-12-12 06:05:59.167899: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:05:59.168220: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:05:59.168748: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:05:59.186977: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 80400000
[2022-12-12 06:05:59.187108: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 80400000
[2022-12-12 06:05:59.187456: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 19994515 / 100000000 nodes ( 19.99 %~20.00 %) | remote 54592894 / 100000000 nodes ( 54.59 %) | cpu 25412591 / 100000000 nodes ( 25.41 %) | 9.54 GB | 3.46157 secs 
[2022-12-12 06:05:59.187564: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 80400000
[2022-12-12 06:05:59.187710: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 19989053 / 100000000 nodes ( 19.99 %~20.00 %) | remote 54598356 / 100000000 nodes ( 54.60 %) | cpu 25412591 / 100000000 nodes ( 25.41 %) | 9.54 GB | 3.46209 secs 
[2022-12-12 06:05:59.188046: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 19989351 / 100000000 nodes ( 19.99 %~20.00 %) | remote 54598058 / 100000000 nodes ( 54.60 %) | cpu 25412591 / 100000000 nodes ( 25.41 %) | 9.54 GB | 3.46197 secs 
[2022-12-12 06:05:59.189229: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 80400000
[2022-12-12 06:05:59.190390: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 80400000
[2022-12-12 06:05:59.190827: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 80400000
[2022-12-12 06:05:59[.2022-12-12 06:05:59191275.: 191284[E: 2022-12-12 06:05:59 E./hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc 191306:/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc: 638:E] 638 eager release cuda mem 80400000] /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu
eager release cuda mem 80400000:
1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 19984368 / 100000000 nodes ( 19.98 %~20.00 %) | remote 54603041 / 100000000 nodes ( 54.60 %) | cpu 25412591 / 100000000 nodes ( 25.41 %) | 9.53 GB | 3.46504 secs 
[2022-12-12 06:05:59.192697: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 19956901 / 100000000 nodes ( 19.96 %~20.00 %) | remote 54630508 / 100000000 nodes ( 54.63 %) | cpu 25412591 / 100000000 nodes ( 25.41 %) | 9.52 GB | 3.4823 secs 
[2022-12-12 06:05:59.194508: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 19970590 / 100000000 nodes ( 19.97 %~20.00 %) | remote 54616819 / 100000000 nodes ( 54.62 %) | cpu 25412591 / 100000000 nodes ( 25.41 %) | 9.53 GB | 3.46832 secs 
[2022-12-12 06:05:59.194768: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2213] before create ctx, mem is 17.84 GB
[2022-12-12 06:05:59.195020: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 19970071 / 100000000 nodes ( 19.97 %~20.00 %) | remote 54617338 / 100000000 nodes ( 54.62 %) | cpu 25412591 / 100000000 nodes ( 25.41 %) | 9.53 GB | 3.46884 secs 
[2022-12-12 06:05:59.195746: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 19967139 / 100000000 nodes ( 19.97 %~20.00 %) | remote 54620270 / 100000000 nodes ( 54.62 %) | cpu 25412591 / 100000000 nodes ( 25.41 %) | 9.53 GB | 3.46914 secs 
[2022-12-12 06:06:00.575267: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2219] after create ctx, mem is 18.10 GB
[2022-12-12 06:06:00.575732: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2226] after create stream, mem is 18.10 GB
[2022-12-12 06:06:00.577488: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2213] before create ctx, mem is 18.10 GB
[2022-12-12 06:06:01.794044: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2219] after create ctx, mem is 18.37 GB
[2022-12-12 06:06:01.824572: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2226] after create stream, mem is 18.37 GB
[2022-12-12 06:06:01.825280: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2213] before create ctx, mem is 18.37 GB
[2022-12-12 06:06:02.935935: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2219] after create ctx, mem is 18.58 GB
[2022-12-12 06:06:02.936075: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2226] after create stream, mem is 18.58 GB
[2022-12-12 06:06:02.936427: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2213] before create ctx, mem is 18.58 GB
[2022-12-12 06:06:03.900400: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2219] after create ctx, mem is 18.80 GB
[2022-12-12 06:06:03.901245: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2226] after create stream, mem is 18.80 GB
[2022-12-12 06:06:03.902300: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2243] before create stream, mem is 18.80 GB
[2022-12-12 06:06:03.903083: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2249] after create stream, mem is 18.80 GB
[2022-12-12 06:06:03.905064: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2213] before create ctx, mem is 18.80 GB
[2022-12-12 06:06:05.283727: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2219] after create ctx, mem is 18.99 GB
[2022-12-12 06:06:05.284226: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2226] after create stream, mem is 18.99 GB
[HCTR][06:06:06.180][ERROR][RK0][main]: replica 3 calling init per replica done, doing barrier
[HCTR][06:06:06.180][ERROR][RK0][tid #140361393493760]: replica 4 calling init per replica done, doing barrier
[HCTR][06:06:06.180][ERROR][RK0][tid #140361536104192]: replica 1 calling init per replica done, doing barrier
[HCTR][06:06:06.180][ERROR][RK0][main]: replica 5 calling init per replica done, doing barrier
[HCTR][06:06:06.180][ERROR][RK0][main]: replica 2 calling init per replica done, doing barrier
[HCTR][06:06:06.180][ERROR][RK0][tid #140361125058304]: replica 6 calling init per replica done, doing barrier
[HCTR][06:06:06.180][ERROR][RK0][tid #140362878281472]: replica 0 calling init per replica done, doing barrier
[HCTR][06:06:06.180][ERROR][RK0][tid #140361259276032]: replica 7 calling init per replica done, doing barrier
[HCTR][06:06:06.180][ERROR][RK0][tid #140361125058304]: replica 6 calling init per replica done, doing barrier done
[HCTR][06:06:06.180][ERROR][RK0][main]: replica 3 calling init per replica done, doing barrier done
[HCTR][06:06:06.180][ERROR][RK0][tid #140361393493760]: replica 4 calling init per replica done, doing barrier done
[HCTR][06:06:06.180][ERROR][RK0][tid #140361259276032]: replica 7 calling init per replica done, doing barrier done
[HCTR][06:06:06.180][ERROR][RK0][tid #140361536104192]: replica 1 calling init per replica done, doing barrier done
[HCTR][06:06:06.180][ERROR][RK0][tid #140362878281472]: replica 0 calling init per replica done, doing barrier done
[HCTR][06:06:06.180][ERROR][RK0][main]: replica 5 calling init per replica done, doing barrier done
[HCTR][06:06:06.180][ERROR][RK0][main]: replica 2 calling init per replica done, doing barrier done
[HCTR][06:06:06.180][ERROR][RK0][tid #140361125058304]: init per replica done
[HCTR][06:06:06.180][ERROR][RK0][main]: init per replica done
[HCTR][06:06:06.180][ERROR][RK0][tid #140361393493760]: init per replica done
[HCTR][06:06:06.180][ERROR][RK0][tid #140361536104192]: init per replica done
[HCTR][06:06:06.180][ERROR][RK0][tid #140361259276032]: init per replica done
[HCTR][06:06:06.180][ERROR][RK0][main]: init per replica done
[HCTR][06:06:06.180][ERROR][RK0][main]: init per replica done
[HCTR][06:06:06.183][ERROR][RK0][tid #140362878281472]: init per replica done
[HCTR][06:06:06.219][ERROR][RK0][tid #140361393493760]: 4 allocated 3276800 at 0x7f8ab6238400
[HCTR][06:06:06.219][ERROR][RK0][tid #140361393493760]: 4 allocated 6553600 at 0x7f8ab6558400
[HCTR][06:06:06.219][ERROR][RK0][tid #140361393493760]: 4 allocated 3276800 at 0x7f8ab6b98400
[HCTR][06:06:06.219][ERROR][RK0][tid #140361393493760]: 4 allocated 6553600 at 0x7f8ab6eb8400
[HCTR][06:06:06.219][ERROR][RK0][tid #140361133451008]: 2 allocated 3276800 at 0x7f8984238400
[HCTR][06:06:06.219][ERROR][RK0][tid #140361133451008]: 2 allocated 6553600 at 0x7f8984558400
[HCTR][06:06:06.219][ERROR][RK0][tid #140361133451008]: 2 allocated 3276800 at 0x7f8984b98400
[HCTR][06:06:06.219][ERROR][RK0][tid #140361133451008]: 2 allocated 6553600 at 0x7f8984eb8400
[HCTR][06:06:06.219][ERROR][RK0][main]: 5 allocated 3276800 at 0x7f8a9c238400
[HCTR][06:06:06.219][ERROR][RK0][tid #140361125058304]: 6 allocated 3276800 at 0x7f8a94238400
[HCTR][06:06:06.219][ERROR][RK0][main]: 5 allocated 6553600 at 0x7f8a9c558400
[HCTR][06:06:06.219][ERROR][RK0][tid #140361125058304]: 6 allocated 6553600 at 0x7f8a94558400
[HCTR][06:06:06.219][ERROR][RK0][main]: 5 allocated 3276800 at 0x7f8a9cb98400
[HCTR][06:06:06.219][ERROR][RK0][tid #140361125058304]: 6 allocated 3276800 at 0x7f8a94b98400
[HCTR][06:06:06.219][ERROR][RK0][main]: 1 allocated 3276800 at 0x7f89b4238400
[HCTR][06:06:06.219][ERROR][RK0][main]: 5 allocated 6553600 at 0x7f8a9ceb8400
[HCTR][06:06:06.219][ERROR][RK0][tid #140361125058304]: 6 allocated 6553600 at 0x7f8a94eb8400
[HCTR][06:06:06.219][ERROR][RK0][main]: 1 allocated 6553600 at 0x7f89b4558400
[HCTR][06:06:06.219][ERROR][RK0][tid #140361259276032]: 7 allocated 3276800 at 0x7f8ab4238400
[HCTR][06:06:06.219][ERROR][RK0][main]: 1 allocated 3276800 at 0x7f89b4b98400
[HCTR][06:06:06.219][ERROR][RK0][main]: 1 allocated 6553600 at 0x7f89b4eb8400
[HCTR][06:06:06.219][ERROR][RK0][tid #140361259276032]: 7 allocated 6553600 at 0x7f8ab4558400
[HCTR][06:06:06.219][ERROR][RK0][tid #140361259276032]: 7 allocated 3276800 at 0x7f8ab4b98400
[HCTR][06:06:06.219][ERROR][RK0][tid #140361267668736]: 3 allocated 3276800 at 0x7f8a04238400
[HCTR][06:06:06.219][ERROR][RK0][tid #140361259276032]: 7 allocated 6553600 at 0x7f8ab4eb8400
[HCTR][06:06:06.219][ERROR][RK0][tid #140361267668736]: 3 allocated 6553600 at 0x7f8a04558400
[HCTR][06:06:06.220][ERROR][RK0][tid #140361267668736]: 3 allocated 3276800 at 0x7f8a04b98400
[HCTR][06:06:06.220][ERROR][RK0][tid #140361267668736]: 3 allocated 6553600 at 0x7f8a04eb8400
[HCTR][06:06:06.222][ERROR][RK0][main]: 0 allocated 3276800 at 0x7f8a2c320000
[HCTR][06:06:06.222][ERROR][RK0][main]: 0 allocated 6553600 at 0x7f8a2c640000
[HCTR][06:06:06.222][ERROR][RK0][main]: 0 allocated 3276800 at 0x7f8a2cc80000
[HCTR][06:06:06.222][ERROR][RK0][main]: 0 allocated 6553600 at 0x7f8a2cfa0000
