2022-12-12 01:39:34.650980: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.656548: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.665155: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.670151: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.674703: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.686786: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.696024: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.706083: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.758119: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.768543: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.771888: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.774158: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.774229: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.775485: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.775951: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.776993: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.777695: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.778792: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.779288: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.780395: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.781012: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.781862: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.782739: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.783348: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.784498: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.784691: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.786133: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.786269: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.787930: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.788812: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.789728: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.790604: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.792372: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.793525: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.794507: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.795468: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.796521: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.797549: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.798492: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.799518: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.804925: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 01:39:34.805880: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.807147: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.808271: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.809277: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.810723: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.812653: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.813073: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.814931: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.814940: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.815322: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.815559: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.817448: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.817545: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.818104: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.818476: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.819832: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.820712: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.821102: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.822888: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.823459: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.825561: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.826155: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.827816: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.828069: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.828765: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.830981: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.831112: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.831649: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.831835: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.834888: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.835341: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.835515: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.835650: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.838008: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.838215: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.838978: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.840823: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.840866: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.841755: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.842660: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.843264: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.843496: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.844461: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.845421: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.845720: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.846102: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.846878: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.851214: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.852234: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.852857: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.853534: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.854640: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.854819: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.855964: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.857043: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.857335: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.864303: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.878322: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.881406: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.892317: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.893598: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.894256: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.894558: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.894857: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.894901: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.897301: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.898591: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.898683: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.898846: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.898888: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.899389: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.901592: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.902670: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.903782: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.903922: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.904014: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.904569: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.905903: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.907206: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.907625: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.907666: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.908477: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.910025: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.910870: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.911285: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.911381: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.912057: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.913574: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.914614: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.914985: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.915027: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.915700: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.916895: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.918098: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.918486: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.918528: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.919260: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.921215: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.921248: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.921601: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.921718: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.922242: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.924577: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.924695: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.924859: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.924962: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.925754: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.928104: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.928306: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.928597: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.928639: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.929295: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.931455: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.931746: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.931839: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.931879: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.932428: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.934878: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.935183: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.935225: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.935321: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.935684: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.938127: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.938569: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.938649: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.938693: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.939071: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.939943: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.941712: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.942277: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.942319: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.942405: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.942836: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.944157: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.945784: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.946304: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.946366: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.946409: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.947072: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.948114: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.950799: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.950990: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.951395: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.951582: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.951928: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.952247: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.953966: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.953999: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.954843: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 01:39:34.955301: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.955405: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.956208: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.956567: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.958431: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.959241: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.959322: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.959423: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.959947: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.960312: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.962408: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.963356: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.963401: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.963655: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.963932: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.964520: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.964614: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.966711: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.967895: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.968144: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.968455: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.968790: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.969312: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.969514: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.971536: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.973119: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.973350: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.973756: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.973997: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.974363: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.974912: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.976818: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.978110: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.978487: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.978704: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.979077: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.979384: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.981478: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.983340: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.983943: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.984044: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.984336: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.984610: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.987853: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.988492: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.988898: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.990564: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 01:39:34.991225: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.991559: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.991606: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 01:39:34.991741: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.992053: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 01:39:34.994341: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.994926: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.996680: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.997268: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.998378: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 01:39:34.999010: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:34.999836: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:35.000343: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:35.001495: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:35.001796: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:35.002765: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:35.003224: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:35.004055: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:35.005313: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:35.005808: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:35.006482: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:35.007331: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:35.007966: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:35.008269: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:35.009342: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:35.010310: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:35.011081: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:35.011764: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:35.012368: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:35.019946: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:35.049687: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:35.049756: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:35.053097: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:35.054642: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:35.058076: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:35.088752: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:35.091291: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:35.095155: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:35.098870: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:35.101337: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:35.107237: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:35.109440: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 01:39:35.112084: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:35.117032: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:35.119053: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:35.123771: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:35.125003: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 01:39:35.134653: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:35.144943: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:35.146368: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:35.150946: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:36.102673: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:36.103303: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:36.103845: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:36.104312: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 01:39:36.104373: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-12 01:39:36.124064: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:36.124711: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:36.125280: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:36.125899: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:36.126650: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:36.127120: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-12 01:39:36.171595: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 01:39:36.171804: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 01:39:36.211939: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12340
2022-12-12 01:39:36.312832: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:36.313687: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:36.314645: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:36.315114: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 01:39:36.315180: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-12 01:39:36.333374: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:36.334443: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:36.334958: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:36.335566: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:36.336160: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:36.336656: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:1/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-12 01:39:36.378768: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:36.379558: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:36.380084: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:36.380556: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 01:39:36.380611: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-12 01:39:36.384320: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:36.384941: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:36.385473: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:36.385937: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 01:39:36.385990: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-12 01:39:36.387310: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:36.387953: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:36.388490: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:36.389022: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 01:39:36.389073: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-12 01:39:36.397367: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:36.398260: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:36.398817: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:36.399433: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:36.400144: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:36.400628: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:2/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-12 01:39:36.403233: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:36.403884: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:36.404618: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:36.405365: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:36.405516: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:36.406253: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:36.406458: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:36.406652: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:36.407883: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:5/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-12 01:39:36.408181: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:36.408237: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:36.409118: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:36.409155: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 01:39:36.409212: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-12 01:39:36.409873: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:36.410396: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:36.410899: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:4/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-12 01:39:36.411007: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:36.411622: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:36.412142: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:36.412614: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 01:39:36.412660: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-12 01:39:36.414036: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 01:39:36.414197: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 01:39:36.416047: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12341
2022-12-12 01:39:36.426306: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:36.426973: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:36.427511: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:36.428085: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:36.428282: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:36.428947: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:36.429446: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:36.429572: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:36.429830: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:3/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-12 01:39:36.430698: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:36.430792: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:36.431667: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 01:39:36.431720: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-12 01:39:36.431838: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:36.432640: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:36.433154: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:36.433631: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:6/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-12 01:39:36.449559: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:36.450223: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:36.450736: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:36.451356: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:36.451887: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:39:36.452363: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:7/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-12 01:39:36.452430: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 01:39:36.452613: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 01:39:36.454378: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12345
2022-12-12 01:39:36.457842: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 01:39:36.458041: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 01:39:36.460094: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12344
2022-12-12 01:39:36.474946: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 01:39:36.475163: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 01:39:36.476954: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12343
2022-12-12 01:39:36.477719: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 01:39:36.477904: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 01:39:36.479764: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12346
2022-12-12 01:39:36.483447: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 01:39:36.483604: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 01:39:36.485295: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12342
2022-12-12 01:39:36.497361: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 01:39:36.497540: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 01:39:36.499459: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12347
[HCTR][01:39:37.754][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][01:39:37.754][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][01:39:37.754][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][01:39:37.763][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][01:39:37.763][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][01:39:37.763][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][01:39:37.766][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][01:39:37.766][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 1it [00:01,  1.61s/it]warmup run: 99it [00:01, 80.28it/s]warmup run: 1it [00:01,  1.52s/it]warmup run: 199it [00:01, 175.87it/s]warmup run: 1it [00:01,  1.52s/it]warmup run: 1it [00:01,  1.52s/it]warmup run: 1it [00:01,  1.51s/it]warmup run: 94it [00:01, 80.25it/s]warmup run: 1it [00:01,  1.54s/it]warmup run: 299it [00:01, 282.35it/s]warmup run: 92it [00:01, 78.59it/s]warmup run: 95it [00:01, 81.48it/s]warmup run: 1it [00:01,  1.51s/it]warmup run: 1it [00:01,  1.48s/it]warmup run: 102it [00:01, 87.92it/s]warmup run: 187it [00:01, 172.87it/s]warmup run: 101it [00:01, 85.59it/s]warmup run: 397it [00:02, 391.11it/s]warmup run: 184it [00:01, 170.21it/s]warmup run: 197it [00:01, 184.19it/s]warmup run: 97it [00:01, 85.10it/s]warmup run: 97it [00:01, 83.61it/s]warmup run: 203it [00:01, 189.28it/s]warmup run: 281it [00:01, 275.93it/s]warmup run: 201it [00:01, 184.58it/s]warmup run: 495it [00:02, 498.06it/s]warmup run: 277it [00:01, 272.35it/s]warmup run: 300it [00:01, 298.36it/s]warmup run: 196it [00:01, 186.01it/s]warmup run: 304it [00:01, 300.57it/s]warmup run: 194it [00:01, 180.87it/s]warmup run: 375it [00:01, 382.53it/s]warmup run: 302it [00:01, 295.01it/s]warmup run: 593it [00:02, 597.43it/s]warmup run: 371it [00:01, 379.54it/s]warmup run: 403it [00:01, 416.31it/s]warmup run: 296it [00:01, 297.75it/s]warmup run: 406it [00:01, 416.96it/s]warmup run: 290it [00:01, 286.43it/s]warmup run: 466it [00:02, 480.28it/s]warmup run: 403it [00:01, 409.39it/s]warmup run: 688it [00:02, 657.45it/s]warmup run: 464it [00:02, 481.73it/s]warmup run: 506it [00:02, 530.83it/s]warmup run: 395it [00:01, 411.04it/s]warmup run: 507it [00:02, 528.19it/s]warmup run: 388it [00:01, 398.46it/s]warmup run: 553it [00:02, 539.33it/s]warmup run: 505it [00:02, 522.37it/s]warmup run: 779it [00:02, 710.67it/s]warmup run: 558it [00:02, 578.19it/s]warmup run: 605it [00:02, 626.66it/s]warmup run: 494it [00:01, 520.34it/s]warmup run: 609it [00:02, 631.44it/s]warmup run: 486it [00:02, 507.42it/s]warmup run: 648it [00:02, 630.92it/s]warmup run: 607it [00:02, 626.01it/s]warmup run: 877it [00:02, 778.70it/s]warmup run: 660it [00:02, 679.49it/s]warmup run: 703it [00:02, 708.00it/s]warmup run: 593it [00:02, 619.95it/s]warmup run: 709it [00:02, 716.50it/s]warmup run: 586it [00:02, 610.73it/s]warmup run: 744it [00:02, 709.65it/s]warmup run: 709it [00:02, 716.30it/s]warmup run: 977it [00:02, 836.99it/s]warmup run: 758it [00:02, 753.13it/s]warmup run: 802it [00:02, 777.28it/s]warmup run: 693it [00:02, 708.56it/s]warmup run: 810it [00:02, 788.86it/s]warmup run: 685it [00:02, 697.16it/s]warmup run: 840it [00:02, 772.49it/s]warmup run: 811it [00:02, 790.73it/s]warmup run: 1080it [00:02, 888.24it/s]warmup run: 857it [00:02, 813.08it/s]warmup run: 792it [00:02, 778.00it/s]warmup run: 912it [00:02, 848.90it/s]warmup run: 900it [00:02, 816.74it/s]warmup run: 783it [00:02, 766.32it/s]warmup run: 934it [00:02, 817.21it/s]warmup run: 914it [00:02, 851.68it/s]warmup run: 1182it [00:02, 925.23it/s]warmup run: 955it [00:02, 857.16it/s]warmup run: 1013it [00:02, 891.56it/s]warmup run: 890it [00:02, 826.41it/s]warmup run: 879it [00:02, 813.79it/s]warmup run: 996it [00:02, 845.53it/s]warmup run: 1026it [00:02, 843.92it/s]warmup run: 1016it [00:02, 895.73it/s]warmup run: 1283it [00:02, 948.91it/s]warmup run: 1052it [00:02, 883.66it/s]warmup run: 1116it [00:02, 930.11it/s]warmup run: 989it [00:02, 869.86it/s]warmup run: 975it [00:02, 849.98it/s]warmup run: 1091it [00:02, 849.93it/s]warmup run: 1119it [00:02, 931.74it/s]warmup run: 1118it [00:02, 854.38it/s]warmup run: 1387it [00:03, 973.50it/s]warmup run: 1148it [00:02, 903.32it/s]warmup run: 1220it [00:02, 958.93it/s]warmup run: 1094it [00:02, 919.44it/s]warmup run: 1072it [00:02, 883.20it/s]warmup run: 1184it [00:02, 868.58it/s]warmup run: 1221it [00:02, 950.01it/s]warmup run: 1215it [00:02, 884.88it/s]warmup run: 1490it [00:03, 987.78it/s]warmup run: 1244it [00:02, 915.35it/s]warmup run: 1322it [00:02, 976.56it/s]warmup run: 1199it [00:02, 953.90it/s]warmup run: 1169it [00:02, 907.66it/s]warmup run: 1276it [00:02, 880.33it/s]warmup run: 1324it [00:02, 970.95it/s]warmup run: 1312it [00:02, 907.12it/s]warmup run: 1594it [00:03, 1001.31it/s]warmup run: 1340it [00:02, 910.52it/s]warmup run: 1426it [00:02, 993.84it/s]warmup run: 1300it [00:02, 965.89it/s]warmup run: 1268it [00:02, 929.45it/s]warmup run: 1374it [00:02, 907.82it/s]warmup run: 1428it [00:02, 989.05it/s]warmup run: 1410it [00:03, 928.03it/s]warmup run: 1696it [00:03, 1005.07it/s]warmup run: 1434it [00:03, 905.63it/s]warmup run: 1530it [00:03, 1005.46it/s]warmup run: 1367it [00:02, 946.25it/s]warmup run: 1401it [00:02, 974.50it/s]warmup run: 1476it [00:03, 939.03it/s]warmup run: 1532it [00:03, 1001.53it/s]warmup run: 1508it [00:03, 940.73it/s]warmup run: 1798it [00:03, 1006.94it/s]warmup run: 1527it [00:03, 901.92it/s]warmup run: 1634it [00:03, 1013.03it/s]warmup run: 1467it [00:03, 959.70it/s]warmup run: 1502it [00:02, 983.71it/s]warmup run: 1576it [00:03, 954.75it/s]warmup run: 1635it [00:03, 1007.65it/s]warmup run: 1605it [00:03, 946.66it/s]warmup run: 1900it [00:03, 1008.98it/s]warmup run: 1619it [00:03, 901.67it/s]warmup run: 1737it [00:03, 1010.27it/s]warmup run: 1567it [00:03, 971.51it/s]warmup run: 1604it [00:03, 992.27it/s]warmup run: 1678it [00:03, 972.95it/s]warmup run: 1703it [00:03, 954.67it/s]warmup run: 1738it [00:03, 1002.31it/s]warmup run: 2003it [00:03, 1013.90it/s]warmup run: 1712it [00:03, 909.47it/s]warmup run: 1668it [00:03, 980.40it/s]warmup run: 1840it [00:03, 1010.73it/s]warmup run: 1705it [00:03, 996.29it/s]warmup run: 1779it [00:03, 983.27it/s]warmup run: 1801it [00:03, 960.28it/s]warmup run: 1840it [00:03, 1005.35it/s]warmup run: 2126it [00:03, 1076.68it/s]warmup run: 1806it [00:03, 917.41it/s]warmup run: 1942it [00:03, 1012.87it/s]warmup run: 1768it [00:03, 984.05it/s]warmup run: 1806it [00:03, 997.20it/s]warmup run: 1880it [00:03, 989.06it/s]warmup run: 1900it [00:03, 966.43it/s]warmup run: 1942it [00:03, 1004.02it/s]warmup run: 2249it [00:03, 1121.19it/s]warmup run: 1904it [00:03, 934.52it/s]warmup run: 2050it [00:03, 1032.37it/s]warmup run: 1907it [00:03, 998.13it/s]warmup run: 1868it [00:03, 980.98it/s]warmup run: 1980it [00:03, 991.85it/s]warmup run: 1997it [00:03, 965.15it/s]warmup run: 2053it [00:03, 1034.87it/s]warmup run: 2372it [00:03, 1153.15it/s]warmup run: 2003it [00:03, 948.56it/s]warmup run: 2170it [00:03, 1081.56it/s]warmup run: 2009it [00:03, 1002.94it/s]warmup run: 1968it [00:03, 984.95it/s]warmup run: 2092it [00:03, 1028.16it/s]warmup run: 2112it [00:03, 1018.21it/s]warmup run: 2176it [00:03, 1091.20it/s]warmup run: 2495it [00:04, 1175.90it/s]warmup run: 2117it [00:03, 1005.09it/s]warmup run: 2291it [00:03, 1117.35it/s]warmup run: 2132it [00:03, 1068.16it/s]warmup run: 2080it [00:03, 1023.19it/s]warmup run: 2207it [00:03, 1063.99it/s]warmup run: 2228it [00:03, 1059.57it/s]warmup run: 2299it [00:03, 1131.19it/s]warmup run: 2619it [00:04, 1192.21it/s]warmup run: 2232it [00:03, 1045.61it/s]warmup run: 2415it [00:03, 1151.28it/s]warmup run: 2254it [00:03, 1112.79it/s]warmup run: 2202it [00:03, 1081.26it/s]warmup run: 2325it [00:03, 1095.99it/s]warmup run: 2348it [00:03, 1100.47it/s]warmup run: 2422it [00:03, 1159.65it/s]warmup run: 2742it [00:04, 1203.42it/s]warmup run: 2346it [00:03, 1073.59it/s]warmup run: 2539it [00:03, 1174.97it/s]warmup run: 2376it [00:03, 1144.64it/s]warmup run: 2324it [00:03, 1121.75it/s]warmup run: 2447it [00:03, 1131.07it/s]warmup run: 2468it [00:04, 1127.58it/s]warmup run: 2545it [00:03, 1179.60it/s]warmup run: 2863it [00:04, 1205.01it/s]warmup run: 2460it [00:04, 1092.47it/s]warmup run: 2663it [00:04, 1193.20it/s]warmup run: 2498it [00:03, 1165.58it/s]warmup run: 2446it [00:03, 1149.73it/s]warmup run: 2569it [00:04, 1155.61it/s]warmup run: 2590it [00:04, 1153.27it/s]warmup run: 2669it [00:04, 1195.02it/s]warmup run: 2986it [00:04, 1211.18it/s]warmup run: 3000it [00:04, 672.67it/s] warmup run: 2574it [00:04, 1105.39it/s]warmup run: 2785it [00:04, 1200.09it/s]warmup run: 2619it [00:03, 1178.45it/s]warmup run: 2568it [00:04, 1168.60it/s]warmup run: 2692it [00:04, 1176.84it/s]warmup run: 2709it [00:04, 1161.49it/s]warmup run: 2791it [00:04, 1199.98it/s]warmup run: 2693it [00:04, 1128.25it/s]warmup run: 2908it [00:04, 1208.30it/s]warmup run: 2741it [00:04, 1188.55it/s]warmup run: 2691it [00:04, 1184.70it/s]warmup run: 2815it [00:04, 1190.72it/s]warmup run: 2830it [00:04, 1173.23it/s]warmup run: 2914it [00:04, 1207.61it/s]warmup run: 3000it [00:04, 696.61it/s] warmup run: 2810it [00:04, 1140.32it/s]warmup run: 2862it [00:04, 1192.73it/s]warmup run: 2812it [00:04, 1190.92it/s]warmup run: 2938it [00:04, 1201.04it/s]warmup run: 3000it [00:04, 691.99it/s] warmup run: 2951it [00:04, 1183.05it/s]warmup run: 3000it [00:04, 680.42it/s] warmup run: 3000it [00:04, 664.73it/s] warmup run: 2928it [00:04, 1151.21it/s]warmup run: 2984it [00:04, 1199.45it/s]warmup run: 2933it [00:04, 1195.03it/s]warmup run: 3000it [00:04, 694.92it/s] warmup run: 3000it [00:04, 683.29it/s] warmup run: 3000it [00:04, 662.57it/s] 



warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]

warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]

warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   5%|▌         | 162/3000 [00:00<00:01, 1618.25it/s]warmup should be done:   6%|▌         | 165/3000 [00:00<00:01, 1648.07it/s]warmup should be done:   5%|▌         | 164/3000 [00:00<00:01, 1639.02it/s]warmup should be done:   6%|▌         | 167/3000 [00:00<00:01, 1667.02it/s]warmup should be done:   6%|▌         | 167/3000 [00:00<00:01, 1664.05it/s]warmup should be done:   5%|▌         | 162/3000 [00:00<00:01, 1615.54it/s]warmup should be done:   6%|▌         | 166/3000 [00:00<00:01, 1652.85it/s]warmup should be done:   6%|▌         | 165/3000 [00:00<00:01, 1641.01it/s]warmup should be done:  11%|█         | 325/3000 [00:00<00:01, 1621.87it/s]warmup should be done:  11%|█         | 332/3000 [00:00<00:01, 1657.87it/s]warmup should be done:  11%|█         | 331/3000 [00:00<00:01, 1651.76it/s]warmup should be done:  11%|█         | 336/3000 [00:00<00:01, 1676.32it/s]warmup should be done:  11%|█         | 333/3000 [00:00<00:01, 1659.19it/s]warmup should be done:  11%|█         | 333/3000 [00:00<00:01, 1663.88it/s]warmup should be done:  11%|█         | 326/3000 [00:00<00:01, 1625.04it/s]warmup should be done:  11%|█         | 335/3000 [00:00<00:01, 1667.94it/s]warmup should be done:  17%|█▋        | 498/3000 [00:00<00:01, 1655.54it/s]warmup should be done:  17%|█▋        | 501/3000 [00:00<00:01, 1668.87it/s]warmup should be done:  17%|█▋        | 504/3000 [00:00<00:01, 1674.27it/s]warmup should be done:  17%|█▋        | 497/3000 [00:00<00:01, 1649.99it/s]warmup should be done:  17%|█▋        | 499/3000 [00:00<00:01, 1654.92it/s]warmup should be done:  17%|█▋        | 502/3000 [00:00<00:01, 1663.44it/s]warmup should be done:  16%|█▋        | 488/3000 [00:00<00:01, 1614.68it/s]warmup should be done:  16%|█▋        | 489/3000 [00:00<00:01, 1616.65it/s]warmup should be done:  22%|██▏       | 664/3000 [00:00<00:01, 1655.13it/s]warmup should be done:  22%|██▏       | 669/3000 [00:00<00:01, 1670.61it/s]warmup should be done:  22%|██▏       | 663/3000 [00:00<00:01, 1650.31it/s]warmup should be done:  22%|██▏       | 672/3000 [00:00<00:01, 1672.22it/s]warmup should be done:  22%|██▏       | 651/3000 [00:00<00:01, 1619.78it/s]warmup should be done:  22%|██▏       | 665/3000 [00:00<00:01, 1651.90it/s]warmup should be done:  22%|██▏       | 654/3000 [00:00<00:01, 1628.19it/s]warmup should be done:  22%|██▏       | 669/3000 [00:00<00:01, 1657.39it/s]warmup should be done:  28%|██▊       | 830/3000 [00:00<00:01, 1654.89it/s]warmup should be done:  28%|██▊       | 837/3000 [00:00<00:01, 1671.00it/s]warmup should be done:  27%|██▋       | 815/3000 [00:00<00:01, 1626.22it/s]warmup should be done:  28%|██▊       | 829/3000 [00:00<00:01, 1650.27it/s]warmup should be done:  27%|██▋       | 821/3000 [00:00<00:01, 1642.32it/s]warmup should be done:  28%|██▊       | 840/3000 [00:00<00:01, 1667.90it/s]warmup should be done:  28%|██▊       | 835/3000 [00:00<00:01, 1650.54it/s]warmup should be done:  28%|██▊       | 831/3000 [00:00<00:01, 1611.07it/s]warmup should be done:  34%|███▎      | 1005/3000 [00:00<00:01, 1670.51it/s]warmup should be done:  33%|███▎      | 978/3000 [00:00<00:01, 1624.49it/s]warmup should be done:  33%|███▎      | 996/3000 [00:00<00:01, 1647.79it/s]warmup should be done:  33%|███▎      | 988/3000 [00:00<00:01, 1648.42it/s]warmup should be done:  34%|███▎      | 1007/3000 [00:00<00:01, 1665.05it/s]warmup should be done:  33%|███▎      | 995/3000 [00:00<00:01, 1644.45it/s]warmup should be done:  33%|███▎      | 1001/3000 [00:00<00:01, 1646.47it/s]warmup should be done:  33%|███▎      | 995/3000 [00:00<00:01, 1620.47it/s]warmup should be done:  38%|███▊      | 1154/3000 [00:00<00:01, 1650.06it/s]warmup should be done:  39%|███▉      | 1173/3000 [00:00<00:01, 1664.76it/s]warmup should be done:  39%|███▊      | 1161/3000 [00:00<00:01, 1641.58it/s]warmup should be done:  38%|███▊      | 1141/3000 [00:00<00:01, 1616.05it/s]warmup should be done:  39%|███▉      | 1174/3000 [00:00<00:01, 1661.31it/s]warmup should be done:  39%|███▊      | 1160/3000 [00:00<00:01, 1638.40it/s]warmup should be done:  39%|███▉      | 1166/3000 [00:00<00:01, 1643.48it/s]warmup should be done:  39%|███▊      | 1159/3000 [00:00<00:01, 1623.74it/s]warmup should be done:  44%|████▍     | 1320/3000 [00:00<00:01, 1650.99it/s]warmup should be done:  45%|████▍     | 1340/3000 [00:00<00:00, 1665.08it/s]warmup should be done:  45%|████▍     | 1341/3000 [00:00<00:00, 1661.77it/s]warmup should be done:  44%|████▍     | 1324/3000 [00:00<00:01, 1638.14it/s]warmup should be done:  44%|████▍     | 1326/3000 [00:00<00:01, 1632.50it/s]warmup should be done:  43%|████▎     | 1303/3000 [00:00<00:01, 1606.10it/s]warmup should be done:  44%|████▍     | 1331/3000 [00:00<00:01, 1640.48it/s]warmup should be done:  44%|████▍     | 1323/3000 [00:00<00:01, 1627.75it/s]warmup should be done:  50%|█████     | 1508/3000 [00:00<00:00, 1661.21it/s]warmup should be done:  50%|████▉     | 1486/3000 [00:00<00:00, 1642.82it/s]warmup should be done:  50%|████▉     | 1490/3000 [00:00<00:00, 1633.53it/s]warmup should be done:  49%|████▉     | 1464/3000 [00:00<00:00, 1602.73it/s]warmup should be done:  50%|████▉     | 1488/3000 [00:00<00:00, 1622.47it/s]warmup should be done:  50%|████▉     | 1496/3000 [00:00<00:00, 1634.13it/s]warmup should be done:  50%|█████     | 1507/3000 [00:00<00:00, 1641.52it/s]warmup should be done:  50%|████▉     | 1487/3000 [00:00<00:00, 1631.12it/s]warmup should be done:  56%|█████▌    | 1675/3000 [00:01<00:00, 1661.05it/s]warmup should be done:  55%|█████▌    | 1651/3000 [00:01<00:00, 1641.98it/s]warmup should be done:  55%|█████▌    | 1655/3000 [00:01<00:00, 1635.79it/s]warmup should be done:  54%|█████▍    | 1625/3000 [00:01<00:00, 1599.81it/s]warmup should be done:  55%|█████▌    | 1661/3000 [00:01<00:00, 1636.09it/s]warmup should be done:  56%|█████▌    | 1672/3000 [00:01<00:00, 1641.78it/s]warmup should be done:  55%|█████▌    | 1651/3000 [00:01<00:00, 1616.60it/s]warmup should be done:  55%|█████▌    | 1651/3000 [00:01<00:00, 1632.46it/s]warmup should be done:  61%|██████▏   | 1842/3000 [00:01<00:00, 1661.29it/s]warmup should be done:  61%|██████    | 1816/3000 [00:01<00:00, 1640.62it/s]warmup should be done:  61%|██████    | 1820/3000 [00:01<00:00, 1637.60it/s]warmup should be done:  60%|█████▉    | 1785/3000 [00:01<00:00, 1597.84it/s]warmup should be done:  61%|██████    | 1826/3000 [00:01<00:00, 1639.96it/s]warmup should be done:  61%|██████    | 1837/3000 [00:01<00:00, 1644.09it/s]warmup should be done:  60%|██████    | 1813/3000 [00:01<00:00, 1616.55it/s]warmup should be done:  60%|██████    | 1815/3000 [00:01<00:00, 1634.59it/s]warmup should be done:  67%|██████▋   | 2009/3000 [00:01<00:00, 1661.24it/s]warmup should be done:  66%|██████▌   | 1981/3000 [00:01<00:00, 1640.88it/s]warmup should be done:  65%|██████▍   | 1945/3000 [00:01<00:00, 1596.30it/s]warmup should be done:  66%|██████▌   | 1984/3000 [00:01<00:00, 1630.09it/s]warmup should be done:  66%|██████▋   | 1993/3000 [00:01<00:00, 1648.52it/s]warmup should be done:  67%|██████▋   | 2004/3000 [00:01<00:00, 1650.60it/s]warmup should be done:  66%|██████▌   | 1977/3000 [00:01<00:00, 1622.76it/s]warmup should be done:  66%|██████▌   | 1980/3000 [00:01<00:00, 1636.62it/s]warmup should be done:  73%|███████▎  | 2176/3000 [00:01<00:00, 1660.95it/s]warmup should be done:  72%|███████▏  | 2160/3000 [00:01<00:00, 1654.19it/s]warmup should be done:  70%|███████   | 2105/3000 [00:01<00:00, 1594.50it/s]warmup should be done:  72%|███████▏  | 2171/3000 [00:01<00:00, 1654.19it/s]warmup should be done:  71%|███████▏  | 2142/3000 [00:01<00:00, 1627.87it/s]warmup should be done:  71%|███████▏  | 2144/3000 [00:01<00:00, 1637.11it/s]warmup should be done:  72%|███████▏  | 2146/3000 [00:01<00:00, 1616.85it/s]warmup should be done:  72%|███████▏  | 2148/3000 [00:01<00:00, 1612.10it/s]warmup should be done:  78%|███████▊  | 2343/3000 [00:01<00:00, 1657.86it/s]warmup should be done:  78%|███████▊  | 2328/3000 [00:01<00:00, 1659.34it/s]warmup should be done:  76%|███████▌  | 2265/3000 [00:01<00:00, 1593.79it/s]warmup should be done:  78%|███████▊  | 2338/3000 [00:01<00:00, 1655.97it/s]warmup should be done:  77%|███████▋  | 2308/3000 [00:01<00:00, 1637.57it/s]warmup should be done:  77%|███████▋  | 2306/3000 [00:01<00:00, 1630.80it/s]warmup should be done:  77%|███████▋  | 2312/3000 [00:01<00:00, 1617.80it/s]warmup should be done:  77%|███████▋  | 2308/3000 [00:01<00:00, 1609.54it/s]warmup should be done:  84%|████████▎ | 2510/3000 [00:01<00:00, 1658.73it/s]warmup should be done:  83%|████████▎ | 2494/3000 [00:01<00:00, 1659.09it/s]warmup should be done:  83%|████████▎ | 2504/3000 [00:01<00:00, 1654.74it/s]warmup should be done:  81%|████████  | 2425/3000 [00:01<00:00, 1588.12it/s]warmup should be done:  82%|████████▏ | 2472/3000 [00:01<00:00, 1633.94it/s]warmup should be done:  82%|████████▏ | 2470/3000 [00:01<00:00, 1627.25it/s]warmup should be done:  82%|████████▏ | 2474/3000 [00:01<00:00, 1617.61it/s]warmup should be done:  82%|████████▏ | 2470/3000 [00:01<00:00, 1611.95it/s]warmup should be done:  89%|████████▉ | 2676/3000 [00:01<00:00, 1659.00it/s]warmup should be done:  89%|████████▊ | 2661/3000 [00:01<00:00, 1661.56it/s]warmup should be done:  89%|████████▉ | 2671/3000 [00:01<00:00, 1657.47it/s]warmup should be done:  86%|████████▌ | 2585/3000 [00:01<00:00, 1589.41it/s]warmup should be done:  88%|████████▊ | 2636/3000 [00:01<00:00, 1634.36it/s]warmup should be done:  88%|████████▊ | 2634/3000 [00:01<00:00, 1630.91it/s]warmup should be done:  88%|████████▊ | 2639/3000 [00:01<00:00, 1625.59it/s]warmup should be done:  88%|████████▊ | 2632/3000 [00:01<00:00, 1602.99it/s]warmup should be done:  95%|█████████▍| 2843/3000 [00:01<00:00, 1661.41it/s]warmup should be done:  94%|█████████▍| 2829/3000 [00:01<00:00, 1664.93it/s]warmup should be done:  95%|█████████▍| 2838/3000 [00:01<00:00, 1659.88it/s]warmup should be done:  91%|█████████▏| 2744/3000 [00:01<00:00, 1589.55it/s]warmup should be done:  93%|█████████▎| 2800/3000 [00:01<00:00, 1634.99it/s]warmup should be done:  93%|█████████▎| 2798/3000 [00:01<00:00, 1632.90it/s]warmup should be done:  93%|█████████▎| 2804/3000 [00:01<00:00, 1630.43it/s]warmup should be done:  93%|█████████▎| 2793/3000 [00:01<00:00, 1603.15it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1663.05it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1657.27it/s]warmup should be done: 100%|█████████▉| 2998/3000 [00:01<00:00, 1669.65it/s]warmup should be done:  97%|█████████▋| 2905/3000 [00:01<00:00, 1595.33it/s]warmup should be done:  99%|█████████▉| 2964/3000 [00:01<00:00, 1635.70it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1654.36it/s]warmup should be done:  99%|█████████▉| 2964/3000 [00:01<00:00, 1639.88it/s]warmup should be done:  99%|█████████▉| 2971/3000 [00:01<00:00, 1639.65it/s]warmup should be done:  99%|█████████▊| 2959/3000 [00:01<00:00, 1618.38it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1635.69it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1634.07it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1634.06it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1627.28it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1602.12it/s]






warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   6%|▌         | 168/3000 [00:00<00:01, 1679.70it/s]warmup should be done:   6%|▌         | 169/3000 [00:00<00:01, 1689.29it/s]warmup should be done:   6%|▌         | 168/3000 [00:00<00:01, 1678.17it/s]warmup should be done:   6%|▌         | 165/3000 [00:00<00:01, 1643.56it/s]warmup should be done:   6%|▌         | 167/3000 [00:00<00:01, 1663.93it/s]warmup should be done:   6%|▌         | 167/3000 [00:00<00:01, 1664.46it/s]warmup should be done:   6%|▌         | 171/3000 [00:00<00:01, 1701.37it/s]warmup should be done:   6%|▌         | 172/3000 [00:00<00:01, 1710.06it/s]warmup should be done:  11%|█         | 337/3000 [00:00<00:01, 1680.31it/s]warmup should be done:  11%|█▏        | 338/3000 [00:00<00:01, 1683.00it/s]warmup should be done:  11%|█         | 337/3000 [00:00<00:01, 1681.25it/s]warmup should be done:  11%|█         | 330/3000 [00:00<00:01, 1642.66it/s]warmup should be done:  11%|█▏        | 342/3000 [00:00<00:01, 1702.99it/s]warmup should be done:  11%|█         | 334/3000 [00:00<00:01, 1662.21it/s]warmup should be done:  11%|█         | 336/3000 [00:00<00:01, 1670.07it/s]warmup should be done:  11%|█▏        | 344/3000 [00:00<00:01, 1711.01it/s]warmup should be done:  17%|█▋        | 507/3000 [00:00<00:01, 1685.54it/s]warmup should be done:  17%|█▋        | 507/3000 [00:00<00:01, 1686.34it/s]warmup should be done:  17%|█▋        | 507/3000 [00:00<00:01, 1688.25it/s]warmup should be done:  17%|█▋        | 513/3000 [00:00<00:01, 1705.39it/s]warmup should be done:  16%|█▋        | 495/3000 [00:00<00:01, 1643.85it/s]warmup should be done:  17%|█▋        | 516/3000 [00:00<00:01, 1712.83it/s]warmup should be done:  17%|█▋        | 501/3000 [00:00<00:01, 1655.34it/s]warmup should be done:  17%|█▋        | 504/3000 [00:00<00:01, 1664.36it/s]warmup should be done:  23%|██▎       | 676/3000 [00:00<00:01, 1688.37it/s]warmup should be done:  22%|██▏       | 661/3000 [00:00<00:01, 1648.51it/s]warmup should be done:  23%|██▎       | 678/3000 [00:00<00:01, 1692.67it/s]warmup should be done:  23%|██▎       | 677/3000 [00:00<00:01, 1687.44it/s]warmup should be done:  23%|██▎       | 685/3000 [00:00<00:01, 1707.42it/s]warmup should be done:  23%|██▎       | 689/3000 [00:00<00:01, 1716.38it/s]warmup should be done:  22%|██▏       | 667/3000 [00:00<00:01, 1656.09it/s]warmup should be done:  22%|██▏       | 671/3000 [00:00<00:01, 1662.07it/s]warmup should be done:  28%|██▊       | 846/3000 [00:00<00:01, 1690.14it/s]warmup should be done:  28%|██▊       | 827/3000 [00:00<00:01, 1650.62it/s]warmup should be done:  28%|██▊       | 847/3000 [00:00<00:01, 1688.92it/s]warmup should be done:  29%|██▊       | 857/3000 [00:00<00:01, 1709.37it/s]warmup should be done:  28%|██▊       | 849/3000 [00:00<00:01, 1695.39it/s]warmup should be done:  29%|██▊       | 862/3000 [00:00<00:01, 1718.12it/s]warmup should be done:  28%|██▊       | 834/3000 [00:00<00:01, 1657.40it/s]warmup should be done:  28%|██▊       | 838/3000 [00:00<00:01, 1661.13it/s]warmup should be done:  34%|███▍      | 1016/3000 [00:00<00:01, 1691.64it/s]warmup should be done:  34%|███▍      | 1019/3000 [00:00<00:01, 1695.59it/s]warmup should be done:  34%|███▍      | 1016/3000 [00:00<00:01, 1687.39it/s]warmup should be done:  34%|███▍      | 1028/3000 [00:00<00:01, 1707.23it/s]warmup should be done:  33%|███▎      | 993/3000 [00:00<00:01, 1647.06it/s]warmup should be done:  34%|███▍      | 1034/3000 [00:00<00:01, 1715.21it/s]warmup should be done:  33%|███▎      | 1000/3000 [00:00<00:01, 1654.72it/s]warmup should be done:  34%|███▎      | 1005/3000 [00:00<00:01, 1642.10it/s]warmup should be done:  40%|███▉      | 1199/3000 [00:00<00:01, 1705.02it/s]warmup should be done:  40%|███▉      | 1189/3000 [00:00<00:01, 1692.33it/s]warmup should be done:  39%|███▊      | 1158/3000 [00:00<00:01, 1645.91it/s]warmup should be done:  40%|███▉      | 1186/3000 [00:00<00:01, 1685.31it/s]warmup should be done:  40%|████      | 1206/3000 [00:00<00:01, 1713.05it/s]warmup should be done:  40%|███▉      | 1185/3000 [00:00<00:01, 1679.02it/s]warmup should be done:  39%|███▉      | 1166/3000 [00:00<00:01, 1652.42it/s]warmup should be done:  39%|███▉      | 1170/3000 [00:00<00:01, 1625.52it/s]warmup should be done:  46%|████▌     | 1371/3000 [00:00<00:00, 1708.50it/s]warmup should be done:  45%|████▌     | 1359/3000 [00:00<00:00, 1694.14it/s]warmup should be done:  45%|████▌     | 1356/3000 [00:00<00:00, 1687.35it/s]warmup should be done:  46%|████▌     | 1379/3000 [00:00<00:00, 1716.69it/s]warmup should be done:  45%|████▌     | 1355/3000 [00:00<00:00, 1683.54it/s]warmup should be done:  44%|████▍     | 1323/3000 [00:00<00:01, 1641.90it/s]warmup should be done:  44%|████▍     | 1332/3000 [00:00<00:01, 1650.51it/s]warmup should be done:  44%|████▍     | 1333/3000 [00:00<00:01, 1614.54it/s]warmup should be done:  51%|█████     | 1529/3000 [00:00<00:00, 1693.71it/s]warmup should be done:  51%|█████▏    | 1543/3000 [00:00<00:00, 1709.39it/s]warmup should be done:  51%|█████     | 1525/3000 [00:00<00:00, 1686.51it/s]warmup should be done:  52%|█████▏    | 1552/3000 [00:00<00:00, 1717.85it/s]warmup should be done:  51%|█████     | 1525/3000 [00:00<00:00, 1685.81it/s]warmup should be done:  50%|████▉     | 1488/3000 [00:00<00:00, 1641.31it/s]warmup should be done:  50%|████▉     | 1498/3000 [00:00<00:00, 1651.03it/s]warmup should be done:  50%|████▉     | 1498/3000 [00:00<00:00, 1624.09it/s]warmup should be done:  57%|█████▋    | 1699/3000 [00:01<00:00, 1694.90it/s]warmup should be done:  57%|█████▋    | 1715/3000 [00:01<00:00, 1709.82it/s]warmup should be done:  57%|█████▋    | 1724/3000 [00:01<00:00, 1718.43it/s]warmup should be done:  56%|█████▋    | 1695/3000 [00:01<00:00, 1687.70it/s]warmup should be done:  56%|█████▋    | 1695/3000 [00:01<00:00, 1687.91it/s]warmup should be done:  55%|█████▌    | 1653/3000 [00:01<00:00, 1641.69it/s]warmup should be done:  56%|█████▌    | 1665/3000 [00:01<00:00, 1653.75it/s]warmup should be done:  55%|█████▌    | 1664/3000 [00:01<00:00, 1632.68it/s]warmup should be done:  62%|██████▏   | 1869/3000 [00:01<00:00, 1696.34it/s]warmup should be done:  63%|██████▎   | 1887/3000 [00:01<00:00, 1710.20it/s]warmup should be done:  63%|██████▎   | 1897/3000 [00:01<00:00, 1719.31it/s]warmup should be done:  62%|██████▏   | 1865/3000 [00:01<00:00, 1688.74it/s]warmup should be done:  62%|██████▏   | 1865/3000 [00:01<00:00, 1690.23it/s]warmup should be done:  61%|██████    | 1818/3000 [00:01<00:00, 1642.18it/s]warmup should be done:  61%|██████    | 1831/3000 [00:01<00:00, 1655.42it/s]warmup should be done:  61%|██████    | 1830/3000 [00:01<00:00, 1638.56it/s]warmup should be done:  68%|██████▊   | 2039/3000 [00:01<00:00, 1691.83it/s]warmup should be done:  69%|██████▊   | 2059/3000 [00:01<00:00, 1711.24it/s]warmup should be done:  69%|██████▉   | 2070/3000 [00:01<00:00, 1719.52it/s]warmup should be done:  68%|██████▊   | 2034/3000 [00:01<00:00, 1685.59it/s]warmup should be done:  68%|██████▊   | 2035/3000 [00:01<00:00, 1689.20it/s]warmup should be done:  66%|██████▌   | 1983/3000 [00:01<00:00, 1638.69it/s]warmup should be done:  67%|██████▋   | 1997/3000 [00:01<00:00, 1653.32it/s]warmup should be done:  66%|██████▋   | 1994/3000 [00:01<00:00, 1637.98it/s]warmup should be done:  75%|███████▍  | 2242/3000 [00:01<00:00, 1718.18it/s]warmup should be done:  74%|███████▎  | 2209/3000 [00:01<00:00, 1688.67it/s]warmup should be done:  74%|███████▍  | 2231/3000 [00:01<00:00, 1709.07it/s]warmup should be done:  73%|███████▎  | 2203/3000 [00:01<00:00, 1685.95it/s]warmup should be done:  73%|███████▎  | 2204/3000 [00:01<00:00, 1685.93it/s]warmup should be done:  72%|███████▏  | 2148/3000 [00:01<00:00, 1640.95it/s]warmup should be done:  72%|███████▏  | 2163/3000 [00:01<00:00, 1652.34it/s]warmup should be done:  72%|███████▏  | 2159/3000 [00:01<00:00, 1639.86it/s]warmup should be done:  79%|███████▉  | 2379/3000 [00:01<00:00, 1690.97it/s]warmup should be done:  80%|████████  | 2414/3000 [00:01<00:00, 1717.18it/s]warmup should be done:  80%|████████  | 2402/3000 [00:01<00:00, 1707.91it/s]warmup should be done:  79%|███████▉  | 2373/3000 [00:01<00:00, 1687.33it/s]warmup should be done:  79%|███████▉  | 2374/3000 [00:01<00:00, 1689.74it/s]warmup should be done:  77%|███████▋  | 2314/3000 [00:01<00:00, 1644.22it/s]warmup should be done:  78%|███████▊  | 2329/3000 [00:01<00:00, 1654.38it/s]warmup should be done:  78%|███████▊  | 2325/3000 [00:01<00:00, 1645.84it/s]warmup should be done:  85%|████████▍ | 2549/3000 [00:01<00:00, 1693.29it/s]warmup should be done:  86%|████████▌ | 2574/3000 [00:01<00:00, 1711.05it/s]warmup should be done:  85%|████████▍ | 2543/3000 [00:01<00:00, 1689.90it/s]warmup should be done:  85%|████████▍ | 2545/3000 [00:01<00:00, 1694.88it/s]warmup should be done:  83%|████████▎ | 2479/3000 [00:01<00:00, 1644.60it/s]warmup should be done:  83%|████████▎ | 2495/3000 [00:01<00:00, 1652.21it/s]warmup should be done:  86%|████████▌ | 2586/3000 [00:01<00:00, 1697.69it/s]warmup should be done:  83%|████████▎ | 2490/3000 [00:01<00:00, 1641.85it/s]warmup should be done:  91%|█████████ | 2719/3000 [00:01<00:00, 1694.85it/s]warmup should be done:  92%|█████████▏| 2747/3000 [00:01<00:00, 1714.51it/s]warmup should be done:  90%|█████████ | 2713/3000 [00:01<00:00, 1691.28it/s]warmup should be done:  91%|█████████ | 2716/3000 [00:01<00:00, 1698.80it/s]warmup should be done:  88%|████████▊ | 2645/3000 [00:01<00:00, 1646.56it/s]warmup should be done:  89%|████████▊ | 2661/3000 [00:01<00:00, 1633.39it/s]warmup should be done:  92%|█████████▏| 2756/3000 [00:01<00:00, 1665.92it/s]warmup should be done:  88%|████████▊ | 2655/3000 [00:01<00:00, 1624.78it/s]warmup should be done:  96%|█████████▋| 2889/3000 [00:01<00:00, 1694.82it/s]warmup should be done:  97%|█████████▋| 2920/3000 [00:01<00:00, 1716.30it/s]warmup should be done:  96%|█████████▌| 2887/3000 [00:01<00:00, 1699.35it/s]warmup should be done:  94%|█████████▎| 2812/3000 [00:01<00:00, 1652.81it/s]warmup should be done:  96%|█████████▌| 2883/3000 [00:01<00:00, 1685.67it/s]warmup should be done:  94%|█████████▍| 2825/3000 [00:01<00:00, 1616.38it/s]warmup should be done:  97%|█████████▋| 2923/3000 [00:01<00:00, 1645.05it/s]warmup should be done:  94%|█████████▍| 2818/3000 [00:01<00:00, 1607.27it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1710.15it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1694.99it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1692.80it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1690.83it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1686.54it/s]warmup should be done:  99%|█████████▉| 2980/3000 [00:01<00:00, 1658.78it/s]warmup should be done: 100%|█████████▉| 2988/3000 [00:01<00:00, 1618.84it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1646.93it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1644.12it/s]warmup should be done:  99%|█████████▉| 2980/3000 [00:01<00:00, 1610.85it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1632.94it/s]
WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f35fbfb22b0>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f35fbfa4190>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f35fc2e9d30>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f35fbfa40d0>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f35fbfa21c0>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f35fc2e89d0>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f35fbfa50d0>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f35fc2e6b80>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
2022-12-12 01:41:06.878472: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f312f02cf70 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 01:41:06.878538: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 01:41:06.888002: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 01:41:06.891640: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f312f028ac0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 01:41:06.891700: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 01:41:06.901218: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 01:41:07.508611: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f312302ce20 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 01:41:07.508681: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 01:41:07.518827: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 01:41:07.643226: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f3126798bf0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 01:41:07.643286: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 01:41:07.651294: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 01:41:07.722214: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f312a82c0c0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 01:41:07.722283: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 01:41:07.727679: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f3126830670 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 01:41:07.727741: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 01:41:07.731689: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 01:41:07.737726: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 01:41:07.738979: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f311b030940 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 01:41:07.739021: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 01:41:07.748327: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 01:41:07.748848: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f312a8337f0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 01:41:07.748907: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 01:41:07.758746: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 01:41:13.984872: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 01:41:14.119492: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 01:41:14.374082: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 01:41:14.439800: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 01:41:14.440871: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 01:41:14.540084: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 01:41:14.761253: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 01:41:14.793785: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
[HCTR][01:42:11.231][ERROR][RK0][tid #139849487079168]: replica 7 reaches 1000, calling init pre replica
[HCTR][01:42:11.231][ERROR][RK0][tid #139849487079168]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][01:42:11.237][ERROR][RK0][tid #139849487079168]: coll ps creation done
[HCTR][01:42:11.237][ERROR][RK0][tid #139849487079168]: replica 7 waits for coll ps creation barrier
[HCTR][01:42:11.460][ERROR][RK0][tid #139850158167808]: replica 1 reaches 1000, calling init pre replica
[HCTR][01:42:11.460][ERROR][RK0][tid #139850158167808]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][01:42:11.468][ERROR][RK0][tid #139850158167808]: coll ps creation done
[HCTR][01:42:11.468][ERROR][RK0][tid #139850158167808]: replica 1 waits for coll ps creation barrier
[HCTR][01:42:11.505][ERROR][RK0][main]: replica 0 reaches 1000, calling init pre replica
[HCTR][01:42:11.505][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][01:42:11.513][ERROR][RK0][main]: coll ps creation done
[HCTR][01:42:11.513][ERROR][RK0][main]: replica 0 waits for coll ps creation barrier
[HCTR][01:42:11.545][ERROR][RK0][main]: replica 5 reaches 1000, calling init pre replica
[HCTR][01:42:11.545][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][01:42:11.550][ERROR][RK0][main]: coll ps creation done
[HCTR][01:42:11.550][ERROR][RK0][main]: replica 5 waits for coll ps creation barrier
[HCTR][01:42:11.573][ERROR][RK0][tid #139849612904192]: replica 3 reaches 1000, calling init pre replica
[HCTR][01:42:11.573][ERROR][RK0][tid #139849612904192]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][01:42:11.578][ERROR][RK0][tid #139849612904192]: coll ps creation done
[HCTR][01:42:11.578][ERROR][RK0][tid #139849612904192]: replica 3 waits for coll ps creation barrier
[HCTR][01:42:11.596][ERROR][RK0][main]: replica 6 reaches 1000, calling init pre replica
[HCTR][01:42:11.596][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][01:42:11.601][ERROR][RK0][main]: coll ps creation done
[HCTR][01:42:11.601][ERROR][RK0][main]: replica 6 waits for coll ps creation barrier
[HCTR][01:42:11.638][ERROR][RK0][tid #139849621296896]: replica 2 reaches 1000, calling init pre replica
[HCTR][01:42:11.638][ERROR][RK0][tid #139849621296896]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][01:42:11.642][ERROR][RK0][tid #139849621296896]: coll ps creation done
[HCTR][01:42:11.643][ERROR][RK0][tid #139849621296896]: replica 2 waits for coll ps creation barrier
[HCTR][01:42:11.662][ERROR][RK0][tid #139849487079168]: replica 4 reaches 1000, calling init pre replica
[HCTR][01:42:11.662][ERROR][RK0][tid #139849487079168]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][01:42:11.669][ERROR][RK0][tid #139849487079168]: coll ps creation done
[HCTR][01:42:11.669][ERROR][RK0][tid #139849487079168]: replica 4 waits for coll ps creation barrier
[HCTR][01:42:11.669][ERROR][RK0][main]: replica 0 preparing frequency
[HCTR][01:42:12.547][ERROR][RK0][main]: replica 0 preparing frequency done
[HCTR][01:42:12.585][ERROR][RK0][main]: replica 0 calling init per replica
[HCTR][01:42:12.585][ERROR][RK0][tid #139849487079168]: replica 7 calling init per replica
[HCTR][01:42:12.585][ERROR][RK0][tid #139849621296896]: replica 2 calling init per replica
[HCTR][01:42:12.585][ERROR][RK0][main]: replica 5 calling init per replica
[HCTR][01:42:12.585][ERROR][RK0][tid #139849487079168]: replica 4 calling init per replica
[HCTR][01:42:12.585][ERROR][RK0][main]: replica 6 calling init per replica
[HCTR][01:42:12.585][ERROR][RK0][tid #139850158167808]: replica 1 calling init per replica
[HCTR][01:42:12.585][ERROR][RK0][tid #139849612904192]: replica 3 calling init per replica
[HCTR][01:42:12.585][ERROR][RK0][main]: Calling build_v2
[HCTR][01:42:12.585][ERROR][RK0][tid #139849487079168]: Calling build_v2
[HCTR][01:42:12.585][ERROR][RK0][tid #139849621296896]: Calling build_v2
[HCTR][01:42:12.585][ERROR][RK0][main]: Calling build_v2
[HCTR][01:42:12.585][ERROR][RK0][tid #139849487079168]: Calling build_v2
[HCTR][01:42:12.585][ERROR][RK0][main]: Calling build_v2
[HCTR][01:42:12.585][ERROR][RK0][tid #139850158167808]: Calling build_v2
[HCTR][01:42:12.585][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][01:42:12.585][ERROR][RK0][tid #139849612904192]: Calling build_v2
[HCTR][01:42:12.585][ERROR][RK0][tid #139849487079168]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][01:42:12.585][ERROR][RK0][tid #139849621296896]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][01:42:12.585][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][01:42:12.585][ERROR][RK0][tid #139849487079168]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][01:42:12.585][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][01:42:12.585][ERROR][RK0][tid #139850158167808]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][01:42:12.585][ERROR][RK0][tid #139849612904192]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[[[[[2022-12-12 01:42:12[[[2022-12-12 01:42:122022-12-12 01:42:122022-12-12 01:42:122022-12-12 01:42:12.2022-12-12 01:42:12.2022-12-12 01:42:12..2022-12-12 01:42:12.585705.585705.585713585712.585710: 585712: 585715: : 585711: E: E: EE: E E E  E /hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc /hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc /hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc /hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc:/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc:/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc::/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc:136:136:136136:136] 136] 136] ] 136] using concurrent impl MPS] using concurrent impl MPS] using concurrent impl MPSusing concurrent impl MPS] using concurrent impl MPS
using concurrent impl MPS
using concurrent impl MPS

using concurrent impl MPS



[2022-12-12 01:42:12.590251: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:178] v100x8, slow pcie
[2022-12-12 01:42:12.590289: E [/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 01:42:12:.196590295] : assigning 8 to cpuE
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:178] v100x8, slow pcie
[2022-12-12 01:42:12.[5903492022-12-12 01:42:12: .E[590348 2022-12-12 01:42:12: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.E:590368 196: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] E:assigning 8 to cpu 178
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] :[v100x8, slow pcie2122022-12-12 01:42:12
] .build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8[590447[
[2022-12-12 01:42:12: 2022-12-12 01:42:122022-12-12 01:42:12.E..[590481 5904925904892022-12-12 01:42:12[: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: : .2022-12-12 01:42:12E:EE590526. 178[  : 590538/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] 2022-12-12 01:42:12/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE: :v100x8, slow pcie.::[ E196
5905922121782022-12-12 01:42:12/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc ] : ] [] .:/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccassigning 8 to cpuEbuild asymm link desc with 8X Tesla V100-SXM2-32GB out of 82022-12-12 01:42:12v100x8, slow pcie590644213:
 
.
: ] 178/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc590695E[remote time is 8.68421[] ::  2022-12-12 01:42:12
2022-12-12 01:42:12v100x8, slow pcie[178E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc..
[2022-12-12 01:42:12[]  :5907805907812022-12-12 01:42:12.2022-12-12 01:42:12v100x8, slow pcie/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc178: : .590809.
:] EE590830: 590862196v100x8, slow pcie[  : E: ] 
2022-12-12 01:42:12/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE Eassigning 8 to cpu.[:: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc 
5909252022-12-12 01:42:12196213/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: .] ] :212:E590974assigning 8 to cpuremote time is 8.68421214] 196 [: 

] build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 01:42:12Ecpu time is 97.0588
[assigning 8 to cpu:. 
2022-12-12 01:42:12[
196[591067/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.2022-12-12 01:42:12] 2022-12-12 01:42:12: :591109.[assigning 8 to cpu.E196: 5911522022-12-12 01:42:12
591160 ] E: .: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccassigning 8 to cpu E591198E:
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[ :  212:2022-12-12 01:42:12/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] 214[.: :build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8] 2022-12-12 01:42:12591274212/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc213
cpu time is 97.0588.: ] :] 
591309E[build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8212remote time is 8.68421:  2022-12-12 01:42:12
] 
E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8 :[[591384
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2122022-12-12 01:42:122022-12-12 01:42:12: :] .[.E212build asymm link desc with 8X Tesla V100-SXM2-32GB out of 85914372022-12-12 01:42:12591437 ] 
: .: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccbuild asymm link desc with 8X Tesla V100-SXM2-32GB out of 8E591470E:
 [:  213/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 01:42:12E[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] :. 2022-12-12 01:42:12:remote time is 8.68421214591526/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.213
] : :591546] cpu time is 97.0588[E213: remote time is 8.68421
2022-12-12 01:42:12 ] E
./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccremote time is 8.68421 591606[:
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: 2022-12-12 01:42:12[213:E.2022-12-12 01:42:12] 213 591666.remote time is 8.68421] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: 591690
remote time is 8.68421:E: 
214 [E] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[2022-12-12 01:42:12 cpu time is 97.0588:2022-12-12 01:42:12./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
214.591751:] 591761: 214cpu time is 97.0588: E] 
E cpu time is 97.0588 /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc::214214] ] cpu time is 97.0588cpu time is 97.0588

[2022-12-12 01:43:31.915236: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:118] solver created. now build & solve
[2022-12-12 01:43:31.955114: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:123] solver built. now solve
block 0 storage is 00010001
	access is	0	0	0	0	4	4	4	4	
block 1 storage is 00100010
	access is	1	1	1	1	5	5	5	5	
block 2 storage is 01000100
	access is	2	2	2	2	6	6	6	6	
block 3 storage is 10001000
	access is	3	3	3	3	7	7	7	7	
block 4 storage is 00000000
	access is	8	8	8	8	8	8	8	8	
[2022-12-12 01:43:32. 85895: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:127] solver solved
[2022-12-12 01:43:32. 85960: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:195] 0 solved master
[2022-12-12 01:43:32. 90170: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 0 solved
[2022-12-12 01:43:32. 90208: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 0 initing device 0
[2022-12-12 01:43:32. 90749: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-12 01:43:32. 90795: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 01:43:32. 91758: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 01:43:32. 92605: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 01:43:32[.2022-12-12 01:43:32105491.: 105500E:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc :/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc202:] 2024 solved] 
7 solved
[2022-12-12 01:43:32.[1056202022-12-12 01:43:32: .E105626 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.ccE: 205/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc] :worker 0 thread 4 initing device 4205
] worker 0 thread 7 initing device 7
[2022-12-12 01:43:32.105721: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 2 solved
[2022-12-12 01:43:32.105788: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 2 initing device 2
[2022-12-12 01:43:32.106099: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-12 01:43:32.106129: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu[:2022-12-12 01:43:321815.] 106149Building Coll Cache with ... num gpu device is 8: 
E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 01:43:32.106212: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980[] 2022-12-12 01:43:32eager alloc mem 381.47 MB.
106232: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-12 01:43:32.106288: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 01:43:32.108411: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 01:43:32.108516: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 01:43:32.108567: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 01:43:32.109400: [E2022-12-12 01:43:32 ./hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc109416:: 202E]  6 solved/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc
:202] [5 solved2022-12-12 01:43:32
.109496: E[ 2022-12-12 01:43:32/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc.:109516205: ] Eworker 0 thread 6 initing device 6 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 5 initing device 5
[2022-12-12 01:43:32.109967: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu[:2022-12-12 01:43:321815.] 109984Building Coll Cache with ... num gpu device is 8: 
E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-12 01:43:32.110035: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu[:2022-12-12 01:43:321980.] 110050eager alloc mem 381.47 MB: 
[E 2022-12-12 01:43:32/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu.:1100581980: ] Eeager alloc mem 381.47 MB 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202[] 2022-12-12 01:43:321 solved.
110143: E [/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc2022-12-12 01:43:32:.202110193] : 3 solvedE
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:[2052022-12-12 01:43:32] .worker 0 thread 1 initing device 1110244
: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 3 initing device 3
[2022-12-12 01:43:32.110733: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-12 01:43:32.110778: E[ 2022-12-12 01:43:32/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu.:1108001815: ] EBuilding Coll Cache with ... num gpu device is 8 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 01:43:32.110869: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:[19802022-12-12 01:43:32] .eager alloc mem 381.47 MB110892
: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 01:43:32.111480: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 01:43:32.111526: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 01:43:32.114761: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 01:43:32.115080: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 01:43:32.115762: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 01:43:32.115877: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 01:43:32.118098: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 01:43:32.118367: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 01:43:32.118875: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 01:43:32.118984: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 01:43:32.169479: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.00 Bytes
[2022-12-12 01:43:32.174618: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5
[2022-12-12 01:43:32.174707: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 01:43:32.175500: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 19.45 MB
[2022-12-12 01:43:32.176039: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 01:43:32.177043: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 01:43:32.177090: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 19.07 MB
[2022-12-12 01:43:32.186959: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 01:43:32.187646: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 01:43:32.187692: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 2.39 GB
[2022-12-12 01:43:32.194087: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.00 Bytes
[2022-12-12 01:43:32.199033: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5
[2022-12-12 01:43:32.199143: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 01:43:32.199952: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 19.45 MB
[2022-12-12 01:43:32.200457: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 01:43:32.201068: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.00 Bytes
[2022-12-12 01:43:32.201424: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 01:43:32.201467: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 19.07 MB
[2022-12-12 01:43:32.201933: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.00 Bytes
[[2022-12-12 01:43:322022-12-12 01:43:32..201996201996: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::19801980] ] eager alloc mem 5.00 Byteseager alloc mem 5.00 Bytes

[2022-12-12 01:43:32.202229: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.00 Bytes
[2022-12-12 01:43:32.205519: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.00 Bytes
[2022-12-12 01:43:32.211636: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5
[2022-12-12 01:43:32.211729: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 01:43:32.212259: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5
[2022-12-12 01:43:32.212343: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 01:43:32.212352: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5
[[2022-12-12 01:43:322022-12-12 01:43:32..212440212431: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::638638] ] eager release cuda mem 400000000eager release cuda mem 5

[2022-12-12 01:43:32.212506: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] [eager alloc mem 19.45 MB[2022-12-12 01:43:32
2022-12-12 01:43:32..212532212526: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::638638] ] eager release cuda mem 400000000eager release cuda mem 5

[2022-12-12 01:43:32[.2022-12-12 01:43:32212651.: 212643E:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc :/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc638:] 638eager release cuda mem 400000000] 
eager release cuda mem 5
[2022-12-12 01:43:32.212780: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 01:43:32.213602: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 19.45 MB
[2022-12-12 01:43:32.214085: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 01:43:32.214316: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 19.45 MB
[2022-12-12 01:43:32.214684: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 01:43:32.214729: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 2.39 GB
[2022-12-12 01:43:32.215184: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 19.45 MB
[2022-12-12 01:43:32.215817: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 19.45 MB
[2022-12-12 01:43:32.216386: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 19.45 MB
[2022-12-12 01:43:32.216635: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 01:43:32.217602: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 01:43:32.217646: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 19.07 MB
[2022-12-12 01:43:32.220217: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 01:43:32.220756: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 01:43:32.221192: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 01:43:32.[2212202022-12-12 01:43:32: .E221237 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuW: 1980/hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc] :eager alloc mem 611.00 KB43[
] 2022-12-12 01:43:32WORKER[0] alloc host memory 19.07 MB.
221262: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 01:43:32.221325: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 01:43:32.221744: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 01:43:32.221792: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 19.07 MB
[2022-12-12 01:43:32.222233: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663[
2022-12-12 01:43:32.222256: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] [eager release cuda mem 6256632022-12-12 01:43:32
.222280: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 19.07 MB[
2022-12-12 01:43:32.222307: [W2022-12-12 01:43:32 ./hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc222314:: 43E]  WORKER[0] alloc host memory 19.07 MB/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc
:638] eager release cuda mem 625663
[2022-12-12 01:43:32.222404: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 19.07 MB
[2022-12-12 01:43:32.226684: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 01:43:32.227336: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 01:43:32.227382: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 2.39 GB
[2022-12-12 01:43:32.232045: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 01:43:32.232654: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 01:43:32.232698: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 2.39 GB
[2022-12-12 01:43:32.235049: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 01:43:32.235113: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 01:43:32.235232: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 01:43:32.235333: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 01:43:32.235659: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 01:43:32.235703: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 2.39 GB
[2022-12-12 01:43:32.235741: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 01:43:32.235786: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 2.39 GB
[2022-12-12 01:43:32.235825: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 01:43:32.235868: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 2.39 GB
[2022-12-12 01:43:32.235926: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 01:43:32.235968: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 2.39 GB
[[[[[[[[2022-12-12 01:43:332022-12-12 01:43:332022-12-12 01:43:332022-12-12 01:43:332022-12-12 01:43:332022-12-12 01:43:332022-12-12 01:43:332022-12-12 01:43:33........125607125607125607125606125607125607125607125607: : : : : : : : EEEEEEEE        /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::::::::19261926192619261926192619261926] ] ] ] ] ] ] ] Device 1 init p2p of link 7Device 4 init p2p of link 5Device 2 init p2p of link 1Device 7 init p2p of link 4Device 6 init p2p of link 0Device 3 init p2p of link 2Device 0 init p2p of link 3Device 5 init p2p of link 6







[[2022-12-12 01:43:33[[[[2022-12-12 01:43:33.2022-12-12 01:43:332022-12-12 01:43:332022-12-12 01:43:33[[2022-12-12 01:43:33.126128...2022-12-12 01:43:332022-12-12 01:43:33.126127: 126138126132126144..126134: E: : : 126154126155: E EEE: : E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu   EE /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980:::/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] 198019801980::1980] eager alloc mem 611.00 KB] ] ] 19801980] eager alloc mem 611.00 KB
eager alloc mem 611.00 KBeager alloc mem 611.00 KBeager alloc mem 611.00 KB] ] eager alloc mem 611.00 KB



eager alloc mem 611.00 KBeager alloc mem 611.00 KB


[[2022-12-12 01:43:33[2022-12-12 01:43:33.2022-12-12 01:43:33.127255.127256: 127265: E: E E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:[[/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:6382022-12-12 01:43:332022-12-12 01:43:33[:[638[] ..2022-12-12 01:43:336382022-12-12 01:43:33] 2022-12-12 01:43:33eager release cuda mem 625663127301127300.] .eager release cuda mem 625663.
: : 127317eager release cuda mem 625663127318
127319EE: 
: :   EEE/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc   ::/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc638638:::] ] 638638638eager release cuda mem 625663eager release cuda mem 625663] ] ] 

eager release cuda mem 625663eager release cuda mem 625663eager release cuda mem 625663


[2022-12-12 01:43:33.140092: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 2
[2022-12-12 01:43:33.140236: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 01:43:33.140288: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 6 init p2p of link 5
[2022-12-12 01:43:33.140453: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 01:43:33.141045: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 01:43:33.141282: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 01:43:33.142627: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 7 init p2p of link 1
[2022-12-12 01:43:33.142781: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 01:43:33.142870: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 3 init p2p of link 0
[2022-12-12 01:43:33.143020: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 01:43:33.143054: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 0 init p2p of link 6
[2022-12-12 01:43:33.143208: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 01:43:33.143292: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 2 init p2p of link 3
[2022-12-12 01:43:33.143456: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 01:43:33.143480: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 4 init p2p of link 7
[2022-12-12 01:43:33.143558: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 01:43:33.143641: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 01:43:33.143670: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 5 init p2p of link 4
[2022-12-12 01:43:33.143820: E[ 2022-12-12 01:43:33/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu.:1438291980: ] Eeager alloc mem 611.00 KB 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 01:43:33.143989: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 01:43:33.144274: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 01:43:33.144454: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 01:43:33.144648: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 01:43:33.152536: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 3
[2022-12-12 01:43:33.152655: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 01:43:33.153463: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 01:43:33.153495: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 6 init p2p of link 4
[2022-12-12 01:43:33.153622: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 01:43:33.153843: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 7 init p2p of link 6
[2022-12-12 01:43:33.153975: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 01:43:33.154429: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 01:43:33.154790: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 01:43:33.155073: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 0 init p2p of link 1
[2022-12-12 01:43:33.155202: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 01:43:33.156045: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 01:43:33.156952: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 3 init p2p of link 5
[2022-12-12 01:43:33.157077: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 01:43:33.157374: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 2 init p2p of link 0
[2022-12-12 01:43:33.157489: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 01:43:33.157597: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 4 init p2p of link 2
[2022-12-12 01:43:33.157712: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 01:43:33.157852: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 01:43:33.158294: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 01:43:33.158493: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 01:43:33.158567: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 5 init p2p of link 7
[2022-12-12 01:43:33.158689: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 01:43:33.159498: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 01:43:33.170313: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 3 init p2p of link 1
[2022-12-12 01:43:33.170433: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 01:43:33.171040: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 4 init p2p of link 6
[2022-12-12 01:43:33.171166: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 01:43:33.171268: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 01:43:33.171634: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 0
[2022-12-12 01:43:33.171751: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 01:43:33.172005: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 01:43:33.172119: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 6 init p2p of link 7
[2022-12-12 01:43:33.172243: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 01:43:33.172564: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 01:43:33.173065: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 01:43:33.173293: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 7 init p2p of link 5
[2022-12-12 01:43:33.173523: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 01:43:33.173548: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 0 init p2p of link 2
[2022-12-12 01:43:33.173662: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 01:43:33.174360: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 01:43:33.174484: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 01:43:33.174887: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 2 init p2p of link 4
[2022-12-12 01:43:33.175001: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 01:43:33.175506: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 5 init p2p of link 3
[2022-12-12 01:43:33.175624: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 01:43:33.175767: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 01:43:33.176393: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 01:43:33.185968: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 20400000
[2022-12-12 01:43:33.187332: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 20400000
[2022-12-12 01:43:33.187673: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 20400000
[2022-12-12 01:43:33.187763: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 5000000 / 100000000 nodes ( 5.00 %~5.00 %) | remote 15000000 / 100000000 nodes ( 15.00 %) | cpu 80000000 / 100000000 nodes ( 80.00 %) | 2.39 GB | 1.0769 secs 
[2022-12-12 01:43:33.188150: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 5000000 / 100000000 nodes ( 5.00 %~5.00 %) | remote 15000000 / 100000000 nodes ( 15.00 %) | cpu 80000000 / 100000000 nodes ( 80.00 %) | 2.39 GB | 1.08187 secs 
[2022-12-12 01:43:33.188250: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 5000000 / 100000000 nodes ( 5.00 %~5.00 %) | remote 15000000 / 100000000 nodes ( 15.00 %) | cpu 80000000 / 100000000 nodes ( 80.00 %) | 2.39 GB | 1.07821 secs 
[2022-12-12 01:43:33.188644: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 20400000
[2022-12-12 01:43:33.189461: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 20400000
[2022-12-12 01:43:33.190109: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 20400000
[2022-12-12 01:43:33.190614: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 5000000 / 100000000 nodes ( 5.00 %~5.00 %) | remote 15000000 / 100000000 nodes ( 15.00 %) | cpu 80000000 / 100000000 nodes ( 80.00 %) | 2.39 GB | 1.08448 secs 
[2022-12-12 01:43:33.190633: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 20400000
[2022-12-12 01:43:33.191071: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 20400000
[2022-12-12 01:43:33.191526: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 5000000 / 100000000 nodes ( 5.00 %~5.00 %) | remote 15000000 / 100000000 nodes ( 15.00 %) | cpu 80000000 / 100000000 nodes ( 80.00 %) | 2.39 GB | 1.08074 secs 
[2022-12-12 01:43:33.191824: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 5000000 / 100000000 nodes ( 5.00 %~5.00 %) | remote 15000000 / 100000000 nodes ( 15.00 %) | cpu 80000000 / 100000000 nodes ( 80.00 %) | 2.39 GB | 1.0818 secs 
[2022-12-12 01:43:33.192745: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 5000000 / 100000000 nodes ( 5.00 %~5.00 %) | remote 15000000 / 100000000 nodes ( 15.00 %) | cpu 80000000 / 100000000 nodes ( 80.00 %) | 2.39 GB | 1.08655 secs 
[2022-12-12 01:43:33.194878: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 5000000 / 100000000 nodes ( 5.00 %~5.00 %) | remote 15000000 / 100000000 nodes ( 15.00 %) | cpu 80000000 / 100000000 nodes ( 80.00 %) | 2.39 GB | 1.10409 secs 
[2022-12-12 01:43:33.196401: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 10.71 GB
[2022-12-12 01:43:34.723798: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 10.97 GB
[2022-12-12 01:43:34.725139: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 10.97 GB
[2022-12-12 01:43:34.726375: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 10.97 GB
[2022-12-12 01:43:35.919821: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 11.24 GB
[2022-12-12 01:43:35.920398: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 11.24 GB
[2022-12-12 01:43:35.921320: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 11.24 GB
[2022-12-12 01:43:37.362899: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 11.45 GB
[2022-12-12 01:43:37.363089: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 11.45 GB
[2022-12-12 01:43:37.363469: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 11.45 GB
[2022-12-12 01:43:38.535072: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 11.66 GB
[2022-12-12 01:43:38.535649: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 11.66 GB
[2022-12-12 01:43:38.535938: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 11.66 GB
[2022-12-12 01:43:39.718372: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 12.12 GB
[2022-12-12 01:43:39.718762: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 12.12 GB
[2022-12-12 01:43:39.720453: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 12.12 GB
[2022-12-12 01:43:40.820432: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 12.32 GB
[2022-12-12 01:43:40.821389: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 12.32 GB
[HCTR][01:43:41.847][ERROR][RK0][tid #139850158167808]: replica 1 calling init per replica done, doing barrier
[HCTR][01:43:41.847][ERROR][RK0][tid #139849487079168]: replica 7 calling init per replica done, doing barrier
[HCTR][01:43:41.847][ERROR][RK0][tid #139849621296896]: replica 2 calling init per replica done, doing barrier
[HCTR][01:43:41.847][ERROR][RK0][main]: replica 6 calling init per replica done, doing barrier
[HCTR][01:43:41.847][ERROR][RK0][tid #139849487079168]: replica 4 calling init per replica done, doing barrier
[HCTR][01:43:41.847][ERROR][RK0][main]: replica 5 calling init per replica done, doing barrier
[HCTR][01:43:41.847][ERROR][RK0][tid #139849612904192]: replica 3 calling init per replica done, doing barrier
[HCTR][01:43:41.847][ERROR][RK0][main]: replica 0 calling init per replica done, doing barrier
[HCTR][01:43:41.847][ERROR][RK0][main]: replica 6 calling init per replica done, doing barrier done
[HCTR][01:43:41.847][ERROR][RK0][tid #139850158167808]: replica 1 calling init per replica done, doing barrier done
[HCTR][01:43:41.847][ERROR][RK0][main]: replica 0 calling init per replica done, doing barrier done
[HCTR][01:43:41.847][ERROR][RK0][tid #139849621296896]: replica 2 calling init per replica done, doing barrier done
[HCTR][01:43:41.847][ERROR][RK0][tid #139849487079168]: replica 7 calling init per replica done, doing barrier done
[HCTR][01:43:41.847][ERROR][RK0][tid #139849487079168]: replica 4 calling init per replica done, doing barrier done
[HCTR][01:43:41.847][ERROR][RK0][main]: replica 5 calling init per replica done, doing barrier done
[HCTR][01:43:41.847][ERROR][RK0][tid #139849612904192]: replica 3 calling init per replica done, doing barrier done
[HCTR][01:43:41.847][ERROR][RK0][main]: init per replica done
[HCTR][01:43:41.847][ERROR][RK0][tid #139850158167808]: init per replica done
[HCTR][01:43:41.847][ERROR][RK0][tid #139849621296896]: init per replica done
[HCTR][01:43:41.847][ERROR][RK0][tid #139849487079168]: init per replica done
[HCTR][01:43:41.847][ERROR][RK0][tid #139849487079168]: init per replica done
[HCTR][01:43:41.847][ERROR][RK0][main]: init per replica done
[HCTR][01:43:41.847][ERROR][RK0][tid #139849612904192]: init per replica done
[HCTR][01:43:41.850][ERROR][RK0][main]: init per replica done
[HCTR][01:43:41.885][ERROR][RK0][main]: 7 allocated 3276800 at 0x7f14d8238400
[HCTR][01:43:41.885][ERROR][RK0][main]: 7 allocated 6553600 at 0x7f14d8558400
[HCTR][01:43:41.886][ERROR][RK0][main]: 7 allocated 3276800 at 0x7f14d8b98400
[HCTR][01:43:41.886][ERROR][RK0][main]: 7 allocated 6553600 at 0x7f14d8eb8400
[HCTR][01:43:41.886][ERROR][RK0][main]: 3 allocated 3276800 at 0x7f14b0238400
[HCTR][01:43:41.886][ERROR][RK0][main]: 3 allocated 6553600 at 0x7f14b0558400
[HCTR][01:43:41.886][ERROR][RK0][main]: 3 allocated 3276800 at 0x7f14b0b98400
[HCTR][01:43:41.886][ERROR][RK0][main]: 3 allocated 6553600 at 0x7f14b0eb8400
[HCTR][01:43:41.886][ERROR][RK0][main]: 1 allocated 3276800 at 0x7f14f0238400
[HCTR][01:43:41.886][ERROR][RK0][tid #139849621296896]: 5 allocated 3276800 at 0x7f1474238400
[HCTR][01:43:41.886][ERROR][RK0][main]: 1 allocated 6553600 at 0x7f14f0558400
[HCTR][01:43:41.886][ERROR][RK0][tid #139849621296896]: 5 allocated 6553600 at 0x7f1474558400
[HCTR][01:43:41.886][ERROR][RK0][main]: 1 allocated 3276800 at 0x7f14f0b98400
[HCTR][01:43:41.886][ERROR][RK0][tid #139849621296896]: 5 allocated 3276800 at 0x7f1474b98400
[HCTR][01:43:41.886][ERROR][RK0][main]: 1 allocated 6553600 at 0x7f14f0eb8400
[HCTR][01:43:41.886][ERROR][RK0][tid #139849621296896]: 5 allocated 6553600 at 0x7f1474eb8400
[HCTR][01:43:41.886][ERROR][RK0][tid #139849487079168]: 4 allocated 3276800 at 0x7f1464238400
[HCTR][01:43:41.886][ERROR][RK0][tid #139849487079168]: 4 allocated 6553600 at 0x7f1464558400
[HCTR][01:43:41.886][ERROR][RK0][tid #139849487079168]: 4 allocated 3276800 at 0x7f1464b98400
[HCTR][01:43:41.886][ERROR][RK0][tid #139849487079168]: 4 allocated 6553600 at 0x7f1464eb8400
[HCTR][01:43:41.886][ERROR][RK0][tid #139849621296896]: 2 allocated 3276800 at 0x7f1510238400
[HCTR][01:43:41.886][ERROR][RK0][tid #139849621296896]: 2 allocated 6553600 at 0x7f1510558400
[HCTR][01:43:41.886][ERROR][RK0][tid #139849621296896]: 2 allocated 3276800 at 0x7f1510b98400
[HCTR][01:43:41.886][ERROR][RK0][tid #139849621296896]: 2 allocated 6553600 at 0x7f1510eb8400
[HCTR][01:43:41.886][ERROR][RK0][main]: 6 allocated 3276800 at 0x7f1470238400
[HCTR][01:43:41.886][ERROR][RK0][main]: 6 allocated 6553600 at 0x7f1470558400
[HCTR][01:43:41.886][ERROR][RK0][main]: 6 allocated 3276800 at 0x7f1470b98400
[HCTR][01:43:41.886][ERROR][RK0][main]: 6 allocated 6553600 at 0x7f1470eb8400
[HCTR][01:43:41.889][ERROR][RK0][tid #139849554188032]: 0 allocated 3276800 at 0x7f1536320000
[HCTR][01:43:41.889][ERROR][RK0][tid #139849554188032]: 0 allocated 6553600 at 0x7f1536640000
[HCTR][01:43:41.889][ERROR][RK0][tid #139849554188032]: 0 allocated 3276800 at 0x7f1536c80000
[HCTR][01:43:41.889][ERROR][RK0][tid #139849554188032]: 0 allocated 6553600 at 0x7f1536fa0000
