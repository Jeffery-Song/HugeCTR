2022-12-12 04:16:51.572022: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.584894: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.588749: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.594256: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.607042: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.613480: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.619135: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.633452: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.685084: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.688567: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.691862: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.692889: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.693790: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.694960: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.696032: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.697093: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.698094: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.699391: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.701157: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.702088: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.702241: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.703763: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.703861: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.705074: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.705388: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.706381: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.706837: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.708251: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.708664: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.710250: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.710666: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.711659: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.712200: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.713328: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.713707: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.715417: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.716464: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.717436: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.718841: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.720614: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.721287: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.722703: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.723761: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.724782: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.725861: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.726182: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 04:16:51.726932: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.728008: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.729102: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.735461: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.736499: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.737451: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.738080: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.738562: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.739811: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.740818: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.741228: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.743003: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.743223: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.745238: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.745507: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.747484: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.747996: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.750101: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.750751: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.751664: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.752622: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.753396: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.754747: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.755517: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.757833: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.758951: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.759227: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.760368: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.762010: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.762125: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.762132: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.763033: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.764587: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.764643: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.765052: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.772917: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.774221: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.775651: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.775888: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.776428: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.776883: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.777181: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.778884: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.779228: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.779997: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.780204: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.780373: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.801861: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.802125: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.816079: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.816789: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.817695: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.817885: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.818403: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.818502: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.819929: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.821261: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.822095: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.822303: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.822585: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.822689: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.823637: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.825909: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.826442: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.827630: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.827743: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.827889: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.828807: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.830490: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.830574: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.831326: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.831585: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.832468: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.834630: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.835105: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.835329: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.835966: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.838028: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.838412: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.838883: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.839378: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.840910: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.841377: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.841950: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.842181: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.843728: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.844174: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.844675: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.844924: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.846500: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.846884: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.847707: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.847900: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.849503: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.849980: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.850832: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.851030: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.852413: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.852663: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.853791: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.854741: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.855084: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.855394: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.856564: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.857472: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.857863: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.858103: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.859471: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.860560: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.860864: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.861317: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.861836: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.862377: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.863303: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.863760: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.864504: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.865313: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.865775: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.866675: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.867970: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.868620: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.868650: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.869763: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.869929: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.870921: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.871749: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.871772: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.873202: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.873435: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.874137: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.874953: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.875034: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.876360: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.876858: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.877562: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.877682: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.878320: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.878439: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.880368: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.880633: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.881294: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.881506: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.882188: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.882266: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.884385: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.884441: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 04:16:51.884682: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.885206: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.885227: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.885766: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.885989: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.888399: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.888621: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.889372: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.889859: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.890073: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.891912: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.892685: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.892759: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.893085: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.893214: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.893304: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 04:16:51.893353: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.895181: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.896184: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.896449: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.896932: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.897268: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.898875: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.899935: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.900472: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.900699: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.900717: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 04:16:51.901783: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.901818: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.902901: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.903204: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.903474: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 04:16:51.905771: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.905941: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.906729: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.907272: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.909155: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.909269: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.909287: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.910343: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.910660: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.911923: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.913111: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.913343: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.914134: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.914742: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.916454: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.917458: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.917678: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.918508: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.920366: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.920497: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.921623: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.922614: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.924595: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.925808: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.926787: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.928903: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.929931: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.960037: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.961861: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.963204: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.964965: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.967283: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.968177: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.970085: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.971586: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.972818: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.987314: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.988831: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.989551: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.991759: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.994386: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.995339: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:51.996665: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:52.029723: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 04:16:52.035355: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:52.038005: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:52.038638: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:52.040540: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:52.041997: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:52.043460: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:52.046165: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:52.055944: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:52.084514: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:52.087093: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:52.089851: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:52.093067: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:52.126547: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 04:16:52.135437: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:52.160643: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 04:16:52.161348: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:52.167780: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:52.169769: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:52.176102: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:52.182244: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:53.056384: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:53.057395: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:53.057948: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:53.058410: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 04:16:53.058471: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-12 04:16:53.077883: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:53.078577: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:53.079109: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:53.079913: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:53.080431: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:53.080914: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-12 04:16:53.127645: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 04:16:53.127857: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 04:16:53.184498: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12340
2022-12-12 04:16:53.305399: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:53.306134: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:53.306676: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:53.307173: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 04:16:53.307241: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-12 04:16:53.325175: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:53.325405: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:53.326080: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:53.326411: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:53.326987: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:53.327644: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:53.328112: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 04:16:53.328169: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-12 04:16:53.328864: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:53.329391: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:53.329873: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:1/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-12 04:16:53.336961: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:53.338406: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:53.339009: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:53.339497: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 04:16:53.339570: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-12 04:16:53.347668: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:53.348300: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:53.348820: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:53.349381: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:53.349899: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:53.350364: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:2/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-12 04:16:53.358295: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:53.358954: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:53.359028: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:53.359884: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:53.360061: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:53.361263: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:53.361343: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:53.362212: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:53.362233: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 04:16:53.362289: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-12 04:16:53.362867: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:4/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-12 04:16:53.380929: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:53.381554: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:53.382327: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:53.382905: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:53.383443: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:53.383928: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:3/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-12 04:16:53.388555: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:53.389183: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:53.389704: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:53.390175: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 04:16:53.390229: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-12 04:16:53.409088: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:53.409785: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:53.410127: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 04:16:53.410302: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:53.410312: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 04:16:53.410905: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:53.411508: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:53.411830: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 04:16:53.411972: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 04:16:53.412116: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12344
2022-12-12 04:16:53.412434: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:7/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-12 04:16:53.413776: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12341
2022-12-12 04:16:53.418348: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:53.418392: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:53.419586: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:53.419615: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:53.420590: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:53.420644: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:53.421486: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 04:16:53.421545: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-12 04:16:53.421586: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 04:16:53.421655: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-12 04:16:53.429332: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 04:16:53.429511: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 04:16:53.431002: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 04:16:53.431197: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 04:16:53.431533: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12342
2022-12-12 04:16:53.433067: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12343
2022-12-12 04:16:53.439982: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:53.440277: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:53.440931: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:53.441266: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:53.441816: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:53.442189: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:53.442849: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:53.443108: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:53.443734: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:53.444027: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:16:53.444597: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:5/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-12 04:16:53.444794: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:6/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-12 04:16:53.459065: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 04:16:53.459286: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 04:16:53.461207: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12347
2022-12-12 04:16:53.490584: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 04:16:53.490757: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 04:16:53.490789: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 04:16:53.490934: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 04:16:53.492564: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12345
2022-12-12 04:16:53.492779: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12346
[HCTR][04:16:54.749][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][04:16:54.755][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][04:16:54.757][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][04:16:54.757][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][04:16:54.757][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][04:16:54.763][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][04:16:54.786][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][04:16:54.786][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 1it [00:01,  1.56s/it]warmup run: 1it [00:01,  1.54s/it]warmup run: 1it [00:01,  1.54s/it]warmup run: 101it [00:01, 84.60it/s]warmup run: 1it [00:01,  1.51s/it]warmup run: 101it [00:01, 85.41it/s]warmup run: 93it [00:01, 78.50it/s]warmup run: 203it [00:01, 184.68it/s]warmup run: 102it [00:01, 87.64it/s]warmup run: 202it [00:01, 185.19it/s]warmup run: 180it [00:01, 163.84it/s]warmup run: 305it [00:01, 295.22it/s]warmup run: 1it [00:01,  1.51s/it]warmup run: 203it [00:01, 188.66it/s]warmup run: 305it [00:01, 298.04it/s]warmup run: 251it [00:01, 236.49it/s]warmup run: 1it [00:01,  1.49s/it]warmup run: 406it [00:01, 408.94it/s]warmup run: 1it [00:01,  1.49s/it]warmup run: 1it [00:01,  1.49s/it]warmup run: 69it [00:01, 59.36it/s]warmup run: 300it [00:01, 294.00it/s]warmup run: 408it [00:01, 415.09it/s]warmup run: 335it [00:01, 332.91it/s]warmup run: 100it [00:01, 87.21it/s]warmup run: 506it [00:02, 518.00it/s]warmup run: 100it [00:01, 87.41it/s]warmup run: 94it [00:01, 82.14it/s]warmup run: 162it [00:01, 154.88it/s]warmup run: 401it [00:01, 409.33it/s]warmup run: 506it [00:02, 519.86it/s]warmup run: 424it [00:02, 436.33it/s]warmup run: 198it [00:01, 186.25it/s]warmup run: 607it [00:02, 620.35it/s]warmup run: 197it [00:01, 185.63it/s]warmup run: 192it [00:01, 182.01it/s]warmup run: 260it [00:01, 266.64it/s]warmup run: 504it [00:02, 525.60it/s]warmup run: 607it [00:02, 622.09it/s]warmup run: 517it [00:02, 538.89it/s]warmup run: 297it [00:01, 295.91it/s]warmup run: 704it [00:02, 697.10it/s]warmup run: 294it [00:01, 293.14it/s]warmup run: 290it [00:01, 291.38it/s]warmup run: 358it [00:01, 382.09it/s]warmup run: 608it [00:02, 633.92it/s]warmup run: 708it [00:02, 710.70it/s]warmup run: 617it [00:02, 644.69it/s]warmup run: 396it [00:01, 409.22it/s]warmup run: 801it [00:02, 757.82it/s]warmup run: 392it [00:01, 405.32it/s]warmup run: 388it [00:01, 403.77it/s]warmup run: 456it [00:02, 494.04it/s]warmup run: 712it [00:02, 727.15it/s]warmup run: 809it [00:02, 783.71it/s]warmup run: 711it [00:02, 716.89it/s]warmup run: 493it [00:01, 514.85it/s]warmup run: 897it [00:02, 798.45it/s]warmup run: 491it [00:01, 515.31it/s]warmup run: 487it [00:01, 514.57it/s]warmup run: 814it [00:02, 799.76it/s]warmup run: 554it [00:02, 595.50it/s]warmup run: 908it [00:02, 837.01it/s]warmup run: 803it [00:02, 768.98it/s]warmup run: 594it [00:02, 618.98it/s]warmup run: 991it [00:02, 828.37it/s]warmup run: 591it [00:02, 617.56it/s]warmup run: 589it [00:02, 621.46it/s]warmup run: 916it [00:02, 856.48it/s]warmup run: 652it [00:02, 683.40it/s]warmup run: 896it [00:02, 811.11it/s]warmup run: 1007it [00:02, 869.23it/s]warmup run: 695it [00:02, 708.91it/s]warmup run: 1084it [00:02, 854.76it/s]warmup run: 691it [00:02, 705.03it/s]warmup run: 690it [00:02, 711.46it/s]warmup run: 1018it [00:02, 900.02it/s]warmup run: 750it [00:02, 754.00it/s]warmup run: 991it [00:02, 847.48it/s]warmup run: 796it [00:02, 781.83it/s]warmup run: 1105it [00:02, 887.62it/s]warmup run: 1181it [00:02, 885.45it/s]warmup run: 788it [00:02, 770.58it/s]warmup run: 792it [00:02, 787.16it/s]warmup run: 1119it [00:02, 928.04it/s]warmup run: 845it [00:02, 801.82it/s]warmup run: 1085it [00:02, 872.66it/s]warmup run: 897it [00:02, 839.88it/s]warmup run: 1202it [00:02, 903.61it/s]warmup run: 1279it [00:02, 910.08it/s]warmup run: 885it [00:02, 820.06it/s]warmup run: 894it [00:02, 846.65it/s]warmup run: 940it [00:02, 841.63it/s]warmup run: 1220it [00:02, 943.86it/s]warmup run: 1179it [00:02, 892.04it/s]warmup run: 999it [00:02, 887.73it/s]warmup run: 1298it [00:02, 910.51it/s]warmup run: 1379it [00:02, 934.90it/s]warmup run: 982it [00:02, 859.29it/s]warmup run: 996it [00:02, 892.35it/s]warmup run: 1038it [00:02, 879.71it/s]warmup run: 1320it [00:02, 957.90it/s]warmup run: 1273it [00:02, 904.44it/s]warmup run: 1101it [00:02, 922.17it/s]warmup run: 1393it [00:02, 919.47it/s]warmup run: 1476it [00:03, 929.92it/s]warmup run: 1079it [00:02, 886.65it/s]warmup run: 1098it [00:02, 925.50it/s]warmup run: 1137it [00:02, 910.78it/s]warmup run: 1422it [00:02, 974.12it/s]warmup run: 1367it [00:03, 912.14it/s]warmup run: 1202it [00:02, 945.66it/s]warmup run: 1488it [00:03, 925.19it/s]warmup run: 1578it [00:03, 954.22it/s]warmup run: 1175it [00:02, 904.23it/s]warmup run: 1199it [00:02, 942.15it/s]warmup run: 1236it [00:02, 933.09it/s]warmup run: 1525it [00:03, 990.41it/s]warmup run: 1461it [00:03, 918.45it/s]warmup run: 1303it [00:02, 960.83it/s]warmup run: 1583it [00:03, 929.77it/s]warmup run: 1680it [00:03, 971.49it/s]warmup run: 1271it [00:02, 913.85it/s]warmup run: 1299it [00:02, 953.54it/s]warmup run: 1334it [00:02, 945.70it/s]warmup run: 1628it [00:03, 1000.85it/s]warmup run: 1555it [00:03, 923.53it/s]warmup run: 1407it [00:02, 983.76it/s]warmup run: 1678it [00:03, 933.91it/s]warmup run: 1781it [00:03, 980.59it/s]warmup run: 1367it [00:02, 923.93it/s]warmup run: 1399it [00:02, 963.42it/s]warmup run: 1433it [00:03, 957.54it/s]warmup run: 1731it [00:03, 1007.24it/s]warmup run: 1649it [00:03, 927.08it/s]warmup run: 1510it [00:02, 994.83it/s]warmup run: 1773it [00:03, 934.93it/s]warmup run: 1883it [00:03, 989.28it/s]warmup run: 1463it [00:02, 933.25it/s]warmup run: 1499it [00:02, 972.86it/s]warmup run: 1535it [00:03, 975.94it/s]warmup run: 1834it [00:03, 1012.50it/s]warmup run: 1744it [00:03, 931.77it/s]warmup run: 1612it [00:03, 993.52it/s]warmup run: 1868it [00:03, 934.10it/s]warmup run: 1984it [00:03, 993.69it/s]warmup run: 1559it [00:03, 936.08it/s]warmup run: 1600it [00:03, 982.40it/s]warmup run: 1638it [00:03, 990.09it/s]warmup run: 1936it [00:03, 1011.77it/s]warmup run: 1841it [00:03, 942.41it/s]warmup run: 1713it [00:03, 987.04it/s]warmup run: 1965it [00:03, 943.91it/s]warmup run: 2101it [00:03, 1045.25it/s]warmup run: 1656it [00:03, 944.36it/s]warmup run: 1700it [00:03, 982.40it/s]warmup run: 1740it [00:03, 996.22it/s]warmup run: 2043it [00:03, 1028.46it/s]warmup run: 1939it [00:03, 950.65it/s]warmup run: 2077it [00:03, 996.08it/s]warmup run: 1813it [00:03, 981.18it/s]warmup run: 2223it [00:03, 1094.85it/s]warmup run: 1754it [00:03, 954.60it/s]warmup run: 1801it [00:03, 988.17it/s]warmup run: 1842it [00:03, 1000.24it/s]warmup run: 2162it [00:03, 1076.47it/s]warmup run: 2044it [00:03, 978.73it/s]warmup run: 2198it [00:03, 1059.23it/s]warmup run: 1912it [00:03, 983.34it/s]warmup run: 2345it [00:03, 1129.43it/s]warmup run: 1852it [00:03, 961.45it/s]warmup run: 1904it [00:03, 998.62it/s]warmup run: 1944it [00:03, 1003.45it/s]warmup run: 2282it [00:03, 1111.01it/s]warmup run: 2164it [00:03, 1043.97it/s]warmup run: 2319it [00:03, 1103.10it/s]warmup run: 2013it [00:03, 990.91it/s]warmup run: 2466it [00:03, 1152.27it/s]warmup run: 1950it [00:03, 964.44it/s]warmup run: 2006it [00:03, 1004.83it/s]warmup run: 2054it [00:03, 1030.70it/s]warmup run: 2401it [00:03, 1133.99it/s]warmup run: 2284it [00:03, 1090.48it/s]warmup run: 2133it [00:03, 1051.14it/s]warmup run: 2440it [00:03, 1132.57it/s]warmup run: 2587it [00:04, 1167.05it/s]warmup run: 2054it [00:03, 985.98it/s]warmup run: 2127it [00:03, 1065.14it/s]warmup run: 2176it [00:03, 1085.06it/s]warmup run: 2521it [00:03, 1151.70it/s]warmup run: 2405it [00:04, 1124.91it/s]warmup run: 2253it [00:03, 1093.78it/s]warmup run: 2561it [00:04, 1153.16it/s]warmup run: 2708it [00:04, 1178.02it/s]warmup run: 2174it [00:03, 1049.40it/s]warmup run: 2249it [00:03, 1108.73it/s]warmup run: 2641it [00:04, 1165.43it/s]warmup run: 2298it [00:03, 1124.07it/s]warmup run: 2527it [00:04, 1151.54it/s]warmup run: 2372it [00:03, 1122.39it/s]warmup run: 2681it [00:04, 1165.12it/s]warmup run: 2828it [00:04, 1182.20it/s]warmup run: 2294it [00:03, 1092.42it/s]warmup run: 2372it [00:03, 1142.53it/s]warmup run: 2760it [00:04, 1171.38it/s]warmup run: 2420it [00:03, 1151.34it/s]warmup run: 2650it [00:04, 1172.83it/s]warmup run: 2492it [00:03, 1144.16it/s]warmup run: 2799it [00:04, 1167.85it/s]warmup run: 2949it [00:04, 1189.22it/s]warmup run: 2414it [00:03, 1122.59it/s]warmup run: 2494it [00:03, 1162.82it/s]warmup run: 2880it [00:04, 1178.84it/s]warmup run: 2542it [00:04, 1169.98it/s]warmup run: 3000it [00:04, 675.67it/s] warmup run: 2771it [00:04, 1182.62it/s]warmup run: 2612it [00:04, 1159.70it/s]warmup run: 2918it [00:04, 1172.22it/s]warmup run: 2532it [00:04, 1138.66it/s]warmup run: 2611it [00:04, 1160.98it/s]warmup run: 3000it [00:04, 1182.30it/s]warmup run: 2664it [00:04, 1182.81it/s]warmup run: 3000it [00:04, 691.75it/s] warmup run: 3000it [00:04, 674.64it/s] warmup run: 2892it [00:04, 1189.09it/s]warmup run: 2729it [00:04, 1159.45it/s]warmup run: 2652it [00:04, 1154.28it/s]warmup run: 2729it [00:04, 1165.25it/s]warmup run: 2784it [00:04, 1185.25it/s]warmup run: 3000it [00:04, 658.55it/s] warmup run: 2847it [00:04, 1164.21it/s]warmup run: 2772it [00:04, 1165.72it/s]warmup run: 2849it [00:04, 1174.54it/s]warmup run: 2905it [00:04, 1191.98it/s]warmup run: 2968it [00:04, 1175.07it/s]warmup run: 3000it [00:04, 689.90it/s] warmup run: 2893it [00:04, 1178.50it/s]warmup run: 3000it [00:04, 680.00it/s] warmup run: 2969it [00:04, 1180.60it/s]warmup run: 3000it [00:04, 691.36it/s] warmup run: 3000it [00:04, 681.56it/s] 
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]





warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   5%|         | 164/3000 [00:00<00:01, 1636.31it/s]warmup should be done:   5%|         | 151/3000 [00:00<00:01, 1507.66it/s]warmup should be done:   5%|         | 163/3000 [00:00<00:01, 1628.97it/s]warmup should be done:   6%|         | 167/3000 [00:00<00:01, 1660.02it/s]warmup should be done:   5%|         | 163/3000 [00:00<00:01, 1622.95it/s]warmup should be done:   5%|         | 163/3000 [00:00<00:01, 1623.89it/s]warmup should be done:   5%|         | 163/3000 [00:00<00:01, 1620.18it/s]warmup should be done:   5%|         | 162/3000 [00:00<00:01, 1610.85it/s]warmup should be done:  11%|         | 328/3000 [00:00<00:01, 1639.45it/s]warmup should be done:  11%|         | 334/3000 [00:00<00:01, 1664.16it/s]warmup should be done:  11%|         | 327/3000 [00:00<00:01, 1631.33it/s]warmup should be done:  11%|         | 329/3000 [00:00<00:01, 1639.95it/s]warmup should be done:  11%|         | 326/3000 [00:00<00:01, 1627.73it/s]warmup should be done:  11%|         | 330/3000 [00:00<00:01, 1648.74it/s]warmup should be done:  11%|         | 328/3000 [00:00<00:01, 1633.36it/s]warmup should be done:  10%|         | 302/3000 [00:00<00:01, 1436.83it/s]warmup should be done:  16%|        | 489/3000 [00:00<00:01, 1628.14it/s]warmup should be done:  16%|        | 492/3000 [00:00<00:01, 1636.26it/s]warmup should be done:  16%|        | 495/3000 [00:00<00:01, 1646.07it/s]warmup should be done:  17%|        | 496/3000 [00:00<00:01, 1651.09it/s]warmup should be done:  16%|        | 491/3000 [00:00<00:01, 1631.47it/s]warmup should be done:  17%|        | 501/3000 [00:00<00:01, 1659.24it/s]warmup should be done:  16%|        | 492/3000 [00:00<00:01, 1617.04it/s]warmup should be done:  15%|        | 462/3000 [00:00<00:01, 1509.15it/s]warmup should be done:  22%|       | 656/3000 [00:00<00:01, 1635.05it/s]warmup should be done:  22%|       | 662/3000 [00:00<00:01, 1652.96it/s]warmup should be done:  22%|       | 653/3000 [00:00<00:01, 1629.03it/s]warmup should be done:  22%|       | 661/3000 [00:00<00:01, 1648.77it/s]warmup should be done:  22%|       | 655/3000 [00:00<00:01, 1630.49it/s]warmup should be done:  22%|       | 667/3000 [00:00<00:01, 1656.90it/s]warmup should be done:  22%|       | 654/3000 [00:00<00:01, 1606.10it/s]warmup should be done:  20%|        | 614/3000 [00:00<00:01, 1425.91it/s]warmup should be done:  27%|       | 820/3000 [00:00<00:01, 1633.85it/s]warmup should be done:  27%|       | 817/3000 [00:00<00:01, 1630.09it/s]warmup should be done:  28%|       | 828/3000 [00:00<00:01, 1651.96it/s]warmup should be done:  28%|       | 833/3000 [00:00<00:01, 1654.48it/s]warmup should be done:  27%|       | 819/3000 [00:00<00:01, 1628.76it/s]warmup should be done:  28%|       | 826/3000 [00:00<00:01, 1641.88it/s]warmup should be done:  27%|       | 815/3000 [00:00<00:01, 1600.30it/s]warmup should be done:  26%|       | 774/3000 [00:00<00:01, 1485.09it/s]warmup should be done:  33%|      | 981/3000 [00:00<00:01, 1629.66it/s]warmup should be done:  33%|      | 984/3000 [00:00<00:01, 1631.36it/s]warmup should be done:  33%|      | 994/3000 [00:00<00:01, 1649.74it/s]warmup should be done:  33%|      | 999/3000 [00:00<00:01, 1651.78it/s]warmup should be done:  33%|      | 982/3000 [00:00<00:01, 1624.59it/s]warmup should be done:  33%|      | 991/3000 [00:00<00:01, 1636.20it/s]warmup should be done:  33%|      | 976/3000 [00:00<00:01, 1595.16it/s]warmup should be done:  31%|       | 924/3000 [00:00<00:01, 1465.48it/s]warmup should be done:  38%|      | 1144/3000 [00:00<00:01, 1626.21it/s]warmup should be done:  39%|      | 1159/3000 [00:00<00:01, 1646.68it/s]warmup should be done:  38%|      | 1148/3000 [00:00<00:01, 1626.86it/s]warmup should be done:  38%|      | 1145/3000 [00:00<00:01, 1620.50it/s]warmup should be done:  39%|      | 1165/3000 [00:00<00:01, 1646.71it/s]warmup should be done:  38%|      | 1155/3000 [00:00<00:01, 1630.03it/s]warmup should be done:  38%|      | 1136/3000 [00:00<00:01, 1588.59it/s]warmup should be done:  36%|      | 1072/3000 [00:00<00:01, 1456.16it/s]warmup should be done:  44%|     | 1307/3000 [00:00<00:01, 1626.99it/s]warmup should be done:  44%|     | 1324/3000 [00:00<00:01, 1645.43it/s]warmup should be done:  44%|     | 1311/3000 [00:00<00:01, 1625.81it/s]warmup should be done:  44%|     | 1330/3000 [00:00<00:01, 1645.98it/s]warmup should be done:  44%|     | 1308/3000 [00:00<00:01, 1620.19it/s]warmup should be done:  44%|     | 1320/3000 [00:00<00:01, 1635.87it/s]warmup should be done:  43%|     | 1295/3000 [00:00<00:01, 1587.26it/s]warmup should be done:  41%|      | 1218/3000 [00:00<00:01, 1424.82it/s]warmup should be done:  49%|     | 1470/3000 [00:00<00:00, 1626.31it/s]warmup should be done:  50%|     | 1490/3000 [00:00<00:00, 1646.54it/s]warmup should be done:  49%|     | 1474/3000 [00:00<00:00, 1625.42it/s]warmup should be done:  50%|     | 1495/3000 [00:00<00:00, 1644.49it/s]warmup should be done:  49%|     | 1471/3000 [00:00<00:00, 1618.88it/s]warmup should be done:  49%|     | 1484/3000 [00:00<00:00, 1631.84it/s]warmup should be done:  48%|     | 1454/3000 [00:00<00:00, 1585.40it/s]warmup should be done:  46%|     | 1369/3000 [00:00<00:01, 1448.57it/s]warmup should be done:  54%|    | 1633/3000 [00:01<00:00, 1626.72it/s]warmup should be done:  55%|    | 1656/3000 [00:01<00:00, 1648.02it/s]warmup should be done:  55%|    | 1637/3000 [00:01<00:00, 1624.92it/s]warmup should be done:  55%|    | 1660/3000 [00:01<00:00, 1643.82it/s]warmup should be done:  54%|    | 1633/3000 [00:01<00:00, 1618.47it/s]warmup should be done:  55%|    | 1648/3000 [00:01<00:00, 1629.85it/s]warmup should be done:  54%|    | 1613/3000 [00:01<00:00, 1584.47it/s]warmup should be done:  51%|     | 1529/3000 [00:01<00:00, 1493.71it/s]warmup should be done:  60%|    | 1796/3000 [00:01<00:00, 1625.98it/s]warmup should be done:  61%|    | 1821/3000 [00:01<00:00, 1647.90it/s]warmup should be done:  60%|    | 1800/3000 [00:01<00:00, 1624.20it/s]warmup should be done:  60%|    | 1795/3000 [00:01<00:00, 1618.37it/s]warmup should be done:  61%|    | 1825/3000 [00:01<00:00, 1643.31it/s]warmup should be done:  60%|    | 1811/3000 [00:01<00:00, 1623.73it/s]warmup should be done:  59%|    | 1772/3000 [00:01<00:00, 1584.82it/s]warmup should be done:  56%|    | 1689/3000 [00:01<00:00, 1525.19it/s]warmup should be done:  65%|   | 1959/3000 [00:01<00:00, 1625.52it/s]warmup should be done:  66%|   | 1986/3000 [00:01<00:00, 1648.21it/s]warmup should be done:  65%|   | 1963/3000 [00:01<00:00, 1624.89it/s]warmup should be done:  65%|   | 1957/3000 [00:01<00:00, 1617.84it/s]warmup should be done:  66%|   | 1990/3000 [00:01<00:00, 1643.51it/s]warmup should be done:  66%|   | 1974/3000 [00:01<00:00, 1618.86it/s]warmup should be done:  64%|   | 1931/3000 [00:01<00:00, 1583.86it/s]warmup should be done:  62%|   | 1849/3000 [00:01<00:00, 1547.04it/s]warmup should be done:  71%|   | 2122/3000 [00:01<00:00, 1625.45it/s]warmup should be done:  72%|  | 2151/3000 [00:01<00:00, 1648.68it/s]warmup should be done:  71%|   | 2126/3000 [00:01<00:00, 1623.88it/s]warmup should be done:  71%|   | 2119/3000 [00:01<00:00, 1617.78it/s]warmup should be done:  72%|  | 2155/3000 [00:01<00:00, 1642.95it/s]warmup should be done:  71%|   | 2136/3000 [00:01<00:00, 1615.33it/s]warmup should be done:  70%|   | 2090/3000 [00:01<00:00, 1582.94it/s]warmup should be done:  67%|   | 2009/3000 [00:01<00:00, 1561.94it/s]warmup should be done:  76%|  | 2285/3000 [00:01<00:00, 1625.09it/s]warmup should be done:  77%|  | 2316/3000 [00:01<00:00, 1647.91it/s]warmup should be done:  76%|  | 2289/3000 [00:01<00:00, 1624.26it/s]warmup should be done:  76%|  | 2281/3000 [00:01<00:00, 1617.14it/s]warmup should be done:  77%|  | 2320/3000 [00:01<00:00, 1644.11it/s]warmup should be done:  77%|  | 2298/3000 [00:01<00:00, 1612.71it/s]warmup should be done:  75%|  | 2249/3000 [00:01<00:00, 1583.01it/s]warmup should be done:  72%|  | 2169/3000 [00:01<00:00, 1571.99it/s]warmup should be done:  82%| | 2448/3000 [00:01<00:00, 1625.66it/s]warmup should be done:  83%| | 2481/3000 [00:01<00:00, 1644.32it/s]warmup should be done:  82%| | 2452/3000 [00:01<00:00, 1622.78it/s]warmup should be done:  81%| | 2443/3000 [00:01<00:00, 1615.94it/s]warmup should be done:  83%| | 2485/3000 [00:01<00:00, 1642.70it/s]warmup should be done:  82%| | 2460/3000 [00:01<00:00, 1607.67it/s]warmup should be done:  80%|  | 2408/3000 [00:01<00:00, 1582.38it/s]warmup should be done:  78%|  | 2329/3000 [00:01<00:00, 1579.53it/s]warmup should be done:  87%| | 2613/3000 [00:01<00:00, 1632.81it/s]warmup should be done:  88%| | 2646/3000 [00:01<00:00, 1644.09it/s]warmup should be done:  87%| | 2605/3000 [00:01<00:00, 1616.49it/s]warmup should be done:  87%| | 2616/3000 [00:01<00:00, 1625.73it/s]warmup should be done:  88%| | 2650/3000 [00:01<00:00, 1644.23it/s]warmup should be done:  87%| | 2621/3000 [00:01<00:00, 1607.31it/s]warmup should be done:  86%| | 2567/3000 [00:01<00:00, 1583.87it/s]warmup should be done:  83%| | 2488/3000 [00:01<00:00, 1580.78it/s]warmup should be done:  93%|| 2778/3000 [00:01<00:00, 1637.71it/s]warmup should be done:  94%|| 2811/3000 [00:01<00:00, 1643.00it/s]warmup should be done:  92%|| 2767/3000 [00:01<00:00, 1616.18it/s]warmup should be done:  93%|| 2780/3000 [00:01<00:00, 1627.30it/s]warmup should be done:  94%|| 2815/3000 [00:01<00:00, 1645.87it/s]warmup should be done:  93%|| 2782/3000 [00:01<00:00, 1607.51it/s]warmup should be done:  91%| | 2726/3000 [00:01<00:00, 1585.30it/s]warmup should be done:  88%| | 2648/3000 [00:01<00:00, 1585.41it/s]warmup should be done:  98%|| 2946/3000 [00:01<00:00, 1647.68it/s]warmup should be done:  99%|| 2977/3000 [00:01<00:00, 1647.85it/s]warmup should be done:  98%|| 2932/3000 [00:01<00:00, 1625.70it/s]warmup should be done:  98%|| 2945/3000 [00:01<00:00, 1632.89it/s]warmup should be done:  99%|| 2982/3000 [00:01<00:00, 1650.20it/s]warmup should be done:  98%|| 2946/3000 [00:01<00:00, 1614.71it/s]warmup should be done:  96%|| 2887/3000 [00:01<00:00, 1590.74it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1647.87it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1646.97it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1631.56it/s]warmup should be done:  94%|| 2808/3000 [00:01<00:00, 1589.34it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1628.38it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1623.62it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1621.65it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1591.85it/s]warmup should be done:  99%|| 2970/3000 [00:01<00:00, 1598.38it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1529.40it/s]



warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]


warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   6%|         | 165/3000 [00:00<00:01, 1649.19it/s]warmup should be done:   6%|         | 167/3000 [00:00<00:01, 1668.75it/s]warmup should be done:   6%|         | 169/3000 [00:00<00:01, 1686.87it/s]warmup should be done:   6%|         | 169/3000 [00:00<00:01, 1683.51it/s]warmup should be done:   6%|         | 167/3000 [00:00<00:01, 1662.29it/s]warmup should be done:   5%|         | 161/3000 [00:00<00:01, 1603.02it/s]warmup should be done:   6%|         | 171/3000 [00:00<00:01, 1701.26it/s]warmup should be done:   6%|         | 165/3000 [00:00<00:01, 1642.39it/s]warmup should be done:  11%|         | 330/3000 [00:00<00:01, 1645.74it/s]warmup should be done:  11%|         | 334/3000 [00:00<00:01, 1666.32it/s]warmup should be done:  11%|        | 338/3000 [00:00<00:01, 1685.89it/s]warmup should be done:  11%|         | 327/3000 [00:00<00:01, 1635.50it/s]warmup should be done:  11%|         | 331/3000 [00:00<00:01, 1650.03it/s]warmup should be done:  11%|        | 339/3000 [00:00<00:01, 1687.40it/s]warmup should be done:  11%|         | 334/3000 [00:00<00:01, 1661.96it/s]warmup should be done:  11%|        | 342/3000 [00:00<00:01, 1686.43it/s]warmup should be done:  17%|        | 501/3000 [00:00<00:01, 1667.48it/s]warmup should be done:  17%|        | 507/3000 [00:00<00:01, 1686.26it/s]warmup should be done:  17%|        | 496/3000 [00:00<00:01, 1648.50it/s]warmup should be done:  16%|        | 493/3000 [00:00<00:01, 1643.63it/s]warmup should be done:  17%|        | 498/3000 [00:00<00:01, 1655.98it/s]warmup should be done:  17%|        | 509/3000 [00:00<00:01, 1690.43it/s]warmup should be done:  17%|        | 501/3000 [00:00<00:01, 1661.55it/s]warmup should be done:  17%|        | 511/3000 [00:00<00:01, 1684.73it/s]warmup should be done:  22%|       | 669/3000 [00:00<00:01, 1671.64it/s]warmup should be done:  23%|       | 677/3000 [00:00<00:01, 1689.27it/s]warmup should be done:  22%|       | 662/3000 [00:00<00:01, 1651.72it/s]warmup should be done:  22%|       | 660/3000 [00:00<00:01, 1653.55it/s]warmup should be done:  22%|       | 665/3000 [00:00<00:01, 1658.81it/s]warmup should be done:  23%|       | 680/3000 [00:00<00:01, 1694.52it/s]warmup should be done:  22%|       | 669/3000 [00:00<00:01, 1665.73it/s]warmup should be done:  23%|       | 680/3000 [00:00<00:01, 1685.97it/s]warmup should be done:  28%|       | 837/3000 [00:00<00:01, 1672.81it/s]warmup should be done:  28%|       | 828/3000 [00:00<00:01, 1652.58it/s]warmup should be done:  28%|       | 847/3000 [00:00<00:01, 1690.55it/s]warmup should be done:  28%|       | 828/3000 [00:00<00:01, 1659.70it/s]warmup should be done:  28%|       | 832/3000 [00:00<00:01, 1661.50it/s]warmup should be done:  28%|       | 851/3000 [00:00<00:01, 1697.04it/s]warmup should be done:  28%|       | 837/3000 [00:00<00:01, 1668.29it/s]warmup should be done:  28%|       | 850/3000 [00:00<00:01, 1688.92it/s]warmup should be done:  34%|      | 1005/3000 [00:00<00:01, 1670.09it/s]warmup should be done:  33%|      | 995/3000 [00:00<00:01, 1661.97it/s]warmup should be done:  33%|      | 1000/3000 [00:00<00:01, 1666.49it/s]warmup should be done:  33%|      | 994/3000 [00:00<00:01, 1651.06it/s]warmup should be done:  34%|      | 1021/3000 [00:00<00:01, 1697.69it/s]warmup should be done:  34%|      | 1017/3000 [00:00<00:01, 1688.04it/s]warmup should be done:  33%|      | 1004/3000 [00:00<00:01, 1667.25it/s]warmup should be done:  34%|      | 1019/3000 [00:00<00:01, 1687.95it/s]warmup should be done:  39%|      | 1168/3000 [00:00<00:01, 1668.41it/s]warmup should be done:  39%|      | 1160/3000 [00:00<00:01, 1651.51it/s]warmup should be done:  40%|      | 1186/3000 [00:00<00:01, 1685.47it/s]warmup should be done:  40%|      | 1191/3000 [00:00<00:01, 1694.99it/s]warmup should be done:  39%|      | 1162/3000 [00:00<00:01, 1656.46it/s]warmup should be done:  39%|      | 1171/3000 [00:00<00:01, 1663.56it/s]warmup should be done:  40%|      | 1188/3000 [00:00<00:01, 1685.09it/s]warmup should be done:  39%|      | 1173/3000 [00:00<00:01, 1654.32it/s]warmup should be done:  44%|     | 1326/3000 [00:00<00:01, 1652.68it/s]warmup should be done:  44%|     | 1335/3000 [00:00<00:00, 1665.93it/s]warmup should be done:  45%|     | 1356/3000 [00:00<00:00, 1689.23it/s]warmup should be done:  45%|     | 1362/3000 [00:00<00:00, 1696.70it/s]warmup should be done:  45%|     | 1339/3000 [00:00<00:00, 1665.84it/s]warmup should be done:  44%|     | 1328/3000 [00:00<00:01, 1652.60it/s]warmup should be done:  45%|     | 1358/3000 [00:00<00:00, 1686.98it/s]warmup should be done:  45%|     | 1341/3000 [00:00<00:00, 1661.28it/s]warmup should be done:  50%|     | 1493/3000 [00:00<00:00, 1656.51it/s]warmup should be done:  51%|     | 1526/3000 [00:00<00:00, 1689.87it/s]warmup should be done:  50%|     | 1502/3000 [00:00<00:00, 1662.96it/s]warmup should be done:  50%|     | 1506/3000 [00:00<00:00, 1666.61it/s]warmup should be done:  51%|     | 1532/3000 [00:00<00:00, 1694.44it/s]warmup should be done:  51%|     | 1527/3000 [00:00<00:00, 1685.11it/s]warmup should be done:  50%|     | 1494/3000 [00:00<00:00, 1649.31it/s]warmup should be done:  50%|     | 1509/3000 [00:00<00:00, 1665.48it/s]warmup should be done:  55%|    | 1660/3000 [00:01<00:00, 1659.74it/s]warmup should be done:  57%|    | 1696/3000 [00:01<00:00, 1691.33it/s]warmup should be done:  56%|    | 1669/3000 [00:01<00:00, 1663.64it/s]warmup should be done:  56%|    | 1674/3000 [00:01<00:00, 1669.42it/s]warmup should be done:  57%|    | 1702/3000 [00:01<00:00, 1694.00it/s]warmup should be done:  57%|    | 1696/3000 [00:01<00:00, 1685.75it/s]warmup should be done:  55%|    | 1659/3000 [00:01<00:00, 1646.37it/s]warmup should be done:  56%|    | 1677/3000 [00:01<00:00, 1668.55it/s]warmup should be done:  61%|    | 1827/3000 [00:01<00:00, 1662.57it/s]warmup should be done:  62%|   | 1866/3000 [00:01<00:00, 1693.05it/s]warmup should be done:  61%|    | 1836/3000 [00:01<00:00, 1664.51it/s]warmup should be done:  61%|   | 1842/3000 [00:01<00:00, 1671.18it/s]warmup should be done:  62%|   | 1872/3000 [00:01<00:00, 1694.80it/s]warmup should be done:  62%|   | 1865/3000 [00:01<00:00, 1686.68it/s]warmup should be done:  61%|    | 1824/3000 [00:01<00:00, 1644.59it/s]warmup should be done:  61%|   | 1844/3000 [00:01<00:00, 1657.62it/s]warmup should be done:  66%|   | 1994/3000 [00:01<00:00, 1664.08it/s]warmup should be done:  68%|   | 2036/3000 [00:01<00:00, 1693.80it/s]warmup should be done:  68%|   | 2042/3000 [00:01<00:00, 1694.68it/s]warmup should be done:  67%|   | 2003/3000 [00:01<00:00, 1661.36it/s]warmup should be done:  67%|   | 2010/3000 [00:01<00:00, 1670.03it/s]warmup should be done:  68%|   | 2034/3000 [00:01<00:00, 1686.97it/s]warmup should be done:  66%|   | 1989/3000 [00:01<00:00, 1640.51it/s]warmup should be done:  67%|   | 2012/3000 [00:01<00:00, 1662.16it/s]warmup should be done:  72%|  | 2163/3000 [00:01<00:00, 1669.71it/s]warmup should be done:  74%|  | 2206/3000 [00:01<00:00, 1693.42it/s]warmup should be done:  74%|  | 2212/3000 [00:01<00:00, 1694.08it/s]warmup should be done:  72%|  | 2170/3000 [00:01<00:00, 1659.68it/s]warmup should be done:  73%|  | 2203/3000 [00:01<00:00, 1685.05it/s]warmup should be done:  73%|  | 2178/3000 [00:01<00:00, 1668.58it/s]warmup should be done:  72%|  | 2154/3000 [00:01<00:00, 1638.90it/s]warmup should be done:  73%|  | 2180/3000 [00:01<00:00, 1665.14it/s]warmup should be done:  78%|  | 2332/3000 [00:01<00:00, 1674.77it/s]warmup should be done:  79%|  | 2382/3000 [00:01<00:00, 1693.12it/s]warmup should be done:  79%|  | 2376/3000 [00:01<00:00, 1688.31it/s]warmup should be done:  79%|  | 2372/3000 [00:01<00:00, 1685.11it/s]warmup should be done:  78%|  | 2337/3000 [00:01<00:00, 1659.76it/s]warmup should be done:  78%|  | 2347/3000 [00:01<00:00, 1671.97it/s]warmup should be done:  78%|  | 2348/3000 [00:01<00:00, 1667.99it/s]warmup should be done:  77%|  | 2318/3000 [00:01<00:00, 1630.13it/s]warmup should be done:  83%| | 2501/3000 [00:01<00:00, 1677.92it/s]warmup should be done:  85%| | 2552/3000 [00:01<00:00, 1694.41it/s]warmup should be done:  85%| | 2541/3000 [00:01<00:00, 1685.27it/s]warmup should be done:  83%| | 2504/3000 [00:01<00:00, 1661.15it/s]warmup should be done:  84%| | 2515/3000 [00:01<00:00, 1672.26it/s]warmup should be done:  85%| | 2545/3000 [00:01<00:00, 1682.30it/s]warmup should be done:  84%| | 2516/3000 [00:01<00:00, 1671.15it/s]warmup should be done:  83%| | 2482/3000 [00:01<00:00, 1622.28it/s]warmup should be done:  89%| | 2670/3000 [00:01<00:00, 1679.29it/s]warmup should be done:  91%| | 2722/3000 [00:01<00:00, 1695.42it/s]warmup should be done:  89%| | 2671/3000 [00:01<00:00, 1663.63it/s]warmup should be done:  90%| | 2711/3000 [00:01<00:00, 1686.74it/s]warmup should be done:  89%| | 2684/3000 [00:01<00:00, 1676.66it/s]warmup should be done:  90%| | 2715/3000 [00:01<00:00, 1686.80it/s]warmup should be done:  89%| | 2684/3000 [00:01<00:00, 1673.39it/s]warmup should be done:  88%| | 2646/3000 [00:01<00:00, 1625.61it/s]warmup should be done:  95%|| 2839/3000 [00:01<00:00, 1681.34it/s]warmup should be done:  96%|| 2892/3000 [00:01<00:00, 1694.15it/s]warmup should be done:  95%|| 2838/3000 [00:01<00:00, 1665.32it/s]warmup should be done:  96%|| 2884/3000 [00:01<00:00, 1687.30it/s]warmup should be done:  96%|| 2880/3000 [00:01<00:00, 1685.33it/s]warmup should be done:  95%|| 2853/3000 [00:01<00:00, 1678.69it/s]warmup should be done:  95%|| 2852/3000 [00:01<00:00, 1672.07it/s]warmup should be done:  94%|| 2810/3000 [00:01<00:00, 1629.28it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1694.19it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1688.52it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1686.31it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1670.88it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1667.71it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1666.22it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1663.27it/s]warmup should be done:  99%|| 2975/3000 [00:01<00:00, 1633.10it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1639.90it/s]
WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f037a787eb0>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f0379c7a250>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f037a782d30>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f037a77fe80>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f0379c6c100>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f037a780730>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f0379c6b0d0>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f0379c6c1f0>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
2022-12-12 04:18:24.477746: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7efeaf029340 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 04:18:24.477816: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 04:18:24.487371: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 04:18:24.650450: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7efeae82fc30 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 04:18:24.650509: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 04:18:24.660661: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 04:18:25.075242: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7efeae830e30 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 04:18:25.075308: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 04:18:25.085681: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 04:18:25.103277: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7efea7031f50 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 04:18:25.103340: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 04:18:25.112312: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 04:18:25.294368: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7efeae82fad0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 04:18:25.294436: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 04:18:25.303258: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 04:18:25.309592: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7efeb28349b0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 04:18:25.309651: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 04:18:25.318719: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 04:18:25.343838: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7efea7031720 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 04:18:25.343903: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 04:18:25.344143: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7efea6f926d0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 04:18:25.344191: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 04:18:25.353435: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 04:18:25.353537: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 04:18:31.670980: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 04:18:31.786279: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 04:18:31.915462: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 04:18:31.965486: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 04:18:32.020879: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 04:18:32.160688: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 04:18:32.217743: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 04:18:32.366791: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
[HCTR][04:19:23.707][ERROR][RK0][main]: replica 5 reaches 1000, calling init pre replica
[HCTR][04:19:23.707][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][04:19:23.715][ERROR][RK0][main]: coll ps creation done
[HCTR][04:19:23.715][ERROR][RK0][main]: replica 5 waits for coll ps creation barrier
[HCTR][04:19:23.785][ERROR][RK0][main]: replica 7 reaches 1000, calling init pre replica
[HCTR][04:19:23.785][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][04:19:23.793][ERROR][RK0][main]: coll ps creation done
[HCTR][04:19:23.793][ERROR][RK0][main]: replica 7 waits for coll ps creation barrier
[HCTR][04:19:23.975][ERROR][RK0][tid #139632473798400]: replica 1 reaches 1000, calling init pre replica
[HCTR][04:19:23.975][ERROR][RK0][tid #139632473798400]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][04:19:23.982][ERROR][RK0][tid #139632473798400]: coll ps creation done
[HCTR][04:19:23.982][ERROR][RK0][tid #139632473798400]: replica 1 waits for coll ps creation barrier
[HCTR][04:19:23.983][ERROR][RK0][tid #139632473798400]: replica 0 reaches 1000, calling init pre replica
[HCTR][04:19:23.984][ERROR][RK0][tid #139632473798400]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][04:19:23.991][ERROR][RK0][tid #139632473798400]: coll ps creation done
[HCTR][04:19:23.991][ERROR][RK0][tid #139632473798400]: replica 0 waits for coll ps creation barrier
[HCTR][04:19:24.113][ERROR][RK0][main]: replica 6 reaches 1000, calling init pre replica
[HCTR][04:19:24.114][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][04:19:24.118][ERROR][RK0][tid #139632608016128]: replica 4 reaches 1000, calling init pre replica
[HCTR][04:19:24.118][ERROR][RK0][tid #139632608016128]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][04:19:24.119][ERROR][RK0][main]: coll ps creation done
[HCTR][04:19:24.119][ERROR][RK0][main]: replica 6 waits for coll ps creation barrier
[HCTR][04:19:24.122][ERROR][RK0][tid #139632608016128]: coll ps creation done
[HCTR][04:19:24.122][ERROR][RK0][tid #139632608016128]: replica 4 waits for coll ps creation barrier
[HCTR][04:19:24.191][ERROR][RK0][main]: replica 2 reaches 1000, calling init pre replica
[HCTR][04:19:24.192][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][04:19:24.196][ERROR][RK0][main]: coll ps creation done
[HCTR][04:19:24.196][ERROR][RK0][main]: replica 2 waits for coll ps creation barrier
[HCTR][04:19:24.200][ERROR][RK0][main]: replica 3 reaches 1000, calling init pre replica
[HCTR][04:19:24.201][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][04:19:24.207][ERROR][RK0][main]: coll ps creation done
[HCTR][04:19:24.207][ERROR][RK0][main]: replica 3 waits for coll ps creation barrier
[HCTR][04:19:24.207][ERROR][RK0][tid #139632473798400]: replica 0 preparing frequency
[HCTR][04:19:25.101][ERROR][RK0][tid #139632473798400]: replica 0 preparing frequency done
[HCTR][04:19:25.149][ERROR][RK0][main]: replica 7 calling init per replica
[HCTR][04:19:25.149][ERROR][RK0][main]: replica 2 calling init per replica
[HCTR][04:19:25.149][ERROR][RK0][tid #139632473798400]: replica 1 calling init per replica
[HCTR][04:19:25.149][ERROR][RK0][tid #139632473798400]: replica 0 calling init per replica
[HCTR][04:19:25.149][ERROR][RK0][main]: replica 3 calling init per replica
[HCTR][04:19:25.149][ERROR][RK0][main]: replica 6 calling init per replica
[HCTR][04:19:25.149][ERROR][RK0][main]: replica 5 calling init per replica
[HCTR][04:19:25.149][ERROR][RK0][tid #139632608016128]: replica 4 calling init per replica
[HCTR][04:19:25.149][ERROR][RK0][main]: Calling build_v2
[HCTR][04:19:25.149][ERROR][RK0][main]: Calling build_v2
[HCTR][04:19:25.149][ERROR][RK0][tid #139632473798400]: Calling build_v2
[HCTR][04:19:25.149][ERROR][RK0][tid #139632473798400]: Calling build_v2
[HCTR][04:19:25.149][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][04:19:25.149][ERROR][RK0][main]: Calling build_v2
[HCTR][04:19:25.149][ERROR][RK0][main]: Calling build_v2
[HCTR][04:19:25.149][ERROR][RK0][main]: Calling build_v2
[HCTR][04:19:25.149][ERROR][RK0][tid #139632608016128]: Calling build_v2
[HCTR][04:19:25.149][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][04:19:25.149][ERROR][RK0][tid #139632473798400]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][04:19:25.149][ERROR][RK0][tid #139632473798400]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][04:19:25.149][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][04:19:25.149][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][04:19:25.149][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][04:19:25.149][ERROR][RK0][tid #139632608016128]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[2022-12-12 04:19:25.[153929: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:2022-12-12 04:19:25178.] 153966v100x8, slow pcie: 
E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:[178[] 2022-12-12 04:19:25v100x8, slow pcie.2022-12-12 04:19:25
154025.: 154011E: [ E2022-12-12 04:19:25/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc .:/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc154052196[:: ] 178Eassigning 0 to cpu]  
2022-12-12 04:19:25v100x8, slow pcie/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.
:154059196: ] E[assigning 0 to cpu 2022-12-12 04:19:25
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.:[154115[178: 2022-12-12 04:19:25[] E2022-12-12 04:19:25.2022-12-12 04:19:25v100x8, slow pcie .154134[.
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc154110: 154150[:2022-12-12 04:19:25: E: 2022-12-12 04:19:25[196.E E.] 154155 2022-12-12 04:19:25/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc 154204assigning 0 to cpu: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.:/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: [
E:154198212:E 178: ] 212 /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] Ebuild asymm link desc with 8X Tesla V100-SXM2-32GB out of 8] 2022-12-12 04:19:25/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:v100x8, slow pcie[ 
build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8.:178
2022-12-12 04:19:25/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
196154259] : [.:[] v100x8, slow pcie[E2022-12-12 04:19:251543511782022-12-12 04:19:25assigning 0 to cpu
2022-12-12 04:19:25 .: ] .
.[154398/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccEv100x8, slow pcie1544051544232022-12-12 04:19:25: [: 
: : .E2022-12-12 04:19:25178/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccEE[154478 .] :  2022-12-12 04:19:25: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc154528v100x8, slow pcie212/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.E:: ] 
::154571 213[Ebuild asymm link desc with 8X Tesla V100-SXM2-32GB out of 8196213: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc]  2022-12-12 04:19:25
] ] E:remote time is 8.68421/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.assigning 0 to cpuremote time is 8.68421[ 196
:154676

2022-12-12 04:19:25/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] 212[: .:[assigning 0 to cpu] 2022-12-12 04:19:25.E1547321962022-12-12 04:19:25
build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8154771 [: ] .
: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 04:19:25Eassigning 0 to cpu154785E:. [[
:  196154825/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 04:19:252022-12-12 04:19:25E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] : :.. :Eassigning 0 to cpu213[154882154883/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc214 
] 2022-12-12 04:19:25: : :] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccremote time is 8.68421.EE214cpu time is 97.0588:
[154954  ] 
2122022-12-12 04:19:25[: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cccpu time is 97.0588] .2022-12-12 04:19:25E::
build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8155033. 213212
: 155069/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] ] E: :remote time is 8.68421[build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8 E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc212
2022-12-12 04:19:25
 :] 212.[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccbuild asymm link desc with 8X Tesla V100-SXM2-32GB out of 8[] 1551942022-12-12 04:19:25:
2022-12-12 04:19:25build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8: .214.[
E155251] 1552632022-12-12 04:19:25 : cpu time is 97.0588: ./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[E
E155317:2022-12-12 04:19:25  : 213./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE] 155356:: remote time is 8.68421: 214213/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
E] ] : cpu time is 97.0588[remote time is 8.68421213/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
:2022-12-12 04:19:25
] 213.remote time is 8.68421[] 155470
2022-12-12 04:19:25remote time is 8.68421: .[
E1555172022-12-12 04:19:25 : ./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[E155547:2022-12-12 04:19:25 : 214./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE] cpu time is 97.0588155572: 
: 214/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE] : cpu time is 97.0588214/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
] :cpu time is 97.0588214
] cpu time is 97.0588
[2022-12-12 04:20:43.891091: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:118] solver created. now build & solve
[2022-12-12 04:20:43.931008: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:123] solver built. now solve
[2022-12-12 04:20:43.931089: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:919] num_cached_nodes = 15000000
[2022-12-12 04:20:44. 41544: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:127] solver solved
[2022-12-12 04:20:44. 41640: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:195] 0 solved master
[2022-12-12 04:20:44.189639: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 0 solved
[2022-12-12 04:20:44.189677: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 0 initing device 0
[2022-12-12 04:20:44.190130: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 04:20:44.191497: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 04:20:44.192334: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 04:20:44.205012: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 5 solved
[2022-12-12 04:20:44.205068: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 5 initing device 5
[2022-12-12 04:20:44.205266: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 1 solved
[2022-12-12 04:20:44.205320: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 1 initing device 1
[[2022-12-12 04:20:44[2022-12-12 04:20:44.2022-12-12 04:20:44.205431.205451: 205441: E: E E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202:1980] 202] 7 solved] eager alloc mem 381.47 MB
2 solved

[2022-12-12 04:20:44.[205535[2022-12-12 04:20:44: 2022-12-12 04:20:44.E.205540 205531: /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc: E:E 205 /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc] /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:worker 0 thread 7 initing device 7:205
202] [] worker 0 thread 2 initing device 22022-12-12 04:20:443 solved
.
205579: E [/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc2022-12-12 04:20:44:.202205654] : 4 solvedE
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc[:[2022-12-12 04:20:442052022-12-12 04:20:44.] .205697worker 0 thread 3 initing device 3205702: 
: EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc::1980205] ] eager alloc mem 381.47 MBworker 0 thread 4 initing device 4

[2022-12-12 04:20:44.205956: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 04:20:44.206000: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 04:20:44.206101: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 04:20:44.206186: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 04:20:44.207891: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 6 solved
[2022-12-12 04:20:44.207944: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 6 initing device 6
[2022-12-12 04:20:44.208324: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 04:20:44.209907: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 04:20:44.210253: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 04:20:44.210300: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 04:20:44.210361: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 04:20:44.210419: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 04:20:44.211043: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 04:20:44.213012: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 04:20:44.214542: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 04:20:44.214776: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 04:20:44.214825: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 04:20:44.214879: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 04:20:44.214928: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 04:20:44.215548: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 04:20:44.217451: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 04:20:44.271329: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 2.00 Bytes
[2022-12-12 04:20:44.271750: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1024.00 Bytes
[2022-12-12 04:20:44.277403: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-12 04:20:44.277510: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 2
[2022-12-12 04:20:44.277556: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 04:20:44.278365: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 57.60 MB
[2022-12-12 04:20:44.278941: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:20:44.280012: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:20:44.280114: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 04:20:44.280786: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 04:20:44.280835: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 7.15 GB
[[[[2022-12-12 04:20:442022-12-12 04:20:442022-12-12 04:20:442022-12-12 04:20:44....300689300689300690300689: : : : EEEE    /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::::1980198019801980] ] ] ] eager alloc mem 2.00 Byteseager alloc mem 2.00 Byteseager alloc mem 2.00 Byteseager alloc mem 2.00 Bytes



[[[2022-12-12 04:20:44[2022-12-12 04:20:442022-12-12 04:20:44.2022-12-12 04:20:44..301150.301151301153: 301154: : E: EE E  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::1980:19801980] 1980] ] eager alloc mem 1024.00 Bytes] eager alloc mem 1024.00 Byteseager alloc mem 1024.00 Bytes
eager alloc mem 1024.00 Bytes


[[2022-12-12 04:20:442022-12-12 04:20:44..303643303643: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::19801980] ] eager alloc mem 2.00 Byteseager alloc mem 2.00 Bytes

[[2022-12-12 04:20:442022-12-12 04:20:44..304032304033: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::19801980] ] eager alloc mem 1024.00 Byteseager alloc mem 1024.00 Bytes

[2022-12-12 04:20:44.304706: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 2.00 Bytes
[2022-12-12 04:20:44.305050: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1024.00 Bytes
[2022-12-12 04:20:44.307214: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-12 04:20:44[.2022-12-12 04:20:44307288.: 307279E:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc :/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc638:] 638eager release cuda mem 2] 
eager release cuda mem 1024
[2022-12-12 04:20:44.[307359[2022-12-12 04:20:44: 2022-12-12 04:20:44.E.307353 307371: /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc: E:E 638 /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc] /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:eager release cuda mem 400000000:638
638] ] eager release cuda mem 1024eager release cuda mem 2

[2022-12-12 04:20:44.307461[: 2022-12-12 04:20:44E. 307467/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc: :E638 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cceager release cuda mem 400000000:
638] eager release cuda mem 2
[2022-12-12 04:20:44.307527: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 04:20:44.307656: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-12 04:20:44.307724: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 2
[2022-12-12 04:20:44.307768: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 04:20:44.320253: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 57.60 MB
[2022-12-12 04:20:44.320762: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 57.60 MB
[2022-12-12 04:20:44.321388: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 57.60 MB
[2022-12-12 04:20:44.321428: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-12 04:20:44.321498: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 2
[2022-12-12 04:20:44.321510: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc[:2022-12-12 04:20:44638.] 321540eager release cuda mem 1024: 
E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 04:20:44.321585: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 2
[2022-12-12 04:20:44.321629: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 04:20:44.321898: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 57.60 MB
[2022-12-12 04:20:44.322614: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-12 04:20:44.322681: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 2
[2022-12-12 04:20:44.322721: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 04:20:44.323628: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 57.60 MB
[2022-12-12 04:20:44.324201: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 57.60 MB
[2022-12-12 04:20:44.324488: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:20:44.324545: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:20:44.324581: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:20:44.325011: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:20:44.325446: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 57.60 MB
[2022-12-12 04:20:44.325505: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:20:44.325570: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[[2022-12-12 04:20:442022-12-12 04:20:44..325596325598: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::6381980] ] eager release cuda mem 625663eager alloc mem 25.25 KB

[2022-12-12 04:20:44.325659: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 04:20:44.325710: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 04:20:44.325993: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:20:44.326028: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:20:44.326111: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 04:20:44.326138: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:20:44.326292: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[[[2022-12-12 04:20:442022-12-12 04:20:442022-12-12 04:20:44...326321326333326330: : : EEE[   2022-12-12 04:20:44/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc.:::32638819801980638: ] ] ] Eeager alloc mem 611.00 KBeager alloc mem 7.15 GBeager release cuda mem 25855 


/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 04:20:44.326497: E[ 2022-12-12 04:20:44/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu.:3265081980: ] Eeager alloc mem 7.15 GB 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 7.15 GB
[2022-12-12 04:20:44.326774: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 04:20:44.326821: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 7.15 GB
[2022-12-12 04:20:44.327016: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:20:44.327100: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:[19802022-12-12 04:20:44] .eager alloc mem 25.25 KB327114
: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:20:44.327221: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 04:20:44.327458: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:20:44.327548: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 04:20:44.327790: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 04:20:44.327846: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 7.15 GB
[2022-12-12 04:20:44.327901: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 04:20:44.327942: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 7.15 GB
[2022-12-12 04:20:44.328217: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 04:20:44.328260: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 7.15 GB
[[[[[[[[2022-12-12 04:20:452022-12-12 04:20:452022-12-12 04:20:452022-12-12 04:20:452022-12-12 04:20:452022-12-12 04:20:452022-12-12 04:20:452022-12-12 04:20:45........812038812035812035812035812035812036812052812052: : : : : : : : EEEEEEEE        /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::::::::19801980198019801980198019801980] ] ] ] ] ] ] ] eager alloc mem 611.00 KBeager alloc mem 611.00 KBeager alloc mem 611.00 KBeager alloc mem 611.00 KBeager alloc mem 611.00 KBeager alloc mem 611.00 KBeager alloc mem 611.00 KBeager alloc mem 611.00 KB







[[[2022-12-12 04:20:45[2022-12-12 04:20:45[2022-12-12 04:20:45.2022-12-12 04:20:45.[2022-12-12 04:20:45.[813178.[8131782022-12-12 04:20:45.8131782022-12-12 04:20:45: 8131872022-12-12 04:20:45: .813190: .E: .E813198: E813202 E813207 : E : /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc : /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccE /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccE:/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccE: /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc: 638: 638/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc] 638/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc] :638] :eager release cuda mem 625663] :eager release cuda mem 625663638] eager release cuda mem 625663638
eager release cuda mem 625663638
] eager release cuda mem 625663
] 
] eager release cuda mem 625663
eager release cuda mem 625663eager release cuda mem 625663


[2022-12-12 04:20:45.813561: [E2022-12-12 04:20:45 ./hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu[813571:2022-12-12 04:20:45: 1980.[E] 8135812022-12-12 04:20:45[[ eager alloc mem 611.00 KB: .2022-12-12 04:20:45[[2022-12-12 04:20:45/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu
E813593.2022-12-12 04:20:452022-12-12 04:20:45.: : 813601..8136081980/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE: 813610813611: ] : E: : Eeager alloc mem 611.00 KB1980/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu EE 
] :/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cueager alloc mem 611.00 KB1980:/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:
] 1980::1980eager alloc mem 611.00 KB] 19801980] 
eager alloc mem 611.00 KB] ] eager alloc mem 611.00 KB
eager alloc mem 611.00 KBeager alloc mem 611.00 KB


[2022-12-12 04:20:45.814374: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:20:45.814447: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB[
2022-12-12 04:20:45.814467: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] [eager release cuda mem 6256632022-12-12 04:20:45
.814502: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:20:45.814531: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] [eager release cuda mem 625663[2022-12-12 04:20:45[
[2022-12-12 04:20:45.[2022-12-12 04:20:45[2022-12-12 04:20:45.8145492022-12-12 04:20:45.2022-12-12 04:20:45.814551: .814557.[814559: E814565: 8145772022-12-12 04:20:45: E : E: .E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccE E814619 /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc: /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc: :638/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:E638] :1980:638 ] eager release cuda mem 625663638] 1980] /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cueager release cuda mem 625663
] eager alloc mem 611.00 KB] eager release cuda mem 625663:
eager release cuda mem 625663
eager alloc mem 611.00 KB
1980

] [eager alloc mem 611.00 KB2022-12-12 04:20:45
.814880: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:[19802022-12-12 04:20:45] .eager alloc mem 611.00 KB[814902
[2022-12-12 04:20:45: 2022-12-12 04:20:45.E.814918 814927: /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu: E:E 1980 /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu] /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:eager alloc mem 611.00 KB:1980
1980] ] eager alloc mem 611.00 KBeager alloc mem 611.00 KB

[2022-12-12 04:20:45.815216: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:20:45.815289: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:20:45.815589: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638[] 2022-12-12 04:20:45eager release cuda mem 625663.
815608: E[ 2022-12-12 04:20:45/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc.:815625638: ] Eeager release cuda mem 625663 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[[2022-12-12 04:20:452022-12-12 04:20:45..815667815667: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:[:19802022-12-12 04:20:45638] .[] eager alloc mem 611.00 KB8156982022-12-12 04:20:45eager release cuda mem 625663[
: .[
2022-12-12 04:20:45[E8157092022-12-12 04:20:45.2022-12-12 04:20:45 : .815723./hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE815730: 815737: : E[: 1980/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE 2022-12-12 04:20:45E] : /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc. eager alloc mem 611.00 KB1980/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:815802/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc
] :638: :eager alloc mem 611.00 KB638] E638
] eager release cuda mem 625663 ] eager release cuda mem 625663
/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cueager release cuda mem 625663
:
1980] eager alloc mem 611.00 KB
[2022-12-12 04:20:45.815998: [E2022-12-12 04:20:45 [./hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu2022-12-12 04:20:45816009:.: 1980816016E] :  [eager alloc mem 611.00 KBE/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu2022-12-12 04:20:45
 :./hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu1980816038:] : 1980eager alloc mem 611.00 KBE] 
 eager alloc mem 611.00 KB/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc
:638] eager release cuda mem 625663
[2022-12-12 04:20:45.816182: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:20:45.816470: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:20:45.816538: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:20:45.816622: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:20:45.816661: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:20:45.816690: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu[:2022-12-12 04:20:451980.] 816703eager alloc mem 611.00 KB: 
E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:[6382022-12-12 04:20:45] .eager release cuda mem 625663816734
: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:20:45.[8167932022-12-12 04:20:45: .E816797 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE: 1980/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc[] :[2022-12-12 04:20:45eager alloc mem 611.00 KB6382022-12-12 04:20:45.
] .816844eager release cuda mem 625663816853: 
: EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::638638] ] eager release cuda mem 625663eager release cuda mem 625663

[[2022-12-12 04:20:452022-12-12 04:20:45..816937816941: : EE [[ /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc2022-12-12 04:20:452022-12-12 04:20:45/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:..:6388169738169741980] : : ] eager release cuda mem 625663EEeager alloc mem 611.00 KB
  
/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::19801980] ] eager alloc mem 611.00 KBeager alloc mem 611.00 KB

[2022-12-12 04:20:45.817097: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:20:45.817284: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:20:45.817351: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:20:45.817450: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:20:45.817492: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:20:45.817516: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:20:45.817558: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:20:45.817612: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:20:45.817677: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:20:45.817789: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:[6382022-12-12 04:20:45[] .2022-12-12 04:20:45eager release cuda mem 625663817807.
: 817811E:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc :/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc638:[] 6382022-12-12 04:20:45eager release cuda mem 625663] .
[eager release cuda mem 6256638178482022-12-12 04:20:45
: .E817872 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccE: [638/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu2022-12-12 04:20:45] :[.eager release cuda mem 62566319802022-12-12 04:20:45817924
] .: eager alloc mem 611.00 KB817940E
:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu :/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu1980:] [1980eager alloc mem 611.00 KB2022-12-12 04:20:45] 
.eager alloc mem 611.00 KB818015
: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:20:45.818102: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:20:45.818169: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:20:45.818265: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:20:45.818308: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:20:45.818331: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:20:45.818375: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:20:45.818432: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:20:45.818500: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:20:45.818713: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:20:45.818772: E[ 2022-12-12 04:20:45[/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc.2022-12-12 04:20:45:818784.[638: 8187872022-12-12 04:20:45] E: .eager release cuda mem 625663 E818800
/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu : :/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccE1980: ] 638/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cceager alloc mem 611.00 KB] :[
eager release cuda mem 6256636382022-12-12 04:20:45
[] .2022-12-12 04:20:45eager release cuda mem 625663818895.
: 818914E:  E[[/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu 2022-12-12 04:20:452022-12-12 04:20:45:/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc..1980:818967818967] 638: : eager alloc mem 611.00 KB] EE
eager release cuda mem 625663  
/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu[/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:2022-12-12 04:20:45:[1980.6382022-12-12 04:20:45] 819083] [.eager alloc mem 611.00 KB: eager release cuda mem 604000002022-12-12 04:20:45819102
E
.:  819123E/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:  :E/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc638 :] /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc638eager release cuda mem 625663:] 
638eager release cuda mem 60400000] 
eager release cuda mem 625663
[2022-12-12 04:20:45.819224: E[ 2022-12-12 04:20:45/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc.:819234638[: ] 2022-12-12 04:20:45Eeager release cuda mem 60400000. 
819248/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc: :E638 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cceager release cuda mem 60400000:
638] eager release cuda mem 625663
[2022-12-12 04:20:45.819309: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 60400000
[2022-12-12 04:20:45.819636: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:20:45.819676: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 60400000
[2022-12-12 04:20:45.819802: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:20:45.819840: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 60400000
[2022-12-12 04:20:45.819894: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:20:45.819932: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 60400000
[2022-12-12 04:20:45.820651: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 15000000 / 100000000 nodes ( 15.00 %~15.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 85000000 / 100000000 nodes ( 85.00 %) | 7.15 GB | 1.61465 secs 
[2022-12-12 04:20:45.821186: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 15000000 / 100000000 nodes ( 15.00 %~15.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 85000000 / 100000000 nodes ( 85.00 %) | 7.15 GB | 1.61574 secs 
[2022-12-12 04:20:45.821433: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 15000000 / 100000000 nodes ( 15.00 %~15.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 85000000 / 100000000 nodes ( 85.00 %) | 7.15 GB | 1.61534 secs 
[2022-12-12 04:20:45.821569: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 15000000 / 100000000 nodes ( 15.00 %~15.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 85000000 / 100000000 nodes ( 85.00 %) | 7.15 GB | 1.61325 secs 
[2022-12-12 04:20:45.822138: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 15000000 / 100000000 nodes ( 15.00 %~15.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 85000000 / 100000000 nodes ( 85.00 %) | 7.15 GB | 1.63202 secs 
[2022-12-12 04:20:45.822377: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 15000000 / 100000000 nodes ( 15.00 %~15.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 85000000 / 100000000 nodes ( 85.00 %) | 7.15 GB | 1.6162 secs 
[2022-12-12 04:20:45.822514: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 15000000 / 100000000 nodes ( 15.00 %~15.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 85000000 / 100000000 nodes ( 85.00 %) | 7.15 GB | 1.61656 secs 
[2022-12-12 04:20:45.822694: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 15000000 / 100000000 nodes ( 15.00 %~15.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 85000000 / 100000000 nodes ( 85.00 %) | 7.15 GB | 1.617 secs 
[HCTR][04:20:45.822][ERROR][RK0][tid #139632608016128]: replica 4 calling init per replica done, doing barrier
[HCTR][04:20:45.822][ERROR][RK0][main]: replica 5 calling init per replica done, doing barrier
[HCTR][04:20:45.822][ERROR][RK0][tid #139632473798400]: replica 1 calling init per replica done, doing barrier
[HCTR][04:20:45.822][ERROR][RK0][main]: replica 7 calling init per replica done, doing barrier
[HCTR][04:20:45.822][ERROR][RK0][main]: replica 6 calling init per replica done, doing barrier
[HCTR][04:20:45.822][ERROR][RK0][main]: replica 3 calling init per replica done, doing barrier
[HCTR][04:20:45.822][ERROR][RK0][tid #139632473798400]: replica 0 calling init per replica done, doing barrier
[HCTR][04:20:45.822][ERROR][RK0][main]: replica 2 calling init per replica done, doing barrier
[HCTR][04:20:45.822][ERROR][RK0][main]: replica 2 calling init per replica done, doing barrier done
[HCTR][04:20:45.822][ERROR][RK0][main]: replica 6 calling init per replica done, doing barrier done
[HCTR][04:20:45.822][ERROR][RK0][main]: replica 3 calling init per replica done, doing barrier done
[HCTR][04:20:45.822][ERROR][RK0][tid #139632473798400]: replica 1 calling init per replica done, doing barrier done
[HCTR][04:20:45.822][ERROR][RK0][main]: replica 5 calling init per replica done, doing barrier done
[HCTR][04:20:45.822][ERROR][RK0][tid #139632608016128]: replica 4 calling init per replica done, doing barrier done
[HCTR][04:20:45.822][ERROR][RK0][tid #139632473798400]: replica 0 calling init per replica done, doing barrier done
[HCTR][04:20:45.822][ERROR][RK0][main]: replica 7 calling init per replica done, doing barrier done
[HCTR][04:20:45.822][ERROR][RK0][main]: init per replica done
[HCTR][04:20:45.822][ERROR][RK0][main]: init per replica done
[HCTR][04:20:45.822][ERROR][RK0][main]: init per replica done
[HCTR][04:20:45.822][ERROR][RK0][tid #139632473798400]: init per replica done
[HCTR][04:20:45.822][ERROR][RK0][main]: init per replica done
[HCTR][04:20:45.822][ERROR][RK0][tid #139632608016128]: init per replica done
[HCTR][04:20:45.822][ERROR][RK0][main]: init per replica done
[HCTR][04:20:45.825][ERROR][RK0][tid #139632473798400]: init per replica done
