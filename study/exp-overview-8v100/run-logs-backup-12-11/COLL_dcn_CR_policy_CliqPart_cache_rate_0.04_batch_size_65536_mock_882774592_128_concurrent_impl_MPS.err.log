2022-12-11 19:40:58.744735: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:58.750150: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:58.760441: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:58.764598: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:58.770734: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:58.777379: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:58.789667: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:58.795921: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:58.855487: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:58.863452: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:58.872739: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:58.877963: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:58.879650: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:58.879968: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:58.882028: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:58.882080: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:58.884686: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:58.884757: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:58.886924: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:58.887274: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:58.889110: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:58.889591: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:58.891240: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:58.891902: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:58.893463: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:58.894192: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:58.895588: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:58.896323: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:58.897845: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:58.899946: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:58.901678: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:58.904613: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:58.907565: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:58.908830: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:58.909727: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:58.910726: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:58.911661: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:58.912736: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:58.914808: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:58.916317: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:58.920423: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:58.921711: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:58.921859: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 19:40:58.922669: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:58.923664: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:58.924655: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:58.925962: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:58.927063: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:58.928178: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:58.931873: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:58.932504: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:58.933447: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:58.934923: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:58.935725: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:58.936586: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:58.937153: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:58.938949: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:58.939327: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:58.941415: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:58.941764: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:58.943379: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:58.943920: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:58.944303: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:58.945907: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:58.946660: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:58.947089: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:58.948427: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:58.949568: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:58.949893: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:58.951017: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:58.952316: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:58.953560: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:58.954916: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:58.955970: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:58.957996: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:58.958058: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:58.959705: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:58.960052: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:58.961295: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:58.961526: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:58.963304: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:58.964103: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:58.972995: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:58.973323: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:58.975501: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:58.976657: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:58.977932: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:58.978122: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:58.979647: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:58.980762: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:58.981484: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:58.982271: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:58.982470: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:58.996977: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:59.000836: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:59.018048: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:59.019195: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:59.019657: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:59.019679: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:59.019929: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:59.021972: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:59.022056: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:59.023483: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:59.025064: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:59.025296: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:59.025848: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:59.026084: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:59.027424: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:59.027602: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:59.030907: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:59.031167: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:59.031349: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:59.032835: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:59.033508: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:59.033603: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:59.036767: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:59.036808: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:59.036905: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:59.037843: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:59.038023: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:59.038223: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:59.041723: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:59.041990: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:59.042182: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:59.042835: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:59.043026: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:59.043072: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:59.046899: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:59.047244: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:59.047713: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:59.047903: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:59.048237: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:59.048827: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:59.051588: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:59.052109: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:59.052734: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:59.053081: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:59.053704: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:59.055734: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:59.056260: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:59.056687: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:59.056881: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:59.059390: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:59.059859: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:59.060172: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:59.060613: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:59.063394: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:59.064329: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:59.064718: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:59.065189: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:59.067275: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:59.067977: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:59.068248: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:59.068386: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:59.071014: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:59.071979: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:59.072022: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:59.073680: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:59.074148: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:59.075068: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:59.075171: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:59.076085: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 19:40:59.077456: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:59.077884: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:59.078106: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:59.078341: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:59.080894: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:59.081369: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:59.081409: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:59.081873: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:59.085130: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:59.085637: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:59.085931: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:59.086789: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:59.087335: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:59.089305: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:59.089597: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:59.089758: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:59.090744: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:59.091387: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:59.093009: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:59.093472: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:59.093799: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:59.094625: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:59.095191: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:59.097010: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:59.097012: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:59.097733: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:59.098742: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:59.099407: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:59.101397: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:59.101508: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:59.102155: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:59.102293: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:59.103695: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:59.104874: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:59.107744: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:59.107817: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:59.108535: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:59.108829: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:59.109649: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:59.110622: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:59.112979: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:59.113138: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:59.114346: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:59.114872: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:59.115628: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:59.117731: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:59.117816: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:59.118253: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 19:40:59.119088: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:59.119534: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:59.120869: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:59.122502: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:59.122658: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:59.123670: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:59.127316: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:59.127452: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:59.127665: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:59.128553: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:59.128623: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 19:40:59.129696: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 19:40:59.131639: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:59.131776: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:59.132174: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:59.132663: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:59.135868: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:59.135899: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:59.136369: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:59.136978: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:59.138290: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:59.139149: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:59.139965: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:59.140036: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:59.141273: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:59.143106: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:59.157447: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:59.158761: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:59.159368: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:59.160916: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:59.163486: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:59.164978: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:59.167327: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:59.167954: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:59.168935: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:59.174112: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:59.174496: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:59.175397: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:59.209411: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:59.209672: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:59.210903: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:59.245388: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:59.245426: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:59.246395: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:59.252706: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:59.253033: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:59.254452: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:59.260499: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:59.261047: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:59.261776: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:59.266113: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:59.271221: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:59.272435: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:59.277129: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:59.279212: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:59.280867: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 19:40:59.289259: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:59.290211: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:59.291054: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:59.296072: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:59.297036: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:59.297278: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:59.301512: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:59.302573: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:59.302848: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:59.342395: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:59.344269: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:59.349564: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:59.350833: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:59.410768: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:59.412751: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 19:40:59.420993: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 19:40:59.422483: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:59.428574: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:59.430647: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:59.435792: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:59.437880: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:40:59.446261: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:41:00.338383: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:41:00.339253: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:41:00.339794: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:41:00.340263: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 19:41:00.340323: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-11 19:41:00.358991: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:41:00.359878: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:41:00.360710: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:41:00.361488: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:41:00.362012: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:41:00.362685: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-11 19:41:00.407428: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 19:41:00.407633: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 19:41:00.467806: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12340
2022-12-11 19:41:00.533099: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:41:00.533905: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:41:00.534659: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:41:00.535124: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 19:41:00.535194: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-11 19:41:00.553373: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:41:00.554016: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:41:00.554517: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:41:00.555284: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:41:00.555999: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:41:00.556473: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:1/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-11 19:41:00.614124: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:41:00.614741: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:41:00.615291: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:41:00.615762: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 19:41:00.615816: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-11 19:41:00.633982: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:41:00.635016: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:41:00.635654: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:41:00.636258: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:41:00.636780: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:41:00.637259: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:2/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-11 19:41:00.645035: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 19:41:00.645218: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 19:41:00.646190: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12341
2022-12-11 19:41:00.646228: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:41:00.647107: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:41:00.647895: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:41:00.648609: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 19:41:00.648669: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-11 19:41:00.652646: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:41:00.653264: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:41:00.654069: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:41:00.654544: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 19:41:00.654595: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-11 19:41:00.666452: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:41:00.667097: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:41:00.667669: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:41:00.668265: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:41:00.668779: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:41:00.669252: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:4/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-11 19:41:00.672365: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:41:00.673727: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:41:00.674252: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:41:00.674815: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:41:00.675364: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:41:00.676081: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:3/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-11 19:41:00.681338: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:41:00.682062: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:41:00.682617: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:41:00.683089: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 19:41:00.683151: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-11 19:41:00.700976: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:41:00.701616: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:41:00.702136: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:41:00.702709: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:41:00.703260: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:41:00.703734: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:6/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-11 19:41:00.706199: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:41:00.706811: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:41:00.707364: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:41:00.707829: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 19:41:00.707882: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-11 19:41:00.714258: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 19:41:00.714435: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 19:41:00.715506: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12344
2022-12-11 19:41:00.715743: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:41:00.716432: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:41:00.716985: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:41:00.717455: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 19:41:00.717509: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-11 19:41:00.721348: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 19:41:00.721513: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 19:41:00.722353: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12343
2022-12-11 19:41:00.725311: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:41:00.725952: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:41:00.726456: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:41:00.727039: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:41:00.727384: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 19:41:00.727534: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 19:41:00.727564: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:41:00.728038: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:5/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-11 19:41:00.728559: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12342
2022-12-11 19:41:00.735105: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:41:00.735760: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:41:00.736284: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:41:00.736855: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:41:00.737381: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:41:00.737852: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:7/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-11 19:41:00.749033: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 19:41:00.749214: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 19:41:00.750332: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12346
2022-12-11 19:41:00.771064: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 19:41:00.771222: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 19:41:00.773039: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12345
2022-12-11 19:41:00.782178: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 19:41:00.782359: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 19:41:00.783420: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12347
[HCTR][19:41:02.057][ERROR][RK0][main]: using mock embedding with 882774585 * 128 elements
[HCTR][19:41:02.057][ERROR][RK0][main]: using mock embedding with 882774585 * 128 elements
[HCTR][19:41:02.057][ERROR][RK0][main]: using mock embedding with 882774585 * 128 elements
[HCTR][19:41:02.059][ERROR][RK0][main]: using mock embedding with 882774585 * 128 elements
[HCTR][19:41:02.060][ERROR][RK0][main]: using mock embedding with 882774585 * 128 elements
[HCTR][19:41:02.060][ERROR][RK0][main]: using mock embedding with 882774585 * 128 elements
[HCTR][19:41:02.060][ERROR][RK0][main]: using mock embedding with 882774585 * 128 elements
[HCTR][19:41:02.060][ERROR][RK0][main]: using mock embedding with 882774585 * 128 elements
warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 1it [00:00,  2.54it/s]warmup run: 1it [00:00,  2.53it/s]warmup run: 1it [00:00,  2.54it/s]warmup run: 1it [00:00,  2.51it/s]warmup run: 1it [00:00,  2.39it/s]warmup run: 64it [00:00, 169.11it/s]warmup run: 64it [00:00, 169.11it/s]warmup run: 69it [00:00, 180.78it/s]warmup run: 66it [00:00, 174.59it/s]warmup run: 61it [00:00, 153.95it/s]warmup run: 112it [00:00, 254.12it/s]warmup run: 113it [00:00, 256.03it/s]warmup run: 120it [00:00, 270.51it/s]warmup run: 115it [00:00, 261.21it/s]warmup run: 108it [00:00, 237.73it/s]warmup run: 158it [00:00, 311.36it/s]warmup run: 161it [00:00, 317.38it/s]warmup run: 170it [00:00, 334.29it/s]warmup run: 166it [00:00, 330.65it/s]warmup run: 158it [00:00, 308.44it/s]warmup run: 207it [00:00, 362.10it/s]warmup run: 207it [00:00, 357.56it/s]warmup run: 220it [00:00, 381.09it/s]warmup run: 218it [00:00, 383.39it/s]warmup run: 212it [00:00, 372.10it/s]warmup run: 257it [00:00, 399.76it/s]warmup run: 260it [00:00, 406.98it/s]warmup run: 270it [00:00, 414.59it/s]warmup run: 275it [00:00, 436.09it/s]warmup run: 266it [00:00, 418.03it/s]warmup run: 313it [00:00, 442.51it/s]warmup run: 306it [00:01, 423.80it/s]warmup run: 320it [00:01, 437.87it/s]warmup run: 332it [00:00, 473.13it/s]warmup run: 319it [00:01, 449.93it/s]warmup run: 366it [00:01, 468.12it/s]warmup run: 355it [00:01, 441.10it/s]warmup run: 370it [00:01, 454.43it/s]warmup run: 389it [00:01, 498.93it/s]warmup run: 372it [00:01, 471.71it/s]warmup run: 418it [00:01, 483.30it/s]warmup run: 403it [00:01, 449.11it/s]warmup run: 420it [00:01, 465.72it/s]warmup run: 446it [00:01, 517.47it/s]warmup run: 424it [00:01, 483.24it/s]warmup run: 469it [00:01, 485.04it/s]warmup run: 457it [00:01, 475.57it/s]warmup run: 472it [00:01, 479.08it/s]warmup run: 503it [00:01, 530.23it/s]warmup run: 476it [00:01, 493.13it/s]warmup run: 1it [00:01,  1.38s/it]warmup run: 1it [00:01,  1.39s/it]warmup run: 1it [00:01,  1.39s/it]warmup run: 522it [00:01, 496.44it/s]warmup run: 510it [00:01, 490.75it/s]warmup run: 560it [00:01, 540.14it/s]warmup run: 522it [00:01, 480.04it/s]warmup run: 529it [00:01, 502.12it/s]warmup run: 99it [00:01, 92.43it/s]warmup run: 96it [00:01, 89.28it/s]warmup run: 89it [00:01, 82.65it/s]warmup run: 565it [00:01, 507.74it/s]warmup run: 577it [00:01, 510.34it/s]warmup run: 617it [00:01, 547.54it/s]warmup run: 580it [00:01, 508.19it/s]warmup run: 584it [00:01, 515.30it/s]warmup run: 196it [00:01, 195.97it/s]warmup run: 190it [00:01, 189.31it/s]warmup run: 177it [00:01, 176.23it/s]warmup run: 620it [00:01, 518.95it/s]warmup run: 632it [00:01, 519.99it/s]warmup run: 674it [00:01, 553.99it/s]warmup run: 637it [00:01, 525.18it/s]warmup run: 637it [00:01, 512.12it/s]warmup run: 291it [00:01, 304.52it/s]warmup run: 287it [00:01, 301.15it/s]warmup run: 269it [00:01, 282.87it/s]warmup run: 675it [00:01, 527.93it/s]warmup run: 688it [00:01, 529.21it/s]warmup run: 693it [00:01, 535.31it/s]warmup run: 731it [00:01, 553.44it/s]warmup run: 689it [00:01, 504.13it/s]warmup run: 386it [00:01, 414.27it/s]warmup run: 384it [00:01, 414.48it/s]warmup run: 364it [00:01, 396.01it/s]warmup run: 730it [00:01, 532.79it/s]warmup run: 742it [00:01, 526.55it/s]warmup run: 788it [00:01, 557.90it/s]warmup run: 748it [00:01, 529.40it/s]warmup run: 740it [00:01, 500.31it/s]warmup run: 481it [00:01, 518.83it/s]warmup run: 481it [00:01, 522.81it/s]warmup run: 460it [00:01, 505.44it/s]warmup run: 785it [00:01, 535.26it/s]warmup run: 795it [00:01, 525.66it/s]warmup run: 845it [00:01, 560.37it/s]warmup run: 802it [00:01, 531.47it/s]warmup run: 791it [00:01, 498.04it/s]warmup run: 576it [00:01, 611.74it/s]warmup run: 579it [00:01, 621.42it/s]warmup run: 556it [00:01, 604.03it/s]warmup run: 839it [00:02, 530.83it/s]warmup run: 902it [00:02, 562.31it/s]warmup run: 856it [00:02, 531.39it/s]warmup run: 848it [00:02, 511.98it/s]warmup run: 842it [00:02, 491.82it/s]warmup run: 671it [00:02, 691.65it/s]warmup run: 651it [00:02, 685.17it/s]warmup run: 678it [00:02, 707.46it/s]warmup run: 896it [00:02, 541.19it/s]warmup run: 959it [00:02, 558.74it/s]warmup run: 910it [00:02, 529.93it/s]warmup run: 900it [00:02, 504.53it/s]warmup run: 892it [00:02, 493.57it/s]warmup run: 766it [00:02, 755.37it/s]warmup run: 748it [00:02, 756.78it/s]warmup run: 776it [00:02, 774.97it/s]warmup run: 955it [00:02, 555.21it/s]warmup run: 1016it [00:02, 560.22it/s]warmup run: 964it [00:02, 528.22it/s]warmup run: 951it [00:02, 496.09it/s]warmup run: 942it [00:02, 494.65it/s]warmup run: 862it [00:02, 808.73it/s]warmup run: 844it [00:02, 809.50it/s]warmup run: 872it [00:02, 781.86it/s]warmup run: 1013it [00:02, 561.90it/s]warmup run: 1073it [00:02, 561.06it/s]warmup run: 1018it [00:02, 528.45it/s]warmup run: 1001it [00:02, 486.00it/s]warmup run: 995it [00:02, 502.86it/s]warmup run: 958it [00:02, 848.38it/s]warmup run: 941it [00:02, 851.27it/s]warmup run: 967it [00:02, 825.87it/s]warmup run: 1071it [00:02, 565.66it/s]warmup run: 1130it [00:02, 561.53it/s]warmup run: 1071it [00:02, 528.15it/s]warmup run: 1050it [00:02, 478.85it/s]warmup run: 1050it [00:02, 515.61it/s]warmup run: 1053it [00:02, 870.74it/s]warmup run: 1038it [00:02, 882.99it/s]warmup run: 1065it [00:02, 865.65it/s]warmup run: 1128it [00:02, 565.41it/s]warmup run: 1188it [00:02, 564.34it/s]warmup run: 1126it [00:02, 534.24it/s]warmup run: 1103it [00:02, 491.50it/s]warmup run: 1103it [00:02, 519.64it/s]warmup run: 1147it [00:02, 888.73it/s]warmup run: 1134it [00:02, 883.09it/s]warmup run: 1160it [00:02, 887.28it/s]warmup run: 1185it [00:02, 564.25it/s]warmup run: 1246it [00:02, 566.68it/s]warmup run: 1183it [00:02, 544.36it/s]warmup run: 1158it [00:02, 508.34it/s]warmup run: 1156it [00:02, 515.99it/s]warmup run: 1242it [00:02, 904.02it/s]warmup run: 1259it [00:02, 914.82it/s]warmup run: 1242it [00:02, 554.88it/s]warmup run: 1304it [00:02, 568.32it/s]warmup run: 1238it [00:02, 542.60it/s]warmup run: 1214it [00:02, 522.55it/s]warmup run: 1209it [00:02, 518.76it/s]warmup run: 1228it [00:02, 721.99it/s]warmup run: 1336it [00:02, 902.15it/s]warmup run: 1359it [00:02, 937.05it/s]warmup run: 1361it [00:02, 567.52it/s]warmup run: 1298it [00:02, 547.72it/s]warmup run: 1293it [00:02, 541.34it/s]warmup run: 1271it [00:02, 534.88it/s]warmup run: 1264it [00:02, 525.72it/s]warmup run: 1418it [00:02, 561.09it/s]warmup run: 1353it [00:02, 543.33it/s]warmup run: 1348it [00:02, 540.95it/s]warmup run: 1309it [00:02, 646.47it/s]warmup run: 1328it [00:02, 543.91it/s]warmup run: 1322it [00:02, 540.22it/s]warmup run: 1429it [00:02, 737.22it/s]warmup run: 1456it [00:02, 799.69it/s]warmup run: 1476it [00:03, 564.14it/s]warmup run: 1408it [00:03, 531.87it/s]warmup run: 1403it [00:03, 530.58it/s]warmup run: 1383it [00:03, 541.85it/s]warmup run: 1380it [00:03, 549.80it/s]warmup run: 1381it [00:03, 615.02it/s]warmup run: 1542it [00:03, 720.11it/s]warmup run: 1533it [00:03, 560.03it/s]warmup run: 1510it [00:03, 653.12it/s]warmup run: 1462it [00:03, 522.12it/s]warmup run: 1457it [00:03, 515.72it/s]warmup run: 1438it [00:03, 533.00it/s]warmup run: 1438it [00:03, 557.43it/s]warmup run: 1447it [00:03, 575.63it/s]warmup run: 1590it [00:03, 561.85it/s]warmup run: 1515it [00:03, 518.70it/s]warmup run: 1509it [00:03, 514.55it/s]warmup run: 1619it [00:03, 671.48it/s]warmup run: 1496it [00:03, 562.22it/s]warmup run: 1492it [00:03, 525.36it/s]warmup run: 1582it [00:03, 600.97it/s]warmup run: 1647it [00:03, 562.99it/s]warmup run: 1508it [00:03, 556.29it/s]warmup run: 1568it [00:03, 521.69it/s]warmup run: 1562it [00:03, 516.32it/s]warmup run: 1554it [00:03, 565.17it/s]warmup run: 1545it [00:03, 521.12it/s]warmup run: 1690it [00:03, 641.17it/s]warmup run: 1647it [00:03, 579.64it/s]warmup run: 1704it [00:03, 563.72it/s]warmup run: 1622it [00:03, 525.16it/s]warmup run: 1566it [00:03, 541.13it/s]warmup run: 1615it [00:03, 519.62it/s]warmup run: 1612it [00:03, 567.21it/s]warmup run: 1598it [00:03, 507.18it/s]warmup run: 1757it [00:03, 618.29it/s]warmup run: 1708it [00:03, 562.84it/s]warmup run: 1761it [00:03, 563.60it/s]warmup run: 1677it [00:03, 530.10it/s]warmup run: 1669it [00:03, 523.36it/s]warmup run: 1622it [00:03, 526.14it/s]warmup run: 1669it [00:03, 562.85it/s]warmup run: 1651it [00:03, 511.28it/s]warmup run: 1821it [00:03, 604.20it/s]warmup run: 1818it [00:03, 564.37it/s]warmup run: 1767it [00:03, 547.37it/s]warmup run: 1733it [00:03, 536.65it/s]warmup run: 1722it [00:03, 522.93it/s]warmup run: 1726it [00:03, 557.98it/s]warmup run: 1676it [00:03, 510.53it/s]warmup run: 1703it [00:03, 503.35it/s]warmup run: 1883it [00:03, 593.06it/s]warmup run: 1875it [00:03, 559.75it/s]warmup run: 1823it [00:03, 531.79it/s]warmup run: 1789it [00:03, 543.31it/s]warmup run: 1775it [00:03, 523.66it/s]warmup run: 1782it [00:03, 550.17it/s]warmup run: 1731it [00:03, 520.58it/s]warmup run: 1755it [00:03, 506.29it/s]warmup run: 1943it [00:03, 585.38it/s]warmup run: 1931it [00:03, 559.70it/s]warmup run: 1846it [00:03, 548.89it/s]warmup run: 1877it [00:03, 531.24it/s]warmup run: 1828it [00:03, 523.35it/s]warmup run: 1839it [00:03, 554.52it/s]warmup run: 1784it [00:03, 510.61it/s]warmup run: 1808it [00:03, 510.90it/s]warmup run: 2002it [00:03, 580.05it/s]warmup run: 1987it [00:03, 559.41it/s]warmup run: 1903it [00:03, 553.61it/s]warmup run: 1931it [00:03, 530.36it/s]warmup run: 1881it [00:03, 521.02it/s]warmup run: 1895it [00:03, 547.42it/s]warmup run: 1836it [00:04, 505.03it/s]warmup run: 1860it [00:04, 504.84it/s]warmup run: 2044it [00:04, 560.89it/s]warmup run: 2061it [00:04, 576.86it/s]warmup run: 1960it [00:04, 557.58it/s]warmup run: 1985it [00:04, 529.78it/s]warmup run: 1935it [00:04, 525.54it/s]warmup run: 1950it [00:04, 543.59it/s]warmup run: 1887it [00:04, 501.67it/s]warmup run: 1911it [00:04, 499.38it/s]warmup run: 2101it [00:04, 562.28it/s]warmup run: 2119it [00:04, 573.51it/s]warmup run: 2017it [00:04, 558.58it/s]warmup run: 2039it [00:04, 527.10it/s]warmup run: 1990it [00:04, 526.31it/s]warmup run: 2006it [00:04, 546.18it/s]warmup run: 1943it [00:04, 517.42it/s]warmup run: 1968it [00:04, 517.52it/s]warmup run: 2159it [00:04, 564.97it/s]warmup run: 2177it [00:04, 571.08it/s]warmup run: 2073it [00:04, 548.35it/s]warmup run: 2092it [00:04, 522.92it/s]warmup run: 2047it [00:04, 537.87it/s]warmup run: 2064it [00:04, 553.45it/s]warmup run: 2000it [00:04, 530.82it/s]warmup run: 2025it [00:04, 531.08it/s]warmup run: 2216it [00:04, 566.35it/s]warmup run: 2235it [00:04, 569.31it/s]warmup run: 2145it [00:04, 524.39it/s]warmup run: 2104it [00:04, 545.01it/s]warmup run: 2128it [00:04, 532.76it/s]warmup run: 2120it [00:04, 553.52it/s]warmup run: 2057it [00:04, 541.43it/s]warmup run: 2079it [00:04, 530.17it/s]warmup run: 2274it [00:04, 568.96it/s]warmup run: 2292it [00:04, 568.01it/s]warmup run: 2202it [00:04, 537.21it/s]warmup run: 2159it [00:04, 545.07it/s]warmup run: 2184it [00:04, 540.34it/s]warmup run: 2176it [00:04, 555.37it/s]warmup run: 2115it [00:04, 550.51it/s]warmup run: 2133it [00:04, 524.41it/s]warmup run: 2332it [00:04, 569.57it/s]warmup run: 2349it [00:04, 565.25it/s]warmup run: 2256it [00:04, 535.75it/s]warmup run: 2217it [00:04, 553.47it/s]warmup run: 2239it [00:04, 535.29it/s]warmup run: 2232it [00:04, 553.90it/s]warmup run: 2173it [00:04, 557.01it/s]warmup run: 2186it [00:04, 513.77it/s]warmup run: 2389it [00:04, 567.91it/s]warmup run: 2406it [00:04, 559.67it/s]warmup run: 2310it [00:04, 527.91it/s]warmup run: 2273it [00:04, 546.13it/s]warmup run: 2293it [00:04, 525.96it/s]warmup run: 2288it [00:04, 541.11it/s]warmup run: 2231it [00:04, 561.75it/s]warmup run: 2240it [00:04, 519.96it/s]warmup run: 2446it [00:04, 566.11it/s]warmup run: 2463it [00:04, 562.43it/s]warmup run: 2364it [00:04, 531.21it/s]warmup run: 2330it [00:04, 551.64it/s]warmup run: 2346it [00:04, 517.43it/s]warmup run: 2343it [00:04, 543.06it/s]warmup run: 2289it [00:04, 565.07it/s]warmup run: 2296it [00:04, 529.58it/s]warmup run: 2503it [00:04, 562.54it/s]warmup run: 2520it [00:04, 559.79it/s]warmup run: 2418it [00:04, 526.92it/s]warmup run: 2387it [00:04, 554.83it/s]warmup run: 2398it [00:04, 514.23it/s]warmup run: 2399it [00:04, 545.70it/s]warmup run: 2346it [00:04, 565.72it/s]warmup run: 2352it [00:04, 537.71it/s]warmup run: 2560it [00:04, 561.52it/s]warmup run: 2576it [00:04, 558.33it/s]warmup run: 2444it [00:04, 556.64it/s]warmup run: 2471it [00:04, 517.65it/s]warmup run: 2450it [00:05, 506.35it/s]warmup run: 2454it [00:05, 542.86it/s]warmup run: 2404it [00:05, 567.81it/s]warmup run: 2410it [00:05, 549.47it/s]warmup run: 2617it [00:05, 560.85it/s]warmup run: 2633it [00:05, 561.59it/s]warmup run: 2501it [00:05, 557.93it/s]warmup run: 2523it [00:05, 512.95it/s]warmup run: 2501it [00:05, 502.26it/s]warmup run: 2461it [00:05, 567.40it/s]warmup run: 2509it [00:05, 532.90it/s]warmup run: 2468it [00:05, 557.85it/s]warmup run: 2557it [00:05, 550.56it/s]warmup run: 2577it [00:05, 520.59it/s]warmup run: 2552it [00:05, 499.35it/s]warmup run: 2519it [00:05, 568.17it/s]warmup run: 2563it [00:05, 522.90it/s]warmup run: 2526it [00:05, 561.55it/s]warmup run: 2613it [00:05, 550.60it/s]warmup run: 2634it [00:05, 533.13it/s]warmup run: 2602it [00:05, 495.83it/s]warmup run: 2576it [00:05, 568.10it/s]warmup run: 2583it [00:05, 563.29it/s]warmup run: 2616it [00:05, 514.63it/s]warmup run: 2674it [00:05, 361.61it/s]warmup run: 2690it [00:05, 330.33it/s]warmup run: 2633it [00:05, 562.68it/s]warmup run: 2730it [00:05, 403.83it/s]warmup run: 2744it [00:05, 370.85it/s]warmup run: 2786it [00:05, 439.20it/s]warmup run: 2688it [00:05, 340.76it/s]warmup run: 2669it [00:05, 349.39it/s]warmup run: 2652it [00:05, 318.23it/s]warmup run: 2796it [00:05, 402.20it/s]warmup run: 2668it [00:05, 334.64it/s]warmup run: 2640it [00:05, 359.80it/s]warmup run: 2843it [00:05, 470.43it/s]warmup run: 2743it [00:05, 384.63it/s]warmup run: 2724it [00:05, 390.49it/s]warmup run: 2690it [00:05, 366.03it/s]warmup run: 2702it [00:05, 356.11it/s]warmup run: 2848it [00:05, 430.11it/s]warmup run: 2697it [00:05, 404.49it/s]warmup run: 2726it [00:05, 385.06it/s]warmup run: 2901it [00:05, 497.57it/s]warmup run: 2800it [00:05, 427.30it/s]warmup run: 2779it [00:05, 426.96it/s]warmup run: 2747it [00:05, 408.93it/s]warmup run: 2751it [00:05, 386.78it/s]warmup run: 2902it [00:05, 456.99it/s]warmup run: 2783it [00:05, 427.71it/s]warmup run: 2753it [00:05, 439.43it/s]warmup run: 2958it [00:05, 515.02it/s]warmup run: 2858it [00:05, 463.61it/s]warmup run: 2834it [00:05, 456.61it/s]warmup run: 2804it [00:05, 445.75it/s]warmup run: 2800it [00:05, 411.39it/s]warmup run: 2959it [00:05, 484.94it/s]warmup run: 2808it [00:05, 466.15it/s]warmup run: 2841it [00:05, 463.98it/s]warmup run: 3000it [00:05, 505.73it/s]warmup run: 2913it [00:05, 484.02it/s]warmup run: 3000it [00:05, 500.56it/s]warmup run: 2891it [00:05, 484.19it/s]warmup run: 2858it [00:06, 467.95it/s]warmup run: 2851it [00:06, 435.69it/s]warmup run: 2893it [00:06, 478.23it/s]warmup run: 2863it [00:06, 486.50it/s]warmup run: 2971it [00:06, 508.62it/s]warmup run: 2944it [00:06, 493.86it/s]warmup run: 2910it [00:06, 476.96it/s]warmup run: 2902it [00:06, 453.31it/s]warmup run: 2951it [00:06, 504.18it/s]warmup run: 2921it [00:06, 509.63it/s]warmup run: 3000it [00:06, 487.56it/s]warmup run: 2997it [00:06, 501.52it/s]warmup run: 2962it [00:06, 487.54it/s]warmup run: 3000it [00:06, 482.72it/s]warmup run: 2956it [00:06, 475.90it/s]warmup run: 3000it [00:06, 481.86it/s]warmup run: 2975it [00:06, 517.60it/s]warmup run: 3000it [00:06, 477.52it/s]warmup run: 3000it [00:06, 476.78it/s]warmup run: 3000it [00:06, 475.47it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]




warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   6%|▌         | 171/3000 [00:00<00:01, 1706.95it/s]warmup should be done:   6%|▌         | 166/3000 [00:00<00:01, 1657.53it/s]warmup should be done:   6%|▌         | 167/3000 [00:00<00:01, 1664.10it/s]warmup should be done:   6%|▌         | 168/3000 [00:00<00:01, 1675.51it/s]warmup should be done:   6%|▌         | 170/3000 [00:00<00:01, 1691.94it/s]warmup should be done:   6%|▌         | 169/3000 [00:00<00:01, 1680.79it/s]warmup should be done:   6%|▌         | 172/3000 [00:00<00:01, 1710.21it/s]warmup should be done:   6%|▌         | 168/3000 [00:00<00:01, 1674.09it/s]warmup should be done:  11%|█         | 333/3000 [00:00<00:01, 1662.69it/s]warmup should be done:  11%|█▏        | 339/3000 [00:00<00:01, 1691.47it/s]warmup should be done:  11%|█         | 337/3000 [00:00<00:01, 1682.54it/s]warmup should be done:  12%|█▏        | 345/3000 [00:00<00:01, 1718.80it/s]warmup should be done:  11%|█▏        | 339/3000 [00:00<00:01, 1691.47it/s]warmup should be done:  11%|█▏        | 338/3000 [00:00<00:01, 1684.08it/s]warmup should be done:  11%|█▏        | 340/3000 [00:00<00:01, 1687.82it/s]warmup should be done:  11%|█▏        | 342/3000 [00:00<00:01, 1690.70it/s]warmup should be done:  17%|█▋        | 500/3000 [00:00<00:01, 1661.56it/s]warmup should be done:  17%|█▋        | 510/3000 [00:00<00:01, 1699.23it/s]warmup should be done:  17%|█▋        | 518/3000 [00:00<00:01, 1720.85it/s]warmup should be done:  17%|█▋        | 508/3000 [00:00<00:01, 1688.07it/s]warmup should be done:  17%|█▋        | 509/3000 [00:00<00:01, 1686.20it/s]warmup should be done:  17%|█▋        | 506/3000 [00:00<00:01, 1676.38it/s]warmup should be done:  17%|█▋        | 509/3000 [00:00<00:01, 1685.41it/s]warmup should be done:  17%|█▋        | 512/3000 [00:00<00:01, 1685.91it/s]warmup should be done:  23%|██▎       | 682/3000 [00:00<00:01, 1704.90it/s]warmup should be done:  22%|██▏       | 667/3000 [00:00<00:01, 1659.92it/s]warmup should be done:  23%|██▎       | 691/3000 [00:00<00:01, 1720.41it/s]warmup should be done:  23%|██▎       | 678/3000 [00:00<00:01, 1689.65it/s]warmup should be done:  23%|██▎       | 678/3000 [00:00<00:01, 1685.49it/s]warmup should be done:  22%|██▏       | 674/3000 [00:00<00:01, 1676.45it/s]warmup should be done:  23%|██▎       | 678/3000 [00:00<00:01, 1684.82it/s]warmup should be done:  23%|██▎       | 681/3000 [00:00<00:01, 1686.99it/s]warmup should be done:  28%|██▊       | 853/3000 [00:00<00:01, 1702.33it/s]warmup should be done:  28%|██▊       | 847/3000 [00:00<00:01, 1687.70it/s]warmup should be done:  28%|██▊       | 847/3000 [00:00<00:01, 1683.60it/s]warmup should be done:  28%|██▊       | 847/3000 [00:00<00:01, 1683.23it/s]warmup should be done:  29%|██▉       | 864/3000 [00:00<00:01, 1717.48it/s]warmup should be done:  28%|██▊       | 842/3000 [00:00<00:01, 1671.53it/s]warmup should be done:  28%|██▊       | 833/3000 [00:00<00:01, 1645.07it/s]warmup should be done:  28%|██▊       | 850/3000 [00:00<00:01, 1677.28it/s]warmup should be done:  34%|███▍      | 1016/3000 [00:00<00:01, 1687.46it/s]warmup should be done:  34%|███▍      | 1024/3000 [00:00<00:01, 1702.07it/s]warmup should be done:  35%|███▍      | 1036/3000 [00:00<00:01, 1716.73it/s]warmup should be done:  34%|███▍      | 1016/3000 [00:00<00:01, 1682.51it/s]warmup should be done:  34%|███▍      | 1016/3000 [00:00<00:01, 1681.36it/s]warmup should be done:  34%|███▎      | 1010/3000 [00:00<00:01, 1671.97it/s]warmup should be done:  33%|███▎      | 998/3000 [00:00<00:01, 1645.70it/s]warmup should be done:  34%|███▍      | 1018/3000 [00:00<00:01, 1673.55it/s]warmup should be done:  40%|███▉      | 1185/3000 [00:00<00:01, 1687.96it/s]warmup should be done:  40%|████      | 1208/3000 [00:00<00:01, 1716.30it/s]warmup should be done:  40%|███▉      | 1185/3000 [00:00<00:01, 1682.20it/s]warmup should be done:  39%|███▉      | 1178/3000 [00:00<00:01, 1671.90it/s]warmup should be done:  40%|███▉      | 1185/3000 [00:00<00:01, 1678.70it/s]warmup should be done:  39%|███▉      | 1164/3000 [00:00<00:01, 1649.49it/s]warmup should be done:  40%|███▉      | 1195/3000 [00:00<00:01, 1691.04it/s]warmup should be done:  40%|███▉      | 1186/3000 [00:00<00:01, 1671.95it/s]warmup should be done:  45%|████▌     | 1354/3000 [00:00<00:00, 1687.04it/s]warmup should be done:  45%|████▌     | 1353/3000 [00:00<00:00, 1677.78it/s]warmup should be done:  45%|████▌     | 1354/3000 [00:00<00:00, 1680.06it/s]warmup should be done:  45%|████▍     | 1346/3000 [00:00<00:00, 1669.69it/s]warmup should be done:  46%|████▌     | 1380/3000 [00:00<00:00, 1706.10it/s]warmup should be done:  44%|████▍     | 1329/3000 [00:00<00:01, 1643.58it/s]warmup should be done:  45%|████▌     | 1355/3000 [00:00<00:00, 1675.25it/s]warmup should be done:  46%|████▌     | 1365/3000 [00:00<00:00, 1681.47it/s]warmup should be done:  51%|█████     | 1523/3000 [00:00<00:00, 1686.34it/s]warmup should be done:  51%|█████     | 1522/3000 [00:00<00:00, 1678.67it/s]warmup should be done:  50%|█████     | 1513/3000 [00:00<00:00, 1668.16it/s]warmup should be done:  50%|████▉     | 1495/3000 [00:00<00:00, 1648.36it/s]warmup should be done:  51%|█████     | 1524/3000 [00:00<00:00, 1679.33it/s]warmup should be done:  52%|█████▏    | 1551/3000 [00:00<00:00, 1698.07it/s]warmup should be done:  51%|█████     | 1523/3000 [00:00<00:00, 1668.67it/s]warmup should be done:  51%|█████     | 1534/3000 [00:00<00:00, 1642.35it/s]warmup should be done:  56%|█████▋    | 1692/3000 [00:01<00:00, 1679.47it/s]warmup should be done:  56%|█████▋    | 1690/3000 [00:01<00:00, 1678.66it/s]warmup should be done:  56%|█████▌    | 1680/3000 [00:01<00:00, 1666.07it/s]warmup should be done:  55%|█████▌    | 1661/3000 [00:01<00:00, 1651.58it/s]warmup should be done:  56%|█████▋    | 1692/3000 [00:01<00:00, 1678.35it/s]warmup should be done:  57%|█████▋    | 1721/3000 [00:01<00:00, 1689.56it/s]warmup should be done:  56%|█████▋    | 1690/3000 [00:01<00:00, 1659.97it/s]warmup should be done:  57%|█████▋    | 1699/3000 [00:01<00:00, 1613.53it/s]warmup should be done:  62%|██████▏   | 1861/3000 [00:01<00:00, 1680.53it/s]warmup should be done:  62%|██████▏   | 1858/3000 [00:01<00:00, 1678.21it/s]warmup should be done:  62%|██████▏   | 1847/3000 [00:01<00:00, 1665.11it/s]warmup should be done:  62%|██████▏   | 1861/3000 [00:01<00:00, 1680.70it/s]warmup should be done:  61%|██████    | 1827/3000 [00:01<00:00, 1645.61it/s]warmup should be done:  62%|██████▏   | 1858/3000 [00:01<00:00, 1664.21it/s]warmup should be done:  63%|██████▎   | 1890/3000 [00:01<00:00, 1686.29it/s]warmup should be done:  62%|██████▏   | 1861/3000 [00:01<00:00, 1594.26it/s]warmup should be done:  68%|██████▊   | 2026/3000 [00:01<00:00, 1678.58it/s]warmup should be done:  68%|██████▊   | 2030/3000 [00:01<00:00, 1681.27it/s]warmup should be done:  67%|██████▋   | 2014/3000 [00:01<00:00, 1666.55it/s]warmup should be done:  66%|██████▋   | 1993/3000 [00:01<00:00, 1649.63it/s]warmup should be done:  68%|██████▊   | 2030/3000 [00:01<00:00, 1679.77it/s]warmup should be done:  68%|██████▊   | 2026/3000 [00:01<00:00, 1668.82it/s]warmup should be done:  69%|██████▊   | 2059/3000 [00:01<00:00, 1684.33it/s]warmup should be done:  67%|██████▋   | 2021/3000 [00:01<00:00, 1581.02it/s]warmup should be done:  73%|███████▎  | 2194/3000 [00:01<00:00, 1678.83it/s]warmup should be done:  73%|███████▎  | 2199/3000 [00:01<00:00, 1682.17it/s]warmup should be done:  73%|███████▎  | 2181/3000 [00:01<00:00, 1667.39it/s]warmup should be done:  72%|███████▏  | 2158/3000 [00:01<00:00, 1642.39it/s]warmup should be done:  73%|███████▎  | 2198/3000 [00:01<00:00, 1673.84it/s]warmup should be done:  73%|███████▎  | 2194/3000 [00:01<00:00, 1671.79it/s]warmup should be done:  74%|███████▍  | 2228/3000 [00:01<00:00, 1683.22it/s]warmup should be done:  73%|███████▎  | 2180/3000 [00:01<00:00, 1573.30it/s]warmup should be done:  79%|███████▊  | 2362/3000 [00:01<00:00, 1678.43it/s]warmup should be done:  79%|███████▉  | 2368/3000 [00:01<00:00, 1682.87it/s]warmup should be done:  78%|███████▊  | 2348/3000 [00:01<00:00, 1667.34it/s]warmup should be done:  78%|███████▊  | 2325/3000 [00:01<00:00, 1648.40it/s]warmup should be done:  79%|███████▉  | 2366/3000 [00:01<00:00, 1674.04it/s]warmup should be done:  79%|███████▊  | 2362/3000 [00:01<00:00, 1673.99it/s]warmup should be done:  80%|███████▉  | 2397/3000 [00:01<00:00, 1682.56it/s]warmup should be done:  78%|███████▊  | 2338/3000 [00:01<00:00, 1567.11it/s]warmup should be done:  84%|████████▍ | 2531/3000 [00:01<00:00, 1679.25it/s]warmup should be done:  85%|████████▍ | 2537/3000 [00:01<00:00, 1683.14it/s]warmup should be done:  84%|████████▍ | 2515/3000 [00:01<00:00, 1667.23it/s]warmup should be done:  84%|████████▍ | 2530/3000 [00:01<00:00, 1675.74it/s]warmup should be done:  83%|████████▎ | 2494/3000 [00:01<00:00, 1658.37it/s]warmup should be done:  84%|████████▍ | 2534/3000 [00:01<00:00, 1666.43it/s]warmup should be done:  86%|████████▌ | 2566/3000 [00:01<00:00, 1681.30it/s]warmup should be done:  83%|████████▎ | 2495/3000 [00:01<00:00, 1564.40it/s]warmup should be done:  90%|████████▉ | 2699/3000 [00:01<00:00, 1679.10it/s]warmup should be done:  89%|████████▉ | 2682/3000 [00:01<00:00, 1667.82it/s]warmup should be done:  90%|█████████ | 2706/3000 [00:01<00:00, 1683.59it/s]warmup should be done:  90%|████████▉ | 2698/3000 [00:01<00:00, 1676.43it/s]warmup should be done:  89%|████████▉ | 2663/3000 [00:01<00:00, 1666.78it/s]warmup should be done:  90%|█████████ | 2701/3000 [00:01<00:00, 1665.69it/s]warmup should be done:  91%|█████████ | 2735/3000 [00:01<00:00, 1682.96it/s]warmup should be done:  88%|████████▊ | 2652/3000 [00:01<00:00, 1562.30it/s]warmup should be done:  96%|█████████▌| 2868/3000 [00:01<00:00, 1681.20it/s]warmup should be done:  96%|█████████▌| 2875/3000 [00:01<00:00, 1684.14it/s]warmup should be done:  95%|█████████▍| 2849/3000 [00:01<00:00, 1658.45it/s]warmup should be done:  96%|█████████▌| 2868/3000 [00:01<00:00, 1680.64it/s]warmup should be done:  94%|█████████▍| 2830/3000 [00:01<00:00, 1659.65it/s]warmup should be done:  96%|█████████▌| 2868/3000 [00:01<00:00, 1665.30it/s]warmup should be done:  97%|█████████▋| 2906/3000 [00:01<00:00, 1690.21it/s]warmup should be done:  94%|█████████▎| 2809/3000 [00:01<00:00, 1560.43it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1697.24it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1684.61it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1680.43it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1677.03it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1674.90it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1667.33it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1670.20it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1655.20it/s]warmup should be done:  99%|█████████▉| 2966/3000 [00:01<00:00, 1561.93it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1614.10it/s]



warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]



warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   6%|▌         | 174/3000 [00:00<00:01, 1736.71it/s]warmup should be done:   6%|▌         | 173/3000 [00:00<00:01, 1725.34it/s]warmup should be done:   6%|▌         | 174/3000 [00:00<00:01, 1735.36it/s]warmup should be done:   5%|▌         | 161/3000 [00:00<00:01, 1601.79it/s]warmup should be done:   6%|▌         | 171/3000 [00:00<00:01, 1701.71it/s]warmup should be done:   6%|▌         | 172/3000 [00:00<00:01, 1711.70it/s]warmup should be done:   6%|▌         | 176/3000 [00:00<00:01, 1750.52it/s]warmup should be done:   6%|▌         | 170/3000 [00:00<00:01, 1691.38it/s]warmup should be done:  12%|█▏        | 346/3000 [00:00<00:01, 1727.46it/s]warmup should be done:  12%|█▏        | 348/3000 [00:00<00:01, 1737.87it/s]warmup should be done:  12%|█▏        | 348/3000 [00:00<00:01, 1735.19it/s]warmup should be done:  11%|█▏        | 344/3000 [00:00<00:01, 1714.88it/s]warmup should be done:  11%|█▏        | 344/3000 [00:00<00:01, 1712.98it/s]warmup should be done:  11%|█▏        | 340/3000 [00:00<00:01, 1692.49it/s]warmup should be done:  11%|█         | 322/3000 [00:00<00:01, 1601.53it/s]warmup should be done:  12%|█▏        | 352/3000 [00:00<00:01, 1751.08it/s]warmup should be done:  17%|█▋        | 519/3000 [00:00<00:01, 1728.27it/s]warmup should be done:  17%|█▋        | 522/3000 [00:00<00:01, 1737.60it/s]warmup should be done:  17%|█▋        | 522/3000 [00:00<00:01, 1734.66it/s]warmup should be done:  17%|█▋        | 517/3000 [00:00<00:01, 1720.94it/s]warmup should be done:  17%|█▋        | 510/3000 [00:00<00:01, 1694.42it/s]warmup should be done:  17%|█▋        | 516/3000 [00:00<00:01, 1712.96it/s]warmup should be done:  18%|█▊        | 528/3000 [00:00<00:01, 1750.95it/s]warmup should be done:  16%|█▌        | 485/3000 [00:00<00:01, 1610.23it/s]warmup should be done:  23%|██▎       | 692/3000 [00:00<00:01, 1728.83it/s]warmup should be done:  23%|██▎       | 697/3000 [00:00<00:01, 1739.58it/s]warmup should be done:  23%|██▎       | 691/3000 [00:00<00:01, 1727.68it/s]warmup should be done:  23%|██▎       | 697/3000 [00:00<00:01, 1736.80it/s]warmup should be done:  23%|██▎       | 681/3000 [00:00<00:01, 1697.89it/s]warmup should be done:  23%|██▎       | 689/3000 [00:00<00:01, 1715.97it/s]warmup should be done:  23%|██▎       | 704/3000 [00:00<00:01, 1751.97it/s]warmup should be done:  22%|██▏       | 648/3000 [00:00<00:01, 1615.39it/s]warmup should be done:  29%|██▉       | 865/3000 [00:00<00:01, 1727.65it/s]warmup should be done:  29%|██▉       | 865/3000 [00:00<00:01, 1731.69it/s]warmup should be done:  29%|██▉       | 871/3000 [00:00<00:01, 1737.82it/s]warmup should be done:  29%|██▉       | 872/3000 [00:00<00:01, 1740.79it/s]warmup should be done:  28%|██▊       | 852/3000 [00:00<00:01, 1699.35it/s]warmup should be done:  29%|██▊       | 862/3000 [00:00<00:01, 1718.87it/s]warmup should be done:  29%|██▉       | 881/3000 [00:00<00:01, 1754.83it/s]warmup should be done:  27%|██▋       | 810/3000 [00:00<00:01, 1612.69it/s]warmup should be done:  35%|███▍      | 1038/3000 [00:00<00:01, 1725.96it/s]warmup should be done:  35%|███▍      | 1045/3000 [00:00<00:01, 1736.74it/s]warmup should be done:  35%|███▍      | 1039/3000 [00:00<00:01, 1729.93it/s]warmup should be done:  35%|███▍      | 1047/3000 [00:00<00:01, 1741.22it/s]warmup should be done:  34%|███▍      | 1022/3000 [00:00<00:01, 1698.26it/s]warmup should be done:  34%|███▍      | 1034/3000 [00:00<00:01, 1718.03it/s]warmup should be done:  35%|███▌      | 1057/3000 [00:00<00:01, 1756.13it/s]warmup should be done:  32%|███▏      | 972/3000 [00:00<00:01, 1608.82it/s]warmup should be done:  40%|████      | 1212/3000 [00:00<00:01, 1727.82it/s]warmup should be done:  41%|████      | 1219/3000 [00:00<00:01, 1733.39it/s]warmup should be done:  40%|████      | 1212/3000 [00:00<00:01, 1727.33it/s]warmup should be done:  40%|███▉      | 1192/3000 [00:00<00:01, 1695.48it/s]warmup should be done:  40%|████      | 1206/3000 [00:00<00:01, 1715.17it/s]warmup should be done:  41%|████      | 1222/3000 [00:00<00:01, 1736.68it/s]warmup should be done:  41%|████      | 1233/3000 [00:00<00:01, 1751.12it/s]warmup should be done:  38%|███▊      | 1133/3000 [00:00<00:01, 1608.19it/s]warmup should be done:  46%|████▋     | 1388/3000 [00:00<00:00, 1735.71it/s]warmup should be done:  46%|████▌     | 1386/3000 [00:00<00:00, 1730.80it/s]warmup should be done:  46%|████▋     | 1394/3000 [00:00<00:00, 1736.96it/s]warmup should be done:  45%|████▌     | 1363/3000 [00:00<00:00, 1698.01it/s]warmup should be done:  46%|████▌     | 1382/3000 [00:00<00:00, 1727.67it/s]warmup should be done:  47%|████▋     | 1397/3000 [00:00<00:00, 1739.87it/s]warmup should be done:  47%|████▋     | 1410/3000 [00:00<00:00, 1756.08it/s]warmup should be done:  43%|████▎     | 1294/3000 [00:00<00:01, 1605.49it/s]warmup should be done:  52%|█████▏    | 1564/3000 [00:00<00:00, 1742.05it/s]warmup should be done:  52%|█████▏    | 1568/3000 [00:00<00:00, 1737.62it/s]warmup should be done:  52%|█████▏    | 1560/3000 [00:00<00:00, 1731.30it/s]warmup should be done:  51%|█████     | 1533/3000 [00:00<00:00, 1698.12it/s]warmup should be done:  52%|█████▏    | 1557/3000 [00:00<00:00, 1732.14it/s]warmup should be done:  52%|█████▏    | 1571/3000 [00:00<00:00, 1738.56it/s]warmup should be done:  53%|█████▎    | 1587/3000 [00:00<00:00, 1758.23it/s]warmup should be done:  49%|████▊     | 1456/3000 [00:00<00:00, 1607.89it/s]warmup should be done:  58%|█████▊    | 1740/3000 [00:01<00:00, 1745.79it/s]warmup should be done:  58%|█████▊    | 1743/3000 [00:01<00:00, 1738.39it/s]warmup should be done:  57%|█████▋    | 1703/3000 [00:01<00:00, 1698.36it/s]warmup should be done:  58%|█████▊    | 1734/3000 [00:01<00:00, 1730.77it/s]warmup should be done:  58%|█████▊    | 1732/3000 [00:01<00:00, 1735.55it/s]warmup should be done:  59%|█████▉    | 1763/3000 [00:01<00:00, 1758.66it/s]warmup should be done:  58%|█████▊    | 1745/3000 [00:01<00:00, 1733.36it/s]warmup should be done:  54%|█████▍    | 1618/3000 [00:01<00:00, 1611.08it/s]warmup should be done:  64%|██████▍   | 1917/3000 [00:01<00:00, 1738.10it/s]warmup should be done:  64%|██████▎   | 1908/3000 [00:01<00:00, 1733.29it/s]warmup should be done:  62%|██████▏   | 1874/3000 [00:01<00:00, 1700.04it/s]warmup should be done:  64%|██████▎   | 1907/3000 [00:01<00:00, 1739.55it/s]warmup should be done:  64%|██████▍   | 1915/3000 [00:01<00:00, 1737.33it/s]warmup should be done:  65%|██████▍   | 1939/3000 [00:01<00:00, 1757.85it/s]warmup should be done:  64%|██████▍   | 1919/3000 [00:01<00:00, 1733.76it/s]warmup should be done:  59%|█████▉    | 1780/3000 [00:01<00:00, 1609.91it/s]warmup should be done:  70%|██████▉   | 2092/3000 [00:01<00:00, 1739.52it/s]warmup should be done:  68%|██████▊   | 2045/3000 [00:01<00:00, 1701.45it/s]warmup should be done:  69%|██████▉   | 2082/3000 [00:01<00:00, 1741.84it/s]warmup should be done:  69%|██████▉   | 2082/3000 [00:01<00:00, 1729.19it/s]warmup should be done:  70%|███████   | 2115/3000 [00:01<00:00, 1756.55it/s]warmup should be done:  70%|██████▉   | 2089/3000 [00:01<00:00, 1733.02it/s]warmup should be done:  70%|██████▉   | 2094/3000 [00:01<00:00, 1735.85it/s]warmup should be done:  65%|██████▍   | 1942/3000 [00:01<00:00, 1609.98it/s]warmup should be done:  76%|███████▌  | 2266/3000 [00:01<00:00, 1739.46it/s]warmup should be done:  75%|███████▌  | 2257/3000 [00:01<00:00, 1741.57it/s]warmup should be done:  76%|███████▋  | 2291/3000 [00:01<00:00, 1754.96it/s]warmup should be done:  76%|███████▌  | 2270/3000 [00:01<00:00, 1741.01it/s]warmup should be done:  75%|███████▌  | 2263/3000 [00:01<00:00, 1729.95it/s]warmup should be done:  70%|███████   | 2104/3000 [00:01<00:00, 1611.64it/s]warmup should be done:  75%|███████▌  | 2255/3000 [00:01<00:00, 1701.34it/s]warmup should be done:  74%|███████▍  | 2216/3000 [00:01<00:00, 1662.33it/s]warmup should be done:  81%|████████▏ | 2440/3000 [00:01<00:00, 1738.77it/s]warmup should be done:  81%|████████  | 2432/3000 [00:01<00:00, 1739.98it/s]warmup should be done:  82%|████████▏ | 2446/3000 [00:01<00:00, 1744.82it/s]warmup should be done:  76%|███████▌  | 2266/3000 [00:01<00:00, 1613.50it/s]warmup should be done:  81%|████████  | 2437/3000 [00:01<00:00, 1726.19it/s]warmup should be done:  82%|████████▏ | 2467/3000 [00:01<00:00, 1740.16it/s]warmup should be done:  81%|████████  | 2426/3000 [00:01<00:00, 1698.20it/s]warmup should be done:  79%|███████▉  | 2383/3000 [00:01<00:00, 1650.16it/s]warmup should be done:  87%|████████▋ | 2614/3000 [00:01<00:00, 1738.20it/s]warmup should be done:  87%|████████▋ | 2621/3000 [00:01<00:00, 1742.11it/s]warmup should be done:  87%|████████▋ | 2610/3000 [00:01<00:00, 1723.81it/s]warmup should be done:  81%|████████  | 2428/3000 [00:01<00:00, 1609.11it/s]warmup should be done:  88%|████████▊ | 2643/3000 [00:01<00:00, 1744.17it/s]warmup should be done:  87%|████████▋ | 2606/3000 [00:01<00:00, 1722.26it/s]warmup should be done:  87%|████████▋ | 2596/3000 [00:01<00:00, 1694.60it/s]warmup should be done:  85%|████████▌ | 2550/3000 [00:01<00:00, 1655.33it/s]warmup should be done:  93%|█████████▎| 2788/3000 [00:01<00:00, 1738.27it/s]warmup should be done:  93%|█████████▎| 2796/3000 [00:01<00:00, 1737.16it/s]warmup should be done:  86%|████████▋ | 2590/3000 [00:01<00:00, 1610.43it/s]warmup should be done:  93%|█████████▎| 2783/3000 [00:01<00:00, 1719.40it/s]warmup should be done:  93%|█████████▎| 2780/3000 [00:01<00:00, 1725.74it/s]warmup should be done:  94%|█████████▍| 2818/3000 [00:01<00:00, 1739.97it/s]warmup should be done:  92%|█████████▏| 2768/3000 [00:01<00:00, 1699.49it/s]warmup should be done:  91%|█████████ | 2720/3000 [00:01<00:00, 1668.29it/s]warmup should be done:  99%|█████████▊| 2962/3000 [00:01<00:00, 1737.49it/s]warmup should be done:  92%|█████████▏| 2752/3000 [00:01<00:00, 1611.66it/s]warmup should be done:  99%|█████████▊| 2956/3000 [00:01<00:00, 1734.09it/s]warmup should be done:  99%|█████████▊| 2956/3000 [00:01<00:00, 1720.00it/s]warmup should be done: 100%|█████████▉| 2994/3000 [00:01<00:00, 1744.92it/s]warmup should be done:  99%|█████████▉| 2970/3000 [00:01<00:00, 1712.88it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1750.38it/s]warmup should be done:  96%|█████████▋| 2889/3000 [00:01<00:00, 1673.39it/s]warmup should be done:  98%|█████████▊| 2938/3000 [00:01<00:00, 1683.52it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1737.45it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1730.18it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1728.59it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1728.03it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1711.99it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1684.09it/s]warmup should be done:  97%|█████████▋| 2914/3000 [00:01<00:00, 1612.16it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1610.10it/s]
WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f45617a9d30>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f45616e81f0>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f45616e80d0>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f45617a7790>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f45616e8100>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f45616e81f0>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f45616e8280>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f45617a6e50>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
2022-12-11 19:42:06.676616: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f4092b85680 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 19:42:06.676677: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 19:42:06.676905: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f409282e4c0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 19:42:06.676963: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 19:42:06.686532: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 19:42:06.686810: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 19:42:07.347526: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f409e82b260 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 19:42:07.347595: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 19:42:07.357179: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 19:42:07.398764: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f4096b80f00 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 19:42:07.398822: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 19:42:07.408136: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 19:42:07.410243: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f408ab81ff0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 19:42:07.410279: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 19:42:07.420260: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 19:42:07.436573: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f4096b89900 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 19:42:07.436629: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 19:42:07.437100: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f409e82de80 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 19:42:07.437142: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 19:42:07.437255: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f408f2ec440 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 19:42:07.437310: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 19:42:07.445958: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 19:42:07.446141: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 19:42:07.446746: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 19:42:09.463268: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 19:42:09.533250: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 19:42:09.746578: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 19:42:09.765387: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 19:42:09.803440: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 19:42:09.814257: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 19:42:09.903067: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 19:42:09.903520: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
[HCTR][19:42:31.708][ERROR][RK0][tid #139916713375488]: replica 6 reaches 1000, calling init pre replica
[HCTR][19:42:31.710][ERROR][RK0][tid #139916713375488]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][19:42:31.722][ERROR][RK0][tid #139916713375488]: coll ps creation done
[HCTR][19:42:31.722][ERROR][RK0][tid #139916713375488]: replica 6 waits for coll ps creation barrier
[HCTR][19:42:31.743][ERROR][RK0][tid #139915849352960]: replica 3 reaches 1000, calling init pre replica
[HCTR][19:42:31.744][ERROR][RK0][tid #139915849352960]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][19:42:31.749][ERROR][RK0][tid #139915849352960]: coll ps creation done
[HCTR][19:42:31.749][ERROR][RK0][tid #139915849352960]: replica 3 waits for coll ps creation barrier
[HCTR][19:42:31.782][ERROR][RK0][main]: replica 7 reaches 1000, calling init pre replica
[HCTR][19:42:31.782][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][19:42:31.789][ERROR][RK0][main]: coll ps creation done
[HCTR][19:42:31.789][ERROR][RK0][main]: replica 7 waits for coll ps creation barrier
[HCTR][19:42:31.791][ERROR][RK0][tid #139915773851392]: replica 5 reaches 1000, calling init pre replica
[HCTR][19:42:31.792][ERROR][RK0][tid #139915773851392]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][19:42:31.796][ERROR][RK0][tid #139915773851392]: coll ps creation done
[HCTR][19:42:31.796][ERROR][RK0][tid #139915773851392]: replica 5 waits for coll ps creation barrier
[HCTR][19:42:31.808][ERROR][RK0][tid #139915908069120]: replica 1 reaches 1000, calling init pre replica
[HCTR][19:42:31.809][ERROR][RK0][tid #139915908069120]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][19:42:31.818][ERROR][RK0][tid #139915908069120]: coll ps creation done
[HCTR][19:42:31.818][ERROR][RK0][tid #139915908069120]: replica 1 waits for coll ps creation barrier
[HCTR][19:42:31.861][ERROR][RK0][tid #139915916461824]: replica 4 reaches 1000, calling init pre replica
[HCTR][19:42:31.862][ERROR][RK0][tid #139915916461824]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][19:42:31.869][ERROR][RK0][tid #139915916461824]: coll ps creation done
[HCTR][19:42:31.869][ERROR][RK0][tid #139915916461824]: replica 4 waits for coll ps creation barrier
[HCTR][19:42:31.920][ERROR][RK0][main]: replica 0 reaches 1000, calling init pre replica
[HCTR][19:42:31.920][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][19:42:31.925][ERROR][RK0][main]: coll ps creation done
[HCTR][19:42:31.925][ERROR][RK0][main]: replica 0 waits for coll ps creation barrier
[HCTR][19:42:31.949][ERROR][RK0][main]: replica 2 reaches 1000, calling init pre replica
[HCTR][19:42:31.950][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][19:42:31.957][ERROR][RK0][main]: coll ps creation done
[HCTR][19:42:31.957][ERROR][RK0][main]: replica 2 waits for coll ps creation barrier
[HCTR][19:42:31.960][ERROR][RK0][main]: replica 0 preparing frequency
[HCTR][19:42:39.076][ERROR][RK0][main]: replica 0 preparing frequency done
[HCTR][19:42:39.110][ERROR][RK0][main]: replica 0 calling init per replica
[HCTR][19:42:39.110][ERROR][RK0][tid #139915908069120]: replica 1 calling init per replica
[HCTR][19:42:39.110][ERROR][RK0][tid #139915916461824]: replica 4 calling init per replica
[HCTR][19:42:39.110][ERROR][RK0][main]: replica 2 calling init per replica
[HCTR][19:42:39.110][ERROR][RK0][tid #139915773851392]: replica 5 calling init per replica
[HCTR][19:42:39.110][ERROR][RK0][tid #139915849352960]: replica 3 calling init per replica
[HCTR][19:42:39.110][ERROR][RK0][main]: replica 7 calling init per replica
[HCTR][19:42:39.110][ERROR][RK0][tid #139916713375488]: replica 6 calling init per replica
[HCTR][19:42:39.110][ERROR][RK0][main]: Calling build_v2
[HCTR][19:42:39.110][ERROR][RK0][tid #139915908069120]: Calling build_v2
[HCTR][19:42:39.110][ERROR][RK0][tid #139915916461824]: Calling build_v2
[HCTR][19:42:39.110][ERROR][RK0][main]: Calling build_v2
[HCTR][19:42:39.110][ERROR][RK0][tid #139915773851392]: Calling build_v2
[HCTR][19:42:39.110][ERROR][RK0][tid #139915849352960]: Calling build_v2
[HCTR][19:42:39.110][ERROR][RK0][main]: Calling build_v2
[HCTR][19:42:39.110][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][19:42:39.110][ERROR][RK0][tid #139916713375488]: Calling build_v2
[HCTR][19:42:39.110][ERROR][RK0][tid #139915908069120]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][19:42:39.110][ERROR][RK0][tid #139915916461824]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][19:42:39.110][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][19:42:39.110][ERROR][RK0][tid #139915773851392]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][19:42:39.110][ERROR][RK0][tid #139915849352960]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][19:42:39.110][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][19:42:39.110][ERROR][RK0][tid #139916713375488]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[[[[[2022-12-11 19:42:39[2022-12-11 19:42:39.2022-12-11 19:42:392022-12-11 19:42:39[2022-12-11 19:42:39.110864.2022-12-11 19:42:392022-12-11 19:42:39[..110864: 110853..110854110853: E: 2022-12-11 19:42:39110889110876: : E E.: : EE /hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc 110908EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc:/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc:   /hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc:136:E/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc::136] 136 ::136136] using concurrent impl MPS] /hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc136136] ] using concurrent impl MPS
using concurrent impl MPS:] ] using concurrent impl MPSusing concurrent impl MPS

136using concurrent impl MPSusing concurrent impl MPS

] 

using concurrent impl MPS
[2022-12-11 19:42:39.115040: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:178] v100x8, slow pcie
[2022-12-11 19:42:39.115076: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[:2022-12-11 19:42:39196.] 115085assigning 8 to cpu: 
E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:178] v100x8, slow pcie
[[2022-12-11 19:42:392022-12-11 19:42:39..115144115140: [: E2022-12-11 19:42:39E . /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc115159/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:: :196[E178] 2022-12-11 19:42:39 ] assigning 8 to cpu./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccv100x8, slow pcie
115182:
: 212E]  [build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-11 19:42:39[
:.[2022-12-11 19:42:391781152272022-12-11 19:42:39.] : .115228[v100x8, slow pcieE115238: 2022-12-11 19:42:39
 : E.[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE [1152592022-12-11 19:42:39: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-11 19:42:39: .196/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:.E115276] :178115286 : assigning 8 to cpu212] : [/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE
] v100x8, slow pcieE2022-12-11 19:42:39: build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8
 .213/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
[[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[115328] :2022-12-11 19:42:392022-12-11 19:42:39:[2022-12-11 19:42:39: remote time is 8.68421178..1962022-12-11 19:42:39.E
] 115378115373] .115385 v100x8, slow pcie: [: assigning 8 to cpu115407: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
E2022-12-11 19:42:39E
: E: .[ E 178/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc1154722022-12-11 19:42:39/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] [:: .:/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:v100x8, slow pcie2022-12-11 19:42:39212E115520178:196
.]  : ] 213] 115563build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[Ev100x8, slow pcie] assigning 8 to cpu: 
:2022-12-11 19:42:39 
remote time is 8.68421
E214./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[
 [] 115641:2022-12-11 19:42:39/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-11 19:42:39[cpu time is 97.0588: 196[.:.2022-12-11 19:42:39
E] 2022-12-11 19:42:39115695212115705. assigning 8 to cpu.: ] : 115727/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
115736Ebuild asymm link desc with 8X Tesla V100-SXM2-32GB out of 8E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: ::  
:E196[E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc196 [] 2022-12-11 19:42:39 :] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-11 19:42:39assigning 8 to cpu./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc213assigning 8 to cpu:.
115892:] 
214115889: 212remote time is 8.68421] : E] 
cpu time is 97.0588E build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8
 [/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[2022-12-11 19:42:39:2022-12-11 19:42:39:2022-12-11 19:42:39.[213.212.1160002022-12-11 19:42:39] 116003] 116010: .remote time is 8.68421: build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8: E116032
E
E :  [ /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-11 19:42:39/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: 2022-12-11 19:42:39:.:212/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.214116114212] :116124] : ] build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8213: cpu time is 97.0588Ebuild asymm link desc with 8X Tesla V100-SXM2-32GB out of 8
] E
 
remote time is 8.68421 /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[:[2022-12-11 19:42:39:2022-12-11 19:42:392142022-12-11 19:42:39.213.] .116240] 116255cpu time is 97.0588116251: remote time is 8.68421: 
: E
EE   [/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-11 19:42:39:::.213214213116341] ] ] : remote time is 8.68421cpu time is 97.0588remote time is 8.68421E


 [/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[2022-12-11 19:42:39:2022-12-11 19:42:39.214.116452] 116459: cpu time is 97.0588: E
E  /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc::214214] ] cpu time is 97.0588cpu time is 97.0588

[2022-12-11 19:44:22.605838: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:118] solver created. now build & solve
[2022-12-11 19:44:22.956668: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:123] solver built. now solve
block 0 storage is 00010001
	access is	0	0	0	0	4	4	4	4	
block 1 storage is 00100010
	access is	1	1	1	1	5	5	5	5	
block 2 storage is 01000100
	access is	2	2	2	2	6	6	6	6	
block 3 storage is 10001000
	access is	3	3	3	3	7	7	7	7	
block 4 storage is 00000000
	access is	8	8	8	8	8	8	8	8	
[2022-12-11 19:44:24.470048: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:127] solver solved
[2022-12-11 19:44:24.470108: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:195] 0 solved master
[2022-12-11 19:44:24.470141: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 0 solved
[2022-12-11 19:44:24.470172: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 0 initing device 0
[2022-12-11 19:44:24.470642: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-11 19:44:24.470684: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 3.29 GB
[2022-12-11 19:44:24.474837: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 3.29 GB
[2022-12-11 19:44:24.478933: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 3.29 GB
[2022-12-11 19:44:24.603165: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 5 solved
[2022-12-11 19:44:24.603244: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 5 initing device 5
[2022-12-11 19:44:24.603388: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 4 solved
[2022-12-11 19:44:24.603466: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 4 initing device 4
[2022-12-11 19:44:24.603653: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-11 19:44:24.603705: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 3.29 GB
[2022-12-11 19:44:24.603861: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-11 19:44:24.603908: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 3.29 GB
[2022-12-11 19:44:24.605063: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 6 solved
[2022-12-11 19:44:24.605123: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 6 initing device 6
[2022-12-11 19:44:24[.2022-12-11 19:44:24605501.: 605495E:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu :/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc1815:] 202Building Coll Cache with ... num gpu device is 8] 
2 solved
[2022-12-11 19:44:24.[6055672022-12-11 19:44:24: .E605572 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.ccE: 205/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu] :worker 0 thread 2 initing device 21980
] eager alloc mem 3.29 GB
[2022-12-11 19:44:24.605956: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-11 19:44:24.605996: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 3.29 GB
[2022-12-11 19:44:24.607538: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 3 solved
[2022-12-11 19:44:24.607593: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 3 initing device 3
[2022-12-11 19:44:24.607963: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-11 19:44:24.608002: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 3.29 GB
[2022-12-11 19:44:24.617664: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 7 solved
[2022-12-11 19:44:24.617734: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 7 initing device 7
[2022-12-11 19:44:24.618218: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-11 19:44:24.618269: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 3.29 GB
[2022-12-11 19:44:24.624085: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 1 solved
[2022-12-11 19:44:24.624142: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 1 initing device 1
[2022-12-11 19:44:24.624596: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-11 19:44:24.624645: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 3.29 GB
[2022-12-11 19:44:24.627370: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 3.29 GB
[2022-12-11 19:44:24.631289: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 3.29 GB
[2022-12-11 19:44:24.631382: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 3.29 GB
[2022-12-11 19:44:24.631482: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 3.29 GB
[2022-12-11 19:44:24.635224: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 3.29 GB
[2022-12-11 19:44:24.646257: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 3.29 GB
[2022-12-11 19:44:24.650173: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 3.29 GB
[2022-12-11 19:44:24.654175: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 3.29 GB
[2022-12-11 19:44:24.658156: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 3.29 GB
[2022-12-11 19:44:24.658239: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 3.29 GB
[2022-12-11 19:44:24.658326: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 3.29 GB
[2022-12-11 19:44:24.662193: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 3.29 GB
[2022-12-11 19:44:24.673301: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 3.29 GB
[2022-12-11 19:44:24.677270: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 3.29 GB
[2022-12-11 19:44:25.149771: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.00 Bytes
[2022-12-11 19:44:25.149978: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1855] using empty feat=27
[2022-12-11 19:44:25.167848: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5
[2022-12-11 19:44:25.168000: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 3531098340
[2022-12-11 19:44:25.172278: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 138.07 MB
[2022-12-11 19:44:25.173073: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:44:25.180390: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:44:25.180803: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 134.70 MB
[2022-12-11 19:44:25.279972: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 19:44:25.284579: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 19:44:25.284637: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 16.88 GB
[2022-12-11 19:44:25.427308: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.00 Bytes
[2022-12-11 19:44:25.427498: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1855] using empty feat=27
[2022-12-11 19:44:25.444632: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5
[2022-12-11 19:44:25.444712: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 3531098340
[2022-12-11 19:44:25.448960: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 138.07 MB
[2022-12-11 19:44:25.449619: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:44:25.456765: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:44:25.457051: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 134.70 MB
[2022-12-11 19:44:25.469607: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.00 Bytes
[[[[[2022-12-11 19:44:252022-12-11 19:44:252022-12-11 19:44:252022-12-11 19:44:252022-12-11 19:44:25.....469667469666469666469666469666: : : : : EEEEE   [  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu2022-12-11 19:44:25/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:::.::19801980198046978319801980] ] ] : ] ] eager alloc mem 5.00 Byteseager alloc mem 5.00 Byteseager alloc mem 5.00 BytesWeager alloc mem 5.00 Byteseager alloc mem 5.00 Bytes


 

/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1855] using empty feat=27
[[[2022-12-11 19:44:252022-12-11 19:44:25[2022-12-11 19:44:25[..2022-12-11 19:44:25.2022-12-11 19:44:25469947469947.469949.: : 469955: 469955WW: W:   W W/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu ::/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu18551855:1855:] ] 1855] 1855using empty feat=27using empty feat=27] using empty feat=27] 

using empty feat=27
using empty feat=27

[2022-12-11 19:44:25.488591: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5
[2022-12-11 19:44:25.[4886882022-12-11 19:44:25: .E488679 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccE: 638/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc] :eager release cuda mem 3531098340638
] eager release cuda mem 5
[2022-12-11 19:44:25.488744: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5[
2022-12-11 19:44:25.488784: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 3531098340
[2022-12-11 19:44:25.488824: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 3531098340[
2022-12-11 19:44:25.488835: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5
[[2022-12-11 19:44:252022-12-11 19:44:25..488914488901: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::638638] ] eager release cuda mem 3531098340eager release cuda mem 5

[2022-12-11 19:44:25.488979: E [/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc2022-12-11 19:44:25:.638489008] : eager release cuda mem 5E
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 3531098340
[2022-12-11 19:44:25.489069: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 3531098340
[2022-12-11 19:44:25.493149: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 138.07 MB
[2022-12-11 19:44:25.497488: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 138.07 MB
[2022-12-11 19:44:25.501998: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 138.07 MB
[2022-12-11 19:44:25.506190: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 138.07 MB
[2022-12-11 19:44:25.510165: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 138.07 MB
[2022-12-11 19:44:25.514247: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 138.07 MB
[2022-12-11 19:44:25.515379: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:44:25.515575: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:44:25.515800: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:44:25.516027: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:44:25.516084: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:44:25.516243: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:44:25.523699: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:44:25.523856: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:44:25.523983: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 134.70 MB
[2022-12-11 19:44:25.524065: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:44:25.524183: [E2022-12-11 19:44:25 ./hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc524199:: 638W]  eager release cuda mem 5518079/hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc
:43] WORKER[0] alloc host memory 134.70 MB
[2022-12-11 19:44:25.524248: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:44:25.524292: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:44:25.524675: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 134.70 MB
[2022-12-11 19:44:25.524816: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 134.70 MB
[2022-12-11 19:44:25.524966: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 134.70 MB
[2022-12-11 19:44:25.525029: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 134.70 MB
[2022-12-11 19:44:25.541289: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 19:44:25.545810: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 19:44:25.545855: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 16.88 GB
[2022-12-11 19:44:25.614278: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 19:44:25.614901: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 19:44:25.616015: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 19:44:25.616695: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 19:44:25.617766: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 19:44:25.618259: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 19:44:25.618823: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 19:44:25.618869: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 16.88 GB
[2022-12-11 19:44:25.619411: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 19:44:25.619455: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 16.88 GB
[2022-12-11 19:44:25.620513: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 19:44:25.620556: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 16.88 GB
[2022-12-11 19:44:25.621198: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 19:44:25.621241: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 16.88 GB
[2022-12-11 19:44:25.622274: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 19:44:25.622319: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 16.88 GB
[2022-12-11 19:44:25.622762: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 19:44:25.622804: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 16.88 GB
[[[[[[[[2022-12-11 19:44:282022-12-11 19:44:282022-12-11 19:44:282022-12-11 19:44:282022-12-11 19:44:282022-12-11 19:44:282022-12-11 19:44:282022-12-11 19:44:28........818360818360818362818360818359818361818366818366: : : : : : : : EEEEEEEE        /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::::::::19261926192619261926192619261926] ] ] ] ] ] ] ] Device 2 init p2p of link 1Device 3 init p2p of link 2Device 4 init p2p of link 5Device 7 init p2p of link 4Device 6 init p2p of link 0Device 0 init p2p of link 3Device 5 init p2p of link 6Device 1 init p2p of link 7







[[[[[[2022-12-11 19:44:282022-12-11 19:44:282022-12-11 19:44:282022-12-11 19:44:28[2022-12-11 19:44:282022-12-11 19:44:28[....2022-12-11 19:44:28..2022-12-11 19:44:28818923818923818923818923.818923818923.: : : : 818939: : 818945EEEE: EE:     E  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu ::::/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu1980198019801980:19801980:] ] ] ] 1980] ] 1980eager alloc mem 5.26 MBeager alloc mem 5.26 MBeager alloc mem 5.26 MBeager alloc mem 5.26 MB] eager alloc mem 5.26 MBeager alloc mem 5.26 MB] 



eager alloc mem 5.26 MB

eager alloc mem 5.26 MB

[2022-12-11 19:44:28.828235: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:44:28.828561: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:44:28.828596: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:44:28.828627: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:44:28.828680: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:44:28.828725: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:44:28.828776: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:44:28.828846: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:44:28.845472: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 6 init p2p of link 5
[2022-12-11 19:44:28.845629: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:44:28.849137: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 2
[2022-12-11 19:44:28.849290: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:44:28.856957: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:44:28.857306: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:44:28.865345: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 3 init p2p of link 0
[2022-12-11 19:44:28.865502: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:44:28.865858: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 4 init p2p of link 7
[2022-12-11 19:44:28.865917: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 0 init p2p of link 6
[2022-12-11 19:44:28.866013: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:44:28.866073: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:44:28.866443: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 7 init p2p of link 1
[2022-12-11 19:44:28.866601: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:44:28.866762: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 2 init p2p of link 3
[2022-12-11 19:44:28.866920: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:44:28.867400: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 5 init p2p of link 4
[2022-12-11 19:44:28.867563: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:44:28.872227: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:44:28.872996: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:44:28.873037: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:44:28.873432: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:44:28.874062: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:44:28.874513: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:44:28.881446: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 3
[2022-12-11 19:44:28[.2022-12-11 19:44:28881562.: 881554E:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu :/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu1980:] 1926eager alloc mem 5.26 MB] 
Device 6 init p2p of link 4
[2022-12-11 19:44:28.881722: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:44:28.887456: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 0 init p2p of link 1
[2022-12-11 19:44:28.887581: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:44:28.889102: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 7 init p2p of link 6
[2022-12-11 19:44:28.889225: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:44:28.893339: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:44:28.893792: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:44:28.896029: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:44:28.896758: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:44:28.899928: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 3 init p2p of link 5
[2022-12-11 19:44:28.900059: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:44:28.904368: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 4 init p2p of link 2
[2022-12-11 19:44:28.904494: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:44:28.905804: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 2 init p2p of link 0
[2022-12-11 19:44:28.905930: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:44:28.906440: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 5 init p2p of link 7
[2022-12-11 19:44:28.906572: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:44:28.909823: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:44:28.912370: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:44:28.913283: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 3 init p2p of link 1
[2022-12-11 19:44:28.913403: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:44:28.914077: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:44:28.914202: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:44:28.920557: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 4 init p2p of link 6
[2022-12-11 19:44:28.920680: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:44:28.920905: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:44:28.928958: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:44:28.935525: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 0
[2022-12-11 19:44:28.935647: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:44:28.938362: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 6 init p2p of link 7
[2022-12-11 19:44:28.938483: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:44:28.939696: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 0 init p2p of link 2
[2022-12-11 19:44:28.939822: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:44:28.940126: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 7 init p2p of link 5
[2022-12-11 19:44:28.940259: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:44:28.942521: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:44:28.945340: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:44:28.946615: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:44:28.947036: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:44:28.955011: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 2 init p2p of link 4
[2022-12-11 19:44:28.955144: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:44:28.956162: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 5 init p2p of link 3
[2022-12-11 19:44:28.956298: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:44:28.961666: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:44:28.962833: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:44:28.964555: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 144775028
[2022-12-11 19:44:28.966208: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 35310982 / 882774585 nodes ( 4.00 %~4.00 %) | remote 105932946 / 882774585 nodes ( 12.00 %) | cpu 741530657 / 882774585 nodes ( 84.00 %) | 16.88 GB | 4.35821 secs 
[2022-12-11 19:44:28.967770: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 144775028
[2022-12-11 19:44:28.968591: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 144775028
[2022-12-11 19:44:28.969333: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 35310982 / 882774585 nodes ( 4.00 %~4.00 %) | remote 105932946 / 882774585 nodes ( 12.00 %) | cpu 741530657 / 882774585 nodes ( 84.00 %) | 16.88 GB | 4.36334 secs 
[2022-12-11 19:44:28.970669: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 35310982 / 882774585 nodes ( 4.00 %~4.00 %) | remote 105932946 / 882774585 nodes ( 12.00 %) | cpu 741530657 / 882774585 nodes ( 84.00 %) | 16.88 GB | 4.36677 secs 
[2022-12-11 19:44:28.971124: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 144775028
[2022-12-11 19:44:28.973187: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 35310982 / 882774585 nodes ( 4.00 %~4.00 %) | remote 105932946 / 882774585 nodes ( 12.00 %) | cpu 741530657 / 882774585 nodes ( 84.00 %) | 16.88 GB | 4.36949 secs 
[2022-12-11 19:44:28.978716: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 144775028
[2022-12-11 19:44:28.980841: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 35310982 / 882774585 nodes ( 4.00 %~4.00 %) | remote 105932946 / 882774585 nodes ( 12.00 %) | cpu 741530657 / 882774585 nodes ( 84.00 %) | 16.88 GB | 4.3562 secs 
[2022-12-11 19:44:28.985653: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 144775028
[2022-12-11 19:44:28.987084: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 144775028
[2022-12-11 19:44:28.987992: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 144775028
[2022-12-11 19:44:29. 10956: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 35310982 / 882774585 nodes ( 4.00 %~4.00 %) | remote 105932946 / 882774585 nodes ( 12.00 %) | cpu 741530657 / 882774585 nodes ( 84.00 %) | 16.88 GB | 4.40539 secs 
[2022-12-11 19:44:29. 11197: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 35310982 / 882774585 nodes ( 4.00 %~4.00 %) | remote 105932946 / 882774585 nodes ( 12.00 %) | cpu 741530657 / 882774585 nodes ( 84.00 %) | 16.88 GB | 4.54052 secs 
[2022-12-11 19:44:29. 11444: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 35310982 / 882774585 nodes ( 4.00 %~4.00 %) | remote 105932946 / 882774585 nodes ( 12.00 %) | cpu 741530657 / 882774585 nodes ( 84.00 %) | 16.88 GB | 4.39318 secs 
[2022-12-11 19:44:29. 11961: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 28.27 GB
[2022-12-11 19:44:30.467362: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 28.53 GB
[2022-12-11 19:44:30.491596: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 28.53 GB
[2022-12-11 19:44:30.492989: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 28.53 GB
[2022-12-11 19:44:32. 72752: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 28.80 GB
[2022-12-11 19:44:32. 72887: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 28.80 GB
[2022-12-11 19:44:32. 73205: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 28.80 GB
[2022-12-11 19:44:33.807595: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 29.01 GB
[2022-12-11 19:44:33.808390: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 29.01 GB
[2022-12-11 19:44:33.810171: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 29.01 GB
[2022-12-11 19:44:35.591965: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 29.23 GB
[2022-12-11 19:44:35.593608: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 29.23 GB
[2022-12-11 19:44:35.594809: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 29.23 GB
[2022-12-11 19:44:37.350050: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 29.68 GB
[2022-12-11 19:44:37.350937: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 29.68 GB
[2022-12-11 19:44:37.351544: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 29.68 GB
[2022-12-11 19:44:39.208916: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 29.88 GB
[2022-12-11 19:44:39.209105: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 29.88 GB
[HCTR][19:44:39.253][ERROR][RK0][main]: replica 7 calling init per replica done, doing barrier
[HCTR][19:44:39.253][ERROR][RK0][main]: replica 0 calling init per replica done, doing barrier
[HCTR][19:44:39.253][ERROR][RK0][tid #139915916461824]: replica 4 calling init per replica done, doing barrier
[HCTR][19:44:39.253][ERROR][RK0][main]: replica 2 calling init per replica done, doing barrier
[HCTR][19:44:39.253][ERROR][RK0][tid #139916713375488]: replica 6 calling init per replica done, doing barrier
[HCTR][19:44:39.253][ERROR][RK0][tid #139915773851392]: replica 5 calling init per replica done, doing barrier
[HCTR][19:44:39.253][ERROR][RK0][tid #139915908069120]: replica 1 calling init per replica done, doing barrier
[HCTR][19:44:39.253][ERROR][RK0][tid #139915849352960]: replica 3 calling init per replica done, doing barrier
[HCTR][19:44:39.253][ERROR][RK0][main]: replica 2 calling init per replica done, doing barrier done
[HCTR][19:44:39.253][ERROR][RK0][tid #139915849352960]: replica 3 calling init per replica done, doing barrier done
[HCTR][19:44:39.253][ERROR][RK0][main]: replica 0 calling init per replica done, doing barrier done
[HCTR][19:44:39.253][ERROR][RK0][main]: replica 7 calling init per replica done, doing barrier done
[HCTR][19:44:39.253][ERROR][RK0][tid #139915773851392]: replica 5 calling init per replica done, doing barrier done
[HCTR][19:44:39.253][ERROR][RK0][tid #139915908069120]: replica 1 calling init per replica done, doing barrier done
[HCTR][19:44:39.253][ERROR][RK0][tid #139915916461824]: replica 4 calling init per replica done, doing barrier done
[HCTR][19:44:39.253][ERROR][RK0][tid #139916713375488]: replica 6 calling init per replica done, doing barrier done
[HCTR][19:44:39.253][ERROR][RK0][main]: init per replica done
[HCTR][19:44:39.253][ERROR][RK0][tid #139915849352960]: init per replica done
[HCTR][19:44:39.253][ERROR][RK0][main]: init per replica done
[HCTR][19:44:39.253][ERROR][RK0][tid #139915773851392]: init per replica done
[HCTR][19:44:39.253][ERROR][RK0][tid #139915908069120]: init per replica done
[HCTR][19:44:39.253][ERROR][RK0][tid #139915916461824]: init per replica done
[HCTR][19:44:39.253][ERROR][RK0][tid #139916713375488]: init per replica done
[HCTR][19:44:39.272][ERROR][RK0][main]: init per replica done
