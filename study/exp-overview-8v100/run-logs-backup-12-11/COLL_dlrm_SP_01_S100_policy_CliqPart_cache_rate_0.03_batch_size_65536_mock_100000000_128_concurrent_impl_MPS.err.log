2022-12-11 22:42:37.575300: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.583251: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.588932: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.595007: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.599662: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.612788: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.618762: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.624264: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.672795: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.673475: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.682478: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.689670: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.693752: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.694982: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.697245: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.697793: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.698883: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.699689: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.700526: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.701257: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.702108: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.702882: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.703633: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.704680: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.705165: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.706936: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.708079: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.709112: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.710041: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.710969: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.711898: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.712810: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.714526: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.715754: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.716805: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.717720: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.718632: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.719573: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.720483: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.721418: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.726757: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 22:42:37.728801: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.730025: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.731573: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.733175: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.733262: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.734876: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.735006: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.736389: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.736533: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.736677: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.738572: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.738850: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.738979: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.740968: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.741125: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.741213: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.743757: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.746265: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.747025: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.748901: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.749900: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.750100: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.751230: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.753007: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.753285: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.754064: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.756233: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.756489: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.757104: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.757532: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.759562: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.759840: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.760268: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.761064: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.761462: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.762925: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.763174: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.763472: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.764584: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.765141: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.766394: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.766473: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.766615: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.767792: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.768548: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.769406: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.769450: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.769490: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.776894: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.778756: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.779258: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.779687: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.780804: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.781489: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.781715: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.783357: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.783944: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.784098: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.787817: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.795356: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.820891: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.821544: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.821563: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.821770: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.821813: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.824546: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.825429: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.825662: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.825735: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.825782: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.825928: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.829517: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.830022: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.830472: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.830519: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.830686: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.833663: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.834113: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.834392: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.834816: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.834917: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.837496: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.837980: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.838263: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.838451: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.838648: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.841046: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.841731: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.842034: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.842286: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.842434: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.844533: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.845491: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.845675: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.845774: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.845850: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.848007: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.848739: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.849009: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.849138: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.849147: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.852219: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.853073: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.853258: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.853394: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.853485: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.855691: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.856901: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.857087: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.857227: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.857471: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.859496: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.860542: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.860746: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.860970: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.861041: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.863240: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.864249: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.864457: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.864716: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.864719: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.866663: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.867795: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.867932: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.868245: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.868393: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.868877: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.870355: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.871630: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.871905: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.872246: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.872538: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.873204: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.874152: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.874541: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.876131: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.876203: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.876844: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.878002: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.878713: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.879092: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.880929: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.881232: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.881479: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 22:42:37.881719: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.881804: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.882348: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.882618: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.884493: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.885019: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.885258: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.885518: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.886204: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.887328: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.889109: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.889169: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.889332: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.889515: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.890121: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.890953: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.891375: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.893596: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.893602: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.893811: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.894014: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.894998: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.895688: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.896218: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.898058: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.898155: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.898461: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.898537: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.899681: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.900349: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.900955: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.902709: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.902823: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.902952: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.903090: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.904054: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.905265: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.907508: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.907821: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.908046: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.908050: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.909030: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.910383: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.912207: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.912359: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.912598: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.912731: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.913924: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.914930: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.917195: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.917730: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.918191: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.918764: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.920801: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.921040: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 22:42:37.921043: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 22:42:37.921421: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.921769: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.924149: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.925006: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.926301: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 22:42:37.927021: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.927489: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.928014: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 22:42:37.929364: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.929658: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.930647: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.930706: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.932926: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.933720: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.934090: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.934277: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.936218: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.936734: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.936919: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.937233: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.937343: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.937643: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.943798: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.944614: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.944820: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.945136: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.949117: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.949572: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.949831: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:37.950112: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:38.010251: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:38.010694: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:38.015573: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:38.016103: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:38.020853: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:38.022673: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:38.028497: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:38.029076: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:38.035144: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:38.039183: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 22:42:38.042849: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 22:42:38.048834: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:38.052267: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:38.056498: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:38.061285: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:38.062174: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:38.071188: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:39.034858: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:39.035502: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:39.036270: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:39.036743: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 22:42:39.036801: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-11 22:42:39.054566: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:39.055381: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:39.055908: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:39.056484: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:39.057008: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:39.057630: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-11 22:42:39.105456: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 22:42:39.105665: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 22:42:39.139836: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12340
2022-12-11 22:42:39.240679: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:39.241286: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:39.242000: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:39.242461: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 22:42:39.242516: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-11 22:42:39.260543: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:39.261326: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:39.262052: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:39.262636: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:39.263181: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:39.263660: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:1/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-11 22:42:39.297135: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:39.297766: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:39.298524: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:39.299053: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 22:42:39.299109: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-11 22:42:39.316986: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:39.317820: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:39.318330: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:39.318920: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:39.319452: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:39.319932: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:2/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-11 22:42:39.322462: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:39.323051: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:39.324084: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:39.324707: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 22:42:39.324754: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-11 22:42:39.327330: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:39.328072: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:39.328512: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:39.328661: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:39.329533: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:39.329612: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 22:42:39.329680: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-11 22:42:39.330275: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:39.330738: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 22:42:39.330788: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-11 22:42:39.338321: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:39.338935: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:39.339720: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:39.340182: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 22:42:39.340239: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-11 22:42:39.342868: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:39.343546: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:39.344117: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:39.344754: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:39.345334: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:39.345964: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:4/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-11 22:42:39.346649: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:39.347255: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:39.347774: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:39.348332: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:39.348523: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 22:42:39.348607: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:39.348658: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 22:42:39.349007: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:39.349585: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:39.349733: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12341
2022-12-11 22:42:39.349838: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:3/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-11 22:42:39.350327: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:39.350922: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:39.351457: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:39.351931: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:6/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-11 22:42:39.358065: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:39.358671: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:39.359189: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:39.359761: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:39.360266: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:39.360923: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:7/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-11 22:42:39.367016: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:39.367707: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:39.368293: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:39.368766: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 22:42:39.368812: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-11 22:42:39.386762: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:39.387442: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:39.387966: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:39.388542: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:39.389066: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:42:39.389532: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:5/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-11 22:42:39.391503: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 22:42:39.391709: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 22:42:39.392837: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12344
2022-12-11 22:42:39.396692: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 22:42:39.396828: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 22:42:39.398260: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12343
2022-12-11 22:42:39.398814: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 22:42:39.398967: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 22:42:39.400175: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12346
2022-12-11 22:42:39.403613: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 22:42:39.403737: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 22:42:39.404709: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12342
2022-12-11 22:42:39.407588: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 22:42:39.407729: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 22:42:39.409427: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12347
2022-12-11 22:42:39.436726: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 22:42:39.436931: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 22:42:39.438693: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12345
[HCTR][22:42:40.718][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][22:42:40.719][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][22:42:40.719][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][22:42:40.719][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][22:42:40.719][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][22:42:40.719][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][22:42:40.719][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][22:42:40.719][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 1it [00:01,  1.61s/it]warmup run: 1it [00:01,  1.62s/it]warmup run: 98it [00:01, 79.73it/s]warmup run: 99it [00:01, 80.04it/s]warmup run: 1it [00:01,  1.55s/it]warmup run: 1it [00:01,  1.52s/it]warmup run: 187it [00:01, 164.30it/s]warmup run: 200it [00:01, 176.53it/s]warmup run: 1it [00:01,  1.52s/it]warmup run: 75it [00:01, 62.93it/s]warmup run: 95it [00:01, 81.19it/s]warmup run: 1it [00:01,  1.51s/it]warmup run: 262it [00:01, 239.95it/s]warmup run: 301it [00:01, 283.81it/s]warmup run: 98it [00:01, 84.09it/s]warmup run: 168it [00:01, 156.04it/s]warmup run: 191it [00:01, 177.03it/s]warmup run: 101it [00:01, 87.00it/s]warmup run: 359it [00:02, 353.76it/s]warmup run: 401it [00:02, 394.97it/s]warmup run: 196it [00:01, 182.18it/s]warmup run: 1it [00:01,  1.47s/it]warmup run: 1it [00:01,  1.50s/it]warmup run: 267it [00:01, 266.67it/s]warmup run: 287it [00:01, 282.23it/s]warmup run: 202it [00:01, 188.28it/s]warmup run: 456it [00:02, 466.01it/s]warmup run: 502it [00:02, 505.83it/s]warmup run: 295it [00:01, 291.30it/s]warmup run: 100it [00:01, 88.22it/s]warmup run: 98it [00:01, 85.00it/s]warmup run: 367it [00:01, 383.45it/s]warmup run: 384it [00:01, 392.60it/s]warmup run: 303it [00:01, 299.34it/s]warmup run: 555it [00:02, 574.00it/s]warmup run: 603it [00:02, 608.82it/s]warmup run: 394it [00:01, 404.07it/s]warmup run: 200it [00:01, 190.53it/s]warmup run: 196it [00:01, 183.93it/s]warmup run: 467it [00:02, 497.80it/s]warmup run: 479it [00:02, 496.29it/s]warmup run: 404it [00:01, 414.27it/s]warmup run: 654it [00:02, 667.58it/s]warmup run: 705it [00:02, 700.90it/s]warmup run: 494it [00:02, 514.98it/s]warmup run: 300it [00:01, 302.22it/s]warmup run: 295it [00:01, 293.53it/s]warmup run: 568it [00:02, 603.85it/s]warmup run: 505it [00:02, 525.36it/s]warmup run: 569it [00:02, 579.17it/s]warmup run: 753it [00:02, 745.17it/s]warmup run: 807it [00:02, 777.63it/s]warmup run: 595it [00:02, 618.80it/s]warmup run: 401it [00:01, 418.39it/s]warmup run: 395it [00:01, 408.15it/s]warmup run: 669it [00:02, 697.24it/s]warmup run: 669it [00:02, 675.38it/s]warmup run: 608it [00:02, 631.16it/s]warmup run: 850it [00:02, 802.08it/s]warmup run: 908it [00:02, 836.28it/s]warmup run: 697it [00:02, 710.48it/s]warmup run: 502it [00:01, 530.84it/s]warmup run: 494it [00:02, 517.39it/s]warmup run: 771it [00:02, 775.21it/s]warmup run: 711it [00:02, 722.11it/s]warmup run: 768it [00:02, 751.23it/s]warmup run: 947it [00:02, 847.27it/s]warmup run: 1009it [00:02, 882.89it/s]warmup run: 798it [00:02, 784.62it/s]warmup run: 606it [00:02, 638.57it/s]warmup run: 595it [00:02, 620.80it/s]warmup run: 873it [00:02, 837.35it/s]warmup run: 812it [00:02, 793.49it/s]warmup run: 865it [00:02, 807.95it/s]warmup run: 1046it [00:02, 885.30it/s]warmup run: 1111it [00:02, 918.79it/s]warmup run: 899it [00:02, 841.82it/s]warmup run: 709it [00:02, 729.42it/s]warmup run: 696it [00:02, 709.70it/s]warmup run: 975it [00:02, 884.33it/s]warmup run: 913it [00:02, 850.13it/s]warmup run: 961it [00:02, 838.93it/s]warmup run: 1144it [00:02, 911.66it/s]warmup run: 1212it [00:02, 942.06it/s]warmup run: 1000it [00:02, 886.13it/s]warmup run: 812it [00:02, 803.88it/s]warmup run: 798it [00:02, 786.75it/s]warmup run: 1077it [00:02, 921.21it/s]warmup run: 1013it [00:02, 888.79it/s]warmup run: 1060it [00:02, 879.05it/s]warmup run: 1243it [00:02, 932.57it/s]warmup run: 1314it [00:02, 963.71it/s]warmup run: 1102it [00:02, 923.09it/s]warmup run: 915it [00:02, 861.19it/s]warmup run: 899it [00:02, 844.84it/s]warmup run: 1179it [00:02, 947.91it/s]warmup run: 1114it [00:02, 921.52it/s]warmup run: 1159it [00:02, 909.15it/s]warmup run: 1342it [00:03, 948.31it/s]warmup run: 1417it [00:03, 982.13it/s]warmup run: 1204it [00:02, 948.70it/s]warmup run: 1018it [00:02, 905.39it/s]warmup run: 999it [00:02, 884.89it/s]warmup run: 1281it [00:02, 966.59it/s]warmup run: 1215it [00:02, 945.04it/s]warmup run: 1259it [00:02, 933.09it/s]warmup run: 1443it [00:03, 966.32it/s]warmup run: 1520it [00:03, 995.58it/s]warmup run: 1307it [00:02, 970.05it/s]warmup run: 1120it [00:02, 937.03it/s]warmup run: 1099it [00:02, 916.39it/s]warmup run: 1386it [00:02, 989.47it/s]warmup run: 1359it [00:02, 951.95it/s]warmup run: 1316it [00:02, 946.25it/s]warmup run: 1545it [00:03, 979.64it/s]warmup run: 1624it [00:03, 1006.06it/s]warmup run: 1409it [00:02, 981.87it/s]warmup run: 1225it [00:02, 966.75it/s]warmup run: 1199it [00:02, 936.81it/s]warmup run: 1490it [00:03, 1004.04it/s]warmup run: 1415it [00:02, 957.85it/s]warmup run: 1647it [00:03, 990.17it/s]warmup run: 1457it [00:03, 945.50it/s]warmup run: 1728it [00:03, 1013.32it/s]warmup run: 1512it [00:03, 994.27it/s]warmup run: 1328it [00:02, 984.42it/s]warmup run: 1299it [00:02, 954.47it/s]warmup run: 1595it [00:03, 1015.17it/s]warmup run: 1750it [00:03, 1001.28it/s]warmup run: 1516it [00:03, 971.37it/s]warmup run: 1555it [00:03, 955.02it/s]warmup run: 1831it [00:03, 1017.66it/s]warmup run: 1615it [00:03, 1004.49it/s]warmup run: 1433it [00:02, 1001.43it/s]warmup run: 1399it [00:02, 966.16it/s]warmup run: 1701it [00:03, 1026.32it/s]warmup run: 1616it [00:03, 979.37it/s]warmup run: 1854it [00:03, 1009.75it/s]warmup run: 1656it [00:03, 970.53it/s]warmup run: 1934it [00:03, 1013.17it/s]warmup run: 1719it [00:03, 1013.85it/s]warmup run: 1538it [00:02, 1013.17it/s]warmup run: 1499it [00:03, 974.02it/s]warmup run: 1805it [00:03, 1030.27it/s]warmup run: 1716it [00:03, 984.77it/s]warmup run: 1956it [00:03, 1008.77it/s]warmup run: 1759it [00:03, 986.48it/s]warmup run: 2041it [00:03, 1028.96it/s]warmup run: 1823it [00:03, 1020.58it/s]warmup run: 1643it [00:03, 1022.53it/s]warmup run: 1599it [00:03, 973.64it/s]warmup run: 1910it [00:03, 1035.02it/s]warmup run: 1816it [00:03, 987.21it/s]warmup run: 2058it [00:03, 1005.70it/s]warmup run: 1864it [00:03, 1003.51it/s]warmup run: 2164it [00:03, 1086.53it/s]warmup run: 1926it [00:03, 1022.00it/s]warmup run: 1748it [00:03, 1028.18it/s]warmup run: 1698it [00:03, 967.20it/s]warmup run: 2016it [00:03, 1041.69it/s]warmup run: 1916it [00:03, 990.16it/s]warmup run: 1968it [00:03, 1014.15it/s]warmup run: 2179it [00:03, 1065.29it/s]warmup run: 2287it [00:03, 1127.35it/s]warmup run: 2034it [00:03, 1038.45it/s]warmup run: 1853it [00:03, 1031.92it/s]warmup run: 1796it [00:03, 964.80it/s]warmup run: 2134it [00:03, 1082.61it/s]warmup run: 2019it [00:03, 1001.75it/s]warmup run: 2084it [00:03, 1056.15it/s]warmup run: 2301it [00:03, 1108.75it/s]warmup run: 2408it [00:03, 1151.46it/s]warmup run: 2152it [00:03, 1079.98it/s]warmup run: 1958it [00:03, 1034.71it/s]warmup run: 1894it [00:03, 967.82it/s]warmup run: 2250it [00:03, 1104.70it/s]warmup run: 2140it [00:03, 1061.57it/s]warmup run: 2207it [00:03, 1106.45it/s]warmup run: 2423it [00:04, 1139.76it/s]warmup run: 2528it [00:04, 1165.92it/s]warmup run: 2272it [00:03, 1113.59it/s]warmup run: 2072it [00:03, 1064.89it/s]warmup run: 1994it [00:03, 974.70it/s]warmup run: 2369it [00:03, 1128.87it/s]warmup run: 2260it [00:03, 1100.06it/s]warmup run: 2330it [00:03, 1141.92it/s]warmup run: 2545it [00:04, 1161.83it/s]warmup run: 2648it [00:04, 1174.25it/s]warmup run: 2392it [00:03, 1138.36it/s]warmup run: 2193it [00:03, 1107.57it/s]warmup run: 2110it [00:03, 1027.67it/s]warmup run: 2488it [00:03, 1147.09it/s]warmup run: 2381it [00:03, 1130.91it/s]warmup run: 2453it [00:03, 1165.90it/s]warmup run: 2665it [00:04, 1172.60it/s]warmup run: 2769it [00:04, 1184.00it/s]warmup run: 2512it [00:03, 1155.88it/s]warmup run: 2314it [00:03, 1137.03it/s]warmup run: 2229it [00:03, 1073.64it/s]warmup run: 2604it [00:04, 1149.87it/s]warmup run: 2502it [00:03, 1152.43it/s]warmup run: 2576it [00:04, 1182.98it/s]warmup run: 2787it [00:04, 1184.09it/s]warmup run: 2890it [00:04, 1191.35it/s]warmup run: 2631it [00:04, 1163.88it/s]warmup run: 2435it [00:03, 1158.05it/s]warmup run: 2345it [00:03, 1098.71it/s]warmup run: 2720it [00:04, 1134.93it/s]warmup run: 2618it [00:04, 1153.65it/s]warmup run: 2908it [00:04, 1191.37it/s]warmup run: 2698it [00:04, 1191.52it/s]warmup run: 3000it [00:04, 676.99it/s] warmup run: 2750it [00:04, 1171.28it/s]warmup run: 2556it [00:03, 1171.87it/s]warmup run: 2460it [00:03, 1113.14it/s]warmup run: 3000it [00:04, 665.89it/s] warmup run: 2834it [00:04, 1125.31it/s]warmup run: 2734it [00:04, 1153.71it/s]warmup run: 2818it [00:04, 1186.90it/s]warmup run: 2870it [00:04, 1178.16it/s]warmup run: 2676it [00:03, 1178.18it/s]warmup run: 2576it [00:04, 1125.08it/s]warmup run: 2947it [00:04, 1120.66it/s]warmup run: 2850it [00:04, 1152.83it/s]warmup run: 2937it [00:04, 1186.50it/s]warmup run: 2989it [00:04, 1181.61it/s]warmup run: 2797it [00:04, 1187.18it/s]warmup run: 3000it [00:04, 690.33it/s] warmup run: 3000it [00:04, 677.41it/s] warmup run: 3000it [00:04, 680.40it/s] warmup run: 2691it [00:04, 1132.25it/s]warmup run: 2970it [00:04, 1165.00it/s]warmup run: 2917it [00:04, 1190.83it/s]warmup run: 3000it [00:04, 687.71it/s] warmup run: 2810it [00:04, 1149.30it/s]warmup run: 3000it [00:04, 704.38it/s] warmup run: 2929it [00:04, 1159.96it/s]warmup run: 3000it [00:04, 683.99it/s] 

warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]


warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   6%|▌         | 165/3000 [00:00<00:01, 1649.27it/s]warmup should be done:   5%|▌         | 155/3000 [00:00<00:01, 1549.53it/s]warmup should be done:   5%|▌         | 161/3000 [00:00<00:01, 1607.09it/s]warmup should be done:   6%|▌         | 166/3000 [00:00<00:01, 1654.31it/s]warmup should be done:   5%|▌         | 162/3000 [00:00<00:01, 1611.29it/s]warmup should be done:   5%|▌         | 164/3000 [00:00<00:01, 1631.17it/s]warmup should be done:   5%|▌         | 164/3000 [00:00<00:01, 1631.14it/s]warmup should be done:   5%|▌         | 156/3000 [00:00<00:01, 1552.01it/s]warmup should be done:  11%|█         | 331/3000 [00:00<00:01, 1652.42it/s]warmup should be done:  11%|█         | 334/3000 [00:00<00:01, 1668.50it/s]warmup should be done:  11%|█         | 324/3000 [00:00<00:01, 1614.47it/s]warmup should be done:  11%|█         | 320/3000 [00:00<00:01, 1603.19it/s]warmup should be done:  11%|█         | 329/3000 [00:00<00:01, 1639.50it/s]warmup should be done:  11%|█         | 324/3000 [00:00<00:01, 1614.84it/s]warmup should be done:  11%|█         | 329/3000 [00:00<00:01, 1638.10it/s]warmup should be done:  11%|█         | 316/3000 [00:00<00:01, 1576.43it/s]warmup should be done:  17%|█▋        | 501/3000 [00:00<00:01, 1667.27it/s]warmup should be done:  16%|█▌        | 486/3000 [00:00<00:01, 1612.96it/s]warmup should be done:  16%|█▌        | 483/3000 [00:00<00:01, 1612.71it/s]warmup should be done:  16%|█▋        | 493/3000 [00:00<00:01, 1635.86it/s]warmup should be done:  16%|█▌        | 486/3000 [00:00<00:01, 1613.17it/s]warmup should be done:  16%|█▋        | 493/3000 [00:00<00:01, 1634.70it/s]warmup should be done:  16%|█▌        | 475/3000 [00:00<00:01, 1578.09it/s]warmup should be done:  17%|█▋        | 497/3000 [00:00<00:01, 1642.76it/s]warmup should be done:  22%|██▏       | 669/3000 [00:00<00:01, 1669.39it/s]warmup should be done:  22%|██▏       | 645/3000 [00:00<00:01, 1613.37it/s]warmup should be done:  22%|██▏       | 657/3000 [00:00<00:01, 1634.61it/s]warmup should be done:  22%|██▏       | 648/3000 [00:00<00:01, 1609.77it/s]warmup should be done:  22%|██▏       | 657/3000 [00:00<00:01, 1632.32it/s]warmup should be done:  22%|██▏       | 648/3000 [00:00<00:01, 1606.43it/s]warmup should be done:  21%|██        | 633/3000 [00:00<00:01, 1570.51it/s]warmup should be done:  22%|██▏       | 662/3000 [00:00<00:01, 1637.80it/s]warmup should be done:  27%|██▋       | 808/3000 [00:00<00:01, 1619.23it/s]warmup should be done:  27%|██▋       | 809/3000 [00:00<00:01, 1606.31it/s]warmup should be done:  26%|██▋       | 791/3000 [00:00<00:01, 1573.62it/s]warmup should be done:  27%|██▋       | 821/3000 [00:00<00:01, 1627.76it/s]warmup should be done:  27%|██▋       | 809/3000 [00:00<00:01, 1596.71it/s]warmup should be done:  27%|██▋       | 821/3000 [00:00<00:01, 1622.00it/s]warmup should be done:  28%|██▊       | 826/3000 [00:00<00:01, 1631.49it/s]warmup should be done:  28%|██▊       | 836/3000 [00:00<00:01, 1645.87it/s]warmup should be done:  32%|███▏      | 971/3000 [00:00<00:01, 1622.30it/s]warmup should be done:  32%|███▏      | 949/3000 [00:00<00:01, 1572.20it/s]warmup should be done:  32%|███▏      | 970/3000 [00:00<00:01, 1599.34it/s]warmup should be done:  33%|███▎      | 984/3000 [00:00<00:01, 1619.49it/s]warmup should be done:  32%|███▏      | 969/3000 [00:00<00:01, 1594.98it/s]warmup should be done:  33%|███▎      | 1002/3000 [00:00<00:01, 1647.54it/s]warmup should be done:  33%|███▎      | 990/3000 [00:00<00:01, 1626.78it/s]warmup should be done:  33%|███▎      | 984/3000 [00:00<00:01, 1616.30it/s]warmup should be done:  38%|███▊      | 1136/3000 [00:00<00:01, 1628.39it/s]warmup should be done:  38%|███▊      | 1130/3000 [00:00<00:01, 1598.59it/s]warmup should be done:  37%|███▋      | 1107/3000 [00:00<00:01, 1568.35it/s]warmup should be done:  38%|███▊      | 1146/3000 [00:00<00:01, 1618.37it/s]warmup should be done:  38%|███▊      | 1129/3000 [00:00<00:01, 1596.05it/s]warmup should be done:  39%|███▉      | 1169/3000 [00:00<00:01, 1651.95it/s]warmup should be done:  38%|███▊      | 1146/3000 [00:00<00:01, 1616.49it/s]warmup should be done:  38%|███▊      | 1153/3000 [00:00<00:01, 1622.03it/s]warmup should be done:  43%|████▎     | 1299/3000 [00:00<00:01, 1626.08it/s]warmup should be done:  43%|████▎     | 1290/3000 [00:00<00:01, 1597.81it/s]warmup should be done:  43%|████▎     | 1289/3000 [00:00<00:01, 1597.26it/s]warmup should be done:  44%|████▎     | 1308/3000 [00:00<00:01, 1616.52it/s]warmup should be done:  42%|████▏     | 1264/3000 [00:00<00:01, 1564.04it/s]warmup should be done:  44%|████▎     | 1308/3000 [00:00<00:01, 1615.74it/s]warmup should be done:  45%|████▍     | 1336/3000 [00:00<00:01, 1654.69it/s]warmup should be done:  44%|████▍     | 1316/3000 [00:00<00:01, 1623.07it/s]warmup should be done:  49%|████▊     | 1462/3000 [00:00<00:00, 1617.17it/s]warmup should be done:  48%|████▊     | 1450/3000 [00:00<00:00, 1596.68it/s]warmup should be done:  48%|████▊     | 1449/3000 [00:00<00:00, 1597.26it/s]warmup should be done:  49%|████▉     | 1470/3000 [00:00<00:00, 1615.66it/s]warmup should be done:  50%|█████     | 1502/3000 [00:00<00:00, 1655.48it/s]warmup should be done:  49%|████▉     | 1470/3000 [00:00<00:00, 1615.33it/s]warmup should be done:  49%|████▉     | 1479/3000 [00:00<00:00, 1623.69it/s]warmup should be done:  47%|████▋     | 1421/3000 [00:00<00:01, 1556.66it/s]warmup should be done:  54%|█████▍    | 1628/3000 [00:01<00:00, 1628.34it/s]warmup should be done:  54%|█████▎    | 1610/3000 [00:01<00:00, 1592.76it/s]warmup should be done:  54%|█████▍    | 1632/3000 [00:01<00:00, 1615.58it/s]warmup should be done:  54%|█████▎    | 1609/3000 [00:01<00:00, 1591.38it/s]warmup should be done:  56%|█████▌    | 1668/3000 [00:01<00:00, 1656.42it/s]warmup should be done:  54%|█████▍    | 1632/3000 [00:01<00:00, 1615.63it/s]warmup should be done:  53%|█████▎    | 1578/3000 [00:01<00:00, 1560.55it/s]warmup should be done:  55%|█████▍    | 1642/3000 [00:01<00:00, 1623.87it/s]warmup should be done:  59%|█████▉    | 1770/3000 [00:01<00:00, 1594.54it/s]warmup should be done:  60%|█████▉    | 1794/3000 [00:01<00:00, 1615.06it/s]warmup should be done:  61%|██████    | 1834/3000 [00:01<00:00, 1657.37it/s]warmup should be done:  59%|█████▉    | 1770/3000 [00:01<00:00, 1594.38it/s]warmup should be done:  60%|█████▉    | 1794/3000 [00:01<00:00, 1615.79it/s]warmup should be done:  60%|██████    | 1805/3000 [00:01<00:00, 1623.66it/s]warmup should be done:  58%|█████▊    | 1736/3000 [00:01<00:00, 1564.01it/s]warmup should be done:  60%|█████▉    | 1791/3000 [00:01<00:00, 1603.82it/s]warmup should be done:  64%|██████▍   | 1932/3000 [00:01<00:00, 1600.09it/s]warmup should be done:  65%|██████▌   | 1957/3000 [00:01<00:00, 1618.74it/s]warmup should be done:  67%|██████▋   | 2001/3000 [00:01<00:00, 1658.56it/s]warmup should be done:  64%|██████▍   | 1931/3000 [00:01<00:00, 1596.54it/s]warmup should be done:  65%|██████▌   | 1957/3000 [00:01<00:00, 1617.25it/s]warmup should be done:  63%|██████▎   | 1893/3000 [00:01<00:00, 1565.50it/s]warmup should be done:  66%|██████▌   | 1968/3000 [00:01<00:00, 1624.83it/s]warmup should be done:  65%|██████▌   | 1957/3000 [00:01<00:00, 1618.01it/s]warmup should be done:  70%|██████▉   | 2095/3000 [00:01<00:00, 1608.58it/s]warmup should be done:  71%|███████   | 2121/3000 [00:01<00:00, 1622.23it/s]warmup should be done:  70%|██████▉   | 2091/3000 [00:01<00:00, 1595.61it/s]warmup should be done:  71%|███████   | 2120/3000 [00:01<00:00, 1619.75it/s]warmup should be done:  72%|███████▏  | 2167/3000 [00:01<00:00, 1653.46it/s]warmup should be done:  68%|██████▊   | 2050/3000 [00:01<00:00, 1566.16it/s]warmup should be done:  71%|███████   | 2131/3000 [00:01<00:00, 1625.51it/s]warmup should be done:  71%|███████   | 2123/3000 [00:01<00:00, 1627.34it/s]warmup should be done:  75%|███████▌  | 2258/3000 [00:01<00:00, 1613.38it/s]warmup should be done:  76%|███████▌  | 2285/3000 [00:01<00:00, 1626.99it/s]warmup should be done:  75%|███████▌  | 2251/3000 [00:01<00:00, 1595.18it/s]warmup should be done:  76%|███████▌  | 2282/3000 [00:01<00:00, 1617.16it/s]warmup should be done:  76%|███████▋  | 2294/3000 [00:01<00:00, 1625.29it/s]warmup should be done:  74%|███████▎  | 2207/3000 [00:01<00:00, 1561.80it/s]warmup should be done:  78%|███████▊  | 2333/3000 [00:01<00:00, 1647.59it/s]warmup should be done:  76%|███████▋  | 2288/3000 [00:01<00:00, 1632.52it/s]warmup should be done:  81%|████████  | 2422/3000 [00:01<00:00, 1619.91it/s]warmup should be done:  82%|████████▏ | 2450/3000 [00:01<00:00, 1633.07it/s]warmup should be done:  80%|████████  | 2411/3000 [00:01<00:00, 1596.60it/s]warmup should be done:  82%|████████▏ | 2445/3000 [00:01<00:00, 1619.43it/s]warmup should be done:  82%|████████▏ | 2457/3000 [00:01<00:00, 1621.09it/s]warmup should be done:  83%|████████▎ | 2498/3000 [00:01<00:00, 1647.95it/s]warmup should be done:  79%|███████▉  | 2364/3000 [00:01<00:00, 1560.94it/s]warmup should be done:  82%|████████▏ | 2454/3000 [00:01<00:00, 1638.15it/s]warmup should be done:  86%|████████▌ | 2586/3000 [00:01<00:00, 1624.51it/s]warmup should be done:  87%|████████▋ | 2616/3000 [00:01<00:00, 1638.81it/s]warmup should be done:  86%|████████▌ | 2574/3000 [00:01<00:00, 1604.29it/s]warmup should be done:  87%|████████▋ | 2609/3000 [00:01<00:00, 1623.76it/s]warmup should be done:  89%|████████▉ | 2664/3000 [00:01<00:00, 1650.53it/s]warmup should be done:  84%|████████▍ | 2521/3000 [00:01<00:00, 1563.53it/s]warmup should be done:  87%|████████▋ | 2620/3000 [00:01<00:00, 1621.49it/s]warmup should be done:  87%|████████▋ | 2620/3000 [00:01<00:00, 1642.99it/s]warmup should be done:  92%|█████████▏| 2750/3000 [00:01<00:00, 1626.97it/s]warmup should be done:  93%|█████████▎| 2780/3000 [00:01<00:00, 1636.81it/s]warmup should be done:  91%|█████████ | 2737/3000 [00:01<00:00, 1609.16it/s]warmup should be done:  92%|█████████▏| 2773/3000 [00:01<00:00, 1626.37it/s]warmup should be done:  89%|████████▉ | 2678/3000 [00:01<00:00, 1565.41it/s]warmup should be done:  94%|█████████▍| 2831/3000 [00:01<00:00, 1655.15it/s]warmup should be done:  93%|█████████▎| 2783/3000 [00:01<00:00, 1622.75it/s]warmup should be done:  93%|█████████▎| 2786/3000 [00:01<00:00, 1646.86it/s]warmup should be done:  97%|█████████▋| 2915/3000 [00:01<00:00, 1632.02it/s]warmup should be done:  98%|█████████▊| 2945/3000 [00:01<00:00, 1639.28it/s]warmup should be done:  97%|█████████▋| 2900/3000 [00:01<00:00, 1614.70it/s]warmup should be done:  98%|█████████▊| 2939/3000 [00:01<00:00, 1635.29it/s]warmup should be done:  94%|█████████▍| 2835/3000 [00:01<00:00, 1565.89it/s]warmup should be done: 100%|█████████▉| 2998/3000 [00:01<00:00, 1658.01it/s]warmup should be done:  98%|█████████▊| 2947/3000 [00:01<00:00, 1625.57it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1655.06it/s]warmup should be done:  98%|█████████▊| 2954/3000 [00:01<00:00, 1655.04it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1629.70it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1627.70it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1626.98it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1623.43it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1612.23it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1602.66it/s]warmup should be done: 100%|█████████▉| 2992/3000 [00:01<00:00, 1566.29it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1565.61it/s]


warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]



warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   6%|▌         | 167/3000 [00:00<00:01, 1669.47it/s]warmup should be done:   6%|▌         | 165/3000 [00:00<00:01, 1643.99it/s]warmup should be done:   6%|▌         | 168/3000 [00:00<00:01, 1675.45it/s]warmup should be done:   6%|▌         | 169/3000 [00:00<00:01, 1684.02it/s]warmup should be done:   5%|▌         | 163/3000 [00:00<00:01, 1622.44it/s]warmup should be done:   6%|▌         | 170/3000 [00:00<00:01, 1690.97it/s]warmup should be done:   6%|▌         | 167/3000 [00:00<00:01, 1660.73it/s]warmup should be done:   6%|▌         | 168/3000 [00:00<00:01, 1671.51it/s]warmup should be done:  11%|█         | 334/3000 [00:00<00:01, 1669.31it/s]warmup should be done:  11%|█▏        | 338/3000 [00:00<00:01, 1687.83it/s]warmup should be done:  11%|█▏        | 338/3000 [00:00<00:01, 1684.41it/s]warmup should be done:  11%|█         | 331/3000 [00:00<00:01, 1647.84it/s]warmup should be done:  11%|█▏        | 341/3000 [00:00<00:01, 1697.82it/s]warmup should be done:  11%|█         | 334/3000 [00:00<00:01, 1661.23it/s]warmup should be done:  11%|█         | 337/3000 [00:00<00:01, 1677.55it/s]warmup should be done:  11%|█         | 326/3000 [00:00<00:01, 1618.01it/s]warmup should be done:  17%|█▋        | 502/3000 [00:00<00:01, 1671.09it/s]warmup should be done:  17%|█▋        | 496/3000 [00:00<00:01, 1648.37it/s]warmup should be done:  17%|█▋        | 508/3000 [00:00<00:01, 1689.52it/s]warmup should be done:  17%|█▋        | 512/3000 [00:00<00:01, 1701.60it/s]warmup should be done:  17%|█▋        | 506/3000 [00:00<00:01, 1681.89it/s]warmup should be done:  17%|█▋        | 501/3000 [00:00<00:01, 1663.37it/s]warmup should be done:  17%|█▋        | 507/3000 [00:00<00:01, 1681.09it/s]warmup should be done:  16%|█▋        | 488/3000 [00:00<00:01, 1605.04it/s]warmup should be done:  22%|██▏       | 670/3000 [00:00<00:01, 1674.14it/s]warmup should be done:  22%|██▏       | 662/3000 [00:00<00:01, 1651.72it/s]warmup should be done:  22%|██▏       | 669/3000 [00:00<00:01, 1668.53it/s]warmup should be done:  23%|██▎       | 683/3000 [00:00<00:01, 1702.12it/s]warmup should be done:  23%|██▎       | 677/3000 [00:00<00:01, 1686.56it/s]warmup should be done:  22%|██▎       | 675/3000 [00:00<00:01, 1681.93it/s]warmup should be done:  23%|██▎       | 677/3000 [00:00<00:01, 1680.22it/s]warmup should be done:  22%|██▏       | 651/3000 [00:00<00:01, 1614.01it/s]warmup should be done:  28%|██▊       | 838/3000 [00:00<00:01, 1675.63it/s]warmup should be done:  28%|██▊       | 847/3000 [00:00<00:01, 1690.73it/s]warmup should be done:  28%|██▊       | 837/3000 [00:00<00:01, 1670.82it/s]warmup should be done:  28%|██▊       | 828/3000 [00:00<00:01, 1648.84it/s]warmup should be done:  28%|██▊       | 854/3000 [00:00<00:01, 1701.08it/s]warmup should be done:  28%|██▊       | 844/3000 [00:00<00:01, 1681.19it/s]warmup should be done:  28%|██▊       | 846/3000 [00:00<00:01, 1674.92it/s]warmup should be done:  27%|██▋       | 815/3000 [00:00<00:01, 1620.32it/s]warmup should be done:  34%|███▎      | 1006/3000 [00:00<00:01, 1673.39it/s]warmup should be done:  34%|███▍      | 1018/3000 [00:00<00:01, 1695.27it/s]warmup should be done:  33%|███▎      | 994/3000 [00:00<00:01, 1650.75it/s]warmup should be done:  34%|███▎      | 1005/3000 [00:00<00:01, 1670.09it/s]warmup should be done:  34%|███▍      | 1025/3000 [00:00<00:01, 1700.93it/s]warmup should be done:  34%|███▍      | 1014/3000 [00:00<00:01, 1684.77it/s]warmup should be done:  33%|███▎      | 978/3000 [00:00<00:01, 1621.63it/s]warmup should be done:  34%|███▍      | 1014/3000 [00:00<00:01, 1673.55it/s]warmup should be done:  39%|███▉      | 1174/3000 [00:00<00:01, 1669.88it/s]warmup should be done:  40%|███▉      | 1189/3000 [00:00<00:01, 1698.22it/s]warmup should be done:  39%|███▉      | 1183/3000 [00:00<00:01, 1685.26it/s]warmup should be done:  39%|███▉      | 1173/3000 [00:00<00:01, 1669.01it/s]warmup should be done:  40%|███▉      | 1196/3000 [00:00<00:01, 1699.92it/s]warmup should be done:  39%|███▊      | 1160/3000 [00:00<00:01, 1643.89it/s]warmup should be done:  38%|███▊      | 1144/3000 [00:00<00:01, 1633.24it/s]warmup should be done:  39%|███▉      | 1182/3000 [00:00<00:01, 1671.98it/s]warmup should be done:  45%|████▌     | 1361/3000 [00:00<00:00, 1703.96it/s]warmup should be done:  45%|████▍     | 1342/3000 [00:00<00:00, 1670.98it/s]warmup should be done:  45%|████▌     | 1352/3000 [00:00<00:00, 1685.77it/s]warmup should be done:  45%|████▍     | 1341/3000 [00:00<00:00, 1669.99it/s]warmup should be done:  46%|████▌     | 1367/3000 [00:00<00:00, 1700.99it/s]warmup should be done:  44%|████▍     | 1313/3000 [00:00<00:01, 1649.09it/s]warmup should be done:  45%|████▌     | 1350/3000 [00:00<00:00, 1670.88it/s]warmup should be done:  44%|████▍     | 1325/3000 [00:00<00:01, 1637.76it/s]warmup should be done:  51%|█████     | 1532/3000 [00:00<00:00, 1705.70it/s]warmup should be done:  51%|█████     | 1521/3000 [00:00<00:00, 1685.24it/s]warmup should be done:  50%|█████     | 1510/3000 [00:00<00:00, 1668.85it/s]warmup should be done:  50%|█████     | 1509/3000 [00:00<00:00, 1670.10it/s]warmup should be done:  51%|█████▏    | 1538/3000 [00:00<00:00, 1699.37it/s]warmup should be done:  49%|████▉     | 1483/3000 [00:00<00:00, 1663.39it/s]warmup should be done:  51%|█████     | 1518/3000 [00:00<00:00, 1673.43it/s]warmup should be done:  50%|████▉     | 1489/3000 [00:00<00:00, 1626.64it/s]warmup should be done:  57%|█████▋    | 1704/3000 [00:01<00:00, 1708.40it/s]warmup should be done:  56%|█████▌    | 1677/3000 [00:01<00:00, 1668.05it/s]warmup should be done:  56%|█████▋    | 1690/3000 [00:01<00:00, 1681.84it/s]warmup should be done:  56%|█████▌    | 1677/3000 [00:01<00:00, 1672.09it/s]warmup should be done:  57%|█████▋    | 1708/3000 [00:01<00:00, 1697.75it/s]warmup should be done:  55%|█████▌    | 1653/3000 [00:01<00:00, 1674.50it/s]warmup should be done:  56%|█████▋    | 1688/3000 [00:01<00:00, 1678.66it/s]warmup should be done:  55%|█████▌    | 1652/3000 [00:01<00:00, 1625.66it/s]warmup should be done:  63%|██████▎   | 1876/3000 [00:01<00:00, 1710.14it/s]warmup should be done:  62%|██████▏   | 1845/3000 [00:01<00:00, 1669.55it/s]warmup should be done:  62%|██████▏   | 1845/3000 [00:01<00:00, 1673.70it/s]warmup should be done:  62%|██████▏   | 1859/3000 [00:01<00:00, 1682.41it/s]warmup should be done:  61%|██████    | 1823/3000 [00:01<00:00, 1681.83it/s]warmup should be done:  63%|██████▎   | 1879/3000 [00:01<00:00, 1699.06it/s]warmup should be done:  62%|██████▏   | 1857/3000 [00:01<00:00, 1680.62it/s]warmup should be done:  60%|██████    | 1815/3000 [00:01<00:00, 1614.49it/s]warmup should be done:  68%|██████▊   | 2048/3000 [00:01<00:00, 1711.65it/s]warmup should be done:  67%|██████▋   | 2013/3000 [00:01<00:00, 1671.11it/s]warmup should be done:  68%|██████▊   | 2028/3000 [00:01<00:00, 1682.11it/s]warmup should be done:  67%|██████▋   | 2013/3000 [00:01<00:00, 1671.70it/s]warmup should be done:  68%|██████▊   | 2050/3000 [00:01<00:00, 1700.76it/s]warmup should be done:  66%|██████▋   | 1993/3000 [00:01<00:00, 1684.42it/s]warmup should be done:  68%|██████▊   | 2026/3000 [00:01<00:00, 1672.41it/s]warmup should be done:  66%|██████▌   | 1979/3000 [00:01<00:00, 1622.10it/s]warmup should be done:  74%|███████▍  | 2220/3000 [00:01<00:00, 1710.78it/s]warmup should be done:  73%|███████▎  | 2181/3000 [00:01<00:00, 1670.74it/s]warmup should be done:  73%|███████▎  | 2181/3000 [00:01<00:00, 1670.76it/s]warmup should be done:  73%|███████▎  | 2197/3000 [00:01<00:00, 1679.98it/s]warmup should be done:  72%|███████▏  | 2163/3000 [00:01<00:00, 1686.68it/s]warmup should be done:  73%|███████▎  | 2194/3000 [00:01<00:00, 1671.98it/s]warmup should be done:  74%|███████▍  | 2221/3000 [00:01<00:00, 1670.66it/s]warmup should be done:  71%|███████▏  | 2142/3000 [00:01<00:00, 1597.10it/s]warmup should be done:  80%|███████▉  | 2392/3000 [00:01<00:00, 1709.71it/s]warmup should be done:  78%|███████▊  | 2333/3000 [00:01<00:00, 1689.81it/s]warmup should be done:  78%|███████▊  | 2349/3000 [00:01<00:00, 1670.68it/s]warmup should be done:  79%|███████▉  | 2366/3000 [00:01<00:00, 1680.37it/s]warmup should be done:  78%|███████▊  | 2349/3000 [00:01<00:00, 1663.35it/s]warmup should be done:  79%|███████▉  | 2363/3000 [00:01<00:00, 1674.89it/s]warmup should be done:  80%|███████▉  | 2390/3000 [00:01<00:00, 1675.87it/s]warmup should be done:  77%|███████▋  | 2302/3000 [00:01<00:00, 1596.01it/s]warmup should be done:  85%|████████▌ | 2564/3000 [00:01<00:00, 1710.20it/s]warmup should be done:  83%|████████▎ | 2503/3000 [00:01<00:00, 1691.06it/s]warmup should be done:  84%|████████▍ | 2517/3000 [00:01<00:00, 1670.35it/s]warmup should be done:  84%|████████▍ | 2535/3000 [00:01<00:00, 1680.54it/s]warmup should be done:  84%|████████▍ | 2517/3000 [00:01<00:00, 1665.42it/s]warmup should be done:  84%|████████▍ | 2533/3000 [00:01<00:00, 1680.81it/s]warmup should be done:  85%|████████▌ | 2561/3000 [00:01<00:00, 1684.12it/s]warmup should be done:  82%|████████▏ | 2464/3000 [00:01<00:00, 1603.00it/s]warmup should be done:  91%|█████████ | 2736/3000 [00:01<00:00, 1710.57it/s]warmup should be done:  89%|████████▉ | 2673/3000 [00:01<00:00, 1690.99it/s]warmup should be done:  90%|█████████ | 2704/3000 [00:01<00:00, 1681.28it/s]warmup should be done:  90%|████████▉ | 2685/3000 [00:01<00:00, 1667.67it/s]warmup should be done:  90%|████████▉ | 2685/3000 [00:01<00:00, 1667.88it/s]warmup should be done:  90%|█████████ | 2704/3000 [00:01<00:00, 1686.64it/s]warmup should be done:  91%|█████████ | 2731/3000 [00:01<00:00, 1686.20it/s]warmup should be done:  88%|████████▊ | 2625/3000 [00:01<00:00, 1596.54it/s]warmup should be done:  97%|█████████▋| 2908/3000 [00:01<00:00, 1709.66it/s]warmup should be done:  95%|█████████▍| 2843/3000 [00:01<00:00, 1689.84it/s]warmup should be done:  96%|█████████▌| 2873/3000 [00:01<00:00, 1680.73it/s]warmup should be done:  95%|█████████▌| 2853/3000 [00:01<00:00, 1668.59it/s]warmup should be done:  95%|█████████▌| 2852/3000 [00:01<00:00, 1665.45it/s]warmup should be done:  96%|█████████▌| 2874/3000 [00:01<00:00, 1689.07it/s]warmup should be done:  97%|█████████▋| 2902/3000 [00:01<00:00, 1690.51it/s]warmup should be done:  93%|█████████▎| 2790/3000 [00:01<00:00, 1612.27it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1703.13it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1693.25it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1681.73it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1680.22it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1669.93it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1667.94it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1665.79it/s]warmup should be done:  98%|█████████▊| 2954/3000 [00:01<00:00, 1619.73it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1623.49it/s]2022-12-11 22:44:15.434175: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f6ca3794e80 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 22:44:15.434236: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 22:44:16.502722: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f5004031360 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 22:44:16.502785: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 22:44:16.510825: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f50d402a490 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 22:44:16.510872: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 22:44:16.512996: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f6cab8306e0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 22:44:16.513058: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 22:44:16.833283: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x1d24a730 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 22:44:16.833354: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 22:44:17.015820: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f50c002e110 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 22:44:17.015888: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 22:44:17.016537: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f6caf82c050 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 22:44:17.016599: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 22:44:17.073862: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f6ca7833790 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 22:44:17.073938: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 22:44:17.693661: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 22:44:18.741052: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 22:44:18.753097: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 22:44:18.842162: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 22:44:19.158244: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 22:44:19.252869: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 22:44:19.343019: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 22:44:19.413601: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 22:44:20.551146: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 22:44:21.590621: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 22:44:21.620742: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 22:44:21.716871: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 22:44:22.072315: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 22:44:22.105187: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 22:44:22.203904: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 22:44:22.308545: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
[HCTR][22:44:59.426][ERROR][RK0][tid #140105423501056]: replica 1 reaches 1000, calling init pre replica
[HCTR][22:44:59.426][ERROR][RK0][tid #140105423501056]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][22:44:59.436][ERROR][RK0][tid #140105423501056]: coll ps creation done
[HCTR][22:44:59.436][ERROR][RK0][tid #140105423501056]: replica 1 waits for coll ps creation barrier
[HCTR][22:44:59.443][ERROR][RK0][tid #140105155065600]: replica 7 reaches 1000, calling init pre replica
[HCTR][22:44:59.443][ERROR][RK0][tid #140105155065600]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][22:44:59.448][ERROR][RK0][tid #140105155065600]: coll ps creation done
[HCTR][22:44:59.448][ERROR][RK0][tid #140105155065600]: replica 7 waits for coll ps creation barrier
[HCTR][22:44:59.462][ERROR][RK0][tid #140105029240576]: replica 3 reaches 1000, calling init pre replica
[HCTR][22:44:59.462][ERROR][RK0][tid #140105029240576]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][22:44:59.470][ERROR][RK0][tid #140105029240576]: coll ps creation done
[HCTR][22:44:59.470][ERROR][RK0][tid #140105029240576]: replica 3 waits for coll ps creation barrier
[HCTR][22:44:59.476][ERROR][RK0][main]: replica 0 reaches 1000, calling init pre replica
[HCTR][22:44:59.476][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][22:44:59.476][ERROR][RK0][tid #140105155065600]: replica 2 reaches 1000, calling init pre replica
[HCTR][22:44:59.476][ERROR][RK0][tid #140105155065600]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][22:44:59.482][ERROR][RK0][tid #140105155065600]: coll ps creation done
[HCTR][22:44:59.482][ERROR][RK0][tid #140105155065600]: replica 2 waits for coll ps creation barrier
[HCTR][22:44:59.485][ERROR][RK0][main]: coll ps creation done
[HCTR][22:44:59.485][ERROR][RK0][main]: replica 0 waits for coll ps creation barrier
[HCTR][22:44:59.491][ERROR][RK0][main]: replica 6 reaches 1000, calling init pre replica
[HCTR][22:44:59.491][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][22:44:59.496][ERROR][RK0][main]: coll ps creation done
[HCTR][22:44:59.496][ERROR][RK0][main]: replica 6 waits for coll ps creation barrier
[HCTR][22:44:59.706][ERROR][RK0][tid #140105146672896]: replica 4 reaches 1000, calling init pre replica
[HCTR][22:44:59.707][ERROR][RK0][tid #140105146672896]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][22:44:59.711][ERROR][RK0][tid #140105146672896]: coll ps creation done
[HCTR][22:44:59.711][ERROR][RK0][tid #140105146672896]: replica 4 waits for coll ps creation barrier
[HCTR][22:44:59.765][ERROR][RK0][main]: replica 5 reaches 1000, calling init pre replica
[HCTR][22:44:59.765][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][22:44:59.772][ERROR][RK0][main]: coll ps creation done
[HCTR][22:44:59.773][ERROR][RK0][main]: replica 5 waits for coll ps creation barrier
[HCTR][22:44:59.773][ERROR][RK0][main]: replica 0 preparing frequency
[HCTR][22:45:00.655][ERROR][RK0][main]: replica 0 preparing frequency done
[HCTR][22:45:00.689][ERROR][RK0][main]: replica 0 calling init per replica
[HCTR][22:45:00.689][ERROR][RK0][main]: replica 5 calling init per replica
[HCTR][22:45:00.689][ERROR][RK0][tid #140105029240576]: replica 3 calling init per replica
[HCTR][22:45:00.689][ERROR][RK0][tid #140105146672896]: replica 4 calling init per replica
[HCTR][22:45:00.689][ERROR][RK0][tid #140105155065600]: replica 7 calling init per replica
[HCTR][22:45:00.689][ERROR][RK0][tid #140105423501056]: replica 1 calling init per replica
[HCTR][22:45:00.689][ERROR][RK0][tid #140105155065600]: replica 2 calling init per replica
[HCTR][22:45:00.689][ERROR][RK0][main]: replica 6 calling init per replica
[HCTR][22:45:00.690][ERROR][RK0][main]: Calling build_v2
[HCTR][22:45:00.690][ERROR][RK0][main]: Calling build_v2
[HCTR][22:45:00.690][ERROR][RK0][tid #140105029240576]: Calling build_v2
[HCTR][22:45:00.690][ERROR][RK0][tid #140105146672896]: Calling build_v2
[HCTR][22:45:00.690][ERROR][RK0][tid #140105155065600]: Calling build_v2
[HCTR][22:45:00.690][ERROR][RK0][tid #140105423501056]: Calling build_v2
[HCTR][22:45:00.690][ERROR][RK0][tid #140105155065600]: Calling build_v2
[HCTR][22:45:00.690][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][22:45:00.690][ERROR][RK0][main]: Calling build_v2
[HCTR][22:45:00.690][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][22:45:00.690][ERROR][RK0][tid #140105029240576]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][22:45:00.690][ERROR][RK0][tid #140105146672896]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][22:45:00.690][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][22:45:00.690][ERROR][RK0][tid #140105155065600]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][22:45:00.690][ERROR][RK0][tid #140105423501056]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][22:45:00.690][ERROR][RK0][tid #140105155065600]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[[[[[2022-12-11 22:45:002022-12-11 22:45:00[[[2022-12-11 22:45:00..2022-12-11 22:45:00.2022-12-11 22:45:006901702022-12-11 22:45:002022-12-11 22:45:006901702022-12-11 22:45:00.690170.: ..: .690181: 690186E690186690180E690186: E:  : :  : E E/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.ccEE/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.ccE /hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc :  : /hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc:/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc136/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc136/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc:136:] ::] :136] 136using concurrent impl MPS136136using concurrent impl MPS136] using concurrent impl MPS] 
] ] 
] using concurrent impl MPS
using concurrent impl MPSusing concurrent impl MPSusing concurrent impl MPSusing concurrent impl MPS




[2022-12-11 22:45:00.694447: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:178] v100x8, slow pcie
[2022-12-11 22:45:00.694490: E[ 2022-12-11 22:45:00/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.:694494196: ] Eassigning 8 to cpu 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:178] v100x8, slow pcie
[[2022-12-11 22:45:002022-12-11 22:45:00..694545694541: : EE[  2022-12-11 22:45:00/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.::694566196178: ] [] Eassigning 8 to cpu2022-12-11 22:45:00v100x8, slow pcie 
.
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc694587:: 212[E] 2022-12-11 22:45:00 build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8[./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
2022-12-11 22:45:00694621[:.: 2022-12-11 22:45:00178694630E.] [:  694636v100x8, slow pcie2022-12-11 22:45:00E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: 
. :E694659/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[196[ : :2022-12-11 22:45:00] 2022-12-11 22:45:00/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE212.assigning 8 to cpu[.: ] 694684
2022-12-11 22:45:00694691178/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccbuild asymm link desc with 8X Tesla V100-SXM2-32GB out of 8: .: ] :
[E694728Ev100x8, slow pcie213[2022-12-11 22:45:00 : [ 
] 2022-12-11 22:45:00./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE2022-12-11 22:45:00/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccremote time is 8.68421.[694771: .:
6947922022-12-11 22:45:00: 178/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc694803196: .[E] :: ] E6948332022-12-11 22:45:00 v100x8, slow pcie178Eassigning 8 to cpu : ./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
]  
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE694875:v100x8, slow pcie/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[: : 178
[:2022-12-11 22:45:00212/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE] 2022-12-11 22:45:00213[.] : v100x8, slow pcie.] 2022-12-11 22:45:00694946build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8196/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
694985remote time is 8.68421.: 
] :: [
694992Eassigning 8 to cpu214E2022-12-11 22:45:00[:  [
]  .2022-12-11 22:45:00E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-11 22:45:00cpu time is 97.0588/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc695060. :.[
:: 695082/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc1966950922022-12-11 22:45:00212E: :] : .]  E196assigning 8 to cpuE695152build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc ] 
 : 
:/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccassigning 8 to cpu/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE196:
[: [] 2132022-12-11 22:45:00214/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-11 22:45:00assigning 8 to cpu] .] :.
remote time is 8.68421695297[cpu time is 97.0588212695303
: 2022-12-11 22:45:00
[] : E.2022-12-11 22:45:00build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8E [695353.
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-11 22:45:00: 695395/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:.[E: :2136954122022-12-11 22:45:00 E212] : ./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc ] remote time is 8.68421E695454:/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccbuild asymm link desc with 8X Tesla V100-SXM2-32GB out of 8
 : 212:
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[E] 214:2022-12-11 22:45:00 build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8[] 212./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
2022-12-11 22:45:00cpu time is 97.0588] 695577:.
build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8[: 213695603
2022-12-11 22:45:00E] : . remote time is 8.68421[E695655/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
2022-12-11 22:45:00 : :./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[E214695699:2022-12-11 22:45:00 ] : 213./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cccpu time is 97.0588E] 695732:
 remote time is 8.68421: 213/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
E] : remote time is 8.68421[213/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
2022-12-11 22:45:00] :.remote time is 8.68421[214695836
2022-12-11 22:45:00] : .[cpu time is 97.0588E6958682022-12-11 22:45:00
 : ./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE695896: : 214/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE] : cpu time is 97.0588214/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
] :cpu time is 97.0588214
] cpu time is 97.0588
[2022-12-11 22:46:20.212878: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:118] solver created. now build & solve
[2022-12-11 22:46:20.253098: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:123] solver built. now solve
block 0 storage is 00010001
	access is	0	0	0	0	4	4	4	4	
block 1 storage is 00100010
	access is	1	1	1	1	5	5	5	5	
block 2 storage is 01000100
	access is	2	2	2	2	6	6	6	6	
block 3 storage is 10001000
	access is	3	3	3	3	7	7	7	7	
block 4 storage is 00000000
	access is	8	8	8	8	8	8	8	8	
[2022-12-11 22:46:20.369679: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:127] solver solved
[2022-12-11 22:46:20.369740: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:195] 0 solved master
[2022-12-11 22:46:20.486307: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 0 solved
[2022-12-11 22:46:20.486365: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 0 initing device 0
[2022-12-11 22:46:20.486881: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-11 22:46:20.486935: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 22:46:20.487929: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 22:46:20.488728: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 22:46:20.501531: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 6 solved
[2022-12-11 22:46:20.501595: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 6 initing device 6
[2022-12-11 22:46:20.501769: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 1 solved
[2022-12-11 22:46:20.501820: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 1 initing device 1
[2022-12-11 22:46:20.501984: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202[] 2022-12-11 22:46:20.4 solved502021
: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu[:2022-12-11 22:46:201815.] 502043Building Coll Cache with ... num gpu device is 8: 
E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] [worker 0 thread 4 initing device 4[[2022-12-11 22:46:20
2022-12-11 22:46:202022-12-11 22:46:20...502049502062502083: E: :  EE/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu  :/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc1980::] 202202eager alloc mem 381.47 MB] ] 
7 solved3 solved

[2022-12-11 22:46:20.[[5022452022-12-11 22:46:202022-12-11 22:46:20: ..E502253502255 : : /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuEE:  1815/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc] ::Building Coll Cache with ... num gpu device is 8205205
] ] worker 0 thread 7 initing device 7worker 0 thread 3 initing device 3

[2022-12-11 22:46:20.502384: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 22:46:20.502519: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-11 22:46:20.502566: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 22:46:20.[5027912022-12-11 22:46:20: .E502798 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE: 1815/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu] :Building Coll Cache with ... num gpu device is 81815
] Building Coll Cache with ... num gpu device is 8
[[2022-12-11 22:46:202022-12-11 22:46:20..502865502867: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::19801980] ] eager alloc mem 381.47 MBeager alloc mem 381.47 MB

[2022-12-11 22:46:20.505476: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 22:46:20.505766: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 22:46:20.505807: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 22:46:20.506348: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 22:46:20.506403: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 22:46:20.509067: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 22:46:20.509298: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 22:46:20.509339: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 22:46:20.509860: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 22:46:20.509915: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 22:46:20.511480: E [/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:2022-12-11 22:46:20202.] 5115015 solved: 
E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:[2022022-12-11 22:46:20] .2 solved511555
: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc[:2022-12-11 22:46:20205.] 511578worker 0 thread 5 initing device 5: 
E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 2 initing device 2
[2022-12-11 22:46:20.511998: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-11 22:46:20.512022: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] [Building Coll Cache with ... num gpu device is 82022-12-11 22:46:20
.512045: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 22:46:20.512073: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 22:46:20.514056: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 22:46:20.514113: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 22:46:20.515419: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 22:46:20.515512: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 22:46:20.568403: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.00 Bytes
[2022-12-11 22:46:20.573796: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5
[2022-12-11 22:46:20.573920: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-11 22:46:20.574735: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 11.83 MB
[2022-12-11 22:46:20.575334: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:46:20.576330: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:46:20.576378: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 11.44 MB
[2022-12-11 22:46:20.581598: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 22:46:20.582391: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 22:46:20.582438: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1.44 GB
[[[[2022-12-11 22:46:20[2022-12-11 22:46:202022-12-11 22:46:202022-12-11 22:46:20.2022-12-11 22:46:20...593761.593759593749593748: 593788: : : E: EEE E   /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:::1980:198019801980] 1980eager alloc mem 5.00 Bytes] ] ] ] 
eager alloc mem 5.00 Byteseager alloc mem 5.00 Byteseager alloc mem 5.00 Byteseager alloc mem 5.00 Bytes



[[2022-12-11 22:46:202022-12-11 22:46:20..597530597530: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::19801980] ] eager alloc mem 5.00 Byteseager alloc mem 5.00 Bytes

[2022-12-11 22:46:20.599963: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5
[2022-12-11 22:46:20.[6000412022-12-11 22:46:20: .E600064 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccE: 638/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc] :eager release cuda mem 5638
] eager release cuda mem 400000000
[2022-12-11 22:46:20.[6001432022-12-11 22:46:20: .E600137 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccE: 638/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc] :eager release cuda mem 400000000638
] eager release cuda mem 5
[2022-12-11 22:46:20.600199: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5
[2022-12-11 22:46:20.600240: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-11 22:46:20[.2022-12-11 22:46:20600281.: 600272E:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc :/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc638:] 638eager release cuda mem 400000000] 
eager release cuda mem 5
[2022-12-11 22:46:20.600378: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-11 22:46:20.601696: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 11.83 MB
[2022-12-11 22:46:20.602445: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 11.83 MB
[2022-12-11 22:46:20.603470: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 11.83 MB
[2022-12-11 22:46:20.604054: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 11.83 MB
[2022-12-11 22:46:20.604392: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5
[2022-12-11 22:46:20.604471: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-11 22:46:20.604567: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 11.83 MB
[2022-12-11 22:46:20.604597: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5
[2022-12-11 22:46:20.604674: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-11 22:46:20.605746: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:46:20.605783: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:46:20.606310: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 11.83 MB
[2022-12-11 22:46:20.606720: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:46:20.606748: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:[6382022-12-11 22:46:20] .eager release cuda mem 625663606766
: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 11.44 MB
[2022-12-11 22:46:20.606801: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 11.44 MB
[2022-12-11 22:46:20.606932: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 11.83 MB
[2022-12-11 22:46:20.607213: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:46:20.607273: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:46:20.607438: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:46:20.608018: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:46:20.608074: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:46:20.608200: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:46:20.608236[: 2022-12-11 22:46:20E. 608246/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc: :W638 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cceager release cuda mem 625663:
43] WORKER[0] alloc host memory 11.44 MB
[2022-12-11 22:46:20.608308: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 11.44 MB
[2022-12-11 22:46:20.608409: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:46:20.608457: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 11.44 MB
[2022-12-11 22:46:20.608996: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:46:20.609045: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:[432022-12-11 22:46:20] .WORKER[0] alloc host memory 11.44 MB609057
: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:46:20.609111: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 11.44 MB
[2022-12-11 22:46:20.615081: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 22:46:20.615730: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 22:46:20.615774: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1.44 GB
[2022-12-11 22:46:20.616124: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 22:46:20.616641: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 22:46:20.616747: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[[2022-12-11 22:46:202022-12-11 22:46:20..616773616790: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::19801980] ] eager alloc mem 25.25 KBeager alloc mem 1.44 GB

[2022-12-11 22:46:20.617249: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 22:46:20.617293: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1.44 GB
[2022-12-11 22:46:20.617405: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 22:46:20.617449: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1.44 GB
[2022-12-11 22:46:20.617483: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 22:46:20.617789: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 22:46:20.618203: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 22:46:20.618248: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1.44 GB
[2022-12-11 22:46:20.618404: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 22:46:20.618449: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1.44 GB
[2022-12-11 22:46:20.618694: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 22:46:20.619308: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 22:46:20.619352: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1.44 GB
[[[[[[[[2022-12-11 22:46:212022-12-11 22:46:212022-12-11 22:46:212022-12-11 22:46:212022-12-11 22:46:212022-12-11 22:46:212022-12-11 22:46:212022-12-11 22:46:21........154918154917154919154918154917154918154917154917: : : : : : : : EEEEEEEE        /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::::::::19261926192619261926192619261926] ] Device 1 init p2p of link 7] ] ] ] ] ] Device 4 init p2p of link 5
Device 0 init p2p of link 3Device 6 init p2p of link 0Device 5 init p2p of link 6Device 2 init p2p of link 1Device 7 init p2p of link 4Device 3 init p2p of link 2






[2022-12-11 22:46:21.155436: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980[[] [2022-12-11 22:46:21[2022-12-11 22:46:21eager alloc mem 611.00 KB2022-12-11 22:46:21.2022-12-11 22:46:21.
.155454[.[155454155456[: 2022-12-11 22:46:211554602022-12-11 22:46:21: : 2022-12-11 22:46:21E.: .EE. 155477E155479  155485/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:  : /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu: :E/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE::E1980 : 19801980 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu1980/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu] ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cueager alloc mem 611.00 KB:] :eager alloc mem 611.00 KBeager alloc mem 611.00 KB:
1980eager alloc mem 611.00 KB1980

1980] 
] ] eager alloc mem 611.00 KBeager alloc mem 611.00 KBeager alloc mem 611.00 KB


[2022-12-11 22:46:21.156267: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:46:21.[1564822022-12-11 22:46:21: .E156487 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc[E:[2022-12-11 22:46:21 6382022-12-11 22:46:21./hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc] .156504:eager release cuda mem 625663[156507: 638
[2022-12-11 22:46:21[: E] 2022-12-11 22:46:21.2022-12-11 22:46:21E eager release cuda mem 625663.156534. /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc
156540: 156543/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:: E: :638E E638]  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc ] eager release cuda mem 625663/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cceager release cuda mem 625663
:638:
638] 638] eager release cuda mem 625663] eager release cuda mem 625663
eager release cuda mem 625663

[2022-12-11 22:46:21.169173: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 2
[2022-12-11 22:46:21.169338: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:46:21.169597: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 6 init p2p of link 5
[2022-12-11 22:46:21.169766: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:46:21.170154: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:46:21.170601: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:46:21.170637: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 4 init p2p of link 7
[2022-12-11 22:46:21.170797: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:46:21.171011: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 5 init p2p of link 4
[2022-12-11 22:46:21.171167: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980[] 2022-12-11 22:46:21eager alloc mem 611.00 KB.
171185: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 0 init p2p of link 6
[2022-12-11 22:46:21.171351: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:46:21.171422: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 2 init p2p of link 3
[2022-12-11 22:46:21.171582: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:46:21.171618: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:46:21.171700: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 3 init p2p of link 0
[2022-12-11 22:46:21.171871: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:46:21.171935: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 7 init p2p of link 1
[2022-12-11 22:46:21.171989: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:46:21.172094: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:46:21.172121: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:46:21.172382: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:46:21.172669: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:46:21.172875: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:46:21.184245: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 3
[2022-12-11 22:46:21.184376: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:46:21.184691: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 0 init p2p of link 1
[2022-12-11 22:46:21.184814: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:46:21.184922: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 6 init p2p of link 4
[2022-12-11 22:46:21.185045: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:46:21.185175: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:46:21.185295: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 7 init p2p of link 6
[2022-12-11 22:46:21.185381: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 4 init p2p of link 2
[2022-12-11 22:46:21.185420: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:46:21.185505: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:46:21.185636: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:46:21.185677: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 5 init p2p of link 7
[2022-12-11 22:46:21.185794: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:46:21.185840: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:46:21.186087: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 2 init p2p of link 0
[2022-12-11 22:46:21[.2022-12-11 22:46:21186224.: 186230E:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc :/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu638[[:] 2022-12-11 22:46:212022-12-11 22:46:211980eager release cuda mem 625663..] 
186270186278eager alloc mem 611.00 KB: : 
EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::1926638] ] Device 3 init p2p of link 5eager release cuda mem 625663

[2022-12-11 22:46:21.186479: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:46:21.186588: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:46:21.187119: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:46:21.187250: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:46:21.201612: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:[19262022-12-11 22:46:21] .Device 4 init p2p of link 6201634
: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 0
[2022-12-11 22:46:21.201745: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:[19802022-12-11 22:46:21] .eager alloc mem 611.00 KB201760
: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[[[2022-12-11 22:46:212022-12-11 22:46:212022-12-11 22:46:21...202561202572202573: : : EEE   /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:::1926638638] ] ] Device 3 init p2p of link 1eager release cuda mem 625663eager release cuda mem 625663


[2022-12-11 22:46:21.[2027702022-12-11 22:46:21: .E202787 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE: 1926/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu] :Device 0 init p2p of link 21980
] eager alloc mem 611.00 KB
[2022-12-11 22:46:21.202946: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:46:21.203220: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 6 init p2p of link 7
[2022-12-11 22:46:21.203341: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:46:21.203649: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:46:21.203698: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 7 init p2p of link 5
[2022-12-11 22:46:21.203754: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:46:21.203825: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:46:21.204146: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:46:21.204299: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 5 init p2p of link 3
[2022-12-11 22:46:21.204414: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[[2022-12-11 22:46:212022-12-11 22:46:21..204633204630: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::6381926] ] eager release cuda mem 625663Device 2 init p2p of link 4

[2022-12-11 22:46:21.204805: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:46:21.205181: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:46:21.205573: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:46:21.218918: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 12399996
[2022-12-11 22:46:21.219040: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 12399996
[2022-12-11 22:46:21.219342: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 12399996
[2022-12-11 22:46:21.219665: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 12399996
[2022-12-11 22:46:21.219968: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 2999999 / 100000000 nodes ( 3.00 %~3.00 %) | remote 8999997 / 100000000 nodes ( 9.00 %) | cpu 88000004 / 100000000 nodes ( 88.00 %) | 1.44 GB | 0.717411 secs 
[2022-12-11 22:46:21.220055: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 12399996
[2022-12-11 22:46:21.220105: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 2999999 / 100000000 nodes ( 3.00 %~3.00 %) | remote 8999997 / 100000000 nodes ( 9.00 %) | cpu 88000004 / 100000000 nodes ( 88.00 %) | 1.44 GB | 0.708071 secs 
[2022-12-11 22:46:21.220257: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 2999999 / 100000000 nodes ( 3.00 %~3.00 %) | remote 8999997 / 100000000 nodes ( 9.00 %) | cpu 88000004 / 100000000 nodes ( 88.00 %) | 1.44 GB | 0.708195 secs 
[2022-12-11 22:46:21.220507: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] [eager release cuda mem 123999962022-12-11 22:46:21
.220536: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 2999999 / 100000000 nodes ( 3.00 %~3.00 %) | remote 8999997 / 100000000 nodes ( 9.00 %) | cpu 88000004 / 100000000 nodes ( 88.00 %) | 1.44 GB | 0.718164 secs 
[2022-12-11 22:46:21.220817: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 12399996
[2022-12-11 22:46:21.220934: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 2999999 / 100000000 nodes ( 3.00 %~3.00 %) | remote 8999997 / 100000000 nodes ( 9.00 %) | cpu 88000004 / 100000000 nodes ( 88.00 %) | 1.44 GB | 0.718085 secs 
[2022-12-11 22:46:21.221106: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 12399996
[2022-12-11 22:46:21.222114: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 2999999 / 100000000 nodes ( 3.00 %~3.00 %) | remote 8999997 / 100000000 nodes ( 9.00 %) | cpu 88000004 / 100000000 nodes ( 88.00 %) | 1.44 GB | 0.735195 secs 
[2022-12-11 22:46:21.224130: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 2999999 / 100000000 nodes ( 3.00 %~3.00 %) | remote 8999997 / 100000000 nodes ( 9.00 %) | cpu 88000004 / 100000000 nodes ( 88.00 %) | 1.44 GB | 0.722058 secs 
[2022-12-11 22:46:21.224337: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 8.07 GB
[2022-12-11 22:46:21.225332: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 2999999 / 100000000 nodes ( 3.00 %~3.00 %) | remote 8999997 / 100000000 nodes ( 9.00 %) | cpu 88000004 / 100000000 nodes ( 88.00 %) | 1.44 GB | 0.722479 secs 
[2022-12-11 22:46:22.642693: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 8.34 GB
[2022-12-11 22:46:22.643081: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 8.34 GB
[2022-12-11 22:46:22.666042: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 8.34 GB
[2022-12-11 22:46:24. 15392: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 8.60 GB
[2022-12-11 22:46:24. 15604: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 8.60 GB
[2022-12-11 22:46:24. 15949: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 8.60 GB
[2022-12-11 22:46:25.170784: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 8.81 GB
[2022-12-11 22:46:25.170915: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 8.81 GB
[2022-12-11 22:46:25.171229: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 8.81 GB
[2022-12-11 22:46:26.433468: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 9.03 GB
[2022-12-11 22:46:26.433624: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 9.03 GB
[2022-12-11 22:46:26.433931: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 9.03 GB
[2022-12-11 22:46:27.769602: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 9.49 GB
[2022-12-11 22:46:27.769768: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 9.49 GB
[2022-12-11 22:46:27.770866: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 9.49 GB
[2022-12-11 22:46:28.950118: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 9.68 GB
[2022-12-11 22:46:28.951363: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 9.68 GB
[HCTR][22:46:30.313][ERROR][RK0][main]: replica 0 calling init per replica done, doing barrier
[HCTR][22:46:30.313][ERROR][RK0][main]: replica 6 calling init per replica done, doing barrier
[HCTR][22:46:30.313][ERROR][RK0][main]: replica 5 calling init per replica done, doing barrier
[HCTR][22:46:30.313][ERROR][RK0][tid #140105423501056]: replica 1 calling init per replica done, doing barrier
[HCTR][22:46:30.313][ERROR][RK0][tid #140105029240576]: replica 3 calling init per replica done, doing barrier
[HCTR][22:46:30.313][ERROR][RK0][tid #140105146672896]: replica 4 calling init per replica done, doing barrier
[HCTR][22:46:30.313][ERROR][RK0][tid #140105155065600]: replica 2 calling init per replica done, doing barrier
[HCTR][22:46:30.313][ERROR][RK0][tid #140105155065600]: replica 7 calling init per replica done, doing barrier
[HCTR][22:46:30.314][ERROR][RK0][main]: replica 0 calling init per replica done, doing barrier done
[HCTR][22:46:30.314][ERROR][RK0][main]: replica 5 calling init per replica done, doing barrier done
[HCTR][22:46:30.314][ERROR][RK0][main]: replica 6 calling init per replica done, doing barrier done
[HCTR][22:46:30.314][ERROR][RK0][tid #140105146672896]: replica 4 calling init per replica done, doing barrier done
[HCTR][22:46:30.314][ERROR][RK0][tid #140105155065600]: replica 7 calling init per replica done, doing barrier done
[HCTR][22:46:30.314][ERROR][RK0][tid #140105155065600]: replica 2 calling init per replica done, doing barrier done
[HCTR][22:46:30.314][ERROR][RK0][tid #140105423501056]: replica 1 calling init per replica done, doing barrier done
[HCTR][22:46:30.314][ERROR][RK0][tid #140105029240576]: replica 3 calling init per replica done, doing barrier done
[HCTR][22:46:30.314][ERROR][RK0][main]: init per replica done
[HCTR][22:46:30.314][ERROR][RK0][main]: init per replica done
[HCTR][22:46:30.314][ERROR][RK0][tid #140105146672896]: init per replica done
[HCTR][22:46:30.314][ERROR][RK0][tid #140105155065600]: init per replica done
[HCTR][22:46:30.314][ERROR][RK0][tid #140105155065600]: init per replica done
[HCTR][22:46:30.314][ERROR][RK0][tid #140105423501056]: init per replica done
[HCTR][22:46:30.314][ERROR][RK0][tid #140105029240576]: init per replica done
[HCTR][22:46:30.316][ERROR][RK0][main]: init per replica done
[HCTR][22:46:30.320][ERROR][RK0][main]: 4 allocated 3276800 at 0x7f6e98d20000
[HCTR][22:46:30.320][ERROR][RK0][main]: 4 allocated 6553600 at 0x7f6e99200000
[HCTR][22:46:30.320][ERROR][RK0][tid #140105423501056]: 1 allocated 3276800 at 0x7f6e98d20000
[HCTR][22:46:30.320][ERROR][RK0][main]: 4 allocated 3276800 at 0x7f6e99840000
[HCTR][22:46:30.320][ERROR][RK0][tid #140105423501056]: 1 allocated 6553600 at 0x7f6e99200000
[HCTR][22:46:30.320][ERROR][RK0][main]: 4 allocated 6553600 at 0x7f6e99b60000
[HCTR][22:46:30.320][ERROR][RK0][tid #140105029240576]: 3 allocated 3276800 at 0x7f6e9ad20000
[HCTR][22:46:30.320][ERROR][RK0][tid #140105423501056]: 1 allocated 3276800 at 0x7f6e99840000
[HCTR][22:46:30.320][ERROR][RK0][main]: 6 allocated 3276800 at 0x7f6e96d20000
[HCTR][22:46:30.320][ERROR][RK0][tid #140105029240576]: 3 allocated 6553600 at 0x7f6e9b200000
[HCTR][22:46:30.320][ERROR][RK0][tid #140105423501056]: 1 allocated 6553600 at 0x7f6e99b60000
[HCTR][22:46:30.320][ERROR][RK0][main]: 6 allocated 6553600 at 0x7f6e97200000
[HCTR][22:46:30.320][ERROR][RK0][tid #140105029240576]: 3 allocated 3276800 at 0x7f6e9b840000
[HCTR][22:46:30.320][ERROR][RK0][main]: 2 allocated 3276800 at 0x7f6e98d20000
[HCTR][22:46:30.320][ERROR][RK0][main]: 6 allocated 3276800 at 0x7f6e97840000
[HCTR][22:46:30.320][ERROR][RK0][tid #140105029240576]: 3 allocated 6553600 at 0x7f6e9bb60000
[HCTR][22:46:30.320][ERROR][RK0][main]: 6 allocated 6553600 at 0x7f6e97b60000
[HCTR][22:46:30.320][ERROR][RK0][main]: 2 allocated 6553600 at 0x7f6e99200000
[HCTR][22:46:30.320][ERROR][RK0][main]: 2 allocated 3276800 at 0x7f6e99840000
[HCTR][22:46:30.320][ERROR][RK0][main]: 2 allocated 6553600 at 0x7f6e99b60000
[HCTR][22:46:30.320][ERROR][RK0][main]: 7 allocated 3276800 at 0x7f6e98d20000
[HCTR][22:46:30.320][ERROR][RK0][main]: 7 allocated 6553600 at 0x7f6e99200000
[HCTR][22:46:30.320][ERROR][RK0][main]: 7 allocated 3276800 at 0x7f6e99840000
[HCTR][22:46:30.320][ERROR][RK0][main]: 7 allocated 6553600 at 0x7f6e99b60000
[HCTR][22:46:30.320][ERROR][RK0][tid #140105222174464]: 5 allocated 3276800 at 0x7f6e98d20000
[HCTR][22:46:30.320][ERROR][RK0][tid #140105222174464]: 5 allocated 6553600 at 0x7f6e99200000
[HCTR][22:46:30.320][ERROR][RK0][tid #140105222174464]: 5 allocated 3276800 at 0x7f6e99840000
[HCTR][22:46:30.320][ERROR][RK0][tid #140105222174464]: 5 allocated 6553600 at 0x7f6e99b60000
[HCTR][22:46:30.323][ERROR][RK0][tid #140105566111488]: 0 allocated 3276800 at 0x7f6e9d920000
[HCTR][22:46:30.323][ERROR][RK0][tid #140105566111488]: 0 allocated 6553600 at 0x7f6e9de00000
[HCTR][22:46:30.323][ERROR][RK0][tid #140105566111488]: 0 allocated 3276800 at 0x7f6e9eb0e800
[HCTR][22:46:30.323][ERROR][RK0][tid #140105566111488]: 0 allocated 6553600 at 0x7f6e9ee2e800








