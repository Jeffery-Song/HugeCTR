2022-12-12 01:45:49.337598: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.343982: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.348980: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.352787: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.359916: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.373371: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.381502: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.386258: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.438432: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.446894: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.449493: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.451934: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.460939: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.479386: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.485554: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.488395: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.489360: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.490314: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.491294: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.492261: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.493317: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.494370: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.495405: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.496369: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.498096: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.499223: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.500147: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.501153: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.502447: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.504267: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.505033: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.505430: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.506677: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.506877: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.508298: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.509357: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.510425: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.511523: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.512596: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.512635: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 01:45:49.513593: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.519797: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.521054: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.522410: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.522499: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.524234: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.524397: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.524428: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.526031: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.526347: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.526350: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.528470: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.528526: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.530580: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.530681: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.532790: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.532945: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.535511: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.536780: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.537386: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.538819: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.539385: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.541270: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.543877: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.544578: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.546138: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.547021: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.547497: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.551688: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.553571: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.554227: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.554256: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.555113: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.556037: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.557007: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.557180: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.557886: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.559401: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.579875: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.585692: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.592700: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.593747: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.594151: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.596598: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.596649: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.596656: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.597456: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.598364: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.599346: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.600821: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.600962: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.601012: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.602785: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.603661: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.604290: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.606010: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.606058: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.606260: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.607526: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.608355: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.608956: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.609799: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.609897: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.609987: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.612958: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.613275: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.613748: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.613819: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.613919: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.617513: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.617626: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.617676: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.617790: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.620540: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.620638: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.620803: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.622810: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.622904: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.623065: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.625124: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.625170: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.625654: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.627748: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.627768: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.628177: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.629951: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.630083: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.630527: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.632419: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.632492: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.632671: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.634802: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.634906: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.635102: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.636924: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.637284: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.637382: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.639173: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.639581: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.639744: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.639931: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.642599: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.642966: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.643523: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.644092: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.644265: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.645525: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.646189: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.647125: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.647242: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.648248: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.648941: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.649849: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.649985: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.650906: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.651862: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.652620: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.652830: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.653994: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.654828: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.655565: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.655628: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.656443: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.656728: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.657931: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.658808: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.658831: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.659817: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.660004: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.661184: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.661812: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.662083: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.662084: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.663531: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.663703: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.664504: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.665271: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.666379: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.666556: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.666649: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.668309: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.669275: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.670780: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.670833: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.671896: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.672483: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.672990: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 01:45:49.673522: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.673674: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.674216: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 01:45:49.674261: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 01:45:49.674646: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.675512: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.676538: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.676612: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.677617: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.678481: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.679765: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.679764: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.680556: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.681552: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.682710: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.682817: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.683551: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.683564: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.683947: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.684085: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.684916: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.686363: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.686680: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.687752: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.687849: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.688128: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.688258: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.688822: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.690969: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.691523: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.691711: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.691813: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.692070: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.692586: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.692977: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.694957: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.695897: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.695931: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.697087: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.726744: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.727385: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.727473: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.728623: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.730864: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.731873: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.732079: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.733099: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.735426: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.736284: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.736616: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.737386: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.740791: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.741507: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.741819: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.742393: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.745267: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.746793: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.748004: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.748184: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.750036: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.751100: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.751791: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.752101: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.755478: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.755583: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.757392: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.758956: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.759018: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.760081: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 01:45:49.760860: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.762019: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.762044: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.764393: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.769633: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.772849: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.773065: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.781000: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.781414: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.783282: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.783512: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.785560: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.786070: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.787289: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.787461: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.818838: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.875661: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.877443: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.878909: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 01:45:49.880095: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.883014: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.888333: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.890300: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 01:45:49.892841: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 01:45:49.893290: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.897725: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.899802: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.901995: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.905081: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.906582: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.909300: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:49.910995: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:50.793832: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:50.794471: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:50.795422: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:50.796088: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 01:45:50.796153: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-12 01:45:50.813805: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:50.814452: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:50.815392: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:50.816092: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:50.816618: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:50.817095: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-12 01:45:50.864396: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 01:45:50.864600: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 01:45:50.904428: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12340
2022-12-12 01:45:51.029702: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:51.030321: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:51.031172: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:51.031642: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 01:45:51.031700: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-12 01:45:51.050461: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:51.051090: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:51.051655: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:51.052237: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:51.052757: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:51.053226: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:1/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-12 01:45:51.057183: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:51.057188: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:51.058464: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:51.058544: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:51.059382: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:51.059513: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:51.060226: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 01:45:51.060295: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-12 01:45:51.060369: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 01:45:51.060419: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-12 01:45:51.077932: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:51.078287: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:51.078749: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:51.079512: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:51.079806: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:51.080629: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:51.081033: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:51.081563: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:51.081926: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:51.082467: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:51.082786: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:2/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-12 01:45:51.083098: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:5/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-12 01:45:51.127999: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 01:45:51.128203: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 01:45:51.130142: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12345
2022-12-12 01:45:51.139158: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:51.139830: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:51.140377: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:51.140834: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 01:45:51.140888: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-12 01:45:51.141644: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 01:45:51.141804: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 01:45:51.143737: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12341
2022-12-12 01:45:51.152397: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:51.152994: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:51.153522: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:51.153985: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 01:45:51.154049: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-12 01:45:51.158610: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:51.159250: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:51.159754: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:51.160332: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:51.160838: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:51.161311: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:7/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-12 01:45:51.165458: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:51.166178: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:51.166722: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:51.167214: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 01:45:51.167277: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-12 01:45:51.172148: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:51.172875: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:51.173540: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:51.173970: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:51.174548: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:51.175093: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:51.175632: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:51.176120: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:51.176604: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:6/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-12 01:45:51.176856: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 01:45:51.176915: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-12 01:45:51.183907: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 01:45:51.184022: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:51.184066: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 01:45:51.184648: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:51.185167: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:51.185720: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12342
2022-12-12 01:45:51.185741: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:51.186266: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:51.186744: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:3/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-12 01:45:51.194044: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:51.194704: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:51.195243: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:51.195831: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:51.196359: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:45:51.196832: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:4/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-12 01:45:51.205672: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 01:45:51.205834: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 01:45:51.208468: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12347
2022-12-12 01:45:51.222164: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 01:45:51.222348: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 01:45:51.224116: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12346
2022-12-12 01:45:51.231880: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 01:45:51.232056: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 01:45:51.233882: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12343
2022-12-12 01:45:51.243602: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 01:45:51.243812: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 01:45:51.245559: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12344
[HCTR][01:45:52.511][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][01:45:52.511][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][01:45:52.515][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][01:45:52.515][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][01:45:52.515][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][01:45:52.515][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][01:45:52.515][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][01:45:52.516][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 1it [00:01,  1.54s/it]warmup run: 1it [00:01,  1.52s/it]warmup run: 1it [00:01,  1.54s/it]warmup run: 102it [00:01, 86.21it/s]warmup run: 86it [00:01, 73.62it/s]warmup run: 91it [00:01, 76.95it/s]warmup run: 1it [00:01,  1.54s/it]warmup run: 202it [00:01, 184.89it/s]warmup run: 171it [00:01, 158.39it/s]warmup run: 1it [00:01,  1.50s/it]warmup run: 183it [00:01, 168.07it/s]warmup run: 1it [00:01,  1.49s/it]warmup run: 95it [00:01, 80.44it/s]warmup run: 1it [00:01,  1.49s/it]warmup run: 303it [00:01, 294.96it/s]warmup run: 1it [00:01,  1.48s/it]warmup run: 271it [00:01, 271.95it/s]warmup run: 92it [00:01, 79.88it/s]warmup run: 275it [00:01, 268.64it/s]warmup run: 96it [00:01, 83.62it/s]warmup run: 190it [00:01, 174.45it/s]warmup run: 100it [00:01, 87.49it/s]warmup run: 405it [00:01, 410.58it/s]warmup run: 96it [00:01, 84.00it/s]warmup run: 372it [00:01, 390.68it/s]warmup run: 184it [00:01, 172.78it/s]warmup run: 373it [00:01, 382.50it/s]warmup run: 190it [00:01, 178.52it/s]warmup run: 285it [00:01, 278.21it/s]warmup run: 200it [00:01, 188.98it/s]warmup run: 504it [00:02, 517.64it/s]warmup run: 192it [00:01, 181.52it/s]warmup run: 471it [00:02, 502.88it/s]warmup run: 276it [00:01, 274.72it/s]warmup run: 471it [00:02, 491.27it/s]warmup run: 283it [00:01, 280.99it/s]warmup run: 379it [00:01, 383.90it/s]warmup run: 299it [00:01, 298.72it/s]warmup run: 604it [00:02, 617.87it/s]warmup run: 287it [00:01, 286.90it/s]warmup run: 573it [00:02, 611.59it/s]warmup run: 376it [00:01, 392.57it/s]warmup run: 569it [00:02, 593.51it/s]warmup run: 376it [00:01, 386.53it/s]warmup run: 474it [00:02, 488.77it/s]warmup run: 394it [00:01, 404.86it/s]warmup run: 704it [00:02, 704.50it/s]warmup run: 383it [00:01, 397.07it/s]warmup run: 676it [00:02, 708.17it/s]warmup run: 477it [00:01, 508.68it/s]warmup run: 672it [00:02, 693.45it/s]warmup run: 469it [00:01, 488.36it/s]warmup run: 571it [00:02, 589.07it/s]warmup run: 486it [00:01, 502.39it/s]warmup run: 803it [00:02, 774.88it/s]warmup run: 480it [00:01, 505.33it/s]warmup run: 777it [00:02, 783.07it/s]warmup run: 578it [00:02, 615.06it/s]warmup run: 772it [00:02, 767.65it/s]warmup run: 566it [00:02, 590.42it/s]warmup run: 663it [00:02, 659.77it/s]warmup run: 588it [00:02, 612.15it/s]warmup run: 902it [00:02, 829.03it/s]warmup run: 579it [00:02, 607.54it/s]warmup run: 680it [00:02, 708.29it/s]warmup run: 876it [00:02, 785.97it/s]warmup run: 869it [00:02, 818.45it/s]warmup run: 668it [00:02, 689.02it/s]warmup run: 691it [00:02, 708.06it/s]warmup run: 754it [00:02, 706.26it/s]warmup run: 1002it [00:02, 873.33it/s]warmup run: 678it [00:02, 695.59it/s]warmup run: 778it [00:02, 775.46it/s]warmup run: 976it [00:02, 841.26it/s]warmup run: 967it [00:02, 860.74it/s]warmup run: 770it [00:02, 770.50it/s]warmup run: 793it [00:02, 785.83it/s]warmup run: 1101it [00:02, 904.92it/s]warmup run: 853it [00:02, 777.10it/s]warmup run: 777it [00:02, 767.80it/s]warmup run: 875it [00:02, 821.98it/s]warmup run: 1078it [00:02, 887.45it/s]warmup run: 1066it [00:02, 896.13it/s]warmup run: 873it [00:02, 837.70it/s]warmup run: 894it [00:02, 844.28it/s]warmup run: 951it [00:02, 830.59it/s]warmup run: 1200it [00:02, 927.00it/s]warmup run: 878it [00:02, 829.28it/s]warmup run: 972it [00:02, 859.05it/s]warmup run: 1180it [00:02, 923.01it/s]warmup run: 1165it [00:02, 922.58it/s]warmup run: 975it [00:02, 884.97it/s]warmup run: 994it [00:02, 886.48it/s]warmup run: 1050it [00:02, 872.48it/s]warmup run: 1299it [00:02, 941.89it/s]warmup run: 978it [00:02, 873.79it/s]warmup run: 1280it [00:02, 942.62it/s]warmup run: 1069it [00:02, 873.72it/s]warmup run: 1263it [00:02, 937.44it/s]warmup run: 1075it [00:02, 914.67it/s]warmup run: 1096it [00:02, 922.46it/s]warmup run: 1148it [00:02, 901.80it/s]warmup run: 1398it [00:02, 952.49it/s]warmup run: 1078it [00:02, 908.44it/s]warmup run: 1382it [00:02, 963.58it/s]warmup run: 1164it [00:02, 886.94it/s]warmup run: 1363it [00:02, 954.32it/s]warmup run: 1178it [00:02, 946.76it/s]warmup run: 1197it [00:02, 946.71it/s]warmup run: 1247it [00:02, 925.25it/s]warmup run: 1497it [00:03, 962.15it/s]warmup run: 1179it [00:02, 935.19it/s]warmup run: 1484it [00:03, 979.60it/s]warmup run: 1266it [00:02, 922.47it/s]warmup run: 1463it [00:03, 967.25it/s]warmup run: 1281it [00:02, 969.79it/s]warmup run: 1298it [00:02, 962.15it/s]warmup run: 1347it [00:02, 944.87it/s]warmup run: 1599it [00:03, 977.13it/s]warmup run: 1279it [00:02, 952.02it/s]warmup run: 1587it [00:03, 991.95it/s]warmup run: 1369it [00:02, 951.92it/s]warmup run: 1563it [00:03, 976.15it/s]warmup run: 1384it [00:02, 986.89it/s]warmup run: 1400it [00:02, 977.86it/s]warmup run: 1447it [00:03, 959.50it/s]warmup run: 1701it [00:03, 989.35it/s]warmup run: 1380it [00:02, 968.56it/s]warmup run: 1688it [00:03, 990.33it/s]warmup run: 1469it [00:03, 964.12it/s]warmup run: 1666it [00:03, 989.67it/s]warmup run: 1487it [00:02, 999.00it/s]warmup run: 1503it [00:02, 991.13it/s]warmup run: 1546it [00:03, 968.35it/s]warmup run: 1803it [00:03, 995.67it/s]warmup run: 1482it [00:02, 981.43it/s]warmup run: 1791it [00:03, 1001.83it/s]warmup run: 1570it [00:03, 974.81it/s]warmup run: 1768it [00:03, 998.14it/s]warmup run: 1591it [00:03, 1010.32it/s]warmup run: 1606it [00:03, 999.86it/s]warmup run: 1646it [00:03, 976.20it/s]warmup run: 1905it [00:03, 1002.68it/s]warmup run: 1583it [00:03, 988.57it/s]warmup run: 1895it [00:03, 1010.61it/s]warmup run: 1671it [00:03, 982.90it/s]warmup run: 1870it [00:03, 1002.24it/s]warmup run: 1694it [00:03, 1013.69it/s]warmup run: 1708it [00:03, 1002.01it/s]warmup run: 2008it [00:03, 1008.28it/s]warmup run: 1745it [00:03, 969.07it/s]warmup run: 1685it [00:03, 995.48it/s]warmup run: 2001it [00:03, 1022.80it/s]warmup run: 1771it [00:03, 986.73it/s]warmup run: 1972it [00:03, 1005.45it/s]warmup run: 1798it [00:03, 1020.40it/s]warmup run: 1810it [00:03, 1004.07it/s]warmup run: 2128it [00:03, 1063.26it/s]warmup run: 1846it [00:03, 981.08it/s]warmup run: 1788it [00:03, 1005.62it/s]warmup run: 2123it [00:03, 1079.24it/s]warmup run: 1872it [00:03, 993.04it/s]warmup run: 2086it [00:03, 1044.54it/s]warmup run: 1901it [00:03, 1021.88it/s]warmup run: 1912it [00:03, 1006.57it/s]warmup run: 2248it [00:03, 1101.80it/s]warmup run: 1946it [00:03, 986.31it/s]warmup run: 1892it [00:03, 1013.74it/s]warmup run: 2245it [00:03, 1121.01it/s]warmup run: 1972it [00:03, 994.87it/s]warmup run: 2207it [00:03, 1091.39it/s]warmup run: 2004it [00:03, 1024.25it/s]warmup run: 2017it [00:03, 1017.57it/s]warmup run: 2368it [00:03, 1130.32it/s]warmup run: 2054it [00:03, 1014.08it/s]warmup run: 1995it [00:03, 1018.14it/s]warmup run: 2367it [00:03, 1149.08it/s]warmup run: 2085it [00:03, 1032.94it/s]warmup run: 2327it [00:03, 1122.63it/s]warmup run: 2126it [00:03, 1081.72it/s]warmup run: 2137it [00:03, 1071.30it/s]warmup run: 2488it [00:03, 1149.95it/s]warmup run: 2174it [00:03, 1068.17it/s]warmup run: 2115it [00:03, 1070.37it/s]warmup run: 2489it [00:03, 1170.03it/s]warmup run: 2203it [00:03, 1075.60it/s]warmup run: 2448it [00:03, 1145.92it/s]warmup run: 2249it [00:03, 1125.52it/s]warmup run: 2257it [00:03, 1108.26it/s]warmup run: 2608it [00:04, 1162.94it/s]warmup run: 2294it [00:03, 1106.75it/s]warmup run: 2236it [00:03, 1111.07it/s]warmup run: 2611it [00:04, 1184.75it/s]warmup run: 2321it [00:03, 1105.08it/s]warmup run: 2568it [00:04, 1161.60it/s]warmup run: 2372it [00:03, 1156.59it/s]warmup run: 2378it [00:03, 1136.83it/s]warmup run: 2728it [00:04, 1172.91it/s]warmup run: 2414it [00:03, 1134.06it/s]warmup run: 2357it [00:03, 1140.03it/s]warmup run: 2733it [00:04, 1194.04it/s]warmup run: 2439it [00:03, 1126.22it/s]warmup run: 2689it [00:04, 1173.76it/s]warmup run: 2496it [00:03, 1178.79it/s]warmup run: 2499it [00:03, 1158.17it/s]warmup run: 2847it [00:04, 1176.44it/s]warmup run: 2534it [00:04, 1152.47it/s]warmup run: 2478it [00:03, 1160.62it/s]warmup run: 2854it [00:04, 1197.02it/s]warmup run: 2557it [00:04, 1141.86it/s]warmup run: 2809it [00:04, 1179.17it/s]warmup run: 2620it [00:04, 1195.02it/s]warmup run: 2621it [00:03, 1173.42it/s]warmup run: 2967it [00:04, 1182.04it/s]warmup run: 2655it [00:04, 1166.81it/s]warmup run: 2599it [00:03, 1173.82it/s]warmup run: 3000it [00:04, 682.84it/s] warmup run: 2976it [00:04, 1202.97it/s]warmup run: 2675it [00:04, 1151.94it/s]warmup run: 2929it [00:04, 1185.15it/s]warmup run: 3000it [00:04, 684.29it/s] warmup run: 2742it [00:04, 1200.31it/s]warmup run: 2741it [00:04, 1179.80it/s]warmup run: 2772it [00:04, 1167.66it/s]warmup run: 2719it [00:04, 1180.61it/s]warmup run: 3000it [00:04, 678.33it/s] warmup run: 2791it [00:04, 1153.29it/s]warmup run: 2863it [00:04, 1201.33it/s]warmup run: 2861it [00:04, 1183.36it/s]warmup run: 2893it [00:04, 1178.33it/s]warmup run: 2839it [00:04, 1184.03it/s]warmup run: 2912it [00:04, 1167.41it/s]warmup run: 2985it [00:04, 1205.22it/s]warmup run: 3000it [00:04, 672.62it/s] warmup run: 2981it [00:04, 1186.88it/s]warmup run: 3000it [00:04, 693.79it/s] warmup run: 3000it [00:04, 693.98it/s] warmup run: 2960it [00:04, 1188.98it/s]warmup run: 3000it [00:04, 681.75it/s] warmup run: 3000it [00:04, 691.63it/s] 


warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]


warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]

warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   6%|▌         | 168/3000 [00:00<00:01, 1677.61it/s]warmup should be done:   5%|▌         | 163/3000 [00:00<00:01, 1625.24it/s]warmup should be done:   5%|▌         | 164/3000 [00:00<00:01, 1635.14it/s]warmup should be done:   6%|▌         | 167/3000 [00:00<00:01, 1661.07it/s]warmup should be done:   6%|▌         | 165/3000 [00:00<00:01, 1644.23it/s]warmup should be done:   6%|▌         | 167/3000 [00:00<00:01, 1661.48it/s]warmup should be done:   6%|▌         | 166/3000 [00:00<00:01, 1651.65it/s]warmup should be done:   5%|▌         | 153/3000 [00:00<00:01, 1521.58it/s]warmup should be done:  11%|█         | 329/3000 [00:00<00:01, 1641.69it/s]warmup should be done:  11%|█         | 334/3000 [00:00<00:01, 1664.13it/s]warmup should be done:  11%|█         | 327/3000 [00:00<00:01, 1629.47it/s]warmup should be done:  11%|█         | 337/3000 [00:00<00:01, 1679.15it/s]warmup should be done:  11%|█         | 331/3000 [00:00<00:01, 1650.38it/s]warmup should be done:  11%|█         | 334/3000 [00:00<00:01, 1662.77it/s]warmup should be done:  11%|█         | 332/3000 [00:00<00:01, 1652.84it/s]warmup should be done:  11%|█         | 316/3000 [00:00<00:01, 1579.77it/s]warmup should be done:  16%|█▋        | 490/3000 [00:00<00:01, 1627.07it/s]warmup should be done:  17%|█▋        | 505/3000 [00:00<00:01, 1675.82it/s]warmup should be done:  17%|█▋        | 501/3000 [00:00<00:01, 1661.36it/s]warmup should be done:  16%|█▋        | 494/3000 [00:00<00:01, 1638.94it/s]warmup should be done:  16%|█▌        | 478/3000 [00:00<00:01, 1593.79it/s]warmup should be done:  17%|█▋        | 497/3000 [00:00<00:01, 1646.42it/s]warmup should be done:  17%|█▋        | 498/3000 [00:00<00:01, 1647.67it/s]warmup should be done:  17%|█▋        | 501/3000 [00:00<00:01, 1655.22it/s]warmup should be done:  22%|██▏       | 658/3000 [00:00<00:01, 1638.75it/s]warmup should be done:  22%|██▏       | 673/3000 [00:00<00:01, 1675.37it/s]warmup should be done:  22%|██▏       | 653/3000 [00:00<00:01, 1624.43it/s]warmup should be done:  22%|██▏       | 663/3000 [00:00<00:01, 1647.50it/s]warmup should be done:  22%|██▏       | 668/3000 [00:00<00:01, 1658.69it/s]warmup should be done:  21%|██▏       | 639/3000 [00:00<00:01, 1596.62it/s]warmup should be done:  22%|██▏       | 662/3000 [00:00<00:01, 1644.27it/s]warmup should be done:  22%|██▏       | 667/3000 [00:00<00:01, 1653.93it/s]warmup should be done:  27%|██▋       | 822/3000 [00:00<00:01, 1637.28it/s]warmup should be done:  28%|██▊       | 841/3000 [00:00<00:01, 1673.40it/s]warmup should be done:  28%|██▊       | 828/3000 [00:00<00:01, 1647.62it/s]warmup should be done:  27%|██▋       | 816/3000 [00:00<00:01, 1621.82it/s]warmup should be done:  28%|██▊       | 834/3000 [00:00<00:01, 1655.84it/s]warmup should be done:  27%|██▋       | 800/3000 [00:00<00:01, 1598.48it/s]warmup should be done:  28%|██▊       | 827/3000 [00:00<00:01, 1641.53it/s]warmup should be done:  28%|██▊       | 833/3000 [00:00<00:01, 1651.41it/s]warmup should be done:  33%|███▎      | 986/3000 [00:00<00:01, 1635.14it/s]warmup should be done:  34%|███▎      | 1009/3000 [00:00<00:01, 1672.80it/s]warmup should be done:  33%|███▎      | 993/3000 [00:00<00:01, 1646.73it/s]warmup should be done:  32%|███▏      | 961/3000 [00:00<00:01, 1600.99it/s]warmup should be done:  33%|███▎      | 979/3000 [00:00<00:01, 1617.65it/s]warmup should be done:  33%|███▎      | 992/3000 [00:00<00:01, 1639.57it/s]warmup should be done:  33%|███▎      | 999/3000 [00:00<00:01, 1650.45it/s]warmup should be done:  33%|███▎      | 1000/3000 [00:00<00:01, 1633.88it/s]warmup should be done:  39%|███▊      | 1158/3000 [00:00<00:01, 1644.32it/s]warmup should be done:  38%|███▊      | 1150/3000 [00:00<00:01, 1630.79it/s]warmup should be done:  39%|███▉      | 1177/3000 [00:00<00:01, 1669.87it/s]warmup should be done:  37%|███▋      | 1122/3000 [00:00<00:01, 1600.71it/s]warmup should be done:  39%|███▊      | 1156/3000 [00:00<00:01, 1639.10it/s]warmup should be done:  38%|███▊      | 1141/3000 [00:00<00:01, 1612.67it/s]warmup should be done:  39%|███▉      | 1165/3000 [00:00<00:01, 1646.71it/s]warmup should be done:  39%|███▉      | 1164/3000 [00:00<00:01, 1634.93it/s]warmup should be done:  44%|████▍     | 1323/3000 [00:00<00:01, 1644.28it/s]warmup should be done:  43%|████▎     | 1283/3000 [00:00<00:01, 1603.14it/s]warmup should be done:  45%|████▍     | 1344/3000 [00:00<00:00, 1668.41it/s]warmup should be done:  44%|████▍     | 1314/3000 [00:00<00:01, 1630.53it/s]warmup should be done:  44%|████▍     | 1322/3000 [00:00<00:01, 1643.20it/s]warmup should be done:  43%|████▎     | 1303/3000 [00:00<00:01, 1612.13it/s]warmup should be done:  44%|████▍     | 1330/3000 [00:00<00:01, 1645.29it/s]warmup should be done:  44%|████▍     | 1329/3000 [00:00<00:01, 1638.68it/s]warmup should be done:  50%|████▉     | 1488/3000 [00:00<00:00, 1644.45it/s]warmup should be done:  48%|████▊     | 1446/3000 [00:00<00:00, 1610.57it/s]warmup should be done:  49%|████▉     | 1478/3000 [00:00<00:00, 1631.09it/s]warmup should be done:  50%|████▉     | 1488/3000 [00:00<00:00, 1646.41it/s]warmup should be done:  50%|█████     | 1511/3000 [00:00<00:00, 1658.75it/s]warmup should be done:  49%|████▉     | 1465/3000 [00:00<00:00, 1610.63it/s]warmup should be done:  50%|████▉     | 1495/3000 [00:00<00:00, 1643.51it/s]warmup should be done:  50%|████▉     | 1494/3000 [00:00<00:00, 1641.25it/s]warmup should be done:  55%|█████▌    | 1653/3000 [00:01<00:00, 1644.16it/s]warmup should be done:  54%|█████▎    | 1610/3000 [00:01<00:00, 1617.39it/s]warmup should be done:  55%|█████▍    | 1643/3000 [00:01<00:00, 1634.46it/s]warmup should be done:  55%|█████▌    | 1654/3000 [00:01<00:00, 1649.93it/s]warmup should be done:  56%|█████▌    | 1677/3000 [00:01<00:00, 1655.38it/s]warmup should be done:  54%|█████▍    | 1627/3000 [00:01<00:00, 1610.30it/s]warmup should be done:  55%|█████▌    | 1660/3000 [00:01<00:00, 1643.32it/s]warmup should be done:  55%|█████▌    | 1659/3000 [00:01<00:00, 1643.75it/s]warmup should be done:  61%|██████    | 1818/3000 [00:01<00:00, 1644.29it/s]warmup should be done:  59%|█████▉    | 1774/3000 [00:01<00:00, 1622.25it/s]warmup should be done:  60%|██████    | 1808/3000 [00:01<00:00, 1637.46it/s]warmup should be done:  61%|██████    | 1820/3000 [00:01<00:00, 1651.78it/s]warmup should be done:  61%|██████▏   | 1844/3000 [00:01<00:00, 1658.50it/s]warmup should be done:  61%|██████    | 1825/3000 [00:01<00:00, 1643.52it/s]warmup should be done:  60%|█████▉    | 1789/3000 [00:01<00:00, 1609.29it/s]warmup should be done:  61%|██████    | 1824/3000 [00:01<00:00, 1644.71it/s]warmup should be done:  65%|██████▍   | 1937/3000 [00:01<00:00, 1624.37it/s]warmup should be done:  66%|██████▌   | 1983/3000 [00:01<00:00, 1644.66it/s]warmup should be done:  66%|██████▌   | 1973/3000 [00:01<00:00, 1639.64it/s]warmup should be done:  67%|██████▋   | 2012/3000 [00:01<00:00, 1662.20it/s]warmup should be done:  66%|██████▋   | 1990/3000 [00:01<00:00, 1643.36it/s]warmup should be done:  66%|██████▋   | 1989/3000 [00:01<00:00, 1645.87it/s]warmup should be done:  66%|██████▌   | 1986/3000 [00:01<00:00, 1635.89it/s]warmup should be done:  65%|██████▌   | 1950/3000 [00:01<00:00, 1598.80it/s]warmup should be done:  70%|███████   | 2100/3000 [00:01<00:00, 1624.37it/s]warmup should be done:  72%|███████▏  | 2148/3000 [00:01<00:00, 1643.67it/s]warmup should be done:  71%|███████▏  | 2138/3000 [00:01<00:00, 1641.14it/s]warmup should be done:  73%|███████▎  | 2179/3000 [00:01<00:00, 1663.44it/s]warmup should be done:  72%|███████▏  | 2154/3000 [00:01<00:00, 1646.81it/s]warmup should be done:  72%|███████▏  | 2155/3000 [00:01<00:00, 1641.65it/s]warmup should be done:  72%|███████▏  | 2152/3000 [00:01<00:00, 1640.27it/s]warmup should be done:  70%|███████   | 2110/3000 [00:01<00:00, 1588.16it/s]warmup should be done:  75%|███████▌  | 2263/3000 [00:01<00:00, 1625.44it/s]warmup should be done:  77%|███████▋  | 2304/3000 [00:01<00:00, 1645.85it/s]warmup should be done:  77%|███████▋  | 2313/3000 [00:01<00:00, 1639.57it/s]warmup should be done:  77%|███████▋  | 2320/3000 [00:01<00:00, 1648.82it/s]warmup should be done:  78%|███████▊  | 2346/3000 [00:01<00:00, 1661.51it/s]warmup should be done:  77%|███████▋  | 2320/3000 [00:01<00:00, 1642.00it/s]warmup should be done:  77%|███████▋  | 2318/3000 [00:01<00:00, 1644.61it/s]warmup should be done:  76%|███████▌  | 2269/3000 [00:01<00:00, 1579.98it/s]warmup should be done:  81%|████████  | 2426/3000 [00:01<00:00, 1624.62it/s]warmup should be done:  82%|████████▏ | 2470/3000 [00:01<00:00, 1648.48it/s]warmup should be done:  84%|████████▍ | 2513/3000 [00:01<00:00, 1663.47it/s]warmup should be done:  83%|████████▎ | 2477/3000 [00:01<00:00, 1630.67it/s]warmup should be done:  83%|████████▎ | 2485/3000 [00:01<00:00, 1647.84it/s]warmup should be done:  83%|████████▎ | 2485/3000 [00:01<00:00, 1639.06it/s]warmup should be done:  83%|████████▎ | 2483/3000 [00:01<00:00, 1645.10it/s]warmup should be done:  81%|████████  | 2431/3000 [00:01<00:00, 1590.15it/s]warmup should be done:  86%|████████▋ | 2590/3000 [00:01<00:00, 1627.93it/s]warmup should be done:  88%|████████▊ | 2637/3000 [00:01<00:00, 1652.95it/s]warmup should be done:  89%|████████▉ | 2680/3000 [00:01<00:00, 1663.51it/s]warmup should be done:  88%|████████▊ | 2650/3000 [00:01<00:00, 1646.97it/s]warmup should be done:  88%|████████▊ | 2641/3000 [00:01<00:00, 1628.13it/s]warmup should be done:  88%|████████▊ | 2650/3000 [00:01<00:00, 1640.50it/s]warmup should be done:  88%|████████▊ | 2649/3000 [00:01<00:00, 1647.75it/s]warmup should be done:  86%|████████▋ | 2595/3000 [00:01<00:00, 1602.66it/s]warmup should be done:  93%|█████████▎| 2803/3000 [00:01<00:00, 1652.94it/s]warmup should be done:  92%|█████████▏| 2753/3000 [00:01<00:00, 1619.12it/s]warmup should be done:  95%|█████████▍| 2848/3000 [00:01<00:00, 1666.82it/s]warmup should be done:  94%|█████████▍| 2815/3000 [00:01<00:00, 1645.77it/s]warmup should be done:  93%|█████████▎| 2804/3000 [00:01<00:00, 1626.96it/s]warmup should be done:  94%|█████████▍| 2815/3000 [00:01<00:00, 1643.16it/s]warmup should be done:  94%|█████████▍| 2816/3000 [00:01<00:00, 1652.68it/s]warmup should be done:  92%|█████████▏| 2758/3000 [00:01<00:00, 1610.75it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1666.83it/s]warmup should be done:  97%|█████████▋| 2917/3000 [00:01<00:00, 1624.34it/s]warmup should be done:  99%|█████████▉| 2969/3000 [00:01<00:00, 1648.91it/s]warmup should be done:  99%|█████████▉| 2980/3000 [00:01<00:00, 1646.68it/s]warmup should be done:  99%|█████████▉| 2967/3000 [00:01<00:00, 1624.25it/s]warmup should be done:  99%|█████████▉| 2981/3000 [00:01<00:00, 1646.09it/s]warmup should be done:  99%|█████████▉| 2983/3000 [00:01<00:00, 1656.02it/s]warmup should be done:  97%|█████████▋| 2923/3000 [00:01<00:00, 1621.94it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1646.65it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1646.40it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1645.68it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1641.08it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1637.83it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1613.47it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1610.96it/s]




warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]

warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   6%|▌         | 168/3000 [00:00<00:01, 1678.08it/s]warmup should be done:   6%|▌         | 168/3000 [00:00<00:01, 1675.20it/s]warmup should be done:   6%|▌         | 171/3000 [00:00<00:01, 1706.34it/s]warmup should be done:   6%|▌         | 167/3000 [00:00<00:01, 1664.08it/s]warmup should be done:   6%|▌         | 171/3000 [00:00<00:01, 1703.54it/s]warmup should be done:   6%|▌         | 165/3000 [00:00<00:01, 1644.93it/s]warmup should be done:   6%|▌         | 166/3000 [00:00<00:01, 1651.28it/s]warmup should be done:   6%|▌         | 166/3000 [00:00<00:01, 1651.47it/s]warmup should be done:  11%|█▏        | 342/3000 [00:00<00:01, 1708.18it/s]warmup should be done:  11%|█         | 331/3000 [00:00<00:01, 1652.05it/s]warmup should be done:  11%|█         | 336/3000 [00:00<00:01, 1673.63it/s]warmup should be done:  11%|█         | 335/3000 [00:00<00:01, 1671.19it/s]warmup should be done:  11%|█         | 336/3000 [00:00<00:01, 1671.83it/s]warmup should be done:  11%|█▏        | 342/3000 [00:00<00:01, 1701.69it/s]warmup should be done:  11%|█         | 334/3000 [00:00<00:01, 1661.94it/s]warmup should be done:  11%|█         | 332/3000 [00:00<00:01, 1647.83it/s]warmup should be done:  17%|█▋        | 498/3000 [00:00<00:01, 1658.55it/s]warmup should be done:  17%|█▋        | 504/3000 [00:00<00:01, 1679.57it/s]warmup should be done:  17%|█▋        | 504/3000 [00:00<00:01, 1673.36it/s]warmup should be done:  17%|█▋        | 504/3000 [00:00<00:01, 1677.27it/s]warmup should be done:  17%|█▋        | 513/3000 [00:00<00:01, 1702.70it/s]warmup should be done:  17%|█▋        | 504/3000 [00:00<00:01, 1671.12it/s]warmup should be done:  17%|█▋        | 498/3000 [00:00<00:01, 1650.69it/s]warmup should be done:  17%|█▋        | 513/3000 [00:00<00:01, 1692.55it/s]warmup should be done:  22%|██▏       | 674/3000 [00:00<00:01, 1685.48it/s]warmup should be done:  22%|██▏       | 665/3000 [00:00<00:01, 1659.20it/s]warmup should be done:  22%|██▎       | 675/3000 [00:00<00:01, 1687.89it/s]warmup should be done:  22%|██▏       | 672/3000 [00:00<00:01, 1673.52it/s]warmup should be done:  22%|██▏       | 673/3000 [00:00<00:01, 1676.90it/s]warmup should be done:  23%|██▎       | 685/3000 [00:00<00:01, 1706.07it/s]warmup should be done:  22%|██▏       | 665/3000 [00:00<00:01, 1654.67it/s]warmup should be done:  23%|██▎       | 685/3000 [00:00<00:01, 1702.07it/s]warmup should be done:  28%|██▊       | 844/3000 [00:00<00:01, 1689.85it/s]warmup should be done:  28%|██▊       | 840/3000 [00:00<00:01, 1675.82it/s]warmup should be done:  28%|██▊       | 831/3000 [00:00<00:01, 1657.55it/s]warmup should be done:  28%|██▊       | 847/3000 [00:00<00:01, 1698.37it/s]warmup should be done:  29%|██▊       | 857/3000 [00:00<00:01, 1708.87it/s]warmup should be done:  28%|██▊       | 842/3000 [00:00<00:01, 1678.42it/s]warmup should be done:  28%|██▊       | 832/3000 [00:00<00:01, 1657.66it/s]warmup should be done:  29%|██▊       | 858/3000 [00:00<00:01, 1708.94it/s]warmup should be done:  34%|███▍      | 1014/3000 [00:00<00:01, 1690.97it/s]warmup should be done:  34%|███▍      | 1018/3000 [00:00<00:01, 1702.24it/s]warmup should be done:  34%|███▎      | 1009/3000 [00:00<00:01, 1677.62it/s]warmup should be done:  34%|███▍      | 1028/3000 [00:00<00:01, 1708.93it/s]warmup should be done:  33%|███▎      | 999/3000 [00:00<00:01, 1662.08it/s]warmup should be done:  34%|███▎      | 1010/3000 [00:00<00:01, 1677.89it/s]warmup should be done:  33%|███▎      | 998/3000 [00:00<00:01, 1656.30it/s]warmup should be done:  34%|███▍      | 1030/3000 [00:00<00:01, 1710.95it/s]warmup should be done:  40%|███▉      | 1189/3000 [00:00<00:01, 1703.40it/s]warmup should be done:  39%|███▉      | 1179/3000 [00:00<00:01, 1683.38it/s]warmup should be done:  39%|███▉      | 1170/3000 [00:00<00:01, 1676.50it/s]warmup should be done:  39%|███▉      | 1184/3000 [00:00<00:01, 1689.12it/s]warmup should be done:  40%|███▉      | 1199/3000 [00:00<00:01, 1705.00it/s]warmup should be done:  39%|███▉      | 1178/3000 [00:00<00:01, 1674.76it/s]warmup should be done:  39%|███▉      | 1164/3000 [00:00<00:01, 1656.34it/s]warmup should be done:  40%|████      | 1202/3000 [00:00<00:01, 1709.34it/s]warmup should be done:  45%|████▌     | 1362/3000 [00:00<00:00, 1708.79it/s]warmup should be done:  45%|████▍     | 1342/3000 [00:00<00:00, 1688.00it/s]warmup should be done:  45%|████▌     | 1354/3000 [00:00<00:00, 1691.12it/s]warmup should be done:  45%|████▍     | 1346/3000 [00:00<00:00, 1676.22it/s]warmup should be done:  46%|████▌     | 1371/3000 [00:00<00:00, 1708.06it/s]warmup should be done:  45%|████▍     | 1348/3000 [00:00<00:00, 1676.16it/s]warmup should be done:  46%|████▌     | 1375/3000 [00:00<00:00, 1713.76it/s]warmup should be done:  44%|████▍     | 1330/3000 [00:00<00:01, 1650.51it/s]warmup should be done:  50%|█████     | 1512/3000 [00:00<00:00, 1690.58it/s]warmup should be done:  50%|█████     | 1514/3000 [00:00<00:00, 1675.27it/s]warmup should be done:  51%|█████▏    | 1542/3000 [00:00<00:00, 1706.51it/s]warmup should be done:  51%|█████     | 1524/3000 [00:00<00:00, 1688.63it/s]warmup should be done:  51%|█████     | 1518/3000 [00:00<00:00, 1682.77it/s]warmup should be done:  52%|█████▏    | 1547/3000 [00:00<00:00, 1712.32it/s]warmup should be done:  51%|█████     | 1533/3000 [00:00<00:00, 1683.11it/s]warmup should be done:  50%|████▉     | 1496/3000 [00:00<00:00, 1609.09it/s]warmup should be done:  56%|█████▌    | 1683/3000 [00:01<00:00, 1695.39it/s]warmup should be done:  56%|█████▋    | 1693/3000 [00:01<00:00, 1688.04it/s]warmup should be done:  57%|█████▋    | 1713/3000 [00:01<00:00, 1703.93it/s]warmup should be done:  56%|█████▋    | 1688/3000 [00:01<00:00, 1686.33it/s]warmup should be done:  57%|█████▋    | 1719/3000 [00:01<00:00, 1711.86it/s]warmup should be done:  57%|█████▋    | 1705/3000 [00:01<00:00, 1691.34it/s]warmup should be done:  56%|█████▌    | 1682/3000 [00:01<00:00, 1652.84it/s]warmup should be done:  55%|█████▌    | 1658/3000 [00:01<00:00, 1532.52it/s]warmup should be done:  62%|██████▏   | 1853/3000 [00:01<00:00, 1692.62it/s]warmup should be done:  63%|██████▎   | 1884/3000 [00:01<00:00, 1704.99it/s]warmup should be done:  62%|██████▏   | 1862/3000 [00:01<00:00, 1685.89it/s]warmup should be done:  62%|██████▏   | 1857/3000 [00:01<00:00, 1686.71it/s]warmup should be done:  63%|██████▎   | 1891/3000 [00:01<00:00, 1713.33it/s]warmup should be done:  63%|██████▎   | 1877/3000 [00:01<00:00, 1698.70it/s]warmup should be done:  62%|██████▏   | 1850/3000 [00:01<00:00, 1659.96it/s]warmup should be done:  60%|██████    | 1813/3000 [00:01<00:00, 1390.36it/s]warmup should be done:  68%|██████▊   | 2055/3000 [00:01<00:00, 1706.20it/s]warmup should be done:  67%|██████▋   | 2024/3000 [00:01<00:00, 1695.45it/s]warmup should be done:  68%|██████▊   | 2031/3000 [00:01<00:00, 1685.82it/s]warmup should be done:  68%|██████▊   | 2026/3000 [00:01<00:00, 1684.86it/s]warmup should be done:  69%|██████▉   | 2063/3000 [00:01<00:00, 1715.28it/s]warmup should be done:  68%|██████▊   | 2048/3000 [00:01<00:00, 1699.98it/s]warmup should be done:  67%|██████▋   | 2017/3000 [00:01<00:00, 1659.68it/s]warmup should be done:  66%|██████▌   | 1967/3000 [00:01<00:00, 1430.07it/s]warmup should be done:  73%|███████▎  | 2195/3000 [00:01<00:00, 1697.65it/s]warmup should be done:  73%|███████▎  | 2200/3000 [00:01<00:00, 1685.65it/s]warmup should be done:  74%|███████▍  | 2226/3000 [00:01<00:00, 1704.06it/s]warmup should be done:  73%|███████▎  | 2195/3000 [00:01<00:00, 1682.95it/s]warmup should be done:  74%|███████▍  | 2235/3000 [00:01<00:00, 1715.76it/s]warmup should be done:  74%|███████▍  | 2220/3000 [00:01<00:00, 1703.22it/s]warmup should be done:  73%|███████▎  | 2187/3000 [00:01<00:00, 1669.46it/s]warmup should be done:  71%|███████   | 2136/3000 [00:01<00:00, 1502.00it/s]warmup should be done:  79%|███████▉  | 2366/3000 [00:01<00:00, 1699.78it/s]warmup should be done:  79%|███████▉  | 2370/3000 [00:01<00:00, 1687.63it/s]warmup should be done:  80%|███████▉  | 2397/3000 [00:01<00:00, 1703.32it/s]warmup should be done:  80%|████████  | 2407/3000 [00:01<00:00, 1715.98it/s]warmup should be done:  79%|███████▉  | 2364/3000 [00:01<00:00, 1681.78it/s]warmup should be done:  79%|███████▊  | 2357/3000 [00:01<00:00, 1678.59it/s]warmup should be done:  80%|███████▉  | 2392/3000 [00:01<00:00, 1706.41it/s]warmup should be done:  77%|███████▋  | 2306/3000 [00:01<00:00, 1556.46it/s]warmup should be done:  85%|████████▍ | 2537/3000 [00:01<00:00, 1702.27it/s]warmup should be done:  85%|████████▍ | 2540/3000 [00:01<00:00, 1690.53it/s]warmup should be done:  86%|████████▌ | 2569/3000 [00:01<00:00, 1705.53it/s]warmup should be done:  86%|████████▌ | 2579/3000 [00:01<00:00, 1716.79it/s]warmup should be done:  84%|████████▍ | 2533/3000 [00:01<00:00, 1679.17it/s]warmup should be done:  85%|████████▌ | 2564/3000 [00:01<00:00, 1709.11it/s]warmup should be done:  84%|████████▍ | 2525/3000 [00:01<00:00, 1651.55it/s]warmup should be done:  83%|████████▎ | 2476/3000 [00:01<00:00, 1596.57it/s]warmup should be done:  90%|█████████ | 2710/3000 [00:01<00:00, 1692.68it/s]warmup should be done:  90%|█████████ | 2708/3000 [00:01<00:00, 1702.53it/s]warmup should be done:  91%|█████████▏| 2741/3000 [00:01<00:00, 1707.22it/s]warmup should be done:  92%|█████████▏| 2752/3000 [00:01<00:00, 1718.85it/s]warmup should be done:  90%|█████████ | 2701/3000 [00:01<00:00, 1676.90it/s]warmup should be done:  91%|█████████ | 2735/3000 [00:01<00:00, 1704.37it/s]warmup should be done:  90%|████████▉ | 2691/3000 [00:01<00:00, 1645.34it/s]warmup should be done:  88%|████████▊ | 2645/3000 [00:01<00:00, 1623.62it/s]warmup should be done:  96%|█████████▌| 2880/3000 [00:01<00:00, 1693.07it/s]warmup should be done:  98%|█████████▊| 2925/3000 [00:01<00:00, 1720.39it/s]warmup should be done:  97%|█████████▋| 2913/3000 [00:01<00:00, 1708.11it/s]warmup should be done:  96%|█████████▌| 2879/3000 [00:01<00:00, 1692.01it/s]warmup should be done:  96%|█████████▌| 2869/3000 [00:01<00:00, 1673.96it/s]warmup should be done:  97%|█████████▋| 2907/3000 [00:01<00:00, 1706.98it/s]warmup should be done:  95%|█████████▌| 2856/3000 [00:01<00:00, 1646.01it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1713.29it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1705.77it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1698.94it/s]warmup should be done:  94%|█████████▍| 2814/3000 [00:01<00:00, 1640.56it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1688.12it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1685.92it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1678.64it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1660.19it/s]warmup should be done:  99%|█████████▉| 2982/3000 [00:01<00:00, 1652.02it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1595.49it/s]
WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7fcddf6770d0>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7fcddf6761f0>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7fcddf745d30>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7fcddf684280>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7fcddf742b20>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7fcddf6741c0>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7fcddf743730>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7fcddf676190>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
2022-12-12 01:47:20.895337: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fc90b02d7a0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 01:47:20.895399: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 01:47:20.902039: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fc912830c00 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 01:47:20.902091: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 01:47:20.905589: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 01:47:20.910944: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 01:47:21.027429: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fc91282c970 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 01:47:21.027483: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 01:47:21.036812: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 01:47:21.803268: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fc90a82c820 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 01:47:21.803334: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 01:47:21.812769: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 01:47:21.813979: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fc90b02d6b0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 01:47:21.814041: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 01:47:21.822064: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 01:47:21.923186: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fc90f032020 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 01:47:21.923253: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 01:47:21.926946: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fc90ef93000 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 01:47:21.927005: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 01:47:21.934306: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 01:47:21.937447: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 01:47:21.940775: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fc90f031460 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 01:47:21.940827: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 01:47:21.950301: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 01:47:28.155592: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 01:47:28.176984: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 01:47:28.253597: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 01:47:28.562276: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 01:47:28.597275: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 01:47:28.665795: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 01:47:28.855376: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 01:47:28.883236: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
[HCTR][01:48:25.351][ERROR][RK0][tid #140502422755072]: replica 7 reaches 1000, calling init pre replica
[HCTR][01:48:25.351][ERROR][RK0][tid #140502422755072]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][01:48:25.357][ERROR][RK0][tid #140502422755072]: coll ps creation done
[HCTR][01:48:25.357][ERROR][RK0][tid #140502422755072]: replica 7 waits for coll ps creation barrier
[HCTR][01:48:25.491][ERROR][RK0][tid #140501734913792]: replica 4 reaches 1000, calling init pre replica
[HCTR][01:48:25.491][ERROR][RK0][tid #140501734913792]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][01:48:25.498][ERROR][RK0][tid #140501734913792]: coll ps creation done
[HCTR][01:48:25.498][ERROR][RK0][tid #140501734913792]: replica 4 waits for coll ps creation barrier
[HCTR][01:48:25.544][ERROR][RK0][main]: replica 2 reaches 1000, calling init pre replica
[HCTR][01:48:25.544][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][01:48:25.549][ERROR][RK0][main]: coll ps creation done
[HCTR][01:48:25.549][ERROR][RK0][main]: replica 2 waits for coll ps creation barrier
[HCTR][01:48:25.603][ERROR][RK0][tid #140501734913792]: replica 3 reaches 1000, calling init pre replica
[HCTR][01:48:25.603][ERROR][RK0][tid #140501734913792]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][01:48:25.610][ERROR][RK0][tid #140501734913792]: coll ps creation done
[HCTR][01:48:25.610][ERROR][RK0][tid #140501734913792]: replica 3 waits for coll ps creation barrier
[HCTR][01:48:25.633][ERROR][RK0][main]: replica 1 reaches 1000, calling init pre replica
[HCTR][01:48:25.633][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][01:48:25.638][ERROR][RK0][main]: coll ps creation done
[HCTR][01:48:25.638][ERROR][RK0][main]: replica 1 waits for coll ps creation barrier
[HCTR][01:48:25.644][ERROR][RK0][main]: replica 6 reaches 1000, calling init pre replica
[HCTR][01:48:25.644][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][01:48:25.650][ERROR][RK0][main]: coll ps creation done
[HCTR][01:48:25.650][ERROR][RK0][main]: replica 6 waits for coll ps creation barrier
[HCTR][01:48:25.688][ERROR][RK0][tid #140501667804928]: replica 5 reaches 1000, calling init pre replica
[HCTR][01:48:25.688][ERROR][RK0][tid #140501667804928]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][01:48:25.693][ERROR][RK0][tid #140501667804928]: coll ps creation done
[HCTR][01:48:25.693][ERROR][RK0][tid #140501667804928]: replica 5 waits for coll ps creation barrier
[HCTR][01:48:25.844][ERROR][RK0][tid #140501726521088]: replica 0 reaches 1000, calling init pre replica
[HCTR][01:48:25.844][ERROR][RK0][tid #140501726521088]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][01:48:25.852][ERROR][RK0][tid #140501726521088]: coll ps creation done
[HCTR][01:48:25.852][ERROR][RK0][tid #140501726521088]: replica 0 waits for coll ps creation barrier
[HCTR][01:48:25.852][ERROR][RK0][tid #140501726521088]: replica 0 preparing frequency
[HCTR][01:48:26.728][ERROR][RK0][tid #140501726521088]: replica 0 preparing frequency done
[HCTR][01:48:26.762][ERROR][RK0][tid #140501726521088]: replica 0 calling init per replica
[HCTR][01:48:26.762][ERROR][RK0][main]: replica 6 calling init per replica
[HCTR][01:48:26.762][ERROR][RK0][tid #140501734913792]: replica 3 calling init per replica
[HCTR][01:48:26.762][ERROR][RK0][main]: replica 1 calling init per replica
[HCTR][01:48:26.762][ERROR][RK0][tid #140502422755072]: replica 7 calling init per replica
[HCTR][01:48:26.762][ERROR][RK0][tid #140501734913792]: replica 4 calling init per replica
[HCTR][01:48:26.762][ERROR][RK0][main]: replica 2 calling init per replica
[HCTR][01:48:26.762][ERROR][RK0][tid #140501667804928]: replica 5 calling init per replica
[HCTR][01:48:26.762][ERROR][RK0][tid #140501726521088]: Calling build_v2
[HCTR][01:48:26.762][ERROR][RK0][main]: Calling build_v2
[HCTR][01:48:26.762][ERROR][RK0][tid #140501734913792]: Calling build_v2
[HCTR][01:48:26.762][ERROR][RK0][main]: Calling build_v2
[HCTR][01:48:26.762][ERROR][RK0][tid #140502422755072]: Calling build_v2
[HCTR][01:48:26.762][ERROR][RK0][tid #140501734913792]: Calling build_v2
[HCTR][01:48:26.762][ERROR][RK0][main]: Calling build_v2
[HCTR][01:48:26.762][ERROR][RK0][tid #140501726521088]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][01:48:26.762][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][01:48:26.762][ERROR][RK0][tid #140501667804928]: Calling build_v2
[HCTR][01:48:26.762][ERROR][RK0][tid #140501734913792]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][01:48:26.762][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][01:48:26.762][ERROR][RK0][tid #140502422755072]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][01:48:26.762][ERROR][RK0][tid #140501734913792]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][01:48:26.762][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][01:48:26.762][ERROR][RK0][tid #140501667804928]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[[2022-12-12 01:48:26.766360: 2022-12-12 01:48:26E. 766403/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: :E178 [] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccv100x8, slow pcie:
1782022-12-12 01:48:26] .v100x8, slow pcie[766444
2022-12-12 01:48:26: .E766480[ : 2022-12-12 01:48:26/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[E.: 766497178/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 01:48:26: ] :.Ev100x8, slow pcie196766491 
] : /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[assigning 0 to cpuE:[
 1962022-12-12 01:48:262022-12-12 01:48:26/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] ..:assigning 0 to cpu766556766536178
: : ] [v100x8, slow pcieEE2022-12-12 01:48:26
  ./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc766597[[::: 2022-12-12 01:48:262022-12-12 01:48:261962022-12-12 01:48:26178E..] [.]  766620766621assigning 0 to cpu766592v100x8, slow pcie/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: 2022-12-12 01:48:26: 
: 
:E.E[E212 [766634  ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 01:48:26[: 2022-12-12 01:48:26/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccbuild asymm link desc with 8X Tesla V100-SXM2-32GB out of 8:.2022-12-12 01:48:26E.::
212766724. 766690196178] : 766752/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: [] ] build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8E: :E2022-12-12 01:48:26assigning 0 to cpuv100x8, slow pcie
 E178 .

/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc ] [/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc766836:/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[v100x8, slow pcie2022-12-12 01:48:26:: 196:2022-12-12 01:48:26
.[178E] 212.7669122022-12-12 01:48:26[]  assigning 0 to cpu] 766938: .2022-12-12 01:48:26v100x8, slow pcie/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8: E766976.:

E : 766999213 /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[E: [[] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:2022-12-12 01:48:26 E2022-12-12 01:48:262022-12-12 01:48:26remote time is 8.68421:213./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc ..
196] 767099:/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc767106767106] remote time is 8.68421[: 212:: : assigning 0 to cpu
2022-12-12 01:48:26E] 196EE
.[ build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8]   7672262022-12-12 01:48:26/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
assigning 0 to cpu/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: .:
::E[767292[196213212 2022-12-12 01:48:26: 2022-12-12 01:48:26] ] ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.[E.assigning 0 to cpuremote time is 8.68421build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8:7673652022-12-12 01:48:26 767370


214: ./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: ] [E767403[:Ecpu time is 97.05882022-12-12 01:48:26 : 2022-12-12 01:48:26214 
./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[E.] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc767476:2022-12-12 01:48:26 767483cpu time is 97.0588:: 213./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: 
212E] 767522:E]  remote time is 8.68421: 212 build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
E] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
: [build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8:2142022-12-12 01:48:26/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[
213] .:2022-12-12 01:48:26] cpu time is 97.0588767672[212.remote time is 8.68421
: 2022-12-12 01:48:26] 767697
E.build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8:  767730[
E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: 2022-12-12 01:48:26 :E.[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc214 7677792022-12-12 01:48:26:] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: .213cpu time is 97.0588:E767815] 
213 : remote time is 8.68421] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE
remote time is 8.68421: 
[214/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 01:48:26[] :.2022-12-12 01:48:26cpu time is 97.0588213767942.
] : 767957remote time is 8.68421E: 
 E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc :/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc214:[] 2142022-12-12 01:48:26cpu time is 97.0588] .
cpu time is 97.0588768026
: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:214] cpu time is 97.0588
[2022-12-12 01:49:45.824171: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:118] solver created. now build & solve
[2022-12-12 01:49:45.864263: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:123] solver built. now solve
[2022-12-12 01:49:45.864336: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:919] num_cached_nodes = 5000000
[2022-12-12 01:49:45.984911: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:127] solver solved
[2022-12-12 01:49:45.984999: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:195] 0 solved master
[2022-12-12 01:49:45.997661: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 0 solved
[2022-12-12 01:49:45.997693: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 0 initing device 0
[2022-12-12 01:49:45.998128: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 01:49:45.998967: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 01:49:45.999639: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 01:49:46. 12805: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 3 solved
[2022-12-12 01:49:46. 12865: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 3 initing device 3
[2022-12-12 01:49:46. 13259: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 01:49:46[.2022-12-12 01:49:46 13319.:  13334E:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc :/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc202:] 2022 solved] 
4 solved
[2022-12-12 01:49:46[.2022-12-12 01:49:46 13402.:  13406E:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc :/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc205:] 205worker 0 thread 2 initing device 2] 
worker 0 thread 4 initing device 4
[[2022-12-12 01:49:462022-12-12 01:49:46.. 13803 13806: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::19801980] ] eager alloc mem 381.47 MBeager alloc mem 381.47 MB

[2022-12-12 01:49:46. 15013: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[[2022-12-12 01:49:462022-12-12 01:49:46.. 15147 15146: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc::202202] ] 5 solved7 solved

[[2022-12-12 01:49:462022-12-12 01:49:46.. 15267 15270: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc::205205] ] worker 0 thread 5 initing device 5worker 0 thread 7 initing device 7

[2022-12-12 01:49:46. 15705: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980[] 2022-12-12 01:49:46eager alloc mem 381.47 MB.
 15726: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 01:49:46. 15873: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 01:49:46. 16021: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 01:49:46. 17293: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 1 solved
[2022-12-12 01:49:46. 17385: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 1 initing device 1
[2022-12-12 01:49:46. 17868: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 01:49:46. 18117: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 01:49:46. 18770: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 6 solved
[2022-12-12 01:49:46. 18856: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 6 initing device 6
[2022-12-12 01:49:46. 19395: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 01:49:46. 19691: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 01:49:46. 19745: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 01:49:46. 19785: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 01:49:46. 20351: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 01:49:46. 22415: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 01:49:46. 23558: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 01:49:46. 23657: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 01:49:46. 23707: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 01:49:46. 25462: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 01:49:46. 25749: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 01:49:46. 75857: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 2.00 Bytes
[2022-12-12 01:49:46. 76247: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1024.00 Bytes
[2022-12-12 01:49:46. 81608: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-12 01:49:46. 81705: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 2
[2022-12-12 01:49:46. 81753: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 01:49:46. 82673: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 19.45 MB
[2022-12-12 01:49:46. 83461: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 01:49:46. 84448: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 01:49:46. 84536: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 01:49:46. 85214: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 01:49:46. 85254: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 2.38 GB
[2022-12-12 01:49:46.102067: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 2.00 Bytes
[2022-12-12 01:49:46.102307: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 2.00 Bytes
[2022-12-12 01:49:46.[102355[2022-12-12 01:49:46: 2022-12-12 01:49:46.E.102374 [102376: /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu2022-12-12 01:49:46: E:.E 1980102417 /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu] : /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:eager alloc mem 2.00 BytesE:1980
 1980] /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu] eager alloc mem 2.00 Bytes:eager alloc mem 2.00 Bytes
1980
] eager alloc mem 1024.00 Bytes
[2022-12-12 01:49:46.102613: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1024.00 Bytes
[2022-12-12 01:49:46.102719: E[ [2022-12-12 01:49:46/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu2022-12-12 01:49:46.:.1027301980102734: ] : Eeager alloc mem 1024.00 BytesE 
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::19801980] ] eager alloc mem 1024.00 Byteseager alloc mem 1024.00 Bytes

[2022-12-12 01:49:46.110003: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 2.00 Bytes
[2022-12-12 01:49:46.110072: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 2.00 Bytes
[2022-12-12 01:49:46.110332: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1024.00 Bytes
[2022-12-12 01:49:46.110419: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1024.00 Bytes
[2022-12-12 01:49:46.112312: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-12 01:49:46.112392: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 2
[2022-12-12 01:49:46.112437: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 01:49:46.115601: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-12 01:49:46.115674: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 2
[2022-12-12 01:49:46.115684: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638[] 2022-12-12 01:49:46eager release cuda mem 1024.
115718: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 01:49:46.115753: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 2
[2022-12-12 01:49:46.115764: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc[:2022-12-12 01:49:46638.] 115797eager release cuda mem 1024: 
E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 01:49:46.115840: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 2
[2022-12-12 01:49:46.115851: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:[6382022-12-12 01:49:46] .eager release cuda mem 1024115885
: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 01:49:46.115924: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 2
[2022-12-12 01:49:46.115974: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 01:49:46.118711: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 19.45 MB
[2022-12-12 01:49:46.119941: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 19.45 MB
[2022-12-12 01:49:46.120462: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 19.45 MB
[2022-12-12 01:49:46.120578: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-12 01:49:46.120649: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 2
[2022-12-12 01:49:46.120690: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 01:49:46.120739: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-12 01:49:46.120812: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 2
[2022-12-12 01:49:46.120855: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 01:49:46.121211: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 19.45 MB
[2022-12-12 01:49:46.131709: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 19.45 MB
[2022-12-12 01:49:46.133698: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 01:49:46.134229: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 19.45 MB
[2022-12-12 01:49:46.134623: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 01:49:46.134709: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 01:49:46.134752: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 19.45 MB
[2022-12-12 01:49:46.135103: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 01:49:46.135160: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 01:49:46.135263: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[[2022-12-12 01:49:462022-12-12 01:49:46..135289135304: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::19801980] ] eager alloc mem 611.00 KBeager alloc mem 2.38 GB

[2022-12-12 01:49:46.135398: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 01:49:46.136024: [E2022-12-12 01:49:46 ./hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc136033:: 638E]  eager release cuda mem 625663/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc
:638] eager release cuda mem 625663
[2022-12-12 01:49:46.136121: [E2022-12-12 01:49:46 ./hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu136130:: 1980E]  eager alloc mem 25.25 KB/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu
:1980] eager alloc mem 25.25 KB
[2022-12-12 01:49:46.136225: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 01:49:46.136306: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 01:49:46.136335: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 01:49:46.136415: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 01:49:46.136690: E[ 2022-12-12 01:49:46/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc.:136700638: ] Eeager release cuda mem 25855 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 01:49:46.136742: E [/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu2022-12-12 01:49:46:.1980136753] : eager alloc mem 2.38 GBE
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 2.38 GB
[2022-12-12 01:49:46.136857: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 01:49:46.136895: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 2.38 GB
[2022-12-12 01:49:46.137091: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 01:49:46.137131: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 2.38 GB
[2022-12-12 01:49:46.138676: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 01:49:46.138726: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 01:49:46.139647: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 01:49:46.139698: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 01:49:46.139735: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 01:49:46.139786: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 01:49:46.140415: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 01:49:46.[1404552022-12-12 01:49:46: .E140458 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE: 1980/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc] :eager alloc mem 2.38 GB638
] eager release cuda mem 25855
[2022-12-12 01:49:46.140521: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 2.38 GB
[[[[[[[[2022-12-12 01:49:462022-12-12 01:49:462022-12-12 01:49:462022-12-12 01:49:462022-12-12 01:49:462022-12-12 01:49:462022-12-12 01:49:462022-12-12 01:49:46........619942619942619942619942619942619942619942619948: : : : : : : : EEEEEEEE        /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::::::::19801980198019801980198019801980] ] ] ] ] ] ] ] eager alloc mem 611.00 KBeager alloc mem 611.00 KBeager alloc mem 611.00 KBeager alloc mem 611.00 KBeager alloc mem 611.00 KBeager alloc mem 611.00 KBeager alloc mem 611.00 KBeager alloc mem 611.00 KB







[[[[2022-12-12 01:49:46[2022-12-12 01:49:462022-12-12 01:49:462022-12-12 01:49:46[.[[2022-12-12 01:49:46...2022-12-12 01:49:466210532022-12-12 01:49:462022-12-12 01:49:46.621054621056621056.: ..621059: : : 621065E621067621068: EEE:  : : E   E/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccEE /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc :  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:::/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc638/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638638638:] ::638] ] ] 638eager release cuda mem 625663638638] eager release cuda mem 625663eager release cuda mem 625663eager release cuda mem 625663] 
] ] eager release cuda mem 625663


eager release cuda mem 625663eager release cuda mem 625663eager release cuda mem 625663



[2022-12-12 01:49:46.621375: E[ [2022-12-12 01:49:46/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu[2022-12-12 01:49:46.:2022-12-12 01:49:46.[6213861980.[6213872022-12-12 01:49:46[: [] 6213912022-12-12 01:49:46: .2022-12-12 01:49:46E2022-12-12 01:49:46eager alloc mem 611.00 KB: .E621401. .
E621408 : 621411/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu621413 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE: :: /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE: E1980E: 1980/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu ]  1980/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu] :/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cueager alloc mem 611.00 KB/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu] :eager alloc mem 611.00 KB1980:
:eager alloc mem 611.00 KB1980
] 19801980
] eager alloc mem 611.00 KB] ] eager alloc mem 611.00 KB
eager alloc mem 611.00 KBeager alloc mem 611.00 KB


[2022-12-12 01:49:46.622220: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 01:49:46.622290: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 01:49:46.622343: E[ 2022-12-12 01:49:46/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc.:622354638[: ] 2022-12-12 01:49:46Eeager release cuda mem 625663. 
622369/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc[: :2022-12-12 01:49:46[E[638.2022-12-12 01:49:46 [2022-12-12 01:49:46] 622388./hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc2022-12-12 01:49:46.[eager release cuda mem 625663: 622401:.6223982022-12-12 01:49:46
E: 638622411: . E] : E622435/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc eager release cuda mem 625663E : :/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc
 [/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccE638:/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc2022-12-12 01:49:46: ] 638:.638/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cueager release cuda mem 625663] 638[622517] :
eager release cuda mem 625663] 2022-12-12 01:49:46: eager release cuda mem 6256631980
eager release cuda mem 625663.E
] 
622575 eager alloc mem 611.00 KB[: /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu
2022-12-12 01:49:46E:. 1980[622652/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu] 2022-12-12 01:49:46: :eager alloc mem 611.00 KB.[E1980
622675[2022-12-12 01:49:46 ] : 2022-12-12 01:49:46./hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cueager alloc mem 611.00 KBE.622690:
 622701: 1980/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu: E] :E eager alloc mem 611.00 KB1980 /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu
] /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:eager alloc mem 611.00 KB:1980
1980] ] eager alloc mem 611.00 KBeager alloc mem 611.00 KB

[2022-12-12 01:49:46.623054: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 01:49:46.623124: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 01:49:46.623397: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 01:49:46.623433: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 01:49:46.623468: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:[19802022-12-12 01:49:46] .eager alloc mem 611.00 KB623482
: [E2022-12-12 01:49:46 ./hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc623501:: 638E]  eager release cuda mem 625663/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu
[:2022-12-12 01:49:461980.] 623529eager alloc mem 611.00 KB: 
E [/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc2022-12-12 01:49:46:.638[623554] 2022-12-12 01:49:46: [[eager release cuda mem 625663.E2022-12-12 01:49:462022-12-12 01:49:46
623565 ..: /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc623573623575E:: :  638[EE/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc] 2022-12-12 01:49:46  :eager release cuda mem 625663./hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu638
623657::] : 6381980eager release cuda mem 625663E] ] 
 eager release cuda mem 625663eager alloc mem 611.00 KB[/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu

2022-12-12 01:49:46:.1980623768] : eager alloc mem 611.00 KBE
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu[:2022-12-12 01:49:461980.] 623813eager alloc mem 611.00 KB: [
E2022-12-12 01:49:46 ./hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu623837:: 1980E]  eager alloc mem 611.00 KB/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu
:[19802022-12-12 01:49:46] .eager alloc mem 611.00 KB623882
: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 01:49:46.623983: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 01:49:46.624226: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 01:49:46.624277: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:[6382022-12-12 01:49:46] .eager release cuda mem 625663624293
: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 01:49:46.624353: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 01:49:46.624519: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:[6382022-12-12 01:49:46] .eager release cuda mem 625663624535
: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 01:49:46.624574: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] [eager release cuda mem 6256632022-12-12 01:49:46
.624593: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu[[:2022-12-12 01:49:462022-12-12 01:49:461980..] 624609624610[eager alloc mem 611.00 KB: [: 2022-12-12 01:49:46
E2022-12-12 01:49:46E. . 624643/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc624649/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu: :: :E638E1980 ]  [] /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cueager release cuda mem 625663/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc2022-12-12 01:49:46eager alloc mem 611.00 KB:
:.
1980638624731] ] : eager alloc mem 611.00 KBeager release cuda mem 625663E

 [/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc2022-12-12 01:49:46:.638624812] : eager release cuda mem 625663E
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:[19802022-12-12 01:49:46] .eager alloc mem 611.00 KB624860
: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 01:49:46.624896: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 01:49:46.625052: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 01:49:46.625098: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] [eager release cuda mem 6256632022-12-12 01:49:46
.625119: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 01:49:46.625168: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 01:49:46.625396: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 01:49:46.625461: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 01:49:46.625501: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 01:49:46.625539: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 01:49:46.625568: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[[2022-12-12 01:49:462022-12-12 01:49:46..625602625603: [: E2022-12-12 01:49:46E .[ /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc6256252022-12-12 01:49:46/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:: .:638E6256501980]  : ] eager release cuda mem 625663/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccEeager alloc mem 611.00 KB
: 
638/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc] :eager release cuda mem 625663638
] eager release cuda mem 625663
[2022-12-12 01:49:46.625775: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[[2022-12-12 01:49:462022-12-12 01:49:46..625809625811: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::1980[1980] 2022-12-12 01:49:46] eager alloc mem 611.00 KB
.eager alloc mem 611.00 KB625870
: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] [eager release cuda mem 6256632022-12-12 01:49:46
.625918: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 01:49:46.625970: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:[19802022-12-12 01:49:46] .eager alloc mem 611.00 KB625986
: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 01:49:46.626210: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 01:49:46.626275: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 01:49:46.626313: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 01:49:46.626377: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 01:49:46.626468: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 01:49:46.626523[: 2022-12-12 01:49:46E. 626533/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc: :E638 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cueager release cuda mem 625663:
1980] eager alloc mem 611.00 KB
[2022-12-12 01:49:46[.2022-12-12 01:49:46[626628.2022-12-12 01:49:46: 626631.E: 626637 E: /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu E:/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc 1980:/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc] 638:eager alloc mem 611.00 KB[[] 638
2022-12-12 01:49:462022-12-12 01:49:46eager release cuda mem 625663] ..
eager release cuda mem 625663626725626738
: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc[/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:2022-12-12 01:49:46:638.638] 626795] eager release cuda mem 625663: [eager release cuda mem 625663
E2022-12-12 01:49:46
 .[/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc6268192022-12-12 01:49:46[:: .2022-12-12 01:49:46638E626841.]  : 626851eager release cuda mem 20400000/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE: 
: E1980/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc ] :/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cceager alloc mem 611.00 KB638:
] 638eager release cuda mem 20400000] 
eager release cuda mem 20400000
[2022-12-12 01:49:46.627018: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 01:49:46.627054: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 20400000
[2022-12-12 01:49:46.627135: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 01:49:46.627173: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 20400000
[2022-12-12 01:49:46.627319: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 01:49:46.627355: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 20400000
[2022-12-12 01:49:46.627478: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 01:49:46.627517: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 20400000
[2022-12-12 01:49:46.627628: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 5000000 / 100000000 nodes ( 5.00 %~5.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 95000000 / 100000000 nodes ( 95.00 %) | 2.38 GB | 0.608239 secs 
[2022-12-12 01:49:46.627683: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[[2022-12-12 01:49:462022-12-12 01:49:46..627717627725: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::1793638] ] Collaborative GPU cache (policy: rep_cache) | local 5000000 / 100000000 nodes ( 5.00 %~5.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 95000000 / 100000000 nodes ( 95.00 %) | 2.38 GB | 0.612018 secs eager release cuda mem 20400000

[2022-12-12 01:49:46.627957: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 5000000 / 100000000 nodes ( 5.00 %~5.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 95000000 / 100000000 nodes ( 95.00 %) | 2.38 GB | 0.614707 secs 
[2022-12-12 01:49:46.628208: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 5000000 / 100000000 nodes ( 5.00 %~5.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 95000000 / 100000000 nodes ( 95.00 %) | 2.38 GB | 0.614411 secs 
[2022-12-12 01:49:46.628770: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 5000000 / 100000000 nodes ( 5.00 %~5.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 95000000 / 100000000 nodes ( 95.00 %) | 2.38 GB | 0.630648 secs 
[2022-12-12 01:49:46.629080: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 5000000 / 100000000 nodes ( 5.00 %~5.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 95000000 / 100000000 nodes ( 95.00 %) | 2.38 GB | 0.61528 secs 
[2022-12-12 01:49:46.629183: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 5000000 / 100000000 nodes ( 5.00 %~5.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 95000000 / 100000000 nodes ( 95.00 %) | 2.38 GB | 0.613469 secs 
[2022-12-12 01:49:46.629283: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 5000000 / 100000000 nodes ( 5.00 %~5.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 95000000 / 100000000 nodes ( 95.00 %) | 2.38 GB | 0.611426 secs 
[HCTR][01:49:46.629][ERROR][RK0][tid #140501726521088]: replica 0 calling init per replica done, doing barrier
[HCTR][01:49:46.629][ERROR][RK0][tid #140501734913792]: replica 3 calling init per replica done, doing barrier
[HCTR][01:49:46.629][ERROR][RK0][tid #140501667804928]: replica 5 calling init per replica done, doing barrier
[HCTR][01:49:46.629][ERROR][RK0][tid #140501734913792]: replica 4 calling init per replica done, doing barrier
[HCTR][01:49:46.629][ERROR][RK0][main]: replica 1 calling init per replica done, doing barrier
[HCTR][01:49:46.629][ERROR][RK0][tid #140502422755072]: replica 7 calling init per replica done, doing barrier
[HCTR][01:49:46.629][ERROR][RK0][main]: replica 2 calling init per replica done, doing barrier
[HCTR][01:49:46.629][ERROR][RK0][main]: replica 6 calling init per replica done, doing barrier
[HCTR][01:49:46.629][ERROR][RK0][main]: replica 6 calling init per replica done, doing barrier done
[HCTR][01:49:46.629][ERROR][RK0][tid #140501734913792]: replica 4 calling init per replica done, doing barrier done
[HCTR][01:49:46.629][ERROR][RK0][tid #140502422755072]: replica 7 calling init per replica done, doing barrier done
[HCTR][01:49:46.629][ERROR][RK0][tid #140501734913792]: replica 3 calling init per replica done, doing barrier done
[HCTR][01:49:46.629][ERROR][RK0][tid #140501726521088]: replica 0 calling init per replica done, doing barrier done
[HCTR][01:49:46.629][ERROR][RK0][main]: replica 2 calling init per replica done, doing barrier done
[HCTR][01:49:46.629][ERROR][RK0][main]: replica 1 calling init per replica done, doing barrier done
[HCTR][01:49:46.629][ERROR][RK0][tid #140501667804928]: replica 5 calling init per replica done, doing barrier done
[HCTR][01:49:46.629][ERROR][RK0][main]: init per replica done
[HCTR][01:49:46.629][ERROR][RK0][tid #140501734913792]: init per replica done
[HCTR][01:49:46.629][ERROR][RK0][main]: init per replica done
[HCTR][01:49:46.629][ERROR][RK0][tid #140502422755072]: init per replica done
[HCTR][01:49:46.629][ERROR][RK0][tid #140501734913792]: init per replica done
[HCTR][01:49:46.629][ERROR][RK0][main]: init per replica done
[HCTR][01:49:46.629][ERROR][RK0][tid #140501667804928]: init per replica done
[HCTR][01:49:46.632][ERROR][RK0][tid #140501726521088]: init per replica done
