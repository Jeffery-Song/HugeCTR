2022-12-12 04:38:49.187710: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.194382: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.200538: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.205912: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.210866: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.224144: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.230710: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.237888: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.292170: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.292504: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.295240: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.295552: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.303715: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.305123: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.307968: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.308537: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.309422: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.310118: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.311104: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.311641: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.312926: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.313345: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.314436: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.315094: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.315911: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.317315: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.318246: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.319249: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.320317: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.321380: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.322424: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.323454: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.325251: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.326278: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.327216: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.328250: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.329376: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.330505: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.331550: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.332546: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.337820: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 04:38:49.340829: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.341944: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.343029: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.344682: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.346720: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.346711: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.347672: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.349354: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.349482: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.349643: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.350560: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.352845: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.352944: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.353064: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.353236: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.353297: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.356234: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.356410: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.356543: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.356862: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.360116: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.360480: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.360863: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.361862: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.362080: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.363930: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.364505: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.364909: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.365861: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.366651: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.367113: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.368126: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.368658: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.369100: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.369618: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.370661: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.371220: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.372232: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.372714: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.373186: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.373222: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.374855: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.375265: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.376495: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.377151: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.377347: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.378736: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.379958: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.381210: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.381465: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.382681: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.383043: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.383737: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.385361: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.385585: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.385890: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.387305: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.387556: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.387638: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.401759: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.401945: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.403209: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.404370: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.405047: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.422531: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.424839: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.427933: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.439926: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.441107: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.441182: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.441186: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.441281: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.443097: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.445035: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.445137: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.445164: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.445301: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.446452: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.448993: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.449150: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.449176: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.450125: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.451190: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.453285: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.453346: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.453368: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.453481: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.455500: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.457070: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.457177: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.457200: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.457342: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.459622: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.461065: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.461170: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.461193: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.461297: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.463300: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.464473: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.464585: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.464673: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.464775: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.466891: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.467770: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.467929: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.468006: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.468153: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.470245: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.471126: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.471205: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.471316: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.471511: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.473656: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.474787: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.474847: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.474971: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.475283: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.477376: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.478344: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.478402: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.478604: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.478827: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.479588: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.480622: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.481404: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.482272: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.482663: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.482735: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.482975: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.484054: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.485495: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.487041: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.487160: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.487382: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.487658: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.488774: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.489703: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.490710: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 04:38:49.491409: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.491619: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.491837: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.491990: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.493095: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.494154: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.495448: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.495699: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.495838: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.496105: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.497484: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.498394: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.500834: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.501028: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.501348: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.501545: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.501975: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.502287: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.502987: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.505076: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.505329: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.505748: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.505916: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.506603: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.506930: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.507565: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.509518: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.509753: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.510475: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.510581: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.511341: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.511634: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.512283: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.515059: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.515722: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.515823: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.516403: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.516773: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.517357: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.520085: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.520685: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.520977: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.521425: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.521866: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.522238: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.524883: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.525418: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.525580: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.526164: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.526601: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.527032: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.529988: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.530872: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.531588: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.531897: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.532327: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.532476: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.535267: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.536158: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.536494: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.536672: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.537318: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.537572: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.541404: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.541948: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.544149: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.544153: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 04:38:49.544638: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 04:38:49.544800: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.545091: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 04:38:49.545196: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 04:38:49.546768: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.547780: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.550067: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.551283: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.552094: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.554766: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.555029: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.555545: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.555762: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.564478: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.565086: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.565145: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.565214: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.565230: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.565276: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.569332: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.569677: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.570049: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.570094: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.570130: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.570223: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.602945: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.603232: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.607772: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.608052: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.642406: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.642692: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.648363: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.648722: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.653411: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.657851: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 04:38:49.662469: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 04:38:49.667794: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.672183: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.672322: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.677481: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.677596: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:49.680631: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:50.695215: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:50.696174: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:50.696695: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:50.697533: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 04:38:50.697590: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-12 04:38:50.716818: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:50.717672: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:50.718204: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:50.718790: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:50.719337: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:50.719995: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-12 04:38:50.766153: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 04:38:50.766379: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 04:38:50.799864: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12340
2022-12-12 04:38:50.873925: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:50.874552: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:50.875822: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:50.876308: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 04:38:50.876360: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-12 04:38:50.894555: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:50.895222: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:50.895957: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:50.896734: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:50.897275: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:50.897966: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:1/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-12 04:38:50.928800: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:50.929414: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:50.929945: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:50.930409: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 04:38:50.930459: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-12 04:38:50.947678: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:50.948693: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:50.949072: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:50.949771: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:50.950473: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:50.950745: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 04:38:50.950800: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-12 04:38:50.951540: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:50.952199: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:50.952712: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:50.953178: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:2/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-12 04:38:50.969648: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:50.970574: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:50.971409: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:50.972264: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:50.972977: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:50.973451: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:3/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-12 04:38:50.977208: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:50.977791: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:50.978324: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:50.978821: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 04:38:50.978880: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-12 04:38:50.981065: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 04:38:50.981233: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 04:38:50.983142: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12341
2022-12-12 04:38:50.995292: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:50.995924: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:50.996444: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:50.996901: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:50.996913: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 04:38:50.996991: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-12 04:38:50.997782: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:50.998325: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:50.999169: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:50.999701: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:51.000177: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:6/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-12 04:38:51.001019: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:51.001576: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:51.002123: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:51.002609: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 04:38:51.002652: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-12 04:38:51.005929: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:51.006504: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:51.007026: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:51.007502: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 04:38:51.007546: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-12 04:38:51.015967: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:51.016642: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:51.017164: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:51.017737: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:51.018258: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:51.018731: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:4/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-12 04:38:51.019114: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 04:38:51.019348: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 04:38:51.020553: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:51.021142: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12343
2022-12-12 04:38:51.021164: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:51.021683: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:51.022260: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:51.022774: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:51.023264: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:7/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-12 04:38:51.026311: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:51.026956: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:51.027474: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:51.028050: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:51.028577: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:38:51.029046: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:5/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-12 04:38:51.040065: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 04:38:51.040256: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 04:38:51.042241: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12342
2022-12-12 04:38:51.046453: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 04:38:51.046640: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 04:38:51.048604: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12346
2022-12-12 04:38:51.064047: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 04:38:51.064241: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 04:38:51.065928: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12344
2022-12-12 04:38:51.069368: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 04:38:51.069525: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 04:38:51.071425: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12347
2022-12-12 04:38:51.074859: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 04:38:51.075008: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 04:38:51.076876: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12345
[HCTR][04:38:52.335][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][04:38:52.335][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][04:38:52.335][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][04:38:52.335][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][04:38:52.335][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][04:38:52.335][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][04:38:52.389][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][04:38:52.389][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 1it [00:01,  1.55s/it]warmup run: 96it [00:01, 80.90it/s]warmup run: 1it [00:01,  1.57s/it]warmup run: 1it [00:01,  1.51s/it]warmup run: 1it [00:01,  1.51s/it]warmup run: 1it [00:01,  1.49s/it]warmup run: 1it [00:01,  1.49s/it]warmup run: 194it [00:01, 177.78it/s]warmup run: 100it [00:01, 82.94it/s]warmup run: 1it [00:01,  1.49s/it]warmup run: 100it [00:01, 86.05it/s]warmup run: 98it [00:01, 84.44it/s]warmup run: 1it [00:01,  1.49s/it]warmup run: 96it [00:01, 83.62it/s]warmup run: 100it [00:01, 86.95it/s]warmup run: 293it [00:01, 286.09it/s]warmup run: 201it [00:01, 181.21it/s]warmup run: 96it [00:01, 83.52it/s]warmup run: 200it [00:01, 186.26it/s]warmup run: 99it [00:01, 86.36it/s]warmup run: 191it [00:01, 177.16it/s]warmup run: 191it [00:01, 179.71it/s]warmup run: 200it [00:01, 187.92it/s]warmup run: 391it [00:01, 397.11it/s]warmup run: 301it [00:01, 288.89it/s]warmup run: 192it [00:01, 180.53it/s]warmup run: 300it [00:01, 296.32it/s]warmup run: 199it [00:01, 187.72it/s]warmup run: 284it [00:01, 278.99it/s]warmup run: 286it [00:01, 284.84it/s]warmup run: 298it [00:01, 296.02it/s]warmup run: 487it [00:02, 501.71it/s]warmup run: 401it [00:01, 401.32it/s]warmup run: 286it [00:01, 284.26it/s]warmup run: 401it [00:01, 411.39it/s]warmup run: 300it [00:01, 299.96it/s]warmup run: 377it [00:01, 384.28it/s]warmup run: 394it [00:01, 404.02it/s]warmup run: 381it [00:01, 392.87it/s]warmup run: 587it [00:02, 605.36it/s]warmup run: 501it [00:02, 510.88it/s]warmup run: 381it [00:01, 392.65it/s]warmup run: 503it [00:02, 525.07it/s]warmup run: 471it [00:02, 487.94it/s]warmup run: 399it [00:01, 412.08it/s]warmup run: 478it [00:01, 501.85it/s]warmup run: 492it [00:01, 512.47it/s]warmup run: 687it [00:02, 695.96it/s]warmup run: 602it [00:02, 614.20it/s]warmup run: 475it [00:01, 496.10it/s]warmup run: 605it [00:02, 629.00it/s]warmup run: 572it [00:02, 598.27it/s]warmup run: 499it [00:01, 523.09it/s]warmup run: 593it [00:02, 618.14it/s]warmup run: 579it [00:02, 608.97it/s]warmup run: 786it [00:02, 767.22it/s]warmup run: 702it [00:02, 701.93it/s]warmup run: 570it [00:02, 592.55it/s]warmup run: 706it [00:02, 715.68it/s]warmup run: 674it [00:02, 695.80it/s]warmup run: 601it [00:02, 628.22it/s]warmup run: 680it [00:02, 701.92it/s]warmup run: 690it [00:02, 698.51it/s]warmup run: 886it [00:02, 827.82it/s]warmup run: 802it [00:02, 775.25it/s]warmup run: 667it [00:02, 680.41it/s]warmup run: 807it [00:02, 787.37it/s]warmup run: 776it [00:02, 774.81it/s]warmup run: 698it [00:02, 705.36it/s]warmup run: 780it [00:02, 775.91it/s]warmup run: 786it [00:02, 763.41it/s]warmup run: 984it [00:02, 864.56it/s]warmup run: 902it [00:02, 832.90it/s]warmup run: 769it [00:02, 764.22it/s]warmup run: 877it [00:02, 836.06it/s]warmup run: 908it [00:02, 843.44it/s]warmup run: 881it [00:02, 836.42it/s]warmup run: 884it [00:02, 818.59it/s]warmup run: 795it [00:02, 754.50it/s]warmup run: 1082it [00:02, 896.46it/s]warmup run: 1002it [00:02, 877.78it/s]warmup run: 868it [00:02, 823.32it/s]warmup run: 1009it [00:02, 887.62it/s]warmup run: 976it [00:02, 875.32it/s]warmup run: 982it [00:02, 882.57it/s]warmup run: 981it [00:02, 859.27it/s]warmup run: 890it [00:02, 804.61it/s]warmup run: 1182it [00:02, 923.40it/s]warmup run: 1104it [00:02, 916.12it/s]warmup run: 966it [00:02, 864.16it/s]warmup run: 1110it [00:02, 920.18it/s]warmup run: 1075it [00:02, 892.63it/s]warmup run: 1082it [00:02, 914.81it/s]warmup run: 987it [00:02, 847.04it/s]warmup run: 1078it [00:02, 882.74it/s]warmup run: 1280it [00:02, 937.83it/s]warmup run: 1205it [00:02, 942.77it/s]warmup run: 1065it [00:02, 896.97it/s]warmup run: 1212it [00:02, 948.57it/s]warmup run: 1172it [00:02, 912.17it/s]warmup run: 1182it [00:02, 936.55it/s]warmup run: 1086it [00:02, 885.19it/s]warmup run: 1174it [00:02, 901.23it/s]warmup run: 1379it [00:02, 952.33it/s]warmup run: 1306it [00:02, 958.20it/s]warmup run: 1164it [00:02, 922.32it/s]warmup run: 1313it [00:02, 961.53it/s]warmup run: 1270it [00:02, 929.11it/s]warmup run: 1282it [00:02, 949.30it/s]warmup run: 1185it [00:02, 913.03it/s]warmup run: 1270it [00:02, 911.68it/s]warmup run: 1479it [00:03, 966.16it/s]warmup run: 1406it [00:02, 969.03it/s]warmup run: 1263it [00:02, 939.23it/s]warmup run: 1415it [00:02, 976.68it/s]warmup run: 1368it [00:02, 941.06it/s]warmup run: 1381it [00:02, 960.32it/s]warmup run: 1283it [00:02, 931.56it/s]warmup run: 1365it [00:02, 919.54it/s]warmup run: 1579it [00:03, 974.09it/s]warmup run: 1508it [00:03, 982.48it/s]warmup run: 1363it [00:02, 954.29it/s]warmup run: 1517it [00:03, 986.51it/s]warmup run: 1466it [00:03, 950.97it/s]warmup run: 1481it [00:02, 971.17it/s]warmup run: 1382it [00:02, 947.99it/s]warmup run: 1460it [00:03, 926.04it/s]warmup run: 1681it [00:03, 987.55it/s]warmup run: 1611it [00:03, 994.79it/s]warmup run: 1463it [00:03, 964.97it/s]warmup run: 1619it [00:03, 993.67it/s]warmup run: 1564it [00:03, 956.52it/s]warmup run: 1481it [00:03, 960.05it/s]warmup run: 1555it [00:03, 929.80it/s]warmup run: 1581it [00:03, 959.93it/s]warmup run: 1781it [00:03, 982.64it/s]warmup run: 1713it [00:03, 1001.63it/s]warmup run: 1563it [00:03, 974.08it/s]warmup run: 1720it [00:03, 989.51it/s]warmup run: 1664it [00:03, 967.54it/s]warmup run: 1583it [00:03, 976.88it/s]warmup run: 1650it [00:03, 932.88it/s]warmup run: 1682it [00:03, 971.75it/s]warmup run: 1880it [00:03, 983.39it/s]warmup run: 1815it [00:03, 1003.53it/s]warmup run: 1662it [00:03, 977.12it/s]warmup run: 1822it [00:03, 995.99it/s]warmup run: 1766it [00:03, 981.46it/s]warmup run: 1685it [00:03, 988.15it/s]warmup run: 1745it [00:03, 937.45it/s]warmup run: 1785it [00:03, 988.28it/s]warmup run: 1979it [00:03, 983.45it/s]warmup run: 1917it [00:03, 1005.94it/s]warmup run: 1761it [00:03, 978.33it/s]warmup run: 1924it [00:03, 1001.62it/s]warmup run: 1868it [00:03, 991.04it/s]warmup run: 1788it [00:03, 998.54it/s]warmup run: 1888it [00:03, 999.86it/s]warmup run: 1840it [00:03, 937.71it/s]warmup run: 2095it [00:03, 1033.76it/s]warmup run: 1860it [00:03, 977.18it/s]warmup run: 2019it [00:03, 969.96it/s] warmup run: 2030it [00:03, 1017.61it/s]warmup run: 1970it [00:03, 998.46it/s]warmup run: 1890it [00:03, 1004.69it/s]warmup run: 1991it [00:03, 1006.69it/s]warmup run: 1935it [00:03, 940.20it/s]warmup run: 2216it [00:03, 1084.93it/s]warmup run: 1959it [00:03, 980.22it/s]warmup run: 2117it [00:03, 972.78it/s]warmup run: 2151it [00:03, 1074.64it/s]warmup run: 2085it [00:03, 1043.48it/s]warmup run: 1993it [00:03, 1010.97it/s]warmup run: 2108it [00:03, 1054.40it/s]warmup run: 2037it [00:03, 963.08it/s]warmup run: 2337it [00:03, 1120.37it/s]warmup run: 2070it [00:03, 1018.71it/s]warmup run: 2233it [00:03, 1027.10it/s]warmup run: 2271it [00:03, 1111.20it/s]warmup run: 2206it [00:03, 1092.86it/s]warmup run: 2111it [00:03, 1060.91it/s]warmup run: 2228it [00:03, 1096.70it/s]warmup run: 2158it [00:03, 1035.56it/s]warmup run: 2457it [00:03, 1143.57it/s]warmup run: 2191it [00:03, 1075.18it/s]warmup run: 2349it [00:03, 1065.73it/s]warmup run: 2392it [00:03, 1138.86it/s]warmup run: 2326it [00:03, 1124.16it/s]warmup run: 2231it [00:03, 1100.93it/s]warmup run: 2348it [00:03, 1126.94it/s]warmup run: 2279it [00:03, 1086.36it/s]warmup run: 2577it [00:04, 1159.22it/s]warmup run: 2314it [00:03, 1120.03it/s]warmup run: 2465it [00:03, 1092.94it/s]warmup run: 2513it [00:03, 1159.68it/s]warmup run: 2446it [00:03, 1144.48it/s]warmup run: 2350it [00:03, 1127.49it/s]warmup run: 2468it [00:03, 1147.82it/s]warmup run: 2400it [00:03, 1121.46it/s]warmup run: 2697it [00:04, 1170.75it/s]warmup run: 2437it [00:03, 1150.41it/s]warmup run: 2582it [00:04, 1114.00it/s]warmup run: 2635it [00:04, 1174.93it/s]warmup run: 2564it [00:04, 1152.17it/s]warmup run: 2469it [00:03, 1144.85it/s]warmup run: 2588it [00:04, 1163.28it/s]warmup run: 2521it [00:04, 1146.94it/s]warmup run: 2815it [00:04, 1172.65it/s]warmup run: 2560it [00:04, 1171.68it/s]warmup run: 2699it [00:04, 1128.23it/s]warmup run: 2755it [00:04, 1181.00it/s]warmup run: 2684it [00:04, 1165.69it/s]warmup run: 2588it [00:04, 1156.72it/s]warmup run: 2709it [00:04, 1175.08it/s]warmup run: 2643it [00:04, 1165.89it/s]warmup run: 2933it [00:04, 1169.92it/s]warmup run: 2683it [00:04, 1186.50it/s]warmup run: 2814it [00:04, 1133.75it/s]warmup run: 3000it [00:04, 677.86it/s] warmup run: 2876it [00:04, 1189.31it/s]warmup run: 2802it [00:04, 1169.78it/s]warmup run: 2707it [00:04, 1164.74it/s]warmup run: 2828it [00:04, 1177.79it/s]warmup run: 2762it [00:04, 1170.44it/s]warmup run: 2804it [00:04, 1192.13it/s]warmup run: 2931it [00:04, 1142.37it/s]warmup run: 2995it [00:04, 1187.85it/s]warmup run: 2922it [00:04, 1177.99it/s]warmup run: 2825it [00:04, 1166.76it/s]warmup run: 2948it [00:04, 1182.15it/s]warmup run: 3000it [00:04, 690.97it/s] warmup run: 2884it [00:04, 1183.80it/s]warmup run: 3000it [00:04, 671.60it/s] warmup run: 2926it [00:04, 1200.20it/s]warmup run: 3000it [00:04, 687.24it/s] warmup run: 3000it [00:04, 680.62it/s] warmup run: 2943it [00:04, 1167.94it/s]warmup run: 3000it [00:04, 679.02it/s] warmup run: 3000it [00:04, 685.59it/s] warmup run: 3000it [00:04, 687.06it/s] 


warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]


warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]

warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   6%|▌         | 165/3000 [00:00<00:01, 1649.55it/s]warmup should be done:   6%|▌         | 167/3000 [00:00<00:01, 1666.52it/s]warmup should be done:   5%|▌         | 157/3000 [00:00<00:01, 1567.61it/s]warmup should be done:   5%|▌         | 155/3000 [00:00<00:01, 1545.52it/s]warmup should be done:   6%|▌         | 166/3000 [00:00<00:01, 1652.52it/s]warmup should be done:   5%|▌         | 161/3000 [00:00<00:01, 1603.49it/s]warmup should be done:   6%|▌         | 168/3000 [00:00<00:01, 1671.72it/s]warmup should be done:   5%|▌         | 162/3000 [00:00<00:01, 1613.51it/s]warmup should be done:  11%|█         | 327/3000 [00:00<00:01, 1636.26it/s]warmup should be done:  11%|█         | 317/3000 [00:00<00:01, 1584.56it/s]warmup should be done:  11%|█         | 332/3000 [00:00<00:01, 1655.81it/s]warmup should be done:  10%|█         | 311/3000 [00:00<00:01, 1550.44it/s]warmup should be done:  11%|█         | 336/3000 [00:00<00:01, 1673.87it/s]warmup should be done:  11%|█         | 325/3000 [00:00<00:01, 1620.01it/s]warmup should be done:  11%|█         | 332/3000 [00:00<00:01, 1642.33it/s]warmup should be done:  11%|█         | 334/3000 [00:00<00:01, 1617.48it/s]warmup should be done:  16%|█▋        | 493/3000 [00:00<00:01, 1645.14it/s]warmup should be done:  17%|█▋        | 498/3000 [00:00<00:01, 1654.55it/s]warmup should be done:  16%|█▋        | 488/3000 [00:00<00:01, 1622.88it/s]warmup should be done:  17%|█▋        | 504/3000 [00:00<00:01, 1670.23it/s]warmup should be done:  16%|█▌        | 467/3000 [00:00<00:01, 1548.39it/s]warmup should be done:  16%|█▌        | 476/3000 [00:00<00:01, 1575.21it/s]warmup should be done:  17%|█▋        | 500/3000 [00:00<00:01, 1634.87it/s]warmup should be done:  17%|█▋        | 497/3000 [00:00<00:01, 1616.53it/s]warmup should be done:  22%|██▏       | 651/3000 [00:00<00:01, 1625.09it/s]warmup should be done:  22%|██▏       | 660/3000 [00:00<00:01, 1651.26it/s]warmup should be done:  22%|██▏       | 664/3000 [00:00<00:01, 1653.37it/s]warmup should be done:  22%|██▏       | 672/3000 [00:00<00:01, 1669.31it/s]warmup should be done:  21%|██        | 622/3000 [00:00<00:01, 1544.46it/s]warmup should be done:  21%|██        | 634/3000 [00:00<00:01, 1571.64it/s]warmup should be done:  22%|██▏       | 667/3000 [00:00<00:01, 1645.18it/s]warmup should be done:  22%|██▏       | 661/3000 [00:00<00:01, 1622.46it/s]warmup should be done:  28%|██▊       | 826/3000 [00:00<00:01, 1653.18it/s]warmup should be done:  28%|██▊       | 830/3000 [00:00<00:01, 1651.38it/s]warmup should be done:  26%|██▌       | 777/3000 [00:00<00:01, 1544.06it/s]warmup should be done:  28%|██▊       | 839/3000 [00:00<00:01, 1666.15it/s]warmup should be done:  26%|██▋       | 795/3000 [00:00<00:01, 1584.58it/s]warmup should be done:  27%|██▋       | 814/3000 [00:00<00:01, 1605.98it/s]warmup should be done:  28%|██▊       | 826/3000 [00:00<00:01, 1631.86it/s]warmup should be done:  28%|██▊       | 833/3000 [00:00<00:01, 1647.39it/s]warmup should be done:  33%|███▎      | 992/3000 [00:00<00:01, 1652.73it/s]warmup should be done:  33%|███▎      | 996/3000 [00:00<00:01, 1649.36it/s]warmup should be done:  34%|███▎      | 1006/3000 [00:00<00:01, 1663.57it/s]warmup should be done:  31%|███       | 932/3000 [00:00<00:01, 1541.97it/s]warmup should be done:  32%|███▏      | 958/3000 [00:00<00:01, 1597.42it/s]warmup should be done:  33%|███▎      | 991/3000 [00:00<00:01, 1636.86it/s]warmup should be done:  33%|███▎      | 998/3000 [00:00<00:01, 1646.93it/s]warmup should be done:  32%|███▎      | 975/3000 [00:00<00:01, 1593.69it/s]warmup should be done:  39%|███▊      | 1158/3000 [00:00<00:01, 1649.25it/s]warmup should be done:  37%|███▋      | 1120/3000 [00:00<00:01, 1601.80it/s]warmup should be done:  39%|███▊      | 1161/3000 [00:00<00:01, 1643.52it/s]warmup should be done:  39%|███▉      | 1173/3000 [00:00<00:01, 1657.80it/s]warmup should be done:  36%|███▌      | 1087/3000 [00:00<00:01, 1536.90it/s]warmup should be done:  38%|███▊      | 1155/3000 [00:00<00:01, 1637.65it/s]warmup should be done:  39%|███▉      | 1163/3000 [00:00<00:01, 1643.74it/s]warmup should be done:  38%|███▊      | 1135/3000 [00:00<00:01, 1587.62it/s]warmup should be done:  44%|████▍     | 1323/3000 [00:00<00:01, 1649.45it/s]warmup should be done:  43%|████▎     | 1282/3000 [00:00<00:01, 1607.14it/s]warmup should be done:  44%|████▍     | 1326/3000 [00:00<00:01, 1643.92it/s]warmup should be done:  41%|████▏     | 1241/3000 [00:00<00:01, 1536.96it/s]warmup should be done:  45%|████▍     | 1339/3000 [00:00<00:01, 1651.03it/s]warmup should be done:  44%|████▍     | 1321/3000 [00:00<00:01, 1642.45it/s]warmup should be done:  44%|████▍     | 1329/3000 [00:00<00:01, 1646.61it/s]warmup should be done:  43%|████▎     | 1294/3000 [00:00<00:01, 1557.43it/s]warmup should be done:  50%|████▉     | 1488/3000 [00:00<00:00, 1649.48it/s]warmup should be done:  48%|████▊     | 1444/3000 [00:00<00:00, 1609.71it/s]warmup should be done:  50%|████▉     | 1491/3000 [00:00<00:00, 1641.67it/s]warmup should be done:  46%|████▋     | 1395/3000 [00:00<00:01, 1535.39it/s]warmup should be done:  50%|████▉     | 1486/3000 [00:00<00:00, 1643.37it/s]warmup should be done:  50%|█████     | 1505/3000 [00:00<00:00, 1645.76it/s]warmup should be done:  50%|████▉     | 1494/3000 [00:00<00:00, 1630.59it/s]warmup should be done:  49%|████▊     | 1457/3000 [00:00<00:00, 1576.75it/s]warmup should be done:  55%|█████▌    | 1653/3000 [00:01<00:00, 1648.49it/s]warmup should be done:  54%|█████▎    | 1606/3000 [00:01<00:00, 1611.43it/s]warmup should be done:  52%|█████▏    | 1549/3000 [00:01<00:00, 1534.92it/s]warmup should be done:  55%|█████▌    | 1656/3000 [00:01<00:00, 1633.09it/s]warmup should be done:  55%|█████▌    | 1652/3000 [00:01<00:00, 1645.78it/s]warmup should be done:  56%|█████▌    | 1670/3000 [00:01<00:00, 1635.49it/s]warmup should be done:  55%|█████▌    | 1658/3000 [00:01<00:00, 1623.89it/s]warmup should be done:  54%|█████▍    | 1620/3000 [00:01<00:00, 1591.57it/s]warmup should be done:  61%|██████    | 1818/3000 [00:01<00:00, 1645.79it/s]warmup should be done:  59%|█████▉    | 1768/3000 [00:01<00:00, 1608.75it/s]warmup should be done:  57%|█████▋    | 1703/3000 [00:01<00:00, 1533.79it/s]warmup should be done:  61%|██████    | 1817/3000 [00:01<00:00, 1645.59it/s]warmup should be done:  61%|██████    | 1820/3000 [00:01<00:00, 1622.46it/s]warmup should be done:  61%|██████    | 1834/3000 [00:01<00:00, 1627.18it/s]warmup should be done:  61%|██████    | 1821/3000 [00:01<00:00, 1618.12it/s]warmup should be done:  59%|█████▉    | 1784/3000 [00:01<00:00, 1603.32it/s]warmup should be done:  66%|██████▌   | 1983/3000 [00:01<00:00, 1643.56it/s]warmup should be done:  64%|██████▍   | 1929/3000 [00:01<00:00, 1604.24it/s]warmup should be done:  62%|██████▏   | 1862/3000 [00:01<00:00, 1549.37it/s]warmup should be done:  66%|██████▌   | 1982/3000 [00:01<00:00, 1644.44it/s]warmup should be done:  66%|██████▌   | 1983/3000 [00:01<00:00, 1614.44it/s]warmup should be done:  67%|██████▋   | 1998/3000 [00:01<00:00, 1629.52it/s]warmup should be done:  66%|██████▌   | 1983/3000 [00:01<00:00, 1614.31it/s]warmup should be done:  65%|██████▍   | 1948/3000 [00:01<00:00, 1611.41it/s]warmup should be done:  72%|███████▏  | 2148/3000 [00:01<00:00, 1645.00it/s]warmup should be done:  67%|██████▋   | 2022/3000 [00:01<00:00, 1563.33it/s]warmup should be done:  70%|██████▉   | 2090/3000 [00:01<00:00, 1603.99it/s]warmup should be done:  72%|███████▏  | 2148/3000 [00:01<00:00, 1646.41it/s]warmup should be done:  72%|███████▏  | 2164/3000 [00:01<00:00, 1637.63it/s]warmup should be done:  72%|███████▏  | 2145/3000 [00:01<00:00, 1610.16it/s]warmup should be done:  72%|███████▏  | 2145/3000 [00:01<00:00, 1612.39it/s]warmup should be done:  70%|███████   | 2111/3000 [00:01<00:00, 1616.41it/s]warmup should be done:  77%|███████▋  | 2313/3000 [00:01<00:00, 1646.09it/s]warmup should be done:  75%|███████▌  | 2252/3000 [00:01<00:00, 1608.71it/s]warmup should be done:  73%|███████▎  | 2182/3000 [00:01<00:00, 1571.97it/s]warmup should be done:  77%|███████▋  | 2314/3000 [00:01<00:00, 1648.01it/s]warmup should be done:  78%|███████▊  | 2330/3000 [00:01<00:00, 1643.39it/s]warmup should be done:  77%|███████▋  | 2307/3000 [00:01<00:00, 1607.43it/s]warmup should be done:  77%|███████▋  | 2308/3000 [00:01<00:00, 1616.66it/s]warmup should be done:  76%|███████▌  | 2273/3000 [00:01<00:00, 1590.03it/s]warmup should be done:  83%|████████▎ | 2478/3000 [00:01<00:00, 1644.55it/s]warmup should be done:  80%|████████  | 2415/3000 [00:01<00:00, 1614.36it/s]warmup should be done:  78%|███████▊  | 2342/3000 [00:01<00:00, 1580.03it/s]warmup should be done:  83%|████████▎ | 2479/3000 [00:01<00:00, 1645.47it/s]warmup should be done:  83%|████████▎ | 2495/3000 [00:01<00:00, 1634.43it/s]warmup should be done:  82%|████████▏ | 2468/3000 [00:01<00:00, 1603.11it/s]warmup should be done:  82%|████████▏ | 2472/3000 [00:01<00:00, 1623.08it/s]warmup should be done:  81%|████████  | 2433/3000 [00:01<00:00, 1581.07it/s]warmup should be done:  88%|████████▊ | 2643/3000 [00:01<00:00, 1644.37it/s]warmup should be done:  86%|████████▌ | 2580/3000 [00:01<00:00, 1622.81it/s]warmup should be done:  83%|████████▎ | 2503/3000 [00:01<00:00, 1586.85it/s]warmup should be done:  88%|████████▊ | 2644/3000 [00:01<00:00, 1643.85it/s]warmup should be done:  89%|████████▊ | 2661/3000 [00:01<00:00, 1640.50it/s]warmup should be done:  88%|████████▊ | 2629/3000 [00:01<00:00, 1602.51it/s]warmup should be done:  88%|████████▊ | 2637/3000 [00:01<00:00, 1628.47it/s]warmup should be done:  86%|████████▋ | 2592/3000 [00:01<00:00, 1581.20it/s]warmup should be done:  94%|█████████▎| 2808/3000 [00:01<00:00, 1639.89it/s]warmup should be done:  91%|█████████▏| 2744/3000 [00:01<00:00, 1627.06it/s]warmup should be done:  89%|████████▉ | 2664/3000 [00:01<00:00, 1593.20it/s]warmup should be done:  94%|█████████▎| 2809/3000 [00:01<00:00, 1645.23it/s]warmup should be done:  94%|█████████▍| 2828/3000 [00:01<00:00, 1647.30it/s]warmup should be done:  93%|█████████▎| 2790/3000 [00:01<00:00, 1602.52it/s]warmup should be done:  93%|█████████▎| 2802/3000 [00:01<00:00, 1632.38it/s]warmup should be done:  92%|█████████▏| 2752/3000 [00:01<00:00, 1584.22it/s]warmup should be done:  99%|█████████▉| 2973/3000 [00:01<00:00, 1640.22it/s]warmup should be done:  97%|█████████▋| 2910/3000 [00:01<00:00, 1635.83it/s]warmup should be done:  94%|█████████▍| 2826/3000 [00:01<00:00, 1599.58it/s]warmup should be done:  99%|█████████▉| 2976/3000 [00:01<00:00, 1651.34it/s]warmup should be done: 100%|█████████▉| 2995/3000 [00:01<00:00, 1653.64it/s]warmup should be done:  98%|█████████▊| 2953/3000 [00:01<00:00, 1610.44it/s]warmup should be done:  99%|█████████▉| 2969/3000 [00:01<00:00, 1641.73it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1648.90it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1644.48it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1642.82it/s]warmup should be done:  97%|█████████▋| 2916/3000 [00:01<00:00, 1597.86it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1632.45it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1624.52it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1610.28it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1596.35it/s]warmup should be done: 100%|█████████▉| 2991/3000 [00:01<00:00, 1612.59it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1566.47it/s]






warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   6%|▌         | 169/3000 [00:00<00:01, 1689.05it/s]warmup should be done:   6%|▌         | 170/3000 [00:00<00:01, 1699.28it/s]warmup should be done:   5%|▌         | 155/3000 [00:00<00:01, 1547.96it/s]warmup should be done:   6%|▌         | 169/3000 [00:00<00:01, 1688.28it/s]warmup should be done:   6%|▌         | 167/3000 [00:00<00:01, 1665.29it/s]warmup should be done:   6%|▌         | 170/3000 [00:00<00:01, 1693.94it/s]warmup should be done:   6%|▌         | 166/3000 [00:00<00:01, 1652.60it/s]warmup should be done:   6%|▌         | 168/3000 [00:00<00:01, 1671.33it/s]warmup should be done:  11%|█         | 325/3000 [00:00<00:01, 1637.13it/s]warmup should be done:  11%|█▏        | 340/3000 [00:00<00:01, 1697.42it/s]warmup should be done:  11%|█▏        | 340/3000 [00:00<00:01, 1696.56it/s]warmup should be done:  11%|█▏        | 338/3000 [00:00<00:01, 1685.65it/s]warmup should be done:  11%|█▏        | 340/3000 [00:00<00:01, 1694.17it/s]warmup should be done:  11%|█         | 335/3000 [00:00<00:01, 1668.46it/s]warmup should be done:  11%|█▏        | 338/3000 [00:00<00:01, 1682.45it/s]warmup should be done:  11%|█         | 332/3000 [00:00<00:01, 1649.27it/s]warmup should be done:  16%|█▋        | 495/3000 [00:00<00:01, 1665.62it/s]warmup should be done:  17%|█▋        | 510/3000 [00:00<00:01, 1698.05it/s]warmup should be done:  17%|█▋        | 503/3000 [00:00<00:01, 1673.62it/s]warmup should be done:  17%|█▋        | 507/3000 [00:00<00:01, 1684.88it/s]warmup should be done:  17%|█▋        | 511/3000 [00:00<00:01, 1699.42it/s]warmup should be done:  17%|█▋        | 510/3000 [00:00<00:01, 1692.40it/s]warmup should be done:  17%|█▋        | 497/3000 [00:00<00:01, 1648.10it/s]warmup should be done:  17%|█▋        | 508/3000 [00:00<00:01, 1686.75it/s]warmup should be done:  22%|██▏       | 664/3000 [00:00<00:01, 1674.86it/s]warmup should be done:  23%|██▎       | 676/3000 [00:00<00:01, 1686.79it/s]warmup should be done:  22%|██▏       | 671/3000 [00:00<00:01, 1675.77it/s]warmup should be done:  23%|██▎       | 680/3000 [00:00<00:01, 1693.55it/s]warmup should be done:  23%|██▎       | 680/3000 [00:00<00:01, 1692.64it/s]warmup should be done:  22%|██▏       | 663/3000 [00:00<00:01, 1650.27it/s]warmup should be done:  23%|██▎       | 677/3000 [00:00<00:01, 1686.53it/s]warmup should be done:  23%|██▎       | 681/3000 [00:00<00:01, 1685.50it/s]warmup should be done:  28%|██▊       | 834/3000 [00:00<00:01, 1682.84it/s]warmup should be done:  28%|██▊       | 839/3000 [00:00<00:01, 1675.54it/s]warmup should be done:  28%|██▊       | 846/3000 [00:00<00:01, 1689.22it/s]warmup should be done:  28%|██▊       | 850/3000 [00:00<00:01, 1693.71it/s]warmup should be done:  28%|██▊       | 850/3000 [00:00<00:01, 1694.28it/s]warmup should be done:  28%|██▊       | 848/3000 [00:00<00:01, 1692.96it/s]warmup should be done:  28%|██▊       | 829/3000 [00:00<00:01, 1647.14it/s]warmup should be done:  28%|██▊       | 852/3000 [00:00<00:01, 1692.75it/s]warmup should be done:  33%|███▎      | 1004/3000 [00:00<00:01, 1688.17it/s]warmup should be done:  34%|███▎      | 1007/3000 [00:00<00:01, 1674.81it/s]warmup should be done:  34%|███▍      | 1015/3000 [00:00<00:01, 1687.43it/s]warmup should be done:  34%|███▍      | 1020/3000 [00:00<00:01, 1691.83it/s]warmup should be done:  34%|███▍      | 1020/3000 [00:00<00:01, 1693.58it/s]warmup should be done:  33%|███▎      | 994/3000 [00:00<00:01, 1645.64it/s]warmup should be done:  34%|███▍      | 1022/3000 [00:00<00:01, 1693.96it/s]warmup should be done:  34%|███▍      | 1018/3000 [00:00<00:01, 1687.11it/s]warmup should be done:  39%|███▉      | 1174/3000 [00:00<00:01, 1691.22it/s]warmup should be done:  39%|███▉      | 1177/3000 [00:00<00:01, 1680.21it/s]warmup should be done:  40%|███▉      | 1190/3000 [00:00<00:01, 1688.57it/s]warmup should be done:  40%|███▉      | 1190/3000 [00:00<00:01, 1690.19it/s]warmup should be done:  39%|███▉      | 1184/3000 [00:00<00:01, 1677.87it/s]warmup should be done:  39%|███▊      | 1159/3000 [00:00<00:01, 1645.11it/s]warmup should be done:  40%|███▉      | 1192/3000 [00:00<00:01, 1692.17it/s]warmup should be done:  40%|███▉      | 1187/3000 [00:00<00:01, 1680.83it/s]warmup should be done:  45%|████▍     | 1345/3000 [00:00<00:00, 1694.29it/s]warmup should be done:  45%|████▍     | 1347/3000 [00:00<00:00, 1684.10it/s]warmup should be done:  45%|████▌     | 1361/3000 [00:00<00:00, 1695.55it/s]warmup should be done:  45%|████▌     | 1361/3000 [00:00<00:00, 1692.69it/s]warmup should be done:  45%|████▌     | 1352/3000 [00:00<00:00, 1673.96it/s]warmup should be done:  44%|████▍     | 1324/3000 [00:00<00:01, 1642.86it/s]warmup should be done:  45%|████▌     | 1364/3000 [00:00<00:00, 1698.16it/s]warmup should be done:  45%|████▌     | 1356/3000 [00:00<00:00, 1679.34it/s]warmup should be done:  50%|█████     | 1515/3000 [00:00<00:00, 1695.46it/s]warmup should be done:  51%|█████     | 1517/3000 [00:00<00:00, 1688.83it/s]warmup should be done:  51%|█████     | 1531/3000 [00:00<00:00, 1694.11it/s]warmup should be done:  51%|█████     | 1532/3000 [00:00<00:00, 1697.69it/s]warmup should be done:  50%|████▉     | 1489/3000 [00:00<00:00, 1644.44it/s]warmup should be done:  51%|█████     | 1520/3000 [00:00<00:00, 1672.25it/s]warmup should be done:  51%|█████     | 1535/3000 [00:00<00:00, 1699.90it/s]warmup should be done:  51%|█████     | 1524/3000 [00:00<00:00, 1678.19it/s]warmup should be done:  56%|█████▌    | 1686/3000 [00:01<00:00, 1696.69it/s]warmup should be done:  56%|█████▌    | 1687/3000 [00:01<00:00, 1691.45it/s]warmup should be done:  57%|█████▋    | 1701/3000 [00:01<00:00, 1695.31it/s]warmup should be done:  57%|█████▋    | 1702/3000 [00:01<00:00, 1697.79it/s]warmup should be done:  55%|█████▌    | 1655/3000 [00:01<00:00, 1647.37it/s]warmup should be done:  57%|█████▋    | 1706/3000 [00:01<00:00, 1702.15it/s]warmup should be done:  56%|█████▋    | 1688/3000 [00:01<00:00, 1671.90it/s]warmup should be done:  56%|█████▋    | 1692/3000 [00:01<00:00, 1678.42it/s]warmup should be done:  62%|██████▏   | 1857/3000 [00:01<00:00, 1699.04it/s]warmup should be done:  62%|██████▏   | 1858/3000 [00:01<00:00, 1695.37it/s]warmup should be done:  62%|██████▏   | 1871/3000 [00:01<00:00, 1696.67it/s]warmup should be done:  62%|██████▏   | 1872/3000 [00:01<00:00, 1697.92it/s]warmup should be done:  61%|██████    | 1822/3000 [00:01<00:00, 1653.13it/s]warmup should be done:  63%|██████▎   | 1877/3000 [00:01<00:00, 1704.11it/s]warmup should be done:  62%|██████▏   | 1858/3000 [00:01<00:00, 1677.41it/s]warmup should be done:  62%|██████▏   | 1861/3000 [00:01<00:00, 1678.83it/s]warmup should be done:  68%|██████▊   | 2027/3000 [00:01<00:00, 1698.18it/s]warmup should be done:  68%|██████▊   | 2029/3000 [00:01<00:00, 1698.16it/s]warmup should be done:  68%|██████▊   | 2043/3000 [00:01<00:00, 1699.16it/s]warmup should be done:  68%|██████▊   | 2048/3000 [00:01<00:00, 1705.76it/s]warmup should be done:  66%|██████▋   | 1991/3000 [00:01<00:00, 1662.59it/s]warmup should be done:  68%|██████▊   | 2041/3000 [00:01<00:00, 1688.02it/s]warmup should be done:  68%|██████▊   | 2027/3000 [00:01<00:00, 1680.85it/s]warmup should be done:  68%|██████▊   | 2029/3000 [00:01<00:00, 1678.72it/s]warmup should be done:  73%|███████▎  | 2197/3000 [00:01<00:00, 1697.74it/s]warmup should be done:  73%|███████▎  | 2199/3000 [00:01<00:00, 1697.68it/s]warmup should be done:  74%|███████▍  | 2213/3000 [00:01<00:00, 1697.82it/s]warmup should be done:  74%|███████▍  | 2219/3000 [00:01<00:00, 1704.90it/s]warmup should be done:  72%|███████▏  | 2161/3000 [00:01<00:00, 1671.33it/s]warmup should be done:  73%|███████▎  | 2196/3000 [00:01<00:00, 1683.04it/s]warmup should be done:  74%|███████▎  | 2210/3000 [00:01<00:00, 1682.46it/s]warmup should be done:  73%|███████▎  | 2197/3000 [00:01<00:00, 1677.20it/s]warmup should be done:  79%|███████▉  | 2367/3000 [00:01<00:00, 1697.02it/s]warmup should be done:  79%|███████▉  | 2369/3000 [00:01<00:00, 1697.75it/s]warmup should be done:  79%|███████▉  | 2383/3000 [00:01<00:00, 1696.34it/s]warmup should be done:  80%|███████▉  | 2390/3000 [00:01<00:00, 1704.47it/s]warmup should be done:  78%|███████▊  | 2331/3000 [00:01<00:00, 1678.94it/s]warmup should be done:  79%|███████▉  | 2365/3000 [00:01<00:00, 1684.90it/s]warmup should be done:  79%|███████▉  | 2379/3000 [00:01<00:00, 1679.69it/s]warmup should be done:  79%|███████▉  | 2365/3000 [00:01<00:00, 1656.84it/s]warmup should be done:  85%|████████▍ | 2537/3000 [00:01<00:00, 1693.32it/s]warmup should be done:  85%|████████▍ | 2539/3000 [00:01<00:00, 1697.56it/s]warmup should be done:  85%|████████▌ | 2553/3000 [00:01<00:00, 1696.54it/s]warmup should be done:  85%|████████▌ | 2561/3000 [00:01<00:00, 1705.29it/s]warmup should be done:  83%|████████▎ | 2501/3000 [00:01<00:00, 1684.69it/s]warmup should be done:  84%|████████▍ | 2535/3000 [00:01<00:00, 1688.77it/s]warmup should be done:  85%|████████▍ | 2548/3000 [00:01<00:00, 1681.12it/s]warmup should be done:  84%|████████▍ | 2532/3000 [00:01<00:00, 1660.49it/s]warmup should be done:  90%|█████████ | 2709/3000 [00:01<00:00, 1698.04it/s]warmup should be done:  90%|█████████ | 2707/3000 [00:01<00:00, 1690.25it/s]warmup should be done:  91%|█████████ | 2723/3000 [00:01<00:00, 1697.12it/s]warmup should be done:  91%|█████████ | 2732/3000 [00:01<00:00, 1706.06it/s]warmup should be done:  89%|████████▉ | 2671/3000 [00:01<00:00, 1687.01it/s]warmup should be done:  90%|█████████ | 2705/3000 [00:01<00:00, 1689.68it/s]warmup should be done:  91%|█████████ | 2717/3000 [00:01<00:00, 1680.98it/s]warmup should be done:  90%|█████████ | 2700/3000 [00:01<00:00, 1664.15it/s]warmup should be done:  96%|█████████▌| 2879/3000 [00:01<00:00, 1693.14it/s]warmup should be done:  96%|█████████▋| 2893/3000 [00:01<00:00, 1694.75it/s]warmup should be done:  96%|█████████▌| 2877/3000 [00:01<00:00, 1684.61it/s]warmup should be done:  96%|█████████▌| 2874/3000 [00:01<00:00, 1688.93it/s]warmup should be done:  97%|█████████▋| 2903/3000 [00:01<00:00, 1703.32it/s]warmup should be done:  95%|█████████▍| 2841/3000 [00:01<00:00, 1688.15it/s]warmup should be done:  96%|█████████▌| 2886/3000 [00:01<00:00, 1678.67it/s]warmup should be done:  96%|█████████▌| 2867/3000 [00:01<00:00, 1663.92it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1699.49it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1695.31it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1688.18it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1687.30it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1685.42it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1683.58it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1674.03it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1665.56it/s]
WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f6b66e441f0>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f6b67184d30>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f6b67182730>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f6b67181e80>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f6b66e430d0>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f6b66e522b0>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f6b66e421c0>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f6b66e44190>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
2022-12-12 04:40:21.911186: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f668682bb30 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 04:40:21.911254: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 04:40:21.919189: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 04:40:21.986110: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f667e834170 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 04:40:21.986169: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 04:40:21.995292: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 04:40:22.897405: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f6682830a50 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 04:40:22.897468: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 04:40:22.898078: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f667e82c460 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 04:40:22.898140: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 04:40:22.898323: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f666e830520 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 04:40:22.898365: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 04:40:22.906936: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 04:40:22.907058: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 04:40:22.907124: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 04:40:22.983827: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f6682f927f0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 04:40:22.983889: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 04:40:22.993361: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 04:40:23.026773: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f668302d7f0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 04:40:23.026842: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 04:40:23.035780: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 04:40:23.039619: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f667a834120 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 04:40:23.039685: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 04:40:23.050286: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 04:40:29.346635: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 04:40:29.352963: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 04:40:29.657313: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 04:40:29.683583: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 04:40:29.693446: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 04:40:29.802665: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 04:40:29.805051: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 04:40:29.855948: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
[HCTR][04:41:21.292][ERROR][RK0][tid #140078521239296]: replica 5 reaches 1000, calling init pre replica
[HCTR][04:41:21.292][ERROR][RK0][tid #140078521239296]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][04:41:21.298][ERROR][RK0][tid #140078521239296]: coll ps creation done
[HCTR][04:41:21.298][ERROR][RK0][tid #140078521239296]: replica 5 waits for coll ps creation barrier
[HCTR][04:41:21.343][ERROR][RK0][tid #140078672242432]: replica 7 reaches 1000, calling init pre replica
[HCTR][04:41:21.343][ERROR][RK0][tid #140078672242432]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][04:41:21.348][ERROR][RK0][tid #140078672242432]: coll ps creation done
[HCTR][04:41:21.348][ERROR][RK0][tid #140078672242432]: replica 7 waits for coll ps creation barrier
[HCTR][04:41:21.564][ERROR][RK0][tid #140078923892480]: replica 3 reaches 1000, calling init pre replica
[HCTR][04:41:21.564][ERROR][RK0][tid #140078923892480]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][04:41:21.569][ERROR][RK0][tid #140078923892480]: coll ps creation done
[HCTR][04:41:21.569][ERROR][RK0][tid #140078923892480]: replica 3 waits for coll ps creation barrier
[HCTR][04:41:21.654][ERROR][RK0][main]: replica 6 reaches 1000, calling init pre replica
[HCTR][04:41:21.654][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][04:41:21.661][ERROR][RK0][main]: coll ps creation done
[HCTR][04:41:21.661][ERROR][RK0][main]: replica 6 waits for coll ps creation barrier
[HCTR][04:41:21.678][ERROR][RK0][tid #140078462523136]: replica 1 reaches 1000, calling init pre replica
[HCTR][04:41:21.678][ERROR][RK0][tid #140078462523136]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][04:41:21.681][ERROR][RK0][tid #140078529632000]: replica 4 reaches 1000, calling init pre replica
[HCTR][04:41:21.682][ERROR][RK0][tid #140078529632000]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][04:41:21.683][ERROR][RK0][tid #140078462523136]: coll ps creation done
[HCTR][04:41:21.683][ERROR][RK0][tid #140078462523136]: replica 1 waits for coll ps creation barrier
[HCTR][04:41:21.689][ERROR][RK0][tid #140078529632000]: coll ps creation done
[HCTR][04:41:21.689][ERROR][RK0][tid #140078529632000]: replica 4 waits for coll ps creation barrier
[HCTR][04:41:21.728][ERROR][RK0][tid #140078529632000]: replica 0 reaches 1000, calling init pre replica
[HCTR][04:41:21.729][ERROR][RK0][tid #140078529632000]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][04:41:21.736][ERROR][RK0][tid #140078529632000]: coll ps creation done
[HCTR][04:41:21.736][ERROR][RK0][tid #140078529632000]: replica 0 waits for coll ps creation barrier
[HCTR][04:41:21.921][ERROR][RK0][tid #140078529632000]: replica 2 reaches 1000, calling init pre replica
[HCTR][04:41:21.921][ERROR][RK0][tid #140078529632000]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][04:41:21.925][ERROR][RK0][tid #140078529632000]: coll ps creation done
[HCTR][04:41:21.925][ERROR][RK0][tid #140078529632000]: replica 2 waits for coll ps creation barrier
[HCTR][04:41:21.925][ERROR][RK0][tid #140078529632000]: replica 0 preparing frequency
[HCTR][04:41:22.890][ERROR][RK0][tid #140078529632000]: replica 0 preparing frequency done
[HCTR][04:41:22.929][ERROR][RK0][tid #140078529632000]: replica 0 calling init per replica
[HCTR][04:41:22.929][ERROR][RK0][main]: replica 6 calling init per replica
[HCTR][04:41:22.929][ERROR][RK0][tid #140078529632000]: replica 4 calling init per replica
[HCTR][04:41:22.929][ERROR][RK0][tid #140078529632000]: replica 2 calling init per replica
[HCTR][04:41:22.929][ERROR][RK0][tid #140078462523136]: replica 1 calling init per replica
[HCTR][04:41:22.929][ERROR][RK0][tid #140078923892480]: replica 3 calling init per replica
[HCTR][04:41:22.929][ERROR][RK0][tid #140078521239296]: replica 5 calling init per replica
[HCTR][04:41:22.929][ERROR][RK0][tid #140078672242432]: replica 7 calling init per replica
[HCTR][04:41:22.929][ERROR][RK0][tid #140078529632000]: Calling build_v2
[HCTR][04:41:22.929][ERROR][RK0][main]: Calling build_v2
[HCTR][04:41:22.929][ERROR][RK0][tid #140078529632000]: Calling build_v2
[HCTR][04:41:22.929][ERROR][RK0][tid #140078529632000]: Calling build_v2
[HCTR][04:41:22.929][ERROR][RK0][tid #140078462523136]: Calling build_v2
[HCTR][04:41:22.929][ERROR][RK0][tid #140078923892480]: Calling build_v2
[HCTR][04:41:22.929][ERROR][RK0][tid #140078521239296]: Calling build_v2
[HCTR][04:41:22.929][ERROR][RK0][tid #140078529632000]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][04:41:22.929][ERROR][RK0][tid #140078923892480]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][04:41:22.929][ERROR][RK0][tid #140078672242432]: Calling build_v2
[HCTR][04:41:22.929][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][04:41:22.929][ERROR][RK0][tid #140078529632000]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][04:41:22.929][ERROR][RK0][tid #140078529632000]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][04:41:22.929][ERROR][RK0][tid #140078462523136]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][04:41:22.929][ERROR][RK0][tid #140078521239296]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][04:41:22.929][ERROR][RK0][tid #140078672242432]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[[[[[[[[2022-12-12 04:41:222022-12-12 04:41:222022-12-12 04:41:222022-12-12 04:41:222022-12-12 04:41:22.2022-12-12 04:41:22.2022-12-12 04:41:22...2022-12-12 04:41:22929323.929323.929323929330929323.: 929341: 929340: : : 929350E: E: EEE:  E E   E/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc /hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc /hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc :/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc:/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc:::/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc136:136:136136136:] ] 136] 136] ] 136using concurrent impl MPSPhaseusing concurrent impl MPSPhase] using concurrent impl MPSPhase] using concurrent impl MPSPhaseusing concurrent impl MPSPhase] 

using concurrent impl MPSPhase
using concurrent impl MPSPhase

using concurrent impl MPSPhase


[2022-12-12 04:41:22.933566: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:178] v100x8, slow pcie
[2022-12-12 04:41:22.933603: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:196] assigning 8 to cpu
[2022-12-12 04:41:22.933620: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:178] [[v100x8, slow pcie2022-12-12 04:41:222022-12-12 04:41:22
..933660933665: : [EE2022-12-12 04:41:22  .[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc9336932022-12-12 04:41:22::: .178212E933703] []  : v100x8, slow pcie2022-12-12 04:41:22build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE
.
: 933747[196/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: 2022-12-12 04:41:22[] :E[.2022-12-12 04:41:22assigning 8 to cpu178 2022-12-12 04:41:22933789.
] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.: 933791v100x8, slow pcie:933804E: 
[178:  E2022-12-12 04:41:22[] E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc .2022-12-12 04:41:22v100x8, slow pcie :[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc933842[.
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc1962022-12-12 04:41:22:: 2022-12-12 04:41:22933863:] [.178E.: 213assigning 8 to cpu2022-12-12 04:41:22933884]  933893E ] 
.: v100x8, slow pcie/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccremote time is 8.68421933931E
:E:
:  178[[196 E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[] 2022-12-12 04:41:222022-12-12 04:41:22] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc :2022-12-12 04:41:22v100x8, slow pcie..assigning 8 to cpu:/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc178.
934047934049
212:] ] 934062: : [196build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8v100x8, slow pcie: EE[2022-12-12 04:41:22] 

E  2022-12-12 04:41:22.assigning 8 to cpu /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[.[934150
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc::2022-12-12 04:41:229341762022-12-12 04:41:22: :212196.: .E214] ] 934222E934227 [] build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8assigning 8 to cpu:  : /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 04:41:22cpu time is 97.0588

E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE:.
 :[ 196934304/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2122022-12-12 04:41:22[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] : :] .2022-12-12 04:41:22:assigning 8 to cpuE196build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8934370.213
 ] 
: 934393] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccassigning 8 to cpuE: remote time is 8.68421:[
 E
212[2022-12-12 04:41:22/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc ] 2022-12-12 04:41:22.:[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccbuild asymm link desc with 8X Tesla V100-SXM2-32GB out of 8.934491213[2022-12-12 04:41:22:
934519: ] 2022-12-12 04:41:22.212: Eremote time is 8.68421[.934529] E 
2022-12-12 04:41:22934548: build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8 /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.[: E
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:9345892022-12-12 04:41:22E :213: [. /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc212] E2022-12-12 04:41:22934641/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc::] remote time is 8.68421 .: 212214build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc934685E] ] 
:: [ build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8cpu time is 97.0588213E2022-12-12 04:41:22/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[

]  .:2022-12-12 04:41:22remote time is 8.68421/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc934781214[.
:: ] 2022-12-12 04:41:22934811213E[cpu time is 97.0588.: ]  2022-12-12 04:41:22
934853Eremote time is 8.68421/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.:  
:934883E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc214: [ :] E2022-12-12 04:41:22/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc213cpu time is 97.0588 .:] 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc934948213remote time is 8.68421:: ] 
214Eremote time is 8.68421]  [
cpu time is 97.0588/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 04:41:22
:[.2142022-12-12 04:41:22935036] .: cpu time is 97.0588935062E
:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc :/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc214:] 214cpu time is 97.0588] 
cpu time is 97.0588
[2022-12-12 04:42:42.749198: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:118] solver created. now build & solve
[2022-12-12 04:42:42.788878: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 381.47 MB
[2022-12-12 04:42:42.788964: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 381.47 MB
[2022-12-12 04:42:42.789995: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:74] mapping nid to rank...
[2022-12-12 04:42:42.858134: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:91] counting slots...
[2022-12-12 04:42:43.257026: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:105] Final num slot is 49
[2022-12-12 04:42:43.257109: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:109] counting blocks...
[2022-12-12 04:42:50.490128: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:118] Final num block is 1025
[2022-12-12 04:42:50.490218: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:123] counting freq and density...
[2022-12-12 04:42:52.398664: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:154] averaging freq and density...
[2022-12-12 04:42:52.398805: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:155] 1025
[2022-12-12 04:42:52.402062: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:123] solver built. now solve
[2022-12-12 04:42:52.402155: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:354] constructing optimal solver, device=8, stream=1
1025 blocks, 8 devices
[2022-12-12 04:42:52.666329: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:527] Add Var...
[2022-12-12 04:42:52.706771: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:545] Capacity...
[2022-12-12 04:42:52.708880: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:548] Connect CPU...
[2022-12-12 04:42:52.745937: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:550] Connect Access To Storage...
[2022-12-12 04:42:53.325754: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:554] Time...
[2022-12-12 04:42:53.328100: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:516] dst 0, total sm is 80
[2022-12-12 04:42:53.331143: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:516] dst 1, total sm is 80
[2022-12-12 04:42:53.334148: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:516] dst 2, total sm is 80
[2022-12-12 04:42:53.337094: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:516] dst 3, total sm is 80
[2022-12-12 04:42:53.340052: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:516] dst 4, total sm is 80
[2022-12-12 04:42:53.342946: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:516] dst 5, total sm is 80
[2022-12-12 04:42:53.345861: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:516] dst 6, total sm is 80
[2022-12-12 04:42:53.348847: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:516] dst 7, total sm is 80
[2022-12-12 04:46:14.472088: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:569] Coll Cache init block placement array
[2022-12-12 04:46:14.481020: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:645] Coll Cache init block placement array done
[2022-12-12 04:46:14.485155: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:647] Coll Cache model reset done
[2022-12-12 04:46:14.529005: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:127] solver solved
[2022-12-12 04:46:14.529119: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:195] 0 solved master
[2022-12-12 04:46:14.529152: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 0 solved
[2022-12-12 04:46:14.529184: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 0 initing device 0
[2022-12-12 04:46:14.529782: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-12 04:46:14.529835: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 04:46:14.531018: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 04:46:14.531720: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 04:46:14.544394: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 3 solved
[2022-12-12 04:46:14.544469: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 3 initing device 3
[[2022-12-12 04:46:142022-12-12 04:46:14..544858544871: : EE  [/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc2022-12-12 04:46:14::.202202544914] ] : 7 solvedE1 solved
 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:[1815[2022-12-12 04:46:14] 2022-12-12 04:46:14.Building Coll Cache with ... num gpu device is 8.544960
544964: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc::205205[] ] 2022-12-12 04:46:14worker 0 thread 7 initing device 7worker 0 thread 1 initing device 1.

545008: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[[2022-12-12 04:46:142022-12-12 04:46:14..545445545445: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::18151815] ] Building Coll Cache with ... num gpu device is 8Building Coll Cache with ... num gpu device is 8

[[2022-12-12 04:46:142022-12-12 04:46:14..545519545519: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::19801980] ] eager alloc mem 381.47 MBeager alloc mem 381.47 MB

[2022-12-12 04:46:14.545592: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 2 solved
[2022-12-12 04:46:14.545660: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 2 initing device 2
[2022-12-12 04:46:14.545671: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 6 solved
[2022-12-12 04:46:14.545724: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 6 initing device 6
[2022-12-12 04:46:14.545793: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 5 solved
[2022-12-12 04:46:14.545864: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 5 initing device 5
[2022-12-12 04:46:14.545888: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 4 solved
[2022-12-12 04:46:14.545950: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 4 initing device 4
[2022-12-12 04:46:14.546097: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-12 04:46:14.546141: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:[19802022-12-12 04:46:14] .eager alloc mem 381.47 MB546153
: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-12 04:46:14.546203: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 04:46:14.546324: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-12 04:46:14.546373: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB[
2022-12-12 04:46:14.546392: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-12 04:46:14.546439: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 04:46:14.548806: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 04:46:14.549259: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 04:46:14.550091: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 04:46:14.550627: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 04:46:14.550674: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 04:46:14.550756: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 04:46:14.550808: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 04:46:14.553272: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 04:46:14.553596: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 04:46:14.554346: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 04:46:14.554392: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 04:46:14.554919: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 04:46:14.554990: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 04:46:14.555042: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 04:46:14.606693: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1.00 KB
[2022-12-12 04:46:14.611934: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1025
[2022-12-12 04:46:14.612045: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 04:46:14.612831: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 57.60 MB
[2022-12-12 04:46:14.613447: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:46:14.614511: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:46:14.614559: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 57.21 MB
[2022-12-12 04:46:14.632227: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1.00 KB[[[
2022-12-12 04:46:14[2022-12-12 04:46:142022-12-12 04:46:14.2022-12-12 04:46:14..632289.632289632298: 632298: : E: EE E  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::1980:19801980] 1980] ] eager alloc mem 1.00 KB] eager alloc mem 1.00 KBeager alloc mem 1.00 KB
eager alloc mem 1.00 KB


[2022-12-12 04:46:14.638597: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1025
[2022-12-12 04:46:14.638682: E [/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc2022-12-12 04:46:14:.638638715] : eager release cuda mem 1025E
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[[2022-12-12 04:46:142022-12-12 04:46:14..638774638798: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::638638] ] eager release cuda mem 1025eager release cuda mem 400000000

[2022-12-12 04:46:14.638863: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1025
[2022-12-12 04:46:14.638921: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000[
2022-12-12 04:46:14.638943: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638[] 2022-12-12 04:46:14eager release cuda mem 400000000.
638952: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1025
[2022-12-12 04:46:14.639063: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 04:46:14.639157: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1.00 KB
[2022-12-12 04:46:14.639447: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1.00 KB
[2022-12-12 04:46:14.639648: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 57.60 MB
[2022-12-12 04:46:14.640669: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 57.60 MB
[2022-12-12 04:46:14.641296: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 57.60 MB
[2022-12-12 04:46:14.641876: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 57.60 MB
[2022-12-12 04:46:14.642509: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 57.60 MB
[2022-12-12 04:46:14.643480: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:46:14.644234: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:46:14.644432: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:46:14.644469: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:46:14.644530[: 2022-12-12 04:46:14E. 644534/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc: :E638 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cueager release cuda mem 625663:
1980] eager alloc mem 611.00 KB
[2022-12-12 04:46:14.644600: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 57.16 MB
[2022-12-12 04:46:14.645275: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:46:14.645324: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 56.85 MB
[2022-12-12 04:46:14.645462: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:46:14.645510: W[ 2022-12-12 04:46:14/hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc.:64551543: ] EWORKER[0] alloc host memory 57.21 MB 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:46:14.[6455662022-12-12 04:46:14: [.E2022-12-12 04:46:14645587 .: /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc645597E::  638W/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc]  :eager release cuda mem 1025/hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc638
:] 43eager release cuda mem 625663] 
WORKER[0] alloc host memory 57.20 MB
[2022-12-12 04:46:14.[6457252022-12-12 04:46:14: .E645732 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccW: 638/hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc] :eager release cuda mem 40000000043
] WORKER[0] alloc host memory 57.20 MB
[2022-12-12 04:46:14.645788: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1025
[2022-12-12 04:46:14.645868: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 04:46:14.646569: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 57.60 MB
[2022-12-12 04:46:14.647289: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 57.60 MB
[2022-12-12 04:46:14.647786: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:46:14.647946: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:46:14.648839: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:46:14.648888: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 56.93 MB
[2022-12-12 04:46:14.648982: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:46:14.649030: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 57.20 MB
[2022-12-12 04:46:14.653061: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 04:46:14.653694: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 04:46:14.653737: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 7.16 GB
[2022-12-12 04:46:14.682889: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[[2022-12-12 04:46:142022-12-12 04:46:14..683285683289: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::19801980] ] eager alloc mem 25.25 KBeager alloc mem 25.25 KB

[2022-12-12 04:46:14.683520: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 04:46:14.683569: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 7.16 GB
[2022-12-12 04:46:14.683759: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[[2022-12-12 04:46:142022-12-12 04:46:14..683929683932: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::638638] ] eager release cuda mem 25855eager release cuda mem 25855

[[2022-12-12 04:46:142022-12-12 04:46:14..683998684000: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::19801980] ] eager alloc mem 7.15 GBeager alloc mem 7.15 GB

[2022-12-12 04:46:14.684361: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 04:46:14.684407: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 7.11 GB
[2022-12-12 04:46:14.684428: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 04:46:14.685024: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 04:46:14.685078: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 7.15 GB
[2022-12-12 04:46:14.686835: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 04:46:14.687441: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 04:46:14.687485: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 7.12 GB
[2022-12-12 04:46:14.687642: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 04:46:14.688250: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 04:46:14.688294: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 7.15 GB
[[[[[[[[2022-12-12 04:46:172022-12-12 04:46:172022-12-12 04:46:172022-12-12 04:46:172022-12-12 04:46:172022-12-12 04:46:172022-12-12 04:46:172022-12-12 04:46:17........186064186064186064186064186064186066186064186065: : : : : : : : EEEEEEEE        /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::::::::19261926192619261926192619261926] ] ] ] Device 6 init p2p of link 0Device 1 init p2p of link 7] ] ] ] Device 2 init p2p of link 1Device 4 init p2p of link 5

Device 7 init p2p of link 4Device 0 init p2p of link 3Device 3 init p2p of link 2Device 5 init p2p of link 6





[[2022-12-12 04:46:172022-12-12 04:46:17..186690186690: [: [E2022-12-12 04:46:17[E2022-12-12 04:46:17[[ .2022-12-12 04:46:17 .2022-12-12 04:46:172022-12-12 04:46:17/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu[186708./hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu186712..:2022-12-12 04:46:17: 186713:: 1867271867161980.E: 1980E: : ] 186740 E]  EEeager alloc mem 611.00 KB: /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu eager alloc mem 611.00 KB/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu  
E:/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu
:/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu 1980:1980::/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu] 1980] 19801980:eager alloc mem 611.00 KB] eager alloc mem 611.00 KB] ] 1980
eager alloc mem 611.00 KB
eager alloc mem 611.00 KBeager alloc mem 611.00 KB] 


eager alloc mem 611.00 KB
[2022-12-12 04:46:17.187728[: 2022-12-12 04:46:17E. 187737/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc: :E638 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cceager release cuda mem 625663:
638] eager release cuda mem 625663
[2022-12-12 04:46:17.[[1878492022-12-12 04:46:172022-12-12 04:46:17: .[.[E1878552022-12-12 04:46:171878552022-12-12 04:46:17[ : .: .2022-12-12 04:46:17/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccE187865E187868.: :  : 187889638/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccE/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccE: ] : : Eeager release cuda mem 625663638/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc638/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc 
] :] :/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cceager release cuda mem 625663638eager release cuda mem 625663638:
] 
] 638eager release cuda mem 625663eager release cuda mem 625663] 

eager release cuda mem 625663
[2022-12-12 04:46:17.203683: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 6 init p2p of link 5
[2022-12-12 04:46:17.203842: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:46:17.203962: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 2
[2022-12-12 04:46:17.204120: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:46:17.204731: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:46:17.205008: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:46:17.205682: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 2 init p2p of link 3
[2022-12-12 04:46:17.205844: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:46:17.206038: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 3 init p2p of link 0
[2022-12-12 04:46:17.206110: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 7 init p2p of link 1
[2022-12-12 04:46:17.206192: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:46:17.206267: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:46:17.206296: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 0 init p2p of link 6
[2022-12-12 04:46:17.206462: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:46:17.206604: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 4 init p2p of link 7
[2022-12-12 04:46:17.206707: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:46:17.206723: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926[] 2022-12-12 04:46:17Device 5 init p2p of link 4.
206759: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:46:17.206899: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:46:17.207058: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:46:17.207093: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:46:17.207306: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:46:17.207628: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:46:17.207782: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:46:17.219821: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 3
[2022-12-12 04:46:17.219911: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] [Device 6 init p2p of link 42022-12-12 04:46:17
.219949: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:46:17.220042: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:46:17.220836: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:46:17.220924: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:46:17.221042: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 7 init p2p of link 6
[2022-12-12 04:46:17.221162: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:46:17.221820: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 0 init p2p of link 1
[2022-12-12 04:46:17.221949: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:46:17.222042: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:46:17.222185: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 2 init p2p of link 0
[2022-12-12 04:46:17.222306: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:46:17.222607: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 3 init p2p of link 5
[2022-12-12 04:46:17.222724: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:46:17.222828: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:46:17.223161: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:46:17.223303: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 4 init p2p of link 2
[2022-12-12 04:46:17.223424: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:46:17.223552: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:46:17.223888: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 5 init p2p of link 7
[2022-12-12 04:46:17.224017: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:46:17.224253: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:46:17.224874: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:46:17.241953: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 0
[2022-12-12 04:46:17.242071: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:46:17.242506: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 4 init p2p of link 6
[2022-12-12 04:46:17.242623: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:46:17.242938: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:46:17.243176: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 6 init p2p of link 7
[2022-12-12 04:46:17.243295: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:46:17.243407: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 3 init p2p of link 1
[2022-12-12 04:46:17.243474: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:46:17.243519: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:46:17.244160: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:46:17.244374: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:46:17.244577: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926[] 2022-12-12 04:46:17Device 7 init p2p of link 5.
244600: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 2 init p2p of link 4
[2022-12-12 04:46:17.244704: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB[
2022-12-12 04:46:17.244724: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:46:17.244768: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 0 init p2p of link 2
[2022-12-12 04:46:17.244889: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:46:17.245557: E[ 2022-12-12 04:46:17/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc.:245571638: ] Eeager release cuda mem 625663 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:46:17.245740: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:46:17.246466: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 5 init p2p of link 3
[2022-12-12 04:46:17.246585: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:46:17.247392: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:46:17.263449: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 60400000
[2022-12-12 04:46:17.263841: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 14904168 / 100000000 nodes ( 14.90 %~15.00 %) | remote 39370649 / 100000000 nodes ( 39.37 %) | cpu 45725183 / 100000000 nodes ( 45.73 %) | 7.11 GB | 2.71748 secs 
[2022-12-12 04:46:17.264130: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 60400000
[2022-12-12 04:46:17.264446: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 60400000
[2022-12-12 04:46:17.264519: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 14985053 / 100000000 nodes ( 14.99 %~15.00 %) | remote 39289764 / 100000000 nodes ( 39.29 %) | cpu 45725183 / 100000000 nodes ( 45.73 %) | 7.15 GB | 2.71901 secs 
[2022-12-12 04:46:17.264824: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 14994559 / 100000000 nodes ( 14.99 %~15.00 %) | remote 39280258 / 100000000 nodes ( 39.28 %) | cpu 45725183 / 100000000 nodes ( 45.73 %) | 7.15 GB | 2.71869 secs 
[2022-12-12 04:46:17.265341: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 60400000
[2022-12-12 04:46:17.266082: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 60400000
[2022-12-12 04:46:17.266160: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 60400000
[2022-12-12 04:46:17.267385: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 60400000
[2022-12-12 04:46:17.267591: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 14993873 / 100000000 nodes ( 14.99 %~15.00 %) | remote 39280944 / 100000000 nodes ( 39.28 %) | cpu 45725183 / 100000000 nodes ( 45.73 %) | 7.15 GB | 2.72116 secs 
[2022-12-12 04:46:17.267818: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 60400000
[2022-12-12 04:46:17.267946: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 14994954 / 100000000 nodes ( 14.99 %~15.00 %) | remote 39279863 / 100000000 nodes ( 39.28 %) | cpu 45725183 / 100000000 nodes ( 45.73 %) | 7.15 GB | 2.72175 secs 
[2022-12-12 04:46:17.268264: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 14924795 / 100000000 nodes ( 14.92 %~15.00 %) | remote 39350022 / 100000000 nodes ( 39.35 %) | cpu 45725183 / 100000000 nodes ( 45.73 %) | 7.12 GB | 2.72327 secs 
[2022-12-12 04:46:17.270433: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 14997133 / 100000000 nodes ( 15.00 %~15.00 %) | remote 39277684 / 100000000 nodes ( 39.28 %) | cpu 45725183 / 100000000 nodes ( 45.73 %) | 7.16 GB | 2.72492 secs 
[2022-12-12 04:46:17.272052: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 14996966 / 100000000 nodes ( 15.00 %~15.00 %) | remote 39277851 / 100000000 nodes ( 39.28 %) | cpu 45725183 / 100000000 nodes ( 45.73 %) | 7.16 GB | 2.74223 secs 
[2022-12-12 04:46:17.273413: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2213] before create ctx, mem is 15.48 GB
[2022-12-12 04:46:18.592749: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2219] after create ctx, mem is 15.74 GB
[2022-12-12 04:46:18.592983: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2226] after create stream, mem is 15.74 GB
[2022-12-12 04:46:18.593560: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2213] before create ctx, mem is 15.74 GB
[2022-12-12 04:46:19.852809: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2219] after create ctx, mem is 16.00 GB
[2022-12-12 04:46:19.853590: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2226] after create stream, mem is 16.00 GB
[2022-12-12 04:46:19.858392: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2213] before create ctx, mem is 16.00 GB
[2022-12-12 04:46:21. 88668: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2219] after create ctx, mem is 16.22 GB
[2022-12-12 04:46:21. 88843: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2226] after create stream, mem is 16.22 GB
[2022-12-12 04:46:21. 89189: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2213] before create ctx, mem is 16.22 GB
[2022-12-12 04:46:22.213859: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2219] after create ctx, mem is 16.43 GB
[2022-12-12 04:46:22.214000: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2226] after create stream, mem is 16.43 GB
[2022-12-12 04:46:22.215568: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2243] before create stream, mem is 16.43 GB
[2022-12-12 04:46:22.215724: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2249] after create stream, mem is 16.43 GB
[2022-12-12 04:46:22.215966: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2213] before create ctx, mem is 16.43 GB
[2022-12-12 04:46:23.411949: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2219] after create ctx, mem is 16.63 GB
[2022-12-12 04:46:23.413002: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2226] after create stream, mem is 16.63 GB
[HCTR][04:46:24.245][ERROR][RK0][main]: replica 6 calling init per replica done, doing barrier
[HCTR][04:46:24.245][ERROR][RK0][tid #140078672242432]: replica 7 calling init per replica done, doing barrier
[HCTR][04:46:24.245][ERROR][RK0][tid #140078529632000]: replica 2 calling init per replica done, doing barrier
[HCTR][04:46:24.245][ERROR][RK0][tid #140078462523136]: replica 1 calling init per replica done, doing barrier
[HCTR][04:46:24.245][ERROR][RK0][tid #140078923892480]: replica 3 calling init per replica done, doing barrier
[HCTR][04:46:24.245][ERROR][RK0][tid #140078529632000]: replica 0 calling init per replica done, doing barrier
[HCTR][04:46:24.245][ERROR][RK0][tid #140078529632000]: replica 4 calling init per replica done, doing barrier
[HCTR][04:46:24.245][ERROR][RK0][tid #140078521239296]: replica 5 calling init per replica done, doing barrier
[HCTR][04:46:24.245][ERROR][RK0][tid #140078529632000]: replica 2 calling init per replica done, doing barrier done
[HCTR][04:46:24.245][ERROR][RK0][tid #140078521239296]: replica 5 calling init per replica done, doing barrier done
[HCTR][04:46:24.245][ERROR][RK0][tid #140078529632000]: replica 4 calling init per replica done, doing barrier done
[HCTR][04:46:24.245][ERROR][RK0][main]: replica 6 calling init per replica done, doing barrier done
[HCTR][04:46:24.245][ERROR][RK0][tid #140078923892480]: replica 3 calling init per replica done, doing barrier done
[HCTR][04:46:24.245][ERROR][RK0][tid #140078529632000]: replica 0 calling init per replica done, doing barrier done
[HCTR][04:46:24.245][ERROR][RK0][tid #140078462523136]: replica 1 calling init per replica done, doing barrier done
[HCTR][04:46:24.245][ERROR][RK0][tid #140078672242432]: replica 7 calling init per replica done, doing barrier done
[HCTR][04:46:24.245][ERROR][RK0][tid #140078529632000]: init per replica done
[HCTR][04:46:24.245][ERROR][RK0][tid #140078521239296]: init per replica done
[HCTR][04:46:24.245][ERROR][RK0][tid #140078529632000]: init per replica done
[HCTR][04:46:24.245][ERROR][RK0][tid #140078462523136]: init per replica done
[HCTR][04:46:24.245][ERROR][RK0][main]: init per replica done
[HCTR][04:46:24.245][ERROR][RK0][tid #140078923892480]: init per replica done
[HCTR][04:46:24.245][ERROR][RK0][tid #140078672242432]: init per replica done
[HCTR][04:46:24.248][ERROR][RK0][tid #140078529632000]: init per replica done
[HCTR][04:46:24.284][ERROR][RK0][main]: 7 allocated 3276800 at 0x7f48fc238400
[HCTR][04:46:24.284][ERROR][RK0][main]: 7 allocated 6553600 at 0x7f48fc558400
[HCTR][04:46:24.284][ERROR][RK0][main]: 7 allocated 3276800 at 0x7f48fcb98400
[HCTR][04:46:24.284][ERROR][RK0][tid #140078529632000]: 4 allocated 3276800 at 0x7f4910238400
[HCTR][04:46:24.284][ERROR][RK0][main]: 7 allocated 6553600 at 0x7f48fceb8400
[HCTR][04:46:24.284][ERROR][RK0][main]: 5 allocated 3276800 at 0x7f493c238400
[HCTR][04:46:24.284][ERROR][RK0][tid #140078529632000]: 4 allocated 6553600 at 0x7f4910558400
[HCTR][04:46:24.284][ERROR][RK0][main]: 5 allocated 6553600 at 0x7f493c558400
[HCTR][04:46:24.284][ERROR][RK0][tid #140078529632000]: 4 allocated 3276800 at 0x7f4910b98400
[HCTR][04:46:24.284][ERROR][RK0][tid #140078529632000]: 4 allocated 6553600 at 0x7f4910eb8400
[HCTR][04:46:24.284][ERROR][RK0][main]: 3 allocated 3276800 at 0x7f4988238400
[HCTR][04:46:24.284][ERROR][RK0][main]: 5 allocated 3276800 at 0x7f493cb98400
[HCTR][04:46:24.284][ERROR][RK0][main]: 3 allocated 6553600 at 0x7f4988558400
[HCTR][04:46:24.284][ERROR][RK0][main]: 5 allocated 6553600 at 0x7f493ceb8400
[HCTR][04:46:24.284][ERROR][RK0][main]: 3 allocated 3276800 at 0x7f4988b98400
[HCTR][04:46:24.284][ERROR][RK0][main]: 3 allocated 6553600 at 0x7f4988eb8400
[HCTR][04:46:24.284][ERROR][RK0][tid #140078789674752]: 6 allocated 3276800 at 0x7f4988238400
[HCTR][04:46:24.284][ERROR][RK0][tid #140078789674752]: 6 allocated 6553600 at 0x7f4988558400
[HCTR][04:46:24.284][ERROR][RK0][tid #140078789674752]: 6 allocated 3276800 at 0x7f4988b98400
[HCTR][04:46:24.284][ERROR][RK0][tid #140078789674752]: 6 allocated 6553600 at 0x7f4988eb8400
[HCTR][04:46:24.284][ERROR][RK0][tid #140078462523136]: 1 allocated 3276800 at 0x7f493c238400
[HCTR][04:46:24.284][ERROR][RK0][main]: 2 allocated 3276800 at 0x7f48b0238400
[HCTR][04:46:24.284][ERROR][RK0][tid #140078462523136]: 1 allocated 6553600 at 0x7f493c558400
[HCTR][04:46:24.284][ERROR][RK0][main]: 2 allocated 6553600 at 0x7f48b0558400
[HCTR][04:46:24.284][ERROR][RK0][tid #140078462523136]: 1 allocated 3276800 at 0x7f493cb98400
[HCTR][04:46:24.284][ERROR][RK0][main]: 2 allocated 3276800 at 0x7f48b0b98400
[HCTR][04:46:24.284][ERROR][RK0][tid #140078462523136]: 1 allocated 6553600 at 0x7f493ceb8400
[HCTR][04:46:24.284][ERROR][RK0][main]: 2 allocated 6553600 at 0x7f48b0eb8400
[HCTR][04:46:24.287][ERROR][RK0][tid #140078529632000]: 0 allocated 3276800 at 0x7f48c8320000
[HCTR][04:46:24.287][ERROR][RK0][tid #140078529632000]: 0 allocated 6553600 at 0x7f48c8640000
[HCTR][04:46:24.287][ERROR][RK0][tid #140078529632000]: 0 allocated 3276800 at 0x7f48c8c80000
[HCTR][04:46:24.287][ERROR][RK0][tid #140078529632000]: 0 allocated 6553600 at 0x7f48c8fa0000
