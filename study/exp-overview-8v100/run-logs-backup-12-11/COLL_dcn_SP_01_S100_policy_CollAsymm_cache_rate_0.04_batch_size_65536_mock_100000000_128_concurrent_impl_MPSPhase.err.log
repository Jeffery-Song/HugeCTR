2022-12-12 00:53:17.042667: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.049735: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.055141: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.068464: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.074792: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.080968: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.093221: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.100221: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.155989: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.156100: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.157441: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.157778: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.158843: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.159372: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.160525: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.161014: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.166466: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.166970: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.168016: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.168916: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.169575: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.170695: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.170996: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.172542: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.172749: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.174382: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.175472: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.176500: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.177559: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.178632: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.179750: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.180851: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.182724: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.183888: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.184890: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.186137: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.187913: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.188698: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.189339: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.190379: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.190885: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.191972: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.192437: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.193567: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.194613: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.195541: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.196501: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.197464: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.198484: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 00:53:17.201021: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.202079: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.203081: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.204344: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.205773: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.207690: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.207723: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.208530: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.209714: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.210009: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.210033: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.210966: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.212687: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.212881: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.213064: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.213727: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.215973: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.216192: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.216853: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.218003: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.218482: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.218758: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.219941: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.221188: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.221916: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.222109: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.223394: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.224104: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.224346: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.225375: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.225611: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.227184: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.227794: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.227973: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.229147: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.229277: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.230726: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.231010: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.231519: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.232613: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.233801: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.233937: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.234564: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.236461: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.236509: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.237053: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.238618: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.238817: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.239542: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.240986: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.241845: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.241969: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.242511: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.250546: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.250709: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.252665: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.253691: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.254528: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.255050: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.256223: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.260376: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.276778: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.278644: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.284740: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.291649: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.293657: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.294188: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.294273: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.294362: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.294567: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.294816: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.297639: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.297718: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.298680: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.298722: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.298764: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.298982: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.299294: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.302920: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.303023: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.303741: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.303827: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.304789: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.304888: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.305999: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.307736: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.308059: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.308741: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.308956: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.309049: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.309268: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.310548: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.312776: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.313061: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.313428: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.313581: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.313835: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.313880: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.315659: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.318110: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.318434: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.318771: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.319021: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.319062: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.319106: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.320973: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.322472: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.323034: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.323628: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.323828: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.323981: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.324029: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.325970: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.327767: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.328136: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.328240: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.328520: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.328756: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.328849: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.331143: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.332435: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.332638: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.332850: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.333082: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.333273: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.333414: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.335644: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.337287: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.337911: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.338146: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.338779: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.338967: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.339107: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.342937: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.343911: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.344257: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.344311: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.344740: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.344978: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.345024: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.348672: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.349300: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.349375: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.349606: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.349700: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.349958: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.351996: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 00:53:17.352759: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.352930: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.353016: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.353439: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.353561: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.353674: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.356650: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.356877: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.357117: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.357254: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.357502: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.357627: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.360882: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.361100: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.361226: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.361409: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.361724: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.361813: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.361818: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.365726: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.366793: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.366884: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.366890: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.367260: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.367515: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.367645: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.370933: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.372043: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.372248: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.372329: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.372443: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.372673: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.372916: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.376490: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.376749: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.376893: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.377007: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.377135: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.377650: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.381274: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.381791: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.381863: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.381908: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.382205: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.382525: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.386351: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.386562: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.386633: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.386740: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.386963: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.387320: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.392470: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.392689: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.392931: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.393122: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.393279: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.396972: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.397053: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 00:53:17.397082: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.397228: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.397476: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.398201: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.401323: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.401461: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.401579: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.401905: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.402644: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.405621: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.406489: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.408778: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.409478: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 00:53:17.409481: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 00:53:17.409529: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 00:53:17.409992: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 00:53:17.410035: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.411980: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.414938: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 00:53:17.419519: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.419937: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.420139: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.420626: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.424889: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.424891: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.426175: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.426236: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.426452: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.458100: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.458273: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.459528: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.459611: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.459839: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:17.463019: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:18.583590: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:18.584443: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:18.585310: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:18.585775: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 00:53:18.585831: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-12 00:53:18.603963: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:18.604607: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:18.605110: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:18.605695: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:18.606229: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:18.606696: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-12 00:53:18.653202: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 00:53:18.653410: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 00:53:18.680470: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12340
2022-12-12 00:53:18.810006: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:18.810633: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:18.811206: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:18.811675: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 00:53:18.811729: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-12 00:53:18.829203: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:18.830243: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:18.830756: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:18.831555: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:18.832354: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:18.832831: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:1/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-12 00:53:18.879104: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:18.879824: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:18.880365: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:18.880826: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 00:53:18.880885: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-12 00:53:18.898027: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:18.898902: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:18.899474: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:18.900065: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:18.900596: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:18.901210: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:2/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-12 00:53:18.910188: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 00:53:18.910388: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 00:53:18.910637: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:18.911249: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:18.911783: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:18.912253: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 00:53:18.912277: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12341
2022-12-12 00:53:18.912305: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-12 00:53:18.912980: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:18.913583: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:18.914104: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:18.914792: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 00:53:18.914844: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-12 00:53:18.915221: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:18.915812: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:18.916344: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:18.916802: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 00:53:18.916849: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-12 00:53:18.919565: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:18.920141: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:18.920829: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:18.921292: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 00:53:18.921341: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-12 00:53:18.929868: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:18.930510: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:18.931011: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:18.931728: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:18.931804: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:18.932925: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:18.932931: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:18.933410: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:18.934167: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:18.934238: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:5/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-12 00:53:18.934855: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:18.935379: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:18.935597: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:18.936328: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:18.936538: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:18.937198: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:4/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-12 00:53:18.937402: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:18.937867: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:7/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-12 00:53:18.938368: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:18.938936: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:18.939462: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:18.940014: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:18.940529: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:18.940988: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:6/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-12 00:53:18.956694: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:18.957331: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:18.957844: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:18.958307: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 00:53:18.958359: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-12 00:53:18.976867: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:18.977574: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:18.978083: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:18.978651: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 00:53:18.978690: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:18.978839: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 00:53:18.979283: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:53:18.979780: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:3/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-12 00:53:18.980672: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12345
2022-12-12 00:53:18.982888: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 00:53:18.983062: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 00:53:18.983262: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 00:53:18.983392: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 00:53:18.985041: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12347
2022-12-12 00:53:18.985217: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12344
2022-12-12 00:53:18.985671: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 00:53:18.985799: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 00:53:18.987607: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12346
2022-12-12 00:53:19.008159: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 00:53:19.008347: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 00:53:19.010443: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12342
2022-12-12 00:53:19.025823: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 00:53:19.026013: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 00:53:19.027758: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12343
[HCTR][00:53:20.300][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][00:53:20.300][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][00:53:20.300][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][00:53:20.300][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][00:53:20.300][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][00:53:20.300][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][00:53:20.300][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][00:53:20.301][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 1it [00:01,  1.57s/it]warmup run: 100it [00:01, 83.29it/s]warmup run: 1it [00:01,  1.53s/it]warmup run: 202it [00:01, 183.10it/s]warmup run: 100it [00:01, 85.17it/s]warmup run: 301it [00:01, 289.51it/s]warmup run: 1it [00:01,  1.52s/it]warmup run: 200it [00:01, 184.59it/s]warmup run: 1it [00:01,  1.49s/it]warmup run: 1it [00:01,  1.49s/it]warmup run: 1it [00:01,  1.55s/it]warmup run: 403it [00:01, 404.95it/s]warmup run: 101it [00:01, 86.50it/s]warmup run: 1it [00:01,  1.49s/it]warmup run: 1it [00:01,  1.47s/it]warmup run: 301it [00:01, 295.21it/s]warmup run: 96it [00:01, 83.54it/s]warmup run: 95it [00:01, 82.77it/s]warmup run: 100it [00:01, 84.13it/s]warmup run: 502it [00:02, 512.50it/s]warmup run: 197it [00:01, 181.73it/s]warmup run: 99it [00:01, 87.27it/s]warmup run: 99it [00:01, 86.33it/s]warmup run: 401it [00:01, 408.27it/s]warmup run: 195it [00:01, 183.96it/s]warmup run: 190it [00:01, 179.03it/s]warmup run: 190it [00:01, 171.64it/s]warmup run: 601it [00:02, 611.78it/s]warmup run: 275it [00:01, 262.35it/s]warmup run: 199it [00:01, 189.51it/s]warmup run: 192it [00:01, 179.84it/s]warmup run: 498it [00:02, 512.32it/s]warmup run: 295it [00:01, 295.23it/s]warmup run: 287it [00:01, 286.88it/s]warmup run: 264it [00:01, 246.98it/s]warmup run: 700it [00:02, 697.61it/s]warmup run: 376it [00:01, 383.06it/s]warmup run: 300it [00:01, 302.36it/s]warmup run: 291it [00:01, 290.30it/s]warmup run: 597it [00:02, 612.09it/s]warmup run: 395it [00:01, 410.23it/s]warmup run: 382it [00:01, 394.64it/s]warmup run: 362it [00:01, 363.57it/s]warmup run: 799it [00:02, 768.34it/s]warmup run: 477it [00:02, 500.35it/s]warmup run: 400it [00:01, 417.12it/s]warmup run: 391it [00:01, 405.70it/s]warmup run: 698it [00:02, 703.86it/s]warmup run: 495it [00:01, 521.65it/s]warmup run: 474it [00:01, 493.69it/s]warmup run: 463it [00:02, 482.68it/s]warmup run: 897it [00:02, 822.62it/s]warmup run: 580it [00:02, 611.12it/s]warmup run: 498it [00:01, 524.01it/s]warmup run: 490it [00:01, 515.29it/s]warmup run: 798it [00:02, 777.21it/s]warmup run: 594it [00:02, 620.84it/s]warmup run: 568it [00:02, 587.64it/s]warmup run: 564it [00:02, 592.78it/s]warmup run: 996it [00:02, 865.77it/s]warmup run: 682it [00:02, 705.56it/s]warmup run: 595it [00:02, 619.21it/s]warmup run: 589it [00:02, 615.98it/s]warmup run: 899it [00:02, 836.53it/s]warmup run: 695it [00:02, 710.11it/s]warmup run: 663it [00:02, 671.49it/s]warmup run: 665it [00:02, 687.74it/s]warmup run: 1094it [00:02, 882.87it/s]warmup run: 785it [00:02, 784.46it/s]warmup run: 690it [00:02, 694.29it/s]warmup run: 687it [00:02, 699.38it/s]warmup run: 998it [00:02, 877.59it/s]warmup run: 792it [00:02, 774.65it/s]warmup run: 759it [00:02, 741.65it/s]warmup run: 765it [00:02, 763.58it/s]warmup run: 1194it [00:02, 915.17it/s]warmup run: 887it [00:02, 844.53it/s]warmup run: 786it [00:02, 760.22it/s]warmup run: 786it [00:02, 771.51it/s]warmup run: 1099it [00:02, 912.97it/s]warmup run: 890it [00:02, 827.01it/s]warmup run: 854it [00:02, 795.37it/s]warmup run: 863it [00:02, 819.85it/s]warmup run: 1293it [00:02, 934.41it/s]warmup run: 987it [00:02, 884.35it/s]warmup run: 882it [00:02, 812.66it/s]warmup run: 883it [00:02, 821.19it/s]warmup run: 1200it [00:02, 940.37it/s]warmup run: 987it [00:02, 857.65it/s]warmup run: 950it [00:02, 839.04it/s]warmup run: 963it [00:02, 866.52it/s]warmup run: 1393it [00:02, 951.03it/s]warmup run: 1087it [00:02, 913.62it/s]warmup run: 979it [00:02, 853.33it/s]warmup run: 1300it [00:02, 955.89it/s]warmup run: 980it [00:02, 854.35it/s]warmup run: 1084it [00:02, 886.83it/s]warmup run: 1044it [00:02, 866.04it/s]warmup run: 1061it [00:02, 897.35it/s]warmup run: 1492it [00:03, 959.44it/s]warmup run: 1189it [00:02, 942.94it/s]warmup run: 1076it [00:02, 883.67it/s]warmup run: 1400it [00:02, 967.17it/s]warmup run: 1078it [00:02, 887.03it/s]warmup run: 1181it [00:02, 909.05it/s]warmup run: 1138it [00:02, 884.60it/s]warmup run: 1161it [00:02, 925.68it/s]warmup run: 1591it [00:03, 967.19it/s]warmup run: 1290it [00:02, 959.88it/s]warmup run: 1174it [00:02, 909.61it/s]warmup run: 1500it [00:03, 974.47it/s]warmup run: 1176it [00:02, 911.22it/s]warmup run: 1278it [00:02, 924.73it/s]warmup run: 1232it [00:02, 897.33it/s]warmup run: 1260it [00:02, 929.03it/s]warmup run: 1690it [00:03, 966.01it/s]warmup run: 1392it [00:02, 975.58it/s]warmup run: 1271it [00:02, 916.74it/s]warmup run: 1277it [00:02, 938.43it/s]warmup run: 1600it [00:03, 975.72it/s]warmup run: 1380it [00:02, 951.71it/s]warmup run: 1327it [00:02, 910.54it/s]warmup run: 1359it [00:02, 946.30it/s]warmup run: 1494it [00:03, 988.07it/s]warmup run: 1790it [00:03, 974.06it/s]warmup run: 1368it [00:02, 931.12it/s]warmup run: 1380it [00:02, 962.67it/s]warmup run: 1699it [00:03, 976.67it/s]warmup run: 1484it [00:03, 975.02it/s]warmup run: 1422it [00:03, 920.99it/s]warmup run: 1461it [00:03, 965.36it/s]warmup run: 1596it [00:03, 996.34it/s]warmup run: 1889it [00:03, 977.91it/s]warmup run: 1464it [00:02, 932.83it/s]warmup run: 1484it [00:03, 983.03it/s]warmup run: 1798it [00:03, 975.04it/s]warmup run: 1589it [00:03, 995.72it/s]warmup run: 1518it [00:03, 930.24it/s]warmup run: 1560it [00:03, 972.43it/s]warmup run: 1988it [00:03, 980.82it/s]warmup run: 1698it [00:03, 1001.29it/s]warmup run: 1587it [00:03, 996.66it/s]warmup run: 1560it [00:03, 938.75it/s]warmup run: 1897it [00:03, 972.46it/s]warmup run: 1693it [00:03, 1007.78it/s]warmup run: 1613it [00:03, 934.30it/s]warmup run: 1660it [00:03, 978.46it/s]warmup run: 1800it [00:03, 1005.63it/s]warmup run: 2105it [00:03, 1034.97it/s]warmup run: 1690it [00:03, 1004.59it/s]warmup run: 1656it [00:03, 941.41it/s]warmup run: 1995it [00:03, 970.06it/s]warmup run: 1797it [00:03, 1017.18it/s]warmup run: 1708it [00:03, 934.88it/s]warmup run: 1759it [00:03, 976.07it/s]warmup run: 2224it [00:03, 1081.11it/s]warmup run: 1903it [00:03, 1011.54it/s]warmup run: 1793it [00:03, 1011.28it/s]warmup run: 1752it [00:03, 942.42it/s]warmup run: 2107it [00:03, 1012.56it/s]warmup run: 1901it [00:03, 1021.99it/s]warmup run: 1804it [00:03, 939.54it/s]warmup run: 1858it [00:03, 979.59it/s]warmup run: 2345it [00:03, 1118.41it/s]warmup run: 2006it [00:03, 1015.49it/s]warmup run: 1897it [00:03, 1017.65it/s]warmup run: 1849it [00:03, 948.24it/s]warmup run: 2225it [00:03, 1060.28it/s]warmup run: 2005it [00:03, 1025.50it/s]warmup run: 1899it [00:03, 942.06it/s]warmup run: 1957it [00:03, 976.24it/s]warmup run: 2467it [00:03, 1146.98it/s]warmup run: 2126it [00:03, 1069.15it/s]warmup run: 2000it [00:03, 1021.25it/s]warmup run: 1945it [00:03, 951.12it/s]warmup run: 2342it [00:03, 1090.38it/s]warmup run: 2128it [00:03, 1084.97it/s]warmup run: 1994it [00:03, 944.03it/s]warmup run: 2066it [00:03, 1008.73it/s]warmup run: 2589it [00:04, 1167.36it/s]warmup run: 2246it [00:03, 1107.77it/s]warmup run: 2123it [00:03, 1081.02it/s]warmup run: 2047it [00:03, 970.87it/s]warmup run: 2459it [00:03, 1114.02it/s]warmup run: 2251it [00:03, 1127.62it/s]warmup run: 2113it [00:03, 1016.07it/s]warmup run: 2187it [00:03, 1066.52it/s]warmup run: 2711it [00:04, 1181.87it/s]warmup run: 2367it [00:03, 1136.17it/s]warmup run: 2246it [00:03, 1125.39it/s]warmup run: 2164it [00:03, 1028.70it/s]warmup run: 2573it [00:04, 1121.49it/s]warmup run: 2374it [00:03, 1157.48it/s]warmup run: 2235it [00:03, 1074.79it/s]warmup run: 2308it [00:03, 1107.41it/s]warmup run: 2832it [00:04, 1188.37it/s]warmup run: 2488it [00:03, 1156.60it/s]warmup run: 2370it [00:03, 1157.36it/s]warmup run: 2281it [00:03, 1069.38it/s]warmup run: 2690it [00:04, 1134.04it/s]warmup run: 2497it [00:03, 1178.16it/s]warmup run: 2357it [00:03, 1115.84it/s]warmup run: 2429it [00:03, 1136.50it/s]warmup run: 2954it [00:04, 1196.30it/s]warmup run: 2609it [00:04, 1170.26it/s]warmup run: 2493it [00:03, 1177.56it/s]warmup run: 2398it [00:03, 1098.37it/s]warmup run: 2806it [00:04, 1139.18it/s]warmup run: 2620it [00:04, 1192.67it/s]warmup run: 3000it [00:04, 676.60it/s] warmup run: 2475it [00:04, 1132.91it/s]warmup run: 2548it [00:04, 1151.20it/s]warmup run: 2729it [00:04, 1179.01it/s]warmup run: 2615it [00:04, 1190.21it/s]warmup run: 2515it [00:03, 1117.00it/s]warmup run: 2923it [00:04, 1146.50it/s]warmup run: 2743it [00:04, 1202.58it/s]warmup run: 2597it [00:04, 1156.29it/s]warmup run: 3000it [00:04, 678.51it/s] warmup run: 2669it [00:04, 1166.91it/s]warmup run: 2848it [00:04, 1180.51it/s]warmup run: 2738it [00:04, 1200.87it/s]warmup run: 2632it [00:04, 1130.87it/s]warmup run: 2865it [00:04, 1206.06it/s]warmup run: 2719it [00:04, 1172.40it/s]warmup run: 2788it [00:04, 1171.53it/s]warmup run: 2968it [00:04, 1185.96it/s]warmup run: 2860it [00:04, 1204.94it/s]warmup run: 2749it [00:04, 1141.29it/s]warmup run: 3000it [00:04, 687.03it/s] warmup run: 2988it [00:04, 1211.09it/s]warmup run: 3000it [00:04, 693.58it/s] warmup run: 2839it [00:04, 1178.41it/s]warmup run: 2906it [00:04, 1172.34it/s]warmup run: 2983it [00:04, 1209.81it/s]warmup run: 2864it [00:04, 1143.75it/s]warmup run: 3000it [00:04, 693.13it/s] warmup run: 2960it [00:04, 1186.78it/s]warmup run: 3000it [00:04, 672.77it/s] warmup run: 3000it [00:04, 674.27it/s] warmup run: 2983it [00:04, 1157.11it/s]warmup run: 3000it [00:04, 679.37it/s] 



warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]


warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   6%|▌         | 166/3000 [00:00<00:01, 1657.38it/s]warmup should be done:   6%|▌         | 165/3000 [00:00<00:01, 1645.43it/s]warmup should be done:   6%|▌         | 168/3000 [00:00<00:01, 1676.33it/s]warmup should be done:   6%|▌         | 165/3000 [00:00<00:01, 1649.06it/s]warmup should be done:   5%|▌         | 163/3000 [00:00<00:01, 1629.46it/s]warmup should be done:   5%|▌         | 157/3000 [00:00<00:01, 1564.72it/s]warmup should be done:   5%|▌         | 164/3000 [00:00<00:01, 1633.19it/s]warmup should be done:   6%|▌         | 167/3000 [00:00<00:01, 1662.16it/s]warmup should be done:  11%|█         | 330/3000 [00:00<00:01, 1645.87it/s]warmup should be done:  11%|█         | 337/3000 [00:00<00:01, 1682.32it/s]warmup should be done:  11%|█         | 333/3000 [00:00<00:01, 1660.32it/s]warmup should be done:  11%|█         | 327/3000 [00:00<00:01, 1633.27it/s]warmup should be done:  11%|█         | 329/3000 [00:00<00:01, 1641.65it/s]warmup should be done:  11%|█         | 319/3000 [00:00<00:01, 1593.73it/s]warmup should be done:  11%|█         | 332/3000 [00:00<00:01, 1656.49it/s]warmup should be done:  11%|█         | 334/3000 [00:00<00:01, 1661.63it/s]warmup should be done:  17%|█▋        | 498/3000 [00:00<00:01, 1655.74it/s]warmup should be done:  16%|█▋        | 495/3000 [00:00<00:01, 1640.99it/s]warmup should be done:  17%|█▋        | 506/3000 [00:00<00:01, 1679.05it/s]warmup should be done:  16%|█▋        | 494/3000 [00:00<00:01, 1638.19it/s]warmup should be done:  17%|█▋        | 500/3000 [00:00<00:01, 1656.57it/s]warmup should be done:  16%|█▌        | 479/3000 [00:00<00:01, 1588.60it/s]warmup should be done:  17%|█▋        | 501/3000 [00:00<00:01, 1654.09it/s]warmup should be done:  16%|█▋        | 491/3000 [00:00<00:01, 1608.05it/s]warmup should be done:  22%|██▏       | 664/3000 [00:00<00:01, 1655.47it/s]warmup should be done:  22%|██▏       | 674/3000 [00:00<00:01, 1676.70it/s]warmup should be done:  22%|██▏       | 666/3000 [00:00<00:01, 1654.59it/s]warmup should be done:  22%|██▏       | 658/3000 [00:00<00:01, 1635.25it/s]warmup should be done:  22%|██▏       | 660/3000 [00:00<00:01, 1637.69it/s]warmup should be done:  22%|██▏       | 668/3000 [00:00<00:01, 1658.60it/s]warmup should be done:  21%|██▏       | 638/3000 [00:00<00:01, 1580.53it/s]warmup should be done:  22%|██▏       | 652/3000 [00:00<00:01, 1605.44it/s]warmup should be done:  28%|██▊       | 830/3000 [00:00<00:01, 1651.49it/s]warmup should be done:  28%|██▊       | 842/3000 [00:00<00:01, 1672.96it/s]warmup should be done:  27%|██▋       | 822/3000 [00:00<00:01, 1633.18it/s]warmup should be done:  27%|██▋       | 824/3000 [00:00<00:01, 1635.59it/s]warmup should be done:  28%|██▊       | 832/3000 [00:00<00:01, 1652.08it/s]warmup should be done:  28%|██▊       | 834/3000 [00:00<00:01, 1657.03it/s]warmup should be done:  27%|██▋       | 797/3000 [00:00<00:01, 1569.36it/s]warmup should be done:  27%|██▋       | 813/3000 [00:00<00:01, 1591.08it/s]warmup should be done:  33%|███▎      | 987/3000 [00:00<00:01, 1637.36it/s]warmup should be done:  33%|███▎      | 989/3000 [00:00<00:01, 1638.55it/s]warmup should be done:  33%|███▎      | 996/3000 [00:00<00:01, 1648.91it/s]warmup should be done:  33%|███▎      | 1000/3000 [00:00<00:01, 1656.29it/s]warmup should be done:  34%|███▎      | 1010/3000 [00:00<00:01, 1669.26it/s]warmup should be done:  33%|███▎      | 998/3000 [00:00<00:01, 1637.99it/s]warmup should be done:  32%|███▏      | 973/3000 [00:00<00:01, 1592.69it/s]warmup should be done:  32%|███▏      | 954/3000 [00:00<00:01, 1542.64it/s]warmup should be done:  39%|███▊      | 1161/3000 [00:00<00:01, 1646.37it/s]warmup should be done:  38%|███▊      | 1151/3000 [00:00<00:01, 1634.50it/s]warmup should be done:  39%|███▉      | 1166/3000 [00:00<00:01, 1654.72it/s]warmup should be done:  38%|███▊      | 1153/3000 [00:00<00:01, 1630.22it/s]warmup should be done:  39%|███▉      | 1177/3000 [00:00<00:01, 1653.73it/s]warmup should be done:  39%|███▊      | 1162/3000 [00:00<00:01, 1625.96it/s]warmup should be done:  38%|███▊      | 1133/3000 [00:00<00:01, 1585.44it/s]warmup should be done:  37%|███▋      | 1109/3000 [00:00<00:01, 1531.53it/s]warmup should be done:  44%|████▍     | 1326/3000 [00:00<00:01, 1645.96it/s]warmup should be done:  44%|████▍     | 1332/3000 [00:00<00:01, 1654.00it/s]warmup should be done:  44%|████▍     | 1315/3000 [00:00<00:01, 1630.52it/s]warmup should be done:  44%|████▍     | 1317/3000 [00:00<00:01, 1618.55it/s]warmup should be done:  45%|████▍     | 1343/3000 [00:00<00:01, 1644.46it/s]warmup should be done:  43%|████▎     | 1292/3000 [00:00<00:01, 1585.58it/s]warmup should be done:  44%|████▍     | 1325/3000 [00:00<00:01, 1617.02it/s]warmup should be done:  42%|████▏     | 1263/3000 [00:00<00:01, 1519.16it/s]warmup should be done:  50%|████▉     | 1491/3000 [00:00<00:00, 1643.56it/s]warmup should be done:  49%|████▉     | 1480/3000 [00:00<00:00, 1635.42it/s]warmup should be done:  50%|████▉     | 1498/3000 [00:00<00:00, 1651.37it/s]warmup should be done:  49%|████▉     | 1479/3000 [00:00<00:00, 1615.35it/s]warmup should be done:  48%|████▊     | 1452/3000 [00:00<00:00, 1588.33it/s]warmup should be done:  50%|████▉     | 1487/3000 [00:00<00:00, 1616.13it/s]warmup should be done:  50%|█████     | 1508/3000 [00:00<00:00, 1624.16it/s]warmup should be done:  47%|████▋     | 1418/3000 [00:00<00:01, 1526.92it/s]warmup should be done:  55%|█████▍    | 1646/3000 [00:01<00:00, 1641.47it/s]warmup should be done:  55%|█████▌    | 1656/3000 [00:01<00:00, 1640.83it/s]warmup should be done:  55%|█████▌    | 1664/3000 [00:01<00:00, 1649.53it/s]warmup should be done:  55%|█████▍    | 1642/3000 [00:01<00:00, 1616.80it/s]warmup should be done:  54%|█████▎    | 1611/3000 [00:01<00:00, 1580.19it/s]warmup should be done:  55%|█████▍    | 1649/3000 [00:01<00:00, 1603.77it/s]warmup should be done:  56%|█████▌    | 1671/3000 [00:01<00:00, 1611.44it/s]warmup should be done:  52%|█████▏    | 1571/3000 [00:01<00:00, 1524.32it/s]warmup should be done:  60%|██████    | 1812/3000 [00:01<00:00, 1644.39it/s]warmup should be done:  61%|██████    | 1821/3000 [00:01<00:00, 1640.45it/s]warmup should be done:  61%|██████    | 1829/3000 [00:01<00:00, 1649.11it/s]warmup should be done:  60%|██████    | 1804/3000 [00:01<00:00, 1617.49it/s]warmup should be done:  59%|█████▉    | 1770/3000 [00:01<00:00, 1579.34it/s]warmup should be done:  60%|██████    | 1814/3000 [00:01<00:00, 1615.71it/s]warmup should be done:  61%|██████    | 1836/3000 [00:01<00:00, 1620.23it/s]warmup should be done:  57%|█████▋    | 1724/3000 [00:01<00:00, 1512.11it/s]warmup should be done:  66%|██████▌   | 1978/3000 [00:01<00:00, 1646.20it/s]warmup should be done:  66%|██████▌   | 1986/3000 [00:01<00:00, 1641.98it/s]warmup should be done:  66%|██████▋   | 1995/3000 [00:01<00:00, 1651.10it/s]warmup should be done:  66%|██████▌   | 1966/3000 [00:01<00:00, 1610.32it/s]warmup should be done:  64%|██████▍   | 1932/3000 [00:01<00:00, 1589.73it/s]warmup should be done:  66%|██████▌   | 1979/3000 [00:01<00:00, 1624.43it/s]warmup should be done:  67%|██████▋   | 2001/3000 [00:01<00:00, 1626.47it/s]warmup should be done:  63%|██████▎   | 1886/3000 [00:01<00:00, 1542.26it/s]warmup should be done:  71%|███████▏  | 2143/3000 [00:01<00:00, 1646.28it/s]warmup should be done:  72%|███████▏  | 2151/3000 [00:01<00:00, 1642.48it/s]warmup should be done:  72%|███████▏  | 2161/3000 [00:01<00:00, 1652.01it/s]warmup should be done:  71%|███████   | 2128/3000 [00:01<00:00, 1608.91it/s]warmup should be done:  70%|██████▉   | 2094/3000 [00:01<00:00, 1597.63it/s]warmup should be done:  71%|███████▏  | 2144/3000 [00:01<00:00, 1630.18it/s]warmup should be done:  72%|███████▏  | 2165/3000 [00:01<00:00, 1630.23it/s]warmup should be done:  68%|██████▊   | 2046/3000 [00:01<00:00, 1556.68it/s]warmup should be done:  77%|███████▋  | 2308/3000 [00:01<00:00, 1645.99it/s]warmup should be done:  77%|███████▋  | 2316/3000 [00:01<00:00, 1644.09it/s]warmup should be done:  78%|███████▊  | 2327/3000 [00:01<00:00, 1653.47it/s]warmup should be done:  76%|███████▋  | 2289/3000 [00:01<00:00, 1606.50it/s]warmup should be done:  75%|███████▌  | 2254/3000 [00:01<00:00, 1594.09it/s]warmup should be done:  77%|███████▋  | 2309/3000 [00:01<00:00, 1634.31it/s]warmup should be done:  78%|███████▊  | 2329/3000 [00:01<00:00, 1630.80it/s]warmup should be done:  74%|███████▎  | 2206/3000 [00:01<00:00, 1568.85it/s]warmup should be done:  83%|████████▎ | 2493/3000 [00:01<00:00, 1652.98it/s]warmup should be done:  83%|████████▎ | 2481/3000 [00:01<00:00, 1641.85it/s]warmup should be done:  82%|████████▏ | 2473/3000 [00:01<00:00, 1630.25it/s]warmup should be done:  82%|████████▏ | 2450/3000 [00:01<00:00, 1600.08it/s]warmup should be done:  81%|████████  | 2416/3000 [00:01<00:00, 1600.37it/s]warmup should be done:  82%|████████▏ | 2473/3000 [00:01<00:00, 1634.69it/s]warmup should be done:  79%|███████▉  | 2367/3000 [00:01<00:00, 1579.61it/s]warmup should be done:  83%|████████▎ | 2493/3000 [00:01<00:00, 1581.09it/s]warmup should be done:  89%|████████▊ | 2659/3000 [00:01<00:00, 1653.11it/s]warmup should be done:  88%|████████▊ | 2646/3000 [00:01<00:00, 1642.38it/s]warmup should be done:  88%|████████▊ | 2637/3000 [00:01<00:00, 1613.98it/s]warmup should be done:  87%|████████▋ | 2611/3000 [00:01<00:00, 1597.70it/s]warmup should be done:  86%|████████▌ | 2577/3000 [00:01<00:00, 1601.20it/s]warmup should be done:  88%|████████▊ | 2638/3000 [00:01<00:00, 1637.03it/s]warmup should be done:  84%|████████▍ | 2527/3000 [00:01<00:00, 1583.55it/s]warmup should be done:  88%|████████▊ | 2652/3000 [00:01<00:00, 1566.94it/s]warmup should be done:  94%|█████████▍| 2825/3000 [00:01<00:00, 1654.13it/s]warmup should be done:  94%|█████████▎| 2811/3000 [00:01<00:00, 1642.15it/s]warmup should be done:  92%|█████████▏| 2771/3000 [00:01<00:00, 1595.44it/s]warmup should be done:  91%|█████████▏| 2738/3000 [00:01<00:00, 1601.09it/s]warmup should be done:  93%|█████████▎| 2803/3000 [00:01<00:00, 1640.08it/s]warmup should be done:  93%|█████████▎| 2799/3000 [00:01<00:00, 1602.88it/s]warmup should be done:  90%|████████▉ | 2689/3000 [00:01<00:00, 1591.54it/s]warmup should be done:  94%|█████████▎| 2809/3000 [00:01<00:00, 1559.77it/s]warmup should be done: 100%|█████████▉| 2992/3000 [00:01<00:00, 1658.20it/s]warmup should be done:  99%|█████████▉| 2977/3000 [00:01<00:00, 1647.17it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1654.34it/s]warmup should be done:  98%|█████████▊| 2933/3000 [00:01<00:00, 1600.75it/s]warmup should be done:  99%|█████████▉| 2969/3000 [00:01<00:00, 1645.09it/s]warmup should be done:  97%|█████████▋| 2903/3000 [00:01<00:00, 1612.89it/s]warmup should be done:  99%|█████████▉| 2967/3000 [00:01<00:00, 1623.48it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1645.29it/s]warmup should be done:  95%|█████████▍| 2849/3000 [00:01<00:00, 1584.09it/s]warmup should be done:  99%|█████████▉| 2970/3000 [00:01<00:00, 1574.04it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1634.84it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1632.39it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1618.89it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1615.01it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1600.40it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1563.57it/s]




warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]

warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   6%|▌         | 167/3000 [00:00<00:01, 1668.71it/s]warmup should be done:   6%|▌         | 169/3000 [00:00<00:01, 1687.92it/s]warmup should be done:   6%|▌         | 171/3000 [00:00<00:01, 1704.81it/s]warmup should be done:   5%|▌         | 162/3000 [00:00<00:01, 1613.70it/s]warmup should be done:   6%|▌         | 169/3000 [00:00<00:01, 1682.95it/s]warmup should be done:   6%|▌         | 165/3000 [00:00<00:01, 1643.58it/s]warmup should be done:   5%|▌         | 164/3000 [00:00<00:01, 1632.74it/s]warmup should be done:   6%|▌         | 165/3000 [00:00<00:01, 1641.01it/s]warmup should be done:  11%|█         | 335/3000 [00:00<00:01, 1672.65it/s]warmup should be done:  11%|█         | 327/3000 [00:00<00:01, 1632.89it/s]warmup should be done:  11%|█         | 332/3000 [00:00<00:01, 1657.11it/s]warmup should be done:  11%|█▏        | 340/3000 [00:00<00:01, 1695.55it/s]warmup should be done:  11%|█▏        | 339/3000 [00:00<00:01, 1689.19it/s]warmup should be done:  11%|█         | 329/3000 [00:00<00:01, 1639.82it/s]warmup should be done:  11%|█▏        | 342/3000 [00:00<00:01, 1679.75it/s]warmup should be done:  11%|█         | 330/3000 [00:00<00:01, 1621.61it/s]warmup should be done:  16%|█▋        | 494/3000 [00:00<00:01, 1643.05it/s]warmup should be done:  17%|█▋        | 503/3000 [00:00<00:01, 1671.92it/s]warmup should be done:  17%|█▋        | 511/3000 [00:00<00:01, 1700.28it/s]warmup should be done:  16%|█▋        | 492/3000 [00:00<00:01, 1636.19it/s]warmup should be done:  17%|█▋        | 500/3000 [00:00<00:01, 1663.08it/s]warmup should be done:  17%|█▋        | 508/3000 [00:00<00:01, 1673.43it/s]warmup should be done:  16%|█▋        | 493/3000 [00:00<00:01, 1616.93it/s]warmup should be done:  17%|█▋        | 511/3000 [00:00<00:01, 1673.87it/s]warmup should be done:  23%|██▎       | 682/3000 [00:00<00:01, 1702.95it/s]warmup should be done:  22%|██▏       | 657/3000 [00:00<00:01, 1640.33it/s]warmup should be done:  22%|██▏       | 659/3000 [00:00<00:01, 1643.33it/s]warmup should be done:  22%|██▏       | 672/3000 [00:00<00:01, 1675.64it/s]warmup should be done:  23%|██▎       | 676/3000 [00:00<00:01, 1674.93it/s]warmup should be done:  22%|██▏       | 667/3000 [00:00<00:01, 1651.78it/s]warmup should be done:  23%|██▎       | 682/3000 [00:00<00:01, 1687.07it/s]warmup should be done:  22%|██▏       | 658/3000 [00:00<00:01, 1627.36it/s]warmup should be done:  28%|██▊       | 844/3000 [00:00<00:01, 1690.53it/s]warmup should be done:  28%|██▊       | 853/3000 [00:00<00:01, 1700.90it/s]warmup should be done:  27%|██▋       | 824/3000 [00:00<00:01, 1641.37it/s]warmup should be done:  28%|██▊       | 835/3000 [00:00<00:01, 1659.79it/s]warmup should be done:  28%|██▊       | 853/3000 [00:00<00:01, 1694.73it/s]warmup should be done:  28%|██▊       | 825/3000 [00:00<00:01, 1641.91it/s]warmup should be done:  28%|██▊       | 844/3000 [00:00<00:01, 1661.97it/s]warmup should be done:  27%|██▋       | 822/3000 [00:00<00:01, 1615.22it/s]warmup should be done:  34%|███▍      | 1015/3000 [00:00<00:01, 1696.92it/s]warmup should be done:  33%|███▎      | 989/3000 [00:00<00:01, 1643.02it/s]warmup should be done:  34%|███▍      | 1024/3000 [00:00<00:01, 1701.44it/s]warmup should be done:  33%|███▎      | 993/3000 [00:00<00:01, 1654.59it/s]warmup should be done:  33%|███▎      | 1001/3000 [00:00<00:01, 1652.65it/s]warmup should be done:  34%|███▍      | 1024/3000 [00:00<00:01, 1697.85it/s]warmup should be done:  34%|███▍      | 1014/3000 [00:00<00:01, 1673.60it/s]warmup should be done:  33%|███▎      | 988/3000 [00:00<00:01, 1629.26it/s]warmup should be done:  40%|███▉      | 1185/3000 [00:00<00:01, 1696.20it/s]warmup should be done:  38%|███▊      | 1155/3000 [00:00<00:01, 1646.22it/s]warmup should be done:  40%|███▉      | 1195/3000 [00:00<00:01, 1700.89it/s]warmup should be done:  39%|███▊      | 1162/3000 [00:00<00:01, 1664.62it/s]warmup should be done:  40%|███▉      | 1195/3000 [00:00<00:01, 1699.47it/s]warmup should be done:  39%|███▉      | 1184/3000 [00:00<00:01, 1681.17it/s]warmup should be done:  38%|███▊      | 1155/3000 [00:00<00:01, 1642.16it/s]warmup should be done:  39%|███▉      | 1167/3000 [00:00<00:01, 1641.28it/s]warmup should be done:  45%|████▌     | 1357/3000 [00:00<00:00, 1701.57it/s]warmup should be done:  46%|████▌     | 1366/3000 [00:00<00:00, 1703.22it/s]warmup should be done:  44%|████▍     | 1320/3000 [00:00<00:01, 1641.44it/s]warmup should be done:  44%|████▍     | 1331/3000 [00:00<00:00, 1670.10it/s]warmup should be done:  46%|████▌     | 1367/3000 [00:00<00:00, 1704.28it/s]warmup should be done:  45%|████▌     | 1354/3000 [00:00<00:00, 1687.01it/s]warmup should be done:  44%|████▍     | 1322/3000 [00:00<00:01, 1647.89it/s]warmup should be done:  44%|████▍     | 1334/3000 [00:00<00:01, 1647.75it/s]warmup should be done:  51%|█████     | 1528/3000 [00:00<00:00, 1703.99it/s]warmup should be done:  51%|█████     | 1537/3000 [00:00<00:00, 1702.28it/s]warmup should be done:  51%|█████▏    | 1538/3000 [00:00<00:00, 1704.93it/s]warmup should be done:  50%|█████     | 1500/3000 [00:00<00:00, 1673.79it/s]warmup should be done:  51%|█████     | 1524/3000 [00:00<00:00, 1689.00it/s]warmup should be done:  50%|████▉     | 1485/3000 [00:00<00:00, 1631.36it/s]warmup should be done:  50%|████▉     | 1487/3000 [00:00<00:00, 1645.79it/s]warmup should be done:  50%|█████     | 1502/3000 [00:00<00:00, 1654.80it/s]warmup should be done:  57%|█████▋    | 1700/3000 [00:01<00:00, 1706.12it/s]warmup should be done:  57%|█████▋    | 1708/3000 [00:01<00:00, 1702.17it/s]warmup should be done:  56%|█████▌    | 1669/3000 [00:01<00:00, 1677.29it/s]warmup should be done:  57%|█████▋    | 1709/3000 [00:01<00:00, 1704.05it/s]warmup should be done:  56%|█████▋    | 1694/3000 [00:01<00:00, 1690.64it/s]warmup should be done:  55%|█████▌    | 1654/3000 [00:01<00:00, 1651.07it/s]warmup should be done:  55%|█████▍    | 1649/3000 [00:01<00:00, 1624.77it/s]warmup should be done:  56%|█████▌    | 1669/3000 [00:01<00:00, 1658.63it/s]warmup should be done:  62%|██████▏   | 1872/3000 [00:01<00:00, 1708.12it/s]warmup should be done:  63%|██████▎   | 1879/3000 [00:01<00:00, 1703.46it/s]warmup should be done:  61%|██████▏   | 1838/3000 [00:01<00:00, 1680.20it/s]warmup should be done:  62%|██████▏   | 1864/3000 [00:01<00:00, 1692.41it/s]warmup should be done:  63%|██████▎   | 1881/3000 [00:01<00:00, 1706.47it/s]warmup should be done:  61%|██████    | 1826/3000 [00:01<00:00, 1669.88it/s]warmup should be done:  61%|██████    | 1835/3000 [00:01<00:00, 1657.45it/s]warmup should be done:  60%|██████    | 1812/3000 [00:01<00:00, 1619.49it/s]warmup should be done:  68%|██████▊   | 2044/3000 [00:01<00:00, 1709.84it/s]warmup should be done:  68%|██████▊   | 2050/3000 [00:01<00:00, 1705.12it/s]warmup should be done:  67%|██████▋   | 2007/3000 [00:01<00:00, 1679.87it/s]warmup should be done:  68%|██████▊   | 2034/3000 [00:01<00:00, 1692.45it/s]warmup should be done:  68%|██████▊   | 2053/3000 [00:01<00:00, 1708.65it/s]warmup should be done:  67%|██████▋   | 1997/3000 [00:01<00:00, 1681.30it/s]warmup should be done:  66%|██████▌   | 1974/3000 [00:01<00:00, 1614.56it/s]warmup should be done:  67%|██████▋   | 2001/3000 [00:01<00:00, 1645.11it/s]warmup should be done:  74%|███████▍  | 2215/3000 [00:01<00:00, 1703.19it/s]warmup should be done:  74%|███████▍  | 2221/3000 [00:01<00:00, 1699.12it/s]warmup should be done:  72%|███████▎  | 2175/3000 [00:01<00:00, 1679.82it/s]warmup should be done:  74%|███████▍  | 2224/3000 [00:01<00:00, 1708.55it/s]warmup should be done:  73%|███████▎  | 2204/3000 [00:01<00:00, 1691.79it/s]warmup should be done:  72%|███████▏  | 2168/3000 [00:01<00:00, 1689.84it/s]warmup should be done:  71%|███████   | 2136/3000 [00:01<00:00, 1614.57it/s]warmup should be done:  72%|███████▏  | 2166/3000 [00:01<00:00, 1635.14it/s]warmup should be done:  80%|███████▉  | 2386/3000 [00:01<00:00, 1698.32it/s]warmup should be done:  80%|███████▉  | 2391/3000 [00:01<00:00, 1694.41it/s]warmup should be done:  78%|███████▊  | 2344/3000 [00:01<00:00, 1681.32it/s]warmup should be done:  80%|███████▉  | 2395/3000 [00:01<00:00, 1708.28it/s]warmup should be done:  79%|███████▉  | 2374/3000 [00:01<00:00, 1691.96it/s]warmup should be done:  78%|███████▊  | 2340/3000 [00:01<00:00, 1696.84it/s]warmup should be done:  77%|███████▋  | 2304/3000 [00:01<00:00, 1633.64it/s]warmup should be done:  78%|███████▊  | 2334/3000 [00:01<00:00, 1645.99it/s]warmup should be done:  85%|████████▌ | 2556/3000 [00:01<00:00, 1697.76it/s]warmup should be done:  85%|████████▌ | 2561/3000 [00:01<00:00, 1693.38it/s]warmup should be done:  84%|████████▍ | 2513/3000 [00:01<00:00, 1683.33it/s]warmup should be done:  86%|████████▌ | 2567/3000 [00:01<00:00, 1709.88it/s]warmup should be done:  85%|████████▍ | 2544/3000 [00:01<00:00, 1693.52it/s]warmup should be done:  84%|████████▎ | 2511/3000 [00:01<00:00, 1698.41it/s]warmup should be done:  82%|████████▎ | 2475/3000 [00:01<00:00, 1654.89it/s]warmup should be done:  83%|████████▎ | 2499/3000 [00:01<00:00, 1640.00it/s]warmup should be done:  91%|█████████ | 2726/3000 [00:01<00:00, 1696.37it/s]warmup should be done:  91%|█████████ | 2731/3000 [00:01<00:00, 1693.75it/s]warmup should be done:  89%|████████▉ | 2682/3000 [00:01<00:00, 1682.55it/s]warmup should be done:  91%|█████████▏| 2738/3000 [00:01<00:00, 1707.85it/s]warmup should be done:  90%|█████████ | 2714/3000 [00:01<00:00, 1692.43it/s]warmup should be done:  89%|████████▉ | 2681/3000 [00:01<00:00, 1689.49it/s]warmup should be done:  88%|████████▊ | 2645/3000 [00:01<00:00, 1666.83it/s]warmup should be done:  89%|████████▉ | 2664/3000 [00:01<00:00, 1632.18it/s]warmup should be done:  97%|█████████▋| 2896/3000 [00:01<00:00, 1694.94it/s]warmup should be done:  97%|█████████▋| 2901/3000 [00:01<00:00, 1691.53it/s]warmup should be done:  97%|█████████▋| 2909/3000 [00:01<00:00, 1705.16it/s]warmup should be done:  95%|█████████▌| 2851/3000 [00:01<00:00, 1680.19it/s]warmup should be done:  96%|█████████▌| 2884/3000 [00:01<00:00, 1689.28it/s]warmup should be done:  94%|█████████▍| 2816/3000 [00:01<00:00, 1679.46it/s]warmup should be done:  95%|█████████▌| 2850/3000 [00:01<00:00, 1682.60it/s]warmup should be done:  94%|█████████▍| 2829/3000 [00:01<00:00, 1636.17it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1701.90it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1697.72it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1695.87it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1686.29it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1668.76it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1664.40it/s]warmup should be done: 100%|█████████▉| 2988/3000 [00:01<00:00, 1689.63it/s]warmup should be done: 100%|█████████▉| 2995/3000 [00:01<00:00, 1640.92it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1648.81it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1646.32it/s]
WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7fc4069d20d0>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7fc4069e22e0>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7fc4069d01c0>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7fc407d27730>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7fc4069d2190>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7fc4069e0280>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7fc4069d21f0>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7fc407d26b20>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
2022-12-12 00:54:50.041603: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fbf3a834200 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 00:54:50.041666: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 00:54:50.051230: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 00:54:50.271535: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fbf4282cb30 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 00:54:50.271593: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 00:54:50.281856: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 00:54:50.545699: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fbf3a830e80 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 00:54:50.545760: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 00:54:50.555300: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 00:54:51.038075: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fbf42830170 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 00:54:51.038143: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 00:54:51.041509: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fbf3b029210 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 00:54:51.041552: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 00:54:51.046144: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 00:54:51.050539: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 00:54:51.114118: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fbf42834980 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 00:54:51.114181: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 00:54:51.121994: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 00:54:51.132871: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fbf3a796650 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 00:54:51.132922: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 00:54:51.142012: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 00:54:51.147963: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fbf3b035990 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 00:54:51.148026: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 00:54:51.155913: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 00:54:57.380676: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 00:54:57.403382: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 00:54:57.712069: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 00:54:57.799295: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 00:54:57.810110: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 00:54:57.981413: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 00:54:58.042419: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 00:54:58.124784: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
[HCTR][00:55:55.779][ERROR][RK0][main]: replica 7 reaches 1000, calling init pre replica
[HCTR][00:55:55.779][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][00:55:55.784][ERROR][RK0][main]: coll ps creation done
[HCTR][00:55:55.784][ERROR][RK0][main]: replica 7 waits for coll ps creation barrier
[HCTR][00:55:56.047][ERROR][RK0][tid #140459590547200]: replica 6 reaches 1000, calling init pre replica
[HCTR][00:55:56.047][ERROR][RK0][tid #140459590547200]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][00:55:56.052][ERROR][RK0][tid #140459590547200]: coll ps creation done
[HCTR][00:55:56.052][ERROR][RK0][tid #140459590547200]: replica 6 waits for coll ps creation barrier
[HCTR][00:55:56.174][ERROR][RK0][main]: replica 1 reaches 1000, calling init pre replica
[HCTR][00:55:56.174][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][00:55:56.178][ERROR][RK0][tid #140459515045632]: replica 3 reaches 1000, calling init pre replica
[HCTR][00:55:56.178][ERROR][RK0][tid #140459515045632]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][00:55:56.179][ERROR][RK0][main]: coll ps creation done
[HCTR][00:55:56.179][ERROR][RK0][main]: replica 1 waits for coll ps creation barrier
[HCTR][00:55:56.181][ERROR][RK0][tid #140459590547200]: replica 0 reaches 1000, calling init pre replica
[HCTR][00:55:56.181][ERROR][RK0][tid #140459590547200]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][00:55:56.183][ERROR][RK0][tid #140459515045632]: coll ps creation done
[HCTR][00:55:56.183][ERROR][RK0][tid #140459515045632]: replica 3 waits for coll ps creation barrier
[HCTR][00:55:56.185][ERROR][RK0][tid #140459590547200]: coll ps creation done
[HCTR][00:55:56.185][ERROR][RK0][tid #140459590547200]: replica 0 waits for coll ps creation barrier
[HCTR][00:55:56.228][ERROR][RK0][main]: replica 2 reaches 1000, calling init pre replica
[HCTR][00:55:56.228][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][00:55:56.237][ERROR][RK0][main]: coll ps creation done
[HCTR][00:55:56.237][ERROR][RK0][main]: replica 2 waits for coll ps creation barrier
[HCTR][00:55:56.424][ERROR][RK0][tid #140459590547200]: replica 4 reaches 1000, calling init pre replica
[HCTR][00:55:56.424][ERROR][RK0][tid #140459590547200]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][00:55:56.429][ERROR][RK0][tid #140459590547200]: coll ps creation done
[HCTR][00:55:56.429][ERROR][RK0][tid #140459590547200]: replica 4 waits for coll ps creation barrier
[HCTR][00:55:56.511][ERROR][RK0][main]: replica 5 reaches 1000, calling init pre replica
[HCTR][00:55:56.511][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][00:55:56.518][ERROR][RK0][main]: coll ps creation done
[HCTR][00:55:56.518][ERROR][RK0][main]: replica 5 waits for coll ps creation barrier
[HCTR][00:55:56.518][ERROR][RK0][tid #140459590547200]: replica 0 preparing frequency
[HCTR][00:55:57.395][ERROR][RK0][tid #140459590547200]: replica 0 preparing frequency done
[HCTR][00:55:57.430][ERROR][RK0][tid #140459590547200]: replica 0 calling init per replica
[HCTR][00:55:57.430][ERROR][RK0][main]: replica 2 calling init per replica
[HCTR][00:55:57.430][ERROR][RK0][tid #140459515045632]: replica 3 calling init per replica
[HCTR][00:55:57.430][ERROR][RK0][main]: replica 5 calling init per replica
[HCTR][00:55:57.430][ERROR][RK0][main]: replica 7 calling init per replica
[HCTR][00:55:57.430][ERROR][RK0][tid #140459590547200]: replica 6 calling init per replica
[HCTR][00:55:57.430][ERROR][RK0][main]: replica 1 calling init per replica
[HCTR][00:55:57.430][ERROR][RK0][tid #140459590547200]: replica 4 calling init per replica
[HCTR][00:55:57.430][ERROR][RK0][tid #140459590547200]: Calling build_v2
[HCTR][00:55:57.430][ERROR][RK0][main]: Calling build_v2
[HCTR][00:55:57.430][ERROR][RK0][tid #140459515045632]: Calling build_v2
[HCTR][00:55:57.430][ERROR][RK0][main]: Calling build_v2
[HCTR][00:55:57.430][ERROR][RK0][main]: Calling build_v2
[HCTR][00:55:57.430][ERROR][RK0][tid #140459590547200]: Calling build_v2
[HCTR][00:55:57.430][ERROR][RK0][main]: Calling build_v2
[HCTR][00:55:57.430][ERROR][RK0][tid #140459590547200]: Calling build_v2
[HCTR][00:55:57.430][ERROR][RK0][tid #140459590547200]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][00:55:57.430][ERROR][RK0][tid #140459515045632]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][00:55:57.430][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][00:55:57.430][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][00:55:57.430][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][00:55:57.430][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][00:55:57.430][ERROR][RK0][tid #140459590547200]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][00:55:57.430][ERROR][RK0][tid #140459590547200]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[[[[[[2022-12-12 00:55:572022-12-12 00:55:572022-12-12 00:55:57.2022-12-12 00:55:57..2022-12-12 00:55:572022-12-12 00:55:57[430318.430318430318[..: 430318: : 2022-12-12 00:55:57430330430318E: EE.: :  E2022-12-12 00:55:57  430348EE/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc ./hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc:   :/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc430369::E/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc136:: 136136 ::] 136E] ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc136136using concurrent impl MPSPhase]  using concurrent impl MPSPhaseusing concurrent impl MPSPhase:] ] 
using concurrent impl MPSPhase/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc

136using concurrent impl MPSPhaseusing concurrent impl MPSPhase
:] 

136using concurrent impl MPSPhase] 
using concurrent impl MPSPhase
[2022-12-12 00:55:57.434997: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:178] v100x8, slow pcie
[2022-12-12 00:55:57.435036: E[ 2022-12-12 00:55:57/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.:435041196: ] Eassigning 8 to cpu 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:178] v100x8, slow pcie
[[2022-12-12 00:55:572022-12-12 00:55:57..435086435091: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc::178[196] 2022-12-12 00:55:57] v100x8, slow pcie.assigning 8 to cpu
435137
: E [/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 00:55:57:.212435172] : build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8E
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:196] assigning 8 to cpu
[[2022-12-12 00:55:57[2022-12-12 00:55:57.2022-12-12 00:55:57.435210.435206: 435216: E: E E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:212:[178] 2132022-12-12 00:55:57] build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8] .v100x8, slow pcie
remote time is 8.68421435261

: E [/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[[2022-12-12 00:55:57:2022-12-12 00:55:572022-12-12 00:55:57.212..435299] 435301435302: build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8: : E
EE   /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[:::2022-12-12 00:55:57196213214.] ] ] 435354assigning 8 to cpuremote time is 8.68421cpu time is 97.0588: 


E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[:2022-12-12 00:55:57213.] 435401remote time is 8.68421: 
E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:[2142022-12-12 00:55:57[] .2022-12-12 00:55:57cpu time is 97.0588435427.
: 435430E:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc :/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc214:] 212cpu time is 97.0588] 
build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8
[2022-12-12 00:55:57.435495: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:213] remote time is 8.68421
[2022-12-12 00:55:57.435524: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:214] cpu time is 97.0588
[2022-12-12 00:55:57.435700: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:178] v100x8, slow pcie
[2022-12-12 00:55:57.435738: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:196[] 2022-12-12 00:55:57assigning 8 to cpu.
435750: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:178] v100x8, slow pcie
[2022-12-12 00:55:57.435792: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[:[2022-12-12 00:55:571962022-12-12 00:55:57.] .435799assigning 8 to cpu435809: 
: EE [ /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 00:55:57/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:[.:1782022-12-12 00:55:57435855212] .: ] v100x8, slow pcie435896: Ebuild asymm link desc with 8X Tesla V100-SXM2-32GB out of 8
E 
 [/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 00:55:57[::.2022-12-12 00:55:57178212435980.] ] : 435999v100x8, slow pciebuild asymm link desc with 8X Tesla V100-SXM2-32GB out of 8E: 

 E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc [[:/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 00:55:572022-12-12 00:55:57196:..] 213436098436102assigning 8 to cpu] : : 
remote time is 8.68421EE
  /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:2022-12-12 00:55:57:.196213436216[] ] : 2022-12-12 00:55:57assigning 8 to cpuremote time is 8.68421E.

 436246/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: [:E2022-12-12 00:55:57214 .] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc436305cpu time is 97.0588[:: 
2022-12-12 00:55:57212E.]  436351build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: 
:E214 ] [/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cccpu time is 97.05882022-12-12 00:55:57:
.212436434] : build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8E
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:213] remote time is 8.68421
[[2022-12-12 00:55:572022-12-12 00:55:57..436496436498: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc::213214] ] remote time is 8.68421cpu time is 97.0588

[2022-12-12 00:55:57.436574: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:214] cpu time is 97.0588
[2022-12-12 00:57:14.921655: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:118] solver created. now build & solve
[2022-12-12 00:57:14.961710: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 381.47 MB
[2022-12-12 00:57:14.961797: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 381.47 MB
[2022-12-12 00:57:14.962910: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:74] mapping nid to rank...
[2022-12-12 00:57:15. 34392: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:91] counting slots...
[2022-12-12 00:57:15.412889: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:105] Final num slot is 49
[2022-12-12 00:57:15.412977: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:109] counting blocks...
[2022-12-12 00:57:21.981449: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:118] Final num block is 1024
[2022-12-12 00:57:21.981541: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:123] counting freq and density...
[2022-12-12 00:57:23.693031: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:154] averaging freq and density...
[2022-12-12 00:57:23.693132: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:155] 1024
[2022-12-12 00:57:23.696219: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:123] solver built. now solve
[2022-12-12 00:57:23.696283: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:354] constructing optimal solver, device=8, stream=1
1024 blocks, 8 devices
[2022-12-12 00:57:23.960107: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:527] Add Var...
[2022-12-12 00:57:23.988035: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:545] Capacity...
[2022-12-12 00:57:23.989474: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:548] Connect CPU...
[2022-12-12 00:57:24.  9962: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:550] Connect Access To Storage...
[2022-12-12 00:57:24.532444: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:554] Time...
[2022-12-12 00:57:24.534685: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:516] dst 0, total sm is 80
[2022-12-12 00:57:24.537679: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:516] dst 1, total sm is 80
[2022-12-12 00:57:24.540540: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:516] dst 2, total sm is 80
[2022-12-12 00:57:24.543417: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:516] dst 3, total sm is 80
[2022-12-12 00:57:24.546279: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:516] dst 4, total sm is 80
[2022-12-12 00:57:24.549145: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:516] dst 5, total sm is 80
[2022-12-12 00:57:24.552014: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:516] dst 6, total sm is 80
[2022-12-12 00:57:24.554868: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:516] dst 7, total sm is 80
[2022-12-12 00:58:23.718487: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:569] Coll Cache init block placement array
[2022-12-12 00:58:23.726916: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:645] Coll Cache init block placement array done
[2022-12-12 00:58:23.729606: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:647] Coll Cache model reset done
[2022-12-12 00:58:23.774733: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:127] solver solved
[2022-12-12 00:58:23.774836: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:195] 0 solved master
[2022-12-12 00:58:23.774871: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 0 solved
[2022-12-12 00:58:23.774902: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 0 initing device 0
[2022-12-12 00:58:23.775561: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-12 00:58:23.775620: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 00:58:23.776704: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 00:58:23.777381: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 00:58:23.790160: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 7 solved
[2022-12-12 00:58:23.790240: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 7 initing device 7
[[2022-12-12 00:58:232022-12-12 00:58:23..790429790430: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc::202202] ] 5 solved
1 solved
[2022-12-12 00:58:23[.2022-12-12 00:58:23790521.: 790526E:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc :/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc205:] 205worker 0 thread 5 initing device 5] 
worker 0 thread 1 initing device 1
[2022-12-12 00:58:23.790580: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 4 solved
[2022-12-12 00:58:23.790635: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 4 initing device 4[
2022-12-12 00:58:23.790631[: 2022-12-12 00:58:23E. 790683/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc: :E202 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:3 solved1815
] Building Coll Cache with ... num gpu device is 8
[2022-12-12 00:58:23.790766: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] [worker 0 thread 3 initing device 32022-12-12 00:58:23
.790789: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 00:58:23.790948: E [/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu2022-12-12 00:58:23:.1815790962] : Building Coll Cache with ... num gpu device is 8E
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8[
2022-12-12 00:58:23.791008: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980[] 2022-12-12 00:58:23eager alloc mem 381.47 MB.
791034: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 00:58:23.791063: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-12 00:58:23.791105: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 00:58:23.791246: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-12 00:58:23.791299: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 00:58:23.791852: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 2 solved
[2022-12-12 00:58:23.791905: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 2 initing device 2
[2022-12-12 00:58:23.791985: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 6 solved
[2022-12-12 00:58:23.792046: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 6 initing device 6
[2022-12-12 00:58:23.792308: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-12 00:58:23.792350: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 00:58:23.792478: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-12 00:58:23.792523: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 00:58:23.795252: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 00:58:23.795306: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 00:58:23.795365: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 00:58:23.795591: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 00:58:23.795671: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 00:58:23.796591: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 00:58:23.797097: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 00:58:23.799593: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 00:58:23.799645: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 00:58:23.799755: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 00:58:23.799872: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 00:58:23.799939: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 00:58:23.800421: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 00:58:23.800917: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 00:58:23.856084: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1024.00 Bytes
[2022-12-12 00:58:23.861326: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-12 00:58:23.861433: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 00:58:23.862247: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 15.64 MB
[2022-12-12 00:58:23.862827: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:58:23.863842: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:58:23.863892: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 15.25 MB
[2022-12-12 00:58:23.867554: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 00:58:23.868270: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 00:58:23.868311: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1.91 GB
[2022-12-12 00:58:23.881013: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1024.00 Bytes
[2022-12-12 00:58:23.[[881141[2022-12-12 00:58:232022-12-12 00:58:23: 2022-12-12 00:58:23..E.881174881174 881174: : /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu: EE:E  1980 /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu] /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::eager alloc mem 1024.00 Bytes:19801980
1980] ] ] eager alloc mem 1024.00 Byteseager alloc mem 1024.00 Byteseager alloc mem 1024.00 Bytes


[2022-12-12 00:58:23.881295: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1024.00 Bytes
[2022-12-12 00:58:23.881371: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1024.00 Bytes
[2022-12-12 00:58:23.885885: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-12 00:58:23.885967: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 00:58:23.887150: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 15.64 MB
[2022-12-12 00:58:23.887608: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc[:2022-12-12 00:58:23638.] 887641eager release cuda mem 1024: 
E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:58:23.887728: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 00:58:23.887786: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[[2022-12-12 00:58:232022-12-12 00:58:23..887864887877: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::638638] ] eager release cuda mem 1024eager release cuda mem 400000000

[2022-12-12 00:58:23.887943: [E2022-12-12 00:58:23 ./hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc887967:: 638E]  eager release cuda mem 1024/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc
:638] eager release cuda mem 400000000
[2022-12-12 00:58:23.888038: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638[] 2022-12-12 00:58:23eager release cuda mem 400000000.
888042: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-12 00:58:23.888124: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 00:58:23.888200: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-12 00:58:23.888323: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 00:58:23.888553: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 15.64 MB
[2022-12-12 00:58:23.888640: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:58:23.888689: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 15.14 MB
[2022-12-12 00:58:23.889534: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 15.64 MB
[2022-12-12 00:58:23.890395: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 15.64 MB
[2022-12-12 00:58:23.890960: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 15.64 MB
[2022-12-12 00:58:23.891507: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 15.64 MB
[2022-12-12 00:58:23.892151: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 15.64 MB
[2022-12-12 00:58:23.892674: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:58:23.893065: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:58:23.893362: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:58:23.893408: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:58:23.893467: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:58:23.893556: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:58:23.893653: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:58:23.893696: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 15.25 MB
[2022-12-12 00:58:23.894034: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:58:23.894078: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 15.23 MB
[2022-12-12 00:58:23.894341: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[[2022-12-12 00:58:232022-12-12 00:58:23..894382894386: : EW  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc::63843] ] eager release cuda mem 625663WORKER[0] alloc host memory 15.21 MB

[2022-12-12 00:58:23.[8944472022-12-12 00:58:23: .E894455 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccW: 638/hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc] :eager release cuda mem 62566343
] WORKER[0] alloc host memory 15.24 MB
[2022-12-12 00:58:23.894511: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 15.24 MB
[2022-12-12 00:58:23.894571: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:58:23.894645: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 15.25 MB
[2022-12-12 00:58:23.900236: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 00:58:23.900852: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 00:58:23.900898: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1.90 GB
[2022-12-12 00:58:23.904857: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 00:58:23.905127: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 00:58:23.905470: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 00:58:23.905484: E [/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu2022-12-12 00:58:23:.1980905513] : eager alloc mem 25.25 KBE
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1.91 GB
[2022-12-12 00:58:23.905743: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 00:58:23.905788: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1.91 GB
[2022-12-12 00:58:23.906099: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 00:58:23.906139: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1.91 GB
[2022-12-12 00:58:23.906309: E [/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu2022-12-12 00:58:23:.1980906327] : eager alloc mem 25.25 KBE
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 00:58:23.906931: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638[] 2022-12-12 00:58:23eager release cuda mem 25855.
906949: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 00:58:23.906982: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980[] 2022-12-12 00:58:23eager alloc mem 1.91 GB.
907000: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1.91 GB
[2022-12-12 00:58:23.907828: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 00:58:23.908455: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 00:58:23.908499: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1.91 GB
[[[[[[[[2022-12-12 00:58:242022-12-12 00:58:242022-12-12 00:58:242022-12-12 00:58:242022-12-12 00:58:242022-12-12 00:58:242022-12-12 00:58:242022-12-12 00:58:24.......539512.539512539513539513539513539512539513: 539521: : : : : : E: EEEEEE E      /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::::::1926:192619261926192619261926] 1926] ] ] ] ] ] Device 2 init p2p of link 1] Device 4 init p2p of link 5Device 7 init p2p of link 4Device 1 init p2p of link 7Device 6 init p2p of link 0Device 5 init p2p of link 6Device 3 init p2p of link 2
Device 0 init p2p of link 3






[[[2022-12-12 00:58:24[2022-12-12 00:58:242022-12-12 00:58:24[.[2022-12-12 00:58:24..2022-12-12 00:58:245400832022-12-12 00:58:24.[540084540083.: .5400912022-12-12 00:58:24: : 540094E540097: .EE:  : E540109  [E/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE : /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu2022-12-12 00:58:24 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE::./hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu1980/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu: 19801980540163:] :1980/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu] ] : 1980eager alloc mem 611.00 KB1980] :eager alloc mem 611.00 KBeager alloc mem 611.00 KBE] 
] eager alloc mem 611.00 KB1980

 eager alloc mem 611.00 KBeager alloc mem 611.00 KB

] /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu
eager alloc mem 611.00 KB:
1980] eager alloc mem 611.00 KB
[[2022-12-12 00:58:242022-12-12 00:58:24..541137541139: [: E2022-12-12 00:58:24E . /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc541154/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:[: :[6382022-12-12 00:58:24E[6382022-12-12 00:58:24] . 2022-12-12 00:58:24] .[eager release cuda mem 625663541176/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc.eager release cuda mem 6256635411812022-12-12 00:58:24
: :541187
: .E638: E[541201 ] E 2022-12-12 00:58:24: /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cceager release cuda mem 625663 /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc.E:
/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:541252 638:638: /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc] 638] E:eager release cuda mem 625663] eager release cuda mem 625663
 638
eager release cuda mem 625663/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc] 
:eager release cuda mem 625663638
] eager release cuda mem 625663
[2022-12-12 00:58:24.554943: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 2
[2022-12-12 00:58:24.555097: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:58:24.555701: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 7 init p2p of link 1
[2022-12-12 00:58:24.555843: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:58:24.555934[: 2022-12-12 00:58:24E. 555931/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc: :E638 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cueager release cuda mem 625663:
1926] Device 2 init p2p of link 3
[2022-12-12 00:58:24.556109: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:58:24.556135: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 6 init p2p of link 5
[2022-12-12 00:58:24.556206: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:[19262022-12-12 00:58:24] .Device 4 init p2p of link 7556224
: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 5 init p2p of link 4
[2022-12-12 00:58:24.556284: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:58:24.556360: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:58:24.556397: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:58:24.556624: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:58:24.556746: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 3 init p2p of link 0
[2022-12-12 00:58:24.556896: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:58:24.556925: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:58:24[.2022-12-12 00:58:24557076.: 557062E:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc :/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu638:] 1926[eager release cuda mem 625663] 2022-12-12 00:58:24
Device 0 init p2p of link 6.
557159: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:[6382022-12-12 00:58:24] .eager release cuda mem 625663557206
: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:58:24.557321: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:58:24.557706: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:58:24.558168: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:58:24.568443: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 3
[2022-12-12 00:58:24.568569: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:58:24.569307: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 7 init p2p of link 6
[2022-12-12 00:58:24.569392: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:58:24.569425: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:58:24.570238: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:58:24.570295: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 2 init p2p of link 0
[2022-12-12 00:58:24.570420: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:58:24.570507: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 6 init p2p of link 4
[2022-12-12 00:58:24.570623: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:58:24.570726: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 4 init p2p of link 2
[2022-12-12 00:58:24.570840: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:58:24.570989: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 5 init p2p of link 7
[2022-12-12 00:58:24.571102: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:58:24.571148: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 0 init p2p of link 1
[2022-12-12 00:58:24.571218: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:58:24.571262: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 3 init p2p of link 5
[2022-12-12 00:58:24.571317: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:58:24.571386: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:58:24.571436: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:58:24.571627: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:58:24.571914: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[[2022-12-12 00:58:242022-12-12 00:58:24..572171572179: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::638638] ] eager release cuda mem 625663eager release cuda mem 625663

[2022-12-12 00:58:24.586595: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 0
[2022-12-12 00:58:24.586715: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:58:24.587009: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 7 init p2p of link 5
[2022-12-12 00:58:24.587102[: 2022-12-12 00:58:24E. 587123/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu: :E1926 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuDevice 4 init p2p of link 6:
1980] eager alloc mem 611.00 KB
[2022-12-12 00:58:24.587245: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:58:24.587525: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:58:24.587740: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 2 init p2p of link 4
[2022-12-12 00:58:24.587799: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 6 init p2p of link 7
[2022-12-12 00:58:24.587854: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:58:24.587913: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:58:24.587978: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[[2022-12-12 00:58:242022-12-12 00:58:24..588050588044: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::6381926] ] eager release cuda mem 625663Device 3 init p2p of link 1

[2022-12-12 00:58:24.588218: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:58:24.588545: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 5 init p2p of link 3
[2022-12-12 00:58:24.588635: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663[
2022-12-12 00:58:24.588658: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:58:24.588710: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:58:24.589023: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:58:24.589129: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 0 init p2p of link 2
[2022-12-12 00:58:24.589267: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:58:24.589442: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:58:24.590068: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:58:24.603516: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 16399996
[2022-12-12 00:58:24.603731: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 16399996
[2022-12-12 00:58:24.603982: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 3995797 / 100000000 nodes ( 4.00 %~4.00 %) | remote 11911688 / 100000000 nodes ( 11.91 %) | cpu 84092515 / 100000000 nodes ( 84.09 %) | 1.91 GB | 0.812952 secs 
[2022-12-12 00:58:24.604089: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 16399996
[2022-12-12 00:58:24.604171: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 3991342 / 100000000 nodes ( 3.99 %~4.00 %) | remote 11916143 / 100000000 nodes ( 11.92 %) | cpu 84092515 / 100000000 nodes ( 84.09 %) | 1.91 GB | 0.811828 secs 
[2022-12-12 00:58:24.604274: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 16399996
[2022-12-12 00:58:24.604424: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 3996053 / 100000000 nodes ( 4.00 %~4.00 %) | remote 11911432 / 100000000 nodes ( 11.91 %) | cpu 84092515 / 100000000 nodes ( 84.09 %) | 1.91 GB | 0.81365 secs 
[2022-12-12 00:58:24.604579: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 16399996
[2022-12-12 00:58:24.604695: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 3987177 / 100000000 nodes ( 3.99 %~4.00 %) | remote 11920308 / 100000000 nodes ( 11.92 %) | cpu 84092515 / 100000000 nodes ( 84.09 %) | 1.91 GB | 0.813597 secs 
[2022-12-12 00:58:24.604788: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 16399996
[2022-12-12 00:58:24.604949: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 3998222 / 100000000 nodes ( 4.00 %~4.00 %) | remote 11909263 / 100000000 nodes ( 11.91 %) | cpu 84092515 / 100000000 nodes ( 84.09 %) | 1.91 GB | 0.81395 secs 
[2022-12-12 00:58:24.605208: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 16399996
[2022-12-12 00:58:24.605760: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 3996982 / 100000000 nodes ( 4.00 %~4.00 %) | remote 11910503 / 100000000 nodes ( 11.91 %) | cpu 84092515 / 100000000 nodes ( 84.09 %) | 1.91 GB | 0.813246 secs 
[2022-12-12 00:58:24.605817: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 16399996
[2022-12-12 00:58:24.608498: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 3968281 / 100000000 nodes ( 3.97 %~4.00 %) | remote 11939204 / 100000000 nodes ( 11.94 %) | cpu 84092515 / 100000000 nodes ( 84.09 %) | 1.90 GB | 0.817212 secs 
[2022-12-12 00:58:24.609482: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 3998138 / 100000000 nodes ( 4.00 %~4.00 %) | remote 11909347 / 100000000 nodes ( 11.91 %) | cpu 84092515 / 100000000 nodes ( 84.09 %) | 1.91 GB | 0.833875 secs 
[2022-12-12 00:58:24.611457: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2213] before create ctx, mem is 10.23 GB
[2022-12-12 00:58:25.976116: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2219] after create ctx, mem is 10.49 GB
[2022-12-12 00:58:25.976600: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2226] after create stream, mem is 10.49 GB
[2022-12-12 00:58:25.977434: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2213] before create ctx, mem is 10.49 GB
[2022-12-12 00:58:27.331324: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2219] after create ctx, mem is 10.76 GB
[2022-12-12 00:58:27.331458: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2226] after create stream, mem is 10.76 GB
[2022-12-12 00:58:27.332518: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2213] before create ctx, mem is 10.76 GB
[2022-12-12 00:58:28.699177: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2219] after create ctx, mem is 10.97 GB
[2022-12-12 00:58:28.704945: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2226] after create stream, mem is 10.97 GB
[2022-12-12 00:58:28.710613: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2213] before create ctx, mem is 10.97 GB
[2022-12-12 00:58:30.102005: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2219] after create ctx, mem is 11.19 GB
[2022-12-12 00:58:30.102289: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2226] after create stream, mem is 11.19 GB
[2022-12-12 00:58:30.102646: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2243] before create stream, mem is 11.19 GB
[2022-12-12 00:58:30.102947: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2249] after create stream, mem is 11.19 GB
[2022-12-12 00:58:30.103414: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2213] before create ctx, mem is 11.19 GB
[2022-12-12 00:58:31.346872: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2219] after create ctx, mem is 11.38 GB
[2022-12-12 00:58:31.347591: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2226] after create stream, mem is 11.38 GB
[HCTR][00:58:32.283][ERROR][RK0][main]: replica 7 calling init per replica done, doing barrier
[HCTR][00:58:32.283][ERROR][RK0][main]: replica 2 calling init per replica done, doing barrier
[HCTR][00:58:32.283][ERROR][RK0][tid #140459590547200]: replica 4 calling init per replica done, doing barrier
[HCTR][00:58:32.283][ERROR][RK0][main]: replica 1 calling init per replica done, doing barrier
[HCTR][00:58:32.283][ERROR][RK0][tid #140459590547200]: replica 6 calling init per replica done, doing barrier
[HCTR][00:58:32.283][ERROR][RK0][main]: replica 5 calling init per replica done, doing barrier
[HCTR][00:58:32.283][ERROR][RK0][tid #140459515045632]: replica 3 calling init per replica done, doing barrier
[HCTR][00:58:32.283][ERROR][RK0][tid #140459590547200]: replica 0 calling init per replica done, doing barrier
[HCTR][00:58:32.283][ERROR][RK0][main]: replica 7 calling init per replica done, doing barrier done
[HCTR][00:58:32.283][ERROR][RK0][main]: replica 1 calling init per replica done, doing barrier done
[HCTR][00:58:32.283][ERROR][RK0][main]: replica 2 calling init per replica done, doing barrier done
[HCTR][00:58:32.283][ERROR][RK0][tid #140459590547200]: replica 0 calling init per replica done, doing barrier done
[HCTR][00:58:32.283][ERROR][RK0][tid #140459590547200]: replica 4 calling init per replica done, doing barrier done
[HCTR][00:58:32.283][ERROR][RK0][tid #140459515045632]: replica 3 calling init per replica done, doing barrier done
[HCTR][00:58:32.283][ERROR][RK0][main]: replica 5 calling init per replica done, doing barrier done
[HCTR][00:58:32.283][ERROR][RK0][tid #140459590547200]: replica 6 calling init per replica done, doing barrier done
[HCTR][00:58:32.283][ERROR][RK0][main]: init per replica done
[HCTR][00:58:32.283][ERROR][RK0][main]: init per replica done
[HCTR][00:58:32.283][ERROR][RK0][main]: init per replica done
[HCTR][00:58:32.283][ERROR][RK0][tid #140459590547200]: init per replica done
[HCTR][00:58:32.283][ERROR][RK0][tid #140459515045632]: init per replica done
[HCTR][00:58:32.283][ERROR][RK0][main]: init per replica done
[HCTR][00:58:32.283][ERROR][RK0][tid #140459590547200]: init per replica done
[HCTR][00:58:32.286][ERROR][RK0][tid #140459590547200]: init per replica done
[HCTR][00:58:32.321][ERROR][RK0][tid #140459590547200]: 7 allocated 3276800 at 0x7fa308238400
[HCTR][00:58:32.321][ERROR][RK0][tid #140459590547200]: 7 allocated 6553600 at 0x7fa308558400
[HCTR][00:58:32.321][ERROR][RK0][tid #140459590547200]: 7 allocated 3276800 at 0x7fa308b98400
[HCTR][00:58:32.321][ERROR][RK0][tid #140459590547200]: 7 allocated 6553600 at 0x7fa308eb8400
[HCTR][00:58:32.321][ERROR][RK0][tid #140460077061888]: 2 allocated 3276800 at 0x7fa388238400
[HCTR][00:58:32.322][ERROR][RK0][tid #140460077061888]: 2 allocated 6553600 at 0x7fa388558400
[HCTR][00:58:32.322][ERROR][RK0][tid #140460077061888]: 2 allocated 3276800 at 0x7fa388b98400
[HCTR][00:58:32.322][ERROR][RK0][tid #140460077061888]: 2 allocated 6553600 at 0x7fa388eb8400
[HCTR][00:58:32.322][ERROR][RK0][main]: 1 allocated 3276800 at 0x7fa398238400
[HCTR][00:58:32.322][ERROR][RK0][tid #140459515045632]: 3 allocated 3276800 at 0x7fa398238400
[HCTR][00:58:32.322][ERROR][RK0][main]: 1 allocated 6553600 at 0x7fa398558400
[HCTR][00:58:32.322][ERROR][RK0][tid #140459515045632]: 3 allocated 6553600 at 0x7fa398558400
[HCTR][00:58:32.322][ERROR][RK0][main]: 1 allocated 3276800 at 0x7fa398b98400
[HCTR][00:58:32.322][ERROR][RK0][main]: 4 allocated 3276800 at 0x7fa2a0238400
[HCTR][00:58:32.322][ERROR][RK0][tid #140459515045632]: 3 allocated 3276800 at 0x7fa398b98400
[HCTR][00:58:32.322][ERROR][RK0][main]: 1 allocated 6553600 at 0x7fa398eb8400
[HCTR][00:58:32.322][ERROR][RK0][main]: 4 allocated 6553600 at 0x7fa2a0558400
[HCTR][00:58:32.322][ERROR][RK0][tid #140459515045632]: 3 allocated 6553600 at 0x7fa398eb8400
[HCTR][00:58:32.322][ERROR][RK0][main]: 4 allocated 3276800 at 0x7fa2a0b98400
[HCTR][00:58:32.322][ERROR][RK0][main]: 4 allocated 6553600 at 0x7fa2a0eb8400
[HCTR][00:58:32.322][ERROR][RK0][main]: 6 allocated 3276800 at 0x7fa340238400
[HCTR][00:58:32.322][ERROR][RK0][main]: 6 allocated 6553600 at 0x7fa340558400
[HCTR][00:58:32.322][ERROR][RK0][main]: 6 allocated 3276800 at 0x7fa340b98400
[HCTR][00:58:32.322][ERROR][RK0][main]: 6 allocated 6553600 at 0x7fa340eb8400
[HCTR][00:58:32.322][ERROR][RK0][main]: 5 allocated 3276800 at 0x7fa398238400
[HCTR][00:58:32.322][ERROR][RK0][main]: 5 allocated 6553600 at 0x7fa398558400
[HCTR][00:58:32.322][ERROR][RK0][main]: 5 allocated 3276800 at 0x7fa398b98400
[HCTR][00:58:32.322][ERROR][RK0][main]: 5 allocated 6553600 at 0x7fa398eb8400
[HCTR][00:58:32.325][ERROR][RK0][tid #140459590547200]: 0 allocated 3276800 at 0x7fa27c320000
[HCTR][00:58:32.325][ERROR][RK0][tid #140459590547200]: 0 allocated 6553600 at 0x7fa27c640000
[HCTR][00:58:32.325][ERROR][RK0][tid #140459590547200]: 0 allocated 3276800 at 0x7fa27cc80000
[HCTR][00:58:32.325][ERROR][RK0][tid #140459590547200]: 0 allocated 6553600 at 0x7fa27cfa0000
