2022-12-12 07:58:37.415803: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.422885: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.431216: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.436046: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.441291: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.453230: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.460080: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.472391: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.525282: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.531003: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.533648: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.534521: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.535850: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.537886: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.538711: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.539069: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.540519: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.540602: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.542084: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.542140: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.543537: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.543634: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.545015: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.545180: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.546357: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.546751: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.547665: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.548312: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.548971: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.549972: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.550475: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.552200: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.554101: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.555273: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.556232: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.557158: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.558137: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.559141: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.560166: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.561183: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.566501: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 07:58:37.568612: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.569747: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.570967: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.572830: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.573392: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.574335: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.575169: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.576224: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.576801: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.577195: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.578961: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.579574: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.579909: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.580231: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.581677: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.582038: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.582891: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.583348: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.585788: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.586153: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.588486: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.588924: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.590002: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.590964: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.591498: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.593149: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.594512: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.596453: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.597166: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.597318: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.597527: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.599502: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.600465: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.600598: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.600760: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.601047: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.602706: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.603400: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.603859: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.604356: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.605673: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.606092: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.606607: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.607206: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.608375: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.608466: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.609234: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.609777: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.622018: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.622206: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.623990: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.624554: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.625503: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.626498: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.626866: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.627691: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.629352: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.637749: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.648207: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.662136: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.662845: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.663706: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.665095: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.665238: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.665446: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.666637: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.667247: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.669148: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.669191: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.669240: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.669636: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.671748: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.673142: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.674178: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.674254: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.674731: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.675527: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.677366: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.677500: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.677646: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.678113: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.678798: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.681201: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.681240: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.681379: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.681879: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.682423: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.685108: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.685148: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.685294: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.685758: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.686454: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.688580: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.688715: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.688899: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.689404: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.689837: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.691979: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.692079: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.692263: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.692662: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.694171: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.695209: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.695314: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.695495: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.696080: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.697787: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.698973: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.699109: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.699259: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.699649: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.701350: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.702445: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.702481: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.702542: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.702826: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.704924: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.705796: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.705837: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.705918: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.706167: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.708431: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.709236: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.709277: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.709458: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.709647: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.709842: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.711894: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.712893: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.712934: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.713172: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.713460: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.713733: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.713855: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.716256: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.717172: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.717404: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.717749: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.718321: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.718633: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.718718: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.721109: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.723373: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.723414: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.723537: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.723580: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.723624: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.723756: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.727462: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.727675: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.727777: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.728034: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.728134: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.728249: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.729858: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 07:58:37.731577: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.731631: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.731745: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.731788: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.731875: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.732582: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.735381: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.735516: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.735518: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.735571: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.735683: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.736597: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.739616: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.739720: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.739778: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.739819: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.740039: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.740319: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.741227: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.743908: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.743955: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.744118: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.744375: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.744646: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.744791: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.745563: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.748739: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.748784: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.748810: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.748937: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.749302: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.749750: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.750163: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.753396: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.753456: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.753459: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.753554: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.754851: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.755112: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.758964: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.759254: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.760127: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.762165: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.762355: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.762842: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 07:58:37.762975: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 07:58:37.763024: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.763601: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 07:58:37.764915: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.765329: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.768038: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.768213: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.769309: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 07:58:37.770592: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.770674: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.773365: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.773586: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.774045: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.774143: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.774579: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.777187: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.777669: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.777723: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.777758: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.778570: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.779561: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.781429: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.782250: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.782266: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.782374: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.782917: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.783971: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.786731: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.827825: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.829433: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.832384: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.833135: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.837472: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.838623: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.871079: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.871725: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.877310: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.879103: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.884521: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.888907: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 07:58:37.893566: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 07:58:37.899195: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.903283: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.904034: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.937646: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.938854: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:37.955176: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:38.898328: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:38.899171: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:38.899939: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:38.900419: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 07:58:38.900474: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-12 07:58:38.917121: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:38.917872: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:38.918389: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:38.919076: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:38.919928: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:38.920415: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-12 07:58:38.966211: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 07:58:38.966413: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 07:58:39.003675: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12340
2022-12-12 07:58:39.108468: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:39.109104: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:39.109628: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:39.110295: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 07:58:39.110352: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-12 07:58:39.127990: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:39.129038: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:39.129551: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:39.130136: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:39.130659: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:39.131155: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:1/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-12 07:58:39.150011: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:39.150944: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:39.151531: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:39.152005: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 07:58:39.152059: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-12 07:58:39.168451: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:39.168956: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:39.169407: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:39.170077: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:39.170419: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:39.171119: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:39.171394: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 07:58:39.171452: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-12 07:58:39.172083: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:39.172604: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:39.173376: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:2/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-12 07:58:39.186207: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:39.186386: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:39.187193: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:39.187366: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:39.188095: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:39.188299: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:39.188938: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 07:58:39.188989: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-12 07:58:39.189101: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 07:58:39.189148: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-12 07:58:39.190584: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:39.191292: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:39.191850: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:39.192574: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:39.193106: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:39.193571: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:6/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-12 07:58:39.206110: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:39.206139: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:39.207277: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:39.207362: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:39.208177: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:39.208310: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:39.209406: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:39.209529: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:39.210324: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:39.210440: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:39.211222: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:4/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-12 07:58:39.211332: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:3/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-12 07:58:39.217450: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 07:58:39.217621: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 07:58:39.219443: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12341
2022-12-12 07:58:39.224967: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:39.225588: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:39.226122: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:39.226858: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 07:58:39.226932: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-12 07:58:39.232317: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:39.232909: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:39.233439: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:39.233897: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 07:58:39.233946: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-12 07:58:39.239647: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 07:58:39.239848: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 07:58:39.241713: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12346
2022-12-12 07:58:39.243562: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:39.244226: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:39.244738: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:39.245334: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:39.245853: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:39.246331: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:7/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-12 07:58:39.250919: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:39.251587: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:39.252110: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:39.252684: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:39.253228: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:58:39.253697: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:5/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-12 07:58:39.254434: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 07:58:39.254586: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 07:58:39.256370: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12342
2022-12-12 07:58:39.257196: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 07:58:39.257234: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 07:58:39.257354: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 07:58:39.257367: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 07:58:39.259067: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12344
2022-12-12 07:58:39.259462: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12343
2022-12-12 07:58:39.292058: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 07:58:39.292249: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 07:58:39.293916: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12347
2022-12-12 07:58:39.298890: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 07:58:39.299036: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 07:58:39.300730: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12345
[HCTR][07:58:40.557][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][07:58:40.569][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][07:58:40.569][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][07:58:40.571][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][07:58:40.572][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][07:58:40.572][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][07:58:40.572][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][07:58:40.572][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 1it [00:01,  1.53s/it]warmup run: 1it [00:01,  1.52s/it]warmup run: 1it [00:01,  1.50s/it]warmup run: 96it [00:01, 81.90it/s]warmup run: 93it [00:01, 79.77it/s]warmup run: 1it [00:01,  1.50s/it]warmup run: 100it [00:01, 86.53it/s]warmup run: 192it [00:01, 177.49it/s]warmup run: 186it [00:01, 172.76it/s]warmup run: 99it [00:01, 85.71it/s]warmup run: 202it [00:01, 189.33it/s]warmup run: 1it [00:01,  1.53s/it]warmup run: 1it [00:01,  1.52s/it]warmup run: 1it [00:01,  1.53s/it]warmup run: 1it [00:01,  1.53s/it]warmup run: 288it [00:01, 282.47it/s]warmup run: 275it [00:01, 269.44it/s]warmup run: 201it [00:01, 188.76it/s]warmup run: 302it [00:01, 299.34it/s]warmup run: 97it [00:01, 82.80it/s]warmup run: 103it [00:01, 88.13it/s]warmup run: 100it [00:01, 84.87it/s]warmup run: 86it [00:01, 73.19it/s]warmup run: 383it [00:01, 389.57it/s]warmup run: 368it [00:01, 375.85it/s]warmup run: 305it [00:01, 304.27it/s]warmup run: 403it [00:01, 414.34it/s]warmup run: 190it [00:01, 175.10it/s]warmup run: 206it [00:01, 190.84it/s]warmup run: 201it [00:01, 185.06it/s]warmup run: 186it [00:01, 173.84it/s]warmup run: 478it [00:02, 493.76it/s]warmup run: 461it [00:02, 478.98it/s]warmup run: 404it [00:01, 415.37it/s]warmup run: 505it [00:02, 528.32it/s]warmup run: 285it [00:01, 279.21it/s]warmup run: 309it [00:01, 303.90it/s]warmup run: 297it [00:01, 288.90it/s]warmup run: 286it [00:01, 284.45it/s]warmup run: 573it [00:02, 589.50it/s]warmup run: 555it [00:02, 575.88it/s]warmup run: 503it [00:02, 523.05it/s]warmup run: 609it [00:02, 636.67it/s]warmup run: 381it [00:01, 388.59it/s]warmup run: 412it [00:01, 420.90it/s]warmup run: 398it [00:01, 404.75it/s]warmup run: 381it [00:01, 391.03it/s]warmup run: 669it [00:02, 674.55it/s]warmup run: 651it [00:02, 663.16it/s]warmup run: 602it [00:02, 621.56it/s]warmup run: 713it [00:02, 729.66it/s]warmup run: 472it [00:02, 486.05it/s]warmup run: 516it [00:02, 536.81it/s]warmup run: 499it [00:02, 517.22it/s]warmup run: 481it [00:02, 504.55it/s]warmup run: 765it [00:02, 744.24it/s]warmup run: 746it [00:02, 733.73it/s]warmup run: 702it [00:02, 708.98it/s]warmup run: 817it [00:02, 806.26it/s]warmup run: 569it [00:02, 587.08it/s]warmup run: 622it [00:02, 646.83it/s]warmup run: 602it [00:02, 624.48it/s]warmup run: 582it [00:02, 610.18it/s]warmup run: 859it [00:02, 791.95it/s]warmup run: 840it [00:02, 787.40it/s]warmup run: 803it [00:02, 783.22it/s]warmup run: 921it [00:02, 865.30it/s]warmup run: 665it [00:02, 672.55it/s]warmup run: 725it [00:02, 734.09it/s]warmup run: 705it [00:02, 716.65it/s]warmup run: 683it [00:02, 701.92it/s]warmup run: 953it [00:02, 826.11it/s]warmup run: 935it [00:02, 829.95it/s]warmup run: 903it [00:02, 837.90it/s]warmup run: 1026it [00:02, 913.56it/s]warmup run: 761it [00:02, 742.56it/s]warmup run: 826it [00:02, 802.53it/s]warmup run: 806it [00:02, 788.30it/s]warmup run: 784it [00:02, 777.98it/s]warmup run: 1051it [00:02, 867.39it/s]warmup run: 1029it [00:02, 856.52it/s]warmup run: 1002it [00:02, 876.93it/s]warmup run: 1130it [00:02, 948.52it/s]warmup run: 855it [00:02, 789.34it/s]warmup run: 927it [00:02, 850.50it/s]warmup run: 906it [00:02, 839.65it/s]warmup run: 882it [00:02, 830.18it/s]warmup run: 1150it [00:02, 900.10it/s]warmup run: 1124it [00:02, 882.24it/s]warmup run: 1101it [00:02, 906.29it/s]warmup run: 1233it [00:02, 971.04it/s]warmup run: 950it [00:02, 831.27it/s]warmup run: 1027it [00:02, 890.12it/s]warmup run: 982it [00:02, 876.27it/s]warmup run: 1005it [00:02, 874.91it/s]warmup run: 1246it [00:02, 906.82it/s]warmup run: 1219it [00:02, 900.18it/s]warmup run: 1200it [00:02, 924.74it/s]warmup run: 1336it [00:02, 982.85it/s]warmup run: 1047it [00:02, 869.36it/s]warmup run: 1129it [00:02, 924.63it/s]warmup run: 1104it [00:02, 904.29it/s]warmup run: 1081it [00:02, 899.50it/s]warmup run: 1344it [00:02, 925.68it/s]warmup run: 1313it [00:02, 908.48it/s]warmup run: 1298it [00:02, 937.03it/s]warmup run: 1439it [00:02, 996.54it/s]warmup run: 1145it [00:02, 900.27it/s]warmup run: 1231it [00:02, 950.28it/s]warmup run: 1205it [00:02, 932.31it/s]warmup run: 1182it [00:02, 930.28it/s]warmup run: 1440it [00:03, 933.72it/s]warmup run: 1407it [00:03, 916.85it/s]warmup run: 1396it [00:02, 949.39it/s]warmup run: 1542it [00:03, 1004.27it/s]warmup run: 1241it [00:02, 915.70it/s]warmup run: 1333it [00:02, 968.16it/s]warmup run: 1304it [00:02, 948.57it/s]warmup run: 1281it [00:02, 945.32it/s]warmup run: 1536it [00:03, 937.56it/s]warmup run: 1501it [00:03, 921.07it/s]warmup run: 1496it [00:03, 963.47it/s]warmup run: 1645it [00:03, 999.04it/s] warmup run: 1339it [00:02, 932.56it/s]warmup run: 1435it [00:02, 980.84it/s]warmup run: 1404it [00:02, 962.85it/s]warmup run: 1383it [00:02, 964.72it/s]warmup run: 1632it [00:03, 935.19it/s]warmup run: 1595it [00:03, 925.81it/s]warmup run: 1596it [00:03, 973.51it/s]warmup run: 1747it [00:03, 985.00it/s]warmup run: 1435it [00:03, 940.51it/s]warmup run: 1537it [00:03, 991.42it/s]warmup run: 1505it [00:03, 975.95it/s]warmup run: 1483it [00:03, 974.63it/s]warmup run: 1733it [00:03, 954.51it/s]warmup run: 1693it [00:03, 941.85it/s]warmup run: 1696it [00:03, 981.25it/s]warmup run: 1847it [00:03, 971.14it/s]warmup run: 1533it [00:03, 949.55it/s]warmup run: 1639it [00:03, 996.72it/s]warmup run: 1606it [00:03, 984.10it/s]warmup run: 1584it [00:03, 984.02it/s]warmup run: 1834it [00:03, 969.42it/s]warmup run: 1791it [00:03, 952.68it/s]warmup run: 1796it [00:03, 982.62it/s]warmup run: 1945it [00:03, 972.18it/s]warmup run: 1630it [00:03, 954.87it/s]warmup run: 1741it [00:03, 1000.48it/s]warmup run: 1684it [00:03, 986.45it/s]warmup run: 1706it [00:03, 970.01it/s]warmup run: 1936it [00:03, 982.74it/s]warmup run: 1887it [00:03, 954.30it/s]warmup run: 1899it [00:03, 994.84it/s]warmup run: 2053it [00:03, 1003.36it/s]warmup run: 1728it [00:03, 961.11it/s]warmup run: 1843it [00:03, 1003.45it/s]warmup run: 1784it [00:03, 986.13it/s]warmup run: 1805it [00:03, 972.10it/s]warmup run: 2045it [00:03, 1013.62it/s]warmup run: 1983it [00:03, 951.13it/s]warmup run: 2002it [00:03, 1002.45it/s]warmup run: 2175it [00:03, 1064.97it/s]warmup run: 1826it [00:03, 964.37it/s]warmup run: 1945it [00:03, 1007.98it/s]warmup run: 1884it [00:03, 988.48it/s]warmup run: 1903it [00:03, 974.11it/s]warmup run: 2167it [00:03, 1074.47it/s]warmup run: 2096it [00:03, 1004.33it/s]warmup run: 2122it [00:03, 1060.75it/s]warmup run: 2297it [00:03, 1109.72it/s]warmup run: 1923it [00:03, 965.13it/s]warmup run: 2056it [00:03, 1037.62it/s]warmup run: 1985it [00:03, 992.38it/s]warmup run: 2002it [00:03, 977.42it/s]warmup run: 2290it [00:03, 1120.83it/s]warmup run: 2215it [00:03, 1058.29it/s]warmup run: 2243it [00:03, 1104.01it/s]warmup run: 2419it [00:03, 1141.07it/s]warmup run: 2026it [00:03, 984.28it/s]warmup run: 2180it [00:03, 1097.53it/s]warmup run: 2098it [00:03, 1032.83it/s]warmup run: 2123it [00:03, 1045.42it/s]warmup run: 2414it [00:03, 1154.13it/s]warmup run: 2334it [00:03, 1095.92it/s]warmup run: 2364it [00:03, 1135.20it/s]warmup run: 2541it [00:03, 1162.90it/s]warmup run: 2149it [00:03, 1056.68it/s]warmup run: 2305it [00:03, 1140.95it/s]warmup run: 2214it [00:03, 1070.15it/s]warmup run: 2245it [00:03, 1095.09it/s]warmup run: 2537it [00:04, 1176.55it/s]warmup run: 2453it [00:04, 1121.99it/s]warmup run: 2485it [00:03, 1157.08it/s]warmup run: 2664it [00:04, 1181.30it/s]warmup run: 2272it [00:03, 1107.03it/s]warmup run: 2430it [00:03, 1171.15it/s]warmup run: 2330it [00:03, 1096.05it/s]warmup run: 2367it [00:03, 1130.09it/s]warmup run: 2660it [00:04, 1192.20it/s]warmup run: 2572it [00:04, 1140.96it/s]warmup run: 2606it [00:04, 1171.98it/s]warmup run: 2394it [00:03, 1140.65it/s]warmup run: 2783it [00:04, 1161.17it/s]warmup run: 2555it [00:03, 1192.24it/s]warmup run: 2489it [00:03, 1154.33it/s]warmup run: 2440it [00:03, 1070.83it/s]warmup run: 2782it [00:04, 1198.84it/s]warmup run: 2692it [00:04, 1155.81it/s]warmup run: 2727it [00:04, 1182.91it/s]warmup run: 2517it [00:04, 1165.18it/s]warmup run: 2680it [00:04, 1207.10it/s]warmup run: 2610it [00:04, 1170.57it/s]warmup run: 2900it [00:04, 1100.92it/s]warmup run: 2548it [00:04, 1046.53it/s]warmup run: 2905it [00:04, 1207.82it/s]warmup run: 2810it [00:04, 1161.11it/s]warmup run: 2847it [00:04, 1186.00it/s]warmup run: 2640it [00:04, 1183.42it/s]warmup run: 2803it [00:04, 1212.13it/s]warmup run: 3000it [00:04, 690.30it/s] warmup run: 2731it [00:04, 1181.45it/s]warmup run: 2670it [00:04, 1095.81it/s]warmup run: 3000it [00:04, 676.57it/s] warmup run: 2928it [00:04, 1165.87it/s]warmup run: 2968it [00:04, 1192.72it/s]warmup run: 2761it [00:04, 1189.74it/s]warmup run: 3000it [00:04, 689.63it/s] warmup run: 2925it [00:04, 1212.44it/s]warmup run: 2850it [00:04, 1182.52it/s]warmup run: 2789it [00:04, 1122.04it/s]warmup run: 3000it [00:04, 666.37it/s] warmup run: 3000it [00:04, 696.44it/s] warmup run: 2881it [00:04, 1177.42it/s]warmup run: 2969it [00:04, 1183.12it/s]warmup run: 2907it [00:04, 1136.62it/s]warmup run: 3000it [00:04, 683.23it/s] warmup run: 2999it [00:04, 1177.99it/s]warmup run: 3000it [00:04, 674.75it/s] warmup run: 3000it [00:04, 673.84it/s] 


warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]

warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]

warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   6%|▌         | 167/3000 [00:00<00:01, 1669.21it/s]warmup should be done:   6%|▌         | 165/3000 [00:00<00:01, 1647.08it/s]warmup should be done:   6%|▌         | 167/3000 [00:00<00:01, 1664.42it/s]warmup should be done:   6%|▌         | 168/3000 [00:00<00:01, 1671.23it/s]warmup should be done:   5%|▌         | 163/3000 [00:00<00:01, 1622.64it/s]warmup should be done:   5%|▍         | 139/3000 [00:00<00:02, 1385.32it/s]warmup should be done:   6%|▌         | 167/3000 [00:00<00:01, 1662.78it/s]warmup should be done:   6%|▌         | 167/3000 [00:00<00:01, 1661.86it/s]warmup should be done:  11%|█         | 335/3000 [00:00<00:01, 1671.54it/s]warmup should be done:  11%|█         | 334/3000 [00:00<00:01, 1667.25it/s]warmup should be done:  10%|█         | 305/3000 [00:00<00:01, 1546.48it/s]warmup should be done:  11%|█         | 332/3000 [00:00<00:01, 1655.83it/s]warmup should be done:  11%|█         | 328/3000 [00:00<00:01, 1635.61it/s]warmup should be done:  11%|█         | 336/3000 [00:00<00:01, 1676.35it/s]warmup should be done:  11%|█         | 336/3000 [00:00<00:01, 1672.94it/s]warmup should be done:  11%|█         | 334/3000 [00:00<00:01, 1664.01it/s]warmup should be done:  17%|█▋        | 504/3000 [00:00<00:01, 1676.58it/s]warmup should be done:  17%|█▋        | 503/3000 [00:00<00:01, 1670.16it/s]warmup should be done:  16%|█▌        | 475/3000 [00:00<00:01, 1612.17it/s]warmup should be done:  17%|█▋        | 501/3000 [00:00<00:01, 1663.78it/s]warmup should be done:  17%|█▋        | 498/3000 [00:00<00:01, 1652.26it/s]warmup should be done:  16%|█▋        | 492/3000 [00:00<00:01, 1631.63it/s]warmup should be done:  17%|█▋        | 501/3000 [00:00<00:01, 1655.99it/s]warmup should be done:  17%|█▋        | 504/3000 [00:00<00:01, 1663.64it/s]warmup should be done:  22%|██▏       | 672/3000 [00:00<00:01, 1676.99it/s]warmup should be done:  21%|██▏       | 644/3000 [00:00<00:01, 1642.51it/s]warmup should be done:  22%|██▏       | 668/3000 [00:00<00:01, 1664.14it/s]warmup should be done:  22%|██▏       | 664/3000 [00:00<00:01, 1650.29it/s]warmup should be done:  22%|██▏       | 667/3000 [00:00<00:01, 1654.44it/s]warmup should be done:  22%|██▏       | 656/3000 [00:00<00:01, 1627.32it/s]warmup should be done:  22%|██▏       | 671/3000 [00:00<00:01, 1660.20it/s]warmup should be done:  22%|██▏       | 671/3000 [00:00<00:01, 1659.47it/s]warmup should be done:  28%|██▊       | 840/3000 [00:00<00:01, 1675.64it/s]warmup should be done:  27%|██▋       | 813/3000 [00:00<00:01, 1656.86it/s]warmup should be done:  28%|██▊       | 835/3000 [00:00<00:01, 1665.76it/s]warmup should be done:  28%|██▊       | 830/3000 [00:00<00:01, 1648.48it/s]warmup should be done:  27%|██▋       | 823/3000 [00:00<00:01, 1639.34it/s]warmup should be done:  28%|██▊       | 833/3000 [00:00<00:01, 1649.97it/s]warmup should be done:  28%|██▊       | 837/3000 [00:00<00:01, 1654.44it/s]warmup should be done:  28%|██▊       | 838/3000 [00:00<00:01, 1651.46it/s]warmup should be done:  33%|███▎      | 1003/3000 [00:00<00:01, 1669.39it/s]warmup should be done:  33%|███▎      | 981/3000 [00:00<00:01, 1662.92it/s]warmup should be done:  34%|███▎      | 1008/3000 [00:00<00:01, 1673.50it/s]warmup should be done:  33%|███▎      | 995/3000 [00:00<00:01, 1647.25it/s]warmup should be done:  33%|███▎      | 990/3000 [00:00<00:01, 1646.58it/s]warmup should be done:  33%|███▎      | 999/3000 [00:00<00:01, 1652.06it/s]warmup should be done:  33%|███▎      | 1003/3000 [00:00<00:01, 1650.51it/s]warmup should be done:  33%|███▎      | 1004/3000 [00:00<00:01, 1646.43it/s]warmup should be done:  38%|███▊      | 1148/3000 [00:00<00:01, 1664.07it/s]warmup should be done:  39%|███▉      | 1171/3000 [00:00<00:01, 1669.57it/s]warmup should be done:  39%|███▉      | 1176/3000 [00:00<00:01, 1669.10it/s]warmup should be done:  39%|███▊      | 1160/3000 [00:00<00:01, 1644.64it/s]warmup should be done:  39%|███▉      | 1165/3000 [00:00<00:01, 1652.80it/s]warmup should be done:  39%|███▊      | 1156/3000 [00:00<00:01, 1647.89it/s]warmup should be done:  39%|███▉      | 1169/3000 [00:00<00:01, 1645.23it/s]warmup should be done:  39%|███▉      | 1169/3000 [00:00<00:01, 1640.02it/s]warmup should be done:  44%|████▍     | 1315/3000 [00:00<00:01, 1665.81it/s]warmup should be done:  45%|████▍     | 1339/3000 [00:00<00:00, 1671.42it/s]warmup should be done:  45%|████▍     | 1343/3000 [00:00<00:00, 1666.76it/s]warmup should be done:  44%|████▍     | 1325/3000 [00:00<00:01, 1645.14it/s]warmup should be done:  44%|████▍     | 1332/3000 [00:00<00:01, 1656.21it/s]warmup should be done:  44%|████▍     | 1322/3000 [00:00<00:01, 1650.17it/s]warmup should be done:  44%|████▍     | 1334/3000 [00:00<00:01, 1642.54it/s]warmup should be done:  44%|████▍     | 1334/3000 [00:00<00:01, 1641.37it/s]warmup should be done:  49%|████▉     | 1483/3000 [00:00<00:00, 1667.36it/s]warmup should be done:  50%|█████     | 1507/3000 [00:00<00:00, 1672.09it/s]warmup should be done:  50%|█████     | 1510/3000 [00:00<00:00, 1666.42it/s]warmup should be done:  50%|████▉     | 1490/3000 [00:00<00:00, 1646.03it/s]warmup should be done:  50%|████▉     | 1498/3000 [00:00<00:00, 1655.31it/s]warmup should be done:  50%|████▉     | 1488/3000 [00:00<00:00, 1651.42it/s]warmup should be done:  50%|████▉     | 1499/3000 [00:00<00:00, 1642.63it/s]warmup should be done:  50%|█████     | 1500/3000 [00:00<00:00, 1646.42it/s]warmup should be done:  55%|█████▌    | 1651/3000 [00:01<00:00, 1669.79it/s]warmup should be done:  56%|█████▌    | 1675/3000 [00:01<00:00, 1674.49it/s]warmup should be done:  56%|█████▌    | 1678/3000 [00:01<00:00, 1667.85it/s]warmup should be done:  55%|█████▌    | 1655/3000 [00:01<00:00, 1646.28it/s]warmup should be done:  55%|█████▌    | 1664/3000 [00:01<00:00, 1656.69it/s]warmup should be done:  55%|█████▌    | 1654/3000 [00:01<00:00, 1652.19it/s]warmup should be done:  55%|█████▌    | 1664/3000 [00:01<00:00, 1643.16it/s]warmup should be done:  56%|█████▌    | 1667/3000 [00:01<00:00, 1651.40it/s]warmup should be done:  61%|██████    | 1819/3000 [00:01<00:00, 1671.74it/s]warmup should be done:  61%|██████▏   | 1843/3000 [00:01<00:00, 1674.54it/s]warmup should be done:  62%|██████▏   | 1845/3000 [00:01<00:00, 1667.82it/s]warmup should be done:  61%|██████    | 1820/3000 [00:01<00:00, 1646.56it/s]warmup should be done:  61%|██████    | 1830/3000 [00:01<00:00, 1656.14it/s]warmup should be done:  61%|██████    | 1820/3000 [00:01<00:00, 1653.74it/s]warmup should be done:  61%|██████    | 1829/3000 [00:01<00:00, 1642.33it/s]warmup should be done:  61%|██████    | 1834/3000 [00:01<00:00, 1654.71it/s]warmup should be done:  67%|██████▋   | 2011/3000 [00:01<00:00, 1675.74it/s]warmup should be done:  66%|██████▌   | 1987/3000 [00:01<00:00, 1667.74it/s]warmup should be done:  67%|██████▋   | 2012/3000 [00:01<00:00, 1667.97it/s]warmup should be done:  66%|██████▌   | 1985/3000 [00:01<00:00, 1646.96it/s]warmup should be done:  67%|██████▋   | 1997/3000 [00:01<00:00, 1657.73it/s]warmup should be done:  66%|██████▌   | 1987/3000 [00:01<00:00, 1655.78it/s]warmup should be done:  66%|██████▋   | 1994/3000 [00:01<00:00, 1642.87it/s]warmup should be done:  67%|██████▋   | 2001/3000 [00:01<00:00, 1657.29it/s]warmup should be done:  73%|███████▎  | 2179/3000 [00:01<00:00, 1675.75it/s]warmup should be done:  72%|███████▏  | 2150/3000 [00:01<00:00, 1647.23it/s]warmup should be done:  73%|███████▎  | 2179/3000 [00:01<00:00, 1666.38it/s]warmup should be done:  72%|███████▏  | 2154/3000 [00:01<00:00, 1662.11it/s]warmup should be done:  72%|███████▏  | 2163/3000 [00:01<00:00, 1657.58it/s]warmup should be done:  72%|███████▏  | 2153/3000 [00:01<00:00, 1656.43it/s]warmup should be done:  72%|███████▏  | 2159/3000 [00:01<00:00, 1642.02it/s]warmup should be done:  72%|███████▏  | 2168/3000 [00:01<00:00, 1658.69it/s]warmup should be done:  78%|███████▊  | 2347/3000 [00:01<00:00, 1671.81it/s]warmup should be done:  77%|███████▋  | 2315/3000 [00:01<00:00, 1647.01it/s]warmup should be done:  77%|███████▋  | 2319/3000 [00:01<00:00, 1657.35it/s]warmup should be done:  78%|███████▊  | 2329/3000 [00:01<00:00, 1656.04it/s]warmup should be done:  78%|███████▊  | 2346/3000 [00:01<00:00, 1659.55it/s]warmup should be done:  77%|███████▋  | 2321/3000 [00:01<00:00, 1653.66it/s]warmup should be done:  78%|███████▊  | 2325/3000 [00:01<00:00, 1644.62it/s]warmup should be done:  78%|███████▊  | 2335/3000 [00:01<00:00, 1657.09it/s]warmup should be done:  84%|████████▍ | 2515/3000 [00:01<00:00, 1672.46it/s]warmup should be done:  83%|████████▎ | 2480/3000 [00:01<00:00, 1643.66it/s]warmup should be done:  83%|████████▎ | 2485/3000 [00:01<00:00, 1654.85it/s]warmup should be done:  83%|████████▎ | 2495/3000 [00:01<00:00, 1653.65it/s]warmup should be done:  84%|████████▎ | 2512/3000 [00:01<00:00, 1654.28it/s]warmup should be done:  83%|████████▎ | 2487/3000 [00:01<00:00, 1644.86it/s]warmup should be done:  83%|████████▎ | 2491/3000 [00:01<00:00, 1647.44it/s]warmup should be done:  83%|████████▎ | 2502/3000 [00:01<00:00, 1660.76it/s]warmup should be done:  89%|████████▉ | 2683/3000 [00:01<00:00, 1674.68it/s]warmup should be done:  88%|████████▊ | 2645/3000 [00:01<00:00, 1644.86it/s]warmup should be done:  89%|████████▊ | 2661/3000 [00:01<00:00, 1655.28it/s]warmup should be done:  88%|████████▊ | 2652/3000 [00:01<00:00, 1656.92it/s]warmup should be done:  89%|████████▉ | 2678/3000 [00:01<00:00, 1652.01it/s]warmup should be done:  88%|████████▊ | 2652/3000 [00:01<00:00, 1642.24it/s]warmup should be done:  89%|████████▊ | 2658/3000 [00:01<00:00, 1651.19it/s]warmup should be done:  89%|████████▉ | 2670/3000 [00:01<00:00, 1663.91it/s]warmup should be done:  95%|█████████▌| 2852/3000 [00:01<00:00, 1677.22it/s]warmup should be done:  94%|█████████▎| 2810/3000 [00:01<00:00, 1645.18it/s]warmup should be done:  94%|█████████▍| 2828/3000 [00:01<00:00, 1658.08it/s]warmup should be done:  94%|█████████▍| 2819/3000 [00:01<00:00, 1659.87it/s]warmup should be done:  95%|█████████▍| 2845/3000 [00:01<00:00, 1655.91it/s]warmup should be done:  94%|█████████▍| 2825/3000 [00:01<00:00, 1655.75it/s]warmup should be done:  94%|█████████▍| 2817/3000 [00:01<00:00, 1639.09it/s]warmup should be done:  95%|█████████▍| 2838/3000 [00:01<00:00, 1667.53it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1672.82it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1664.69it/s]warmup should be done:  99%|█████████▉| 2977/3000 [00:01<00:00, 1649.78it/s]warmup should be done: 100%|█████████▉| 2987/3000 [00:01<00:00, 1665.54it/s]warmup should be done: 100%|█████████▉| 2995/3000 [00:01<00:00, 1659.54it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1658.70it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1656.25it/s]warmup should be done: 100%|█████████▉| 2992/3000 [00:01<00:00, 1659.35it/s]warmup should be done:  99%|█████████▉| 2982/3000 [00:01<00:00, 1640.59it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1652.06it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1650.89it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1647.22it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1645.00it/s]





warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]

warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   6%|▌         | 170/3000 [00:00<00:01, 1696.54it/s]warmup should be done:   6%|▌         | 168/3000 [00:00<00:01, 1674.42it/s]warmup should be done:   6%|▌         | 173/3000 [00:00<00:01, 1724.25it/s]warmup should be done:   6%|▌         | 170/3000 [00:00<00:01, 1695.19it/s]warmup should be done:   6%|▌         | 172/3000 [00:00<00:01, 1712.34it/s]warmup should be done:   6%|▌         | 171/3000 [00:00<00:01, 1701.66it/s]warmup should be done:   6%|▌         | 173/3000 [00:00<00:01, 1720.54it/s]warmup should be done:   6%|▌         | 170/3000 [00:00<00:01, 1690.33it/s]warmup should be done:  11%|█▏        | 340/3000 [00:00<00:01, 1693.00it/s]warmup should be done:  12%|█▏        | 346/3000 [00:00<00:01, 1722.19it/s]warmup should be done:  11%|█▏        | 340/3000 [00:00<00:01, 1691.92it/s]warmup should be done:  11%|█▏        | 342/3000 [00:00<00:01, 1700.54it/s]warmup should be done:  12%|█▏        | 346/3000 [00:00<00:01, 1719.61it/s]warmup should be done:  11%|█▏        | 344/3000 [00:00<00:01, 1709.29it/s]warmup should be done:  11%|█         | 336/3000 [00:00<00:01, 1666.79it/s]warmup should be done:  11%|█▏        | 340/3000 [00:00<00:01, 1673.96it/s]warmup should be done:  17%|█▋        | 510/3000 [00:00<00:01, 1694.38it/s]warmup should be done:  17%|█▋        | 510/3000 [00:00<00:01, 1691.68it/s]warmup should be done:  17%|█▋        | 518/3000 [00:00<00:01, 1717.80it/s]warmup should be done:  17%|█▋        | 504/3000 [00:00<00:01, 1671.30it/s]warmup should be done:  17%|█▋        | 516/3000 [00:00<00:01, 1711.13it/s]warmup should be done:  17%|█▋        | 519/3000 [00:00<00:01, 1719.95it/s]warmup should be done:  17%|█▋        | 513/3000 [00:00<00:01, 1700.92it/s]warmup should be done:  17%|█▋        | 508/3000 [00:00<00:01, 1675.10it/s]warmup should be done:  23%|██▎       | 681/3000 [00:00<00:01, 1700.27it/s]warmup should be done:  23%|██▎       | 680/3000 [00:00<00:01, 1693.87it/s]warmup should be done:  23%|██▎       | 690/3000 [00:00<00:01, 1717.12it/s]warmup should be done:  23%|██▎       | 685/3000 [00:00<00:01, 1706.53it/s]warmup should be done:  22%|██▏       | 673/3000 [00:00<00:01, 1676.10it/s]warmup should be done:  23%|██▎       | 689/3000 [00:00<00:01, 1715.05it/s]warmup should be done:  23%|██▎       | 691/3000 [00:00<00:01, 1707.51it/s]warmup should be done:  23%|██▎       | 677/3000 [00:00<00:01, 1677.17it/s]warmup should be done:  28%|██▊       | 854/3000 [00:00<00:01, 1708.64it/s]warmup should be done:  28%|██▊       | 852/3000 [00:00<00:01, 1702.93it/s]warmup should be done:  29%|██▊       | 857/3000 [00:00<00:01, 1710.79it/s]warmup should be done:  29%|██▉       | 863/3000 [00:00<00:01, 1719.87it/s]warmup should be done:  28%|██▊       | 841/3000 [00:00<00:01, 1675.33it/s]warmup should be done:  29%|██▊       | 862/3000 [00:00<00:01, 1717.50it/s]warmup should be done:  29%|██▉       | 864/3000 [00:00<00:01, 1714.40it/s]warmup should be done:  28%|██▊       | 845/3000 [00:00<00:01, 1675.49it/s]warmup should be done:  34%|███▍      | 1027/3000 [00:00<00:01, 1713.00it/s]warmup should be done:  34%|███▍      | 1035/3000 [00:00<00:01, 1719.15it/s]warmup should be done:  34%|███▍      | 1025/3000 [00:00<00:01, 1709.02it/s]warmup should be done:  34%|███▍      | 1029/3000 [00:00<00:01, 1709.75it/s]warmup should be done:  34%|███▍      | 1034/3000 [00:00<00:01, 1717.14it/s]warmup should be done:  35%|███▍      | 1036/3000 [00:00<00:01, 1715.98it/s]warmup should be done:  34%|███▍      | 1013/3000 [00:00<00:01, 1674.28it/s]warmup should be done:  34%|███▎      | 1009/3000 [00:00<00:01, 1660.65it/s]warmup should be done:  40%|███▉      | 1197/3000 [00:00<00:01, 1712.56it/s]warmup should be done:  40%|███▉      | 1199/3000 [00:00<00:01, 1713.30it/s]warmup should be done:  40%|████      | 1200/3000 [00:00<00:01, 1706.44it/s]warmup should be done:  40%|████      | 1206/3000 [00:00<00:01, 1714.64it/s]warmup should be done:  40%|████      | 1207/3000 [00:00<00:01, 1708.86it/s]warmup should be done:  40%|████      | 1208/3000 [00:00<00:01, 1714.33it/s]warmup should be done:  39%|███▉      | 1181/3000 [00:00<00:01, 1673.59it/s]warmup should be done:  39%|███▉      | 1178/3000 [00:00<00:01, 1669.44it/s]warmup should be done:  46%|████▌     | 1371/3000 [00:00<00:00, 1719.88it/s]warmup should be done:  46%|████▌     | 1371/3000 [00:00<00:00, 1709.87it/s]warmup should be done:  46%|████▌     | 1374/3000 [00:00<00:00, 1714.97it/s]warmup should be done:  46%|████▌     | 1379/3000 [00:00<00:00, 1716.90it/s]warmup should be done:  46%|████▌     | 1378/3000 [00:00<00:00, 1708.73it/s]warmup should be done:  46%|████▌     | 1381/3000 [00:00<00:00, 1719.06it/s]warmup should be done:  45%|████▌     | 1350/3000 [00:00<00:00, 1675.72it/s]warmup should be done:  45%|████▍     | 1348/3000 [00:00<00:00, 1676.13it/s]warmup should be done:  51%|█████▏    | 1544/3000 [00:00<00:00, 1722.81it/s]warmup should be done:  52%|█████▏    | 1547/3000 [00:00<00:00, 1719.13it/s]warmup should be done:  52%|█████▏    | 1551/3000 [00:00<00:00, 1716.37it/s]warmup should be done:  52%|█████▏    | 1554/3000 [00:00<00:00, 1721.60it/s]warmup should be done:  52%|█████▏    | 1549/3000 [00:00<00:00, 1705.88it/s]warmup should be done:  51%|█████▏    | 1542/3000 [00:00<00:00, 1701.78it/s]warmup should be done:  51%|█████     | 1517/3000 [00:00<00:00, 1678.38it/s]warmup should be done:  51%|█████     | 1518/3000 [00:00<00:00, 1673.17it/s]warmup should be done:  57%|█████▋    | 1718/3000 [00:01<00:00, 1725.57it/s]warmup should be done:  57%|█████▋    | 1720/3000 [00:01<00:00, 1721.95it/s]warmup should be done:  57%|█████▋    | 1723/3000 [00:01<00:00, 1716.17it/s]warmup should be done:  58%|█████▊    | 1727/3000 [00:01<00:00, 1721.71it/s]warmup should be done:  57%|█████▋    | 1720/3000 [00:01<00:00, 1703.71it/s]warmup should be done:  57%|█████▋    | 1713/3000 [00:01<00:00, 1695.93it/s]warmup should be done:  56%|█████▌    | 1687/3000 [00:01<00:00, 1682.83it/s]warmup should be done:  56%|█████▌    | 1687/3000 [00:01<00:00, 1676.03it/s]warmup should be done:  63%|██████▎   | 1892/3000 [00:01<00:00, 1728.28it/s]warmup should be done:  63%|██████▎   | 1893/3000 [00:01<00:00, 1724.13it/s]warmup should be done:  63%|██████▎   | 1896/3000 [00:01<00:00, 1717.35it/s]warmup should be done:  63%|██████▎   | 1900/3000 [00:01<00:00, 1723.57it/s]warmup should be done:  63%|██████▎   | 1891/3000 [00:01<00:00, 1703.69it/s]warmup should be done:  63%|██████▎   | 1883/3000 [00:01<00:00, 1695.33it/s]warmup should be done:  62%|██████▏   | 1856/3000 [00:01<00:00, 1679.87it/s]warmup should be done:  62%|██████▏   | 1857/3000 [00:01<00:00, 1686.49it/s]warmup should be done:  69%|██████▉   | 2066/3000 [00:01<00:00, 1729.78it/s]warmup should be done:  69%|██████▉   | 2066/3000 [00:01<00:00, 1725.61it/s]warmup should be done:  69%|██████▉   | 2073/3000 [00:01<00:00, 1724.90it/s]warmup should be done:  69%|██████▉   | 2069/3000 [00:01<00:00, 1718.37it/s]warmup should be done:  69%|██████▊   | 2062/3000 [00:01<00:00, 1703.93it/s]warmup should be done:  67%|██████▋   | 2024/3000 [00:01<00:00, 1679.71it/s]warmup should be done:  68%|██████▊   | 2054/3000 [00:01<00:00, 1699.21it/s]warmup should be done:  68%|██████▊   | 2027/3000 [00:01<00:00, 1688.00it/s]warmup should be done:  75%|███████▍  | 2239/3000 [00:01<00:00, 1729.37it/s]warmup should be done:  75%|███████▍  | 2239/3000 [00:01<00:00, 1724.52it/s]warmup should be done:  75%|███████▍  | 2246/3000 [00:01<00:00, 1723.16it/s]warmup should be done:  75%|███████▍  | 2241/3000 [00:01<00:00, 1717.24it/s]warmup should be done:  73%|███████▎  | 2192/3000 [00:01<00:00, 1677.60it/s]warmup should be done:  74%|███████▍  | 2225/3000 [00:01<00:00, 1699.93it/s]warmup should be done:  74%|███████▍  | 2233/3000 [00:01<00:00, 1698.08it/s]warmup should be done:  73%|███████▎  | 2196/3000 [00:01<00:00, 1688.06it/s]warmup should be done:  80%|████████  | 2412/3000 [00:01<00:00, 1728.45it/s]warmup should be done:  80%|████████  | 2412/3000 [00:01<00:00, 1721.07it/s]warmup should be done:  80%|████████  | 2413/3000 [00:01<00:00, 1714.39it/s]warmup should be done:  81%|████████  | 2419/3000 [00:01<00:00, 1718.91it/s]warmup should be done:  79%|███████▊  | 2362/3000 [00:01<00:00, 1684.19it/s]warmup should be done:  80%|███████▉  | 2398/3000 [00:01<00:00, 1705.95it/s]warmup should be done:  79%|███████▉  | 2366/3000 [00:01<00:00, 1689.33it/s]warmup should be done:  80%|████████  | 2403/3000 [00:01<00:00, 1691.09it/s]warmup should be done:  86%|████████▌ | 2585/3000 [00:01<00:00, 1725.44it/s]warmup should be done:  86%|████████▌ | 2585/3000 [00:01<00:00, 1719.15it/s]warmup should be done:  84%|████████▍ | 2533/3000 [00:01<00:00, 1691.75it/s]warmup should be done:  86%|████████▋ | 2591/3000 [00:01<00:00, 1716.54it/s]warmup should be done:  86%|████████▌ | 2570/3000 [00:01<00:00, 1709.48it/s]warmup should be done:  85%|████████▍ | 2536/3000 [00:01<00:00, 1690.40it/s]warmup should be done:  86%|████████▌ | 2585/3000 [00:01<00:00, 1699.63it/s]warmup should be done:  86%|████████▌ | 2573/3000 [00:01<00:00, 1688.98it/s]warmup should be done:  92%|█████████▏| 2759/3000 [00:01<00:00, 1727.47it/s]warmup should be done:  92%|█████████▏| 2759/3000 [00:01<00:00, 1722.79it/s]warmup should be done:  90%|█████████ | 2705/3000 [00:01<00:00, 1699.31it/s]warmup should be done:  92%|█████████▏| 2763/3000 [00:01<00:00, 1715.24it/s]warmup should be done:  91%|█████████▏| 2744/3000 [00:01<00:00, 1716.28it/s]warmup should be done:  90%|█████████ | 2706/3000 [00:01<00:00, 1691.61it/s]warmup should be done:  91%|█████████▏| 2742/3000 [00:01<00:00, 1687.92it/s]warmup should be done:  92%|█████████▏| 2755/3000 [00:01<00:00, 1693.65it/s]warmup should be done:  98%|█████████▊| 2932/3000 [00:01<00:00, 1718.07it/s]warmup should be done:  98%|█████████▊| 2933/3000 [00:01<00:00, 1725.50it/s]warmup should be done:  96%|█████████▌| 2877/3000 [00:01<00:00, 1704.20it/s]warmup should be done:  98%|█████████▊| 2935/3000 [00:01<00:00, 1713.59it/s]warmup should be done:  97%|█████████▋| 2917/3000 [00:01<00:00, 1720.22it/s]warmup should be done:  96%|█████████▌| 2876/3000 [00:01<00:00, 1690.52it/s]warmup should be done:  98%|█████████▊| 2925/3000 [00:01<00:00, 1689.05it/s]warmup should be done:  97%|█████████▋| 2911/3000 [00:01<00:00, 1666.61it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1717.65it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1717.19it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1716.88it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1707.57it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1707.21it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1696.97it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1685.80it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1682.50it/s]
WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7fe2f546bb80>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7fe2f50f91f0>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7fe2f50fa1c0>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7fe2f50f9190>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7fe2f50fc0d0>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7fe2f51072b0>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7fe2f546ed30>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7fe2f546c730>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
2022-12-12 08:00:09.934977: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fde22834320 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 08:00:09.934998: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fde13029a90 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 08:00:09.935054: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 08:00:09.935061: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 08:00:09.944308: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 08:00:09.946950: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 08:00:09.979823: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fde170314f0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 08:00:09.979885: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 08:00:09.987728: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 08:00:10.141348: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fde16f924f0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 08:00:10.141409: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 08:00:10.149353: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 08:00:10.388089: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fde1b0313e0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 08:00:10.388151: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 08:00:10.397922: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 08:00:10.565567: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fde1702d670 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 08:00:10.565631: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 08:00:10.568258: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fde17029350 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 08:00:10.568309: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 08:00:10.576138: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 08:00:10.576181: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 08:00:10.594437: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fde1a834630 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 08:00:10.594504: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 08:00:10.605403: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 08:00:17.094093: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 08:00:17.111587: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 08:00:17.116253: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 08:00:17.144180: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 08:00:17.250623: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 08:00:17.305512: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 08:00:17.645730: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 08:00:17.650489: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
[HCTR][08:01:06.526][ERROR][RK0][main]: replica 7 reaches 1000, calling init pre replica
[HCTR][08:01:06.526][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][08:01:06.532][ERROR][RK0][main]: coll ps creation done
[HCTR][08:01:06.532][ERROR][RK0][main]: replica 7 waits for coll ps creation barrier
[HCTR][08:01:06.553][ERROR][RK0][main]: replica 5 reaches 1000, calling init pre replica
[HCTR][08:01:06.554][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][08:01:06.558][ERROR][RK0][main]: coll ps creation done
[HCTR][08:01:06.558][ERROR][RK0][main]: replica 5 waits for coll ps creation barrier
[HCTR][08:01:06.794][ERROR][RK0][tid #140591996335872]: replica 6 reaches 1000, calling init pre replica
[HCTR][08:01:06.794][ERROR][RK0][tid #140591996335872]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][08:01:06.801][ERROR][RK0][tid #140591996335872]: coll ps creation done
[HCTR][08:01:06.802][ERROR][RK0][tid #140591996335872]: replica 6 waits for coll ps creation barrier
[HCTR][08:01:06.830][ERROR][RK0][tid #140592424134400]: replica 0 reaches 1000, calling init pre replica
[HCTR][08:01:06.830][ERROR][RK0][tid #140592424134400]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][08:01:06.837][ERROR][RK0][tid #140592424134400]: coll ps creation done
[HCTR][08:01:06.838][ERROR][RK0][tid #140592424134400]: replica 0 waits for coll ps creation barrier
[HCTR][08:01:06.864][ERROR][RK0][tid #140592063444736]: replica 1 reaches 1000, calling init pre replica
[HCTR][08:01:06.864][ERROR][RK0][tid #140592063444736]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][08:01:06.873][ERROR][RK0][tid #140592063444736]: coll ps creation done
[HCTR][08:01:06.873][ERROR][RK0][tid #140592063444736]: replica 1 waits for coll ps creation barrier
[HCTR][08:01:06.924][ERROR][RK0][tid #140592424134400]: replica 4 reaches 1000, calling init pre replica
[HCTR][08:01:06.924][ERROR][RK0][tid #140592424134400]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][08:01:06.928][ERROR][RK0][tid #140592424134400]: coll ps creation done
[HCTR][08:01:06.928][ERROR][RK0][tid #140592424134400]: replica 4 waits for coll ps creation barrier
[HCTR][08:01:07.000][ERROR][RK0][tid #140592617068288]: replica 2 reaches 1000, calling init pre replica
[HCTR][08:01:07.000][ERROR][RK0][tid #140592617068288]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][08:01:07.005][ERROR][RK0][tid #140592617068288]: coll ps creation done
[HCTR][08:01:07.005][ERROR][RK0][tid #140592617068288]: replica 2 waits for coll ps creation barrier
[HCTR][08:01:07.081][ERROR][RK0][tid #140592818394880]: replica 3 reaches 1000, calling init pre replica
[HCTR][08:01:07.082][ERROR][RK0][tid #140592818394880]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][08:01:07.089][ERROR][RK0][tid #140592818394880]: coll ps creation done
[HCTR][08:01:07.089][ERROR][RK0][tid #140592818394880]: replica 3 waits for coll ps creation barrier
[HCTR][08:01:07.089][ERROR][RK0][tid #140592424134400]: replica 0 preparing frequency
[HCTR][08:01:07.922][ERROR][RK0][tid #140592424134400]: replica 0 preparing frequency done
[HCTR][08:01:07.970][ERROR][RK0][tid #140592424134400]: replica 0 calling init per replica
[HCTR][08:01:07.970][ERROR][RK0][main]: replica 7 calling init per replica
[HCTR][08:01:07.970][ERROR][RK0][main]: replica 5 calling init per replica
[HCTR][08:01:07.970][ERROR][RK0][tid #140592818394880]: replica 3 calling init per replica
[HCTR][08:01:07.970][ERROR][RK0][tid #140592424134400]: replica 4 calling init per replica
[HCTR][08:01:07.970][ERROR][RK0][tid #140592617068288]: replica 2 calling init per replica
[HCTR][08:01:07.970][ERROR][RK0][tid #140591996335872]: replica 6 calling init per replica
[HCTR][08:01:07.970][ERROR][RK0][tid #140592063444736]: replica 1 calling init per replica
[HCTR][08:01:07.970][ERROR][RK0][tid #140592424134400]: Calling build_v2
[HCTR][08:01:07.970][ERROR][RK0][main]: Calling build_v2
[HCTR][08:01:07.970][ERROR][RK0][main]: Calling build_v2
[HCTR][08:01:07.970][ERROR][RK0][tid #140592818394880]: Calling build_v2
[HCTR][08:01:07.970][ERROR][RK0][tid #140592424134400]: Calling build_v2
[HCTR][08:01:07.970][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][08:01:07.970][ERROR][RK0][tid #140592617068288]: Calling build_v2
[HCTR][08:01:07.970][ERROR][RK0][tid #140591996335872]: Calling build_v2
[HCTR][08:01:07.970][ERROR][RK0][tid #140592424134400]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][08:01:07.970][ERROR][RK0][tid #140592063444736]: Calling build_v2
[HCTR][08:01:07.970][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][08:01:07.970][ERROR][RK0][tid #140592818394880]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][08:01:07.970][ERROR][RK0][tid #140592424134400]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][08:01:07.970][ERROR][RK0][tid #140592617068288]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][08:01:07.970][ERROR][RK0][tid #140591996335872]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][08:01:07.970][ERROR][RK0][tid #140592063444736]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[[[[2022-12-12 08:01:072022-12-12 08:01:072022-12-12 08:01:07..2022-12-12 08:01:07.[[[9708719708792022-12-12 08:01:07.970879[: 2022-12-12 08:01:07: 2022-12-12 08:01:07.970884: E.E.970898: E 2022-12-12 08:01:07970898 970898: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc.: /hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc:970926E:E /hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc:136:  136 /hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc:136] E/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc] /hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc:136] using concurrent impl MPS :using concurrent impl MPS:136] using concurrent impl MPS
/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc136
136] using concurrent impl MPS
:] ] using concurrent impl MPS
136using concurrent impl MPSusing concurrent impl MPS
] 

using concurrent impl MPS
[2022-12-12 08:01:07.975208: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:178] v100x8, slow pcie
[2022-12-12 08:01:07.975246: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:[1962022-12-12 08:01:07] .assigning 8 to cpu975255
: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:178] v100x8, slow pcie
[2022-12-12 08:01:07.975301: [E2022-12-12 08:01:07 ./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc975304:: [196E2022-12-12 08:01:07]  .assigning 8 to cpu/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc975331
:: 178[E] 2022-12-12 08:01:07 v100x8, slow pcie./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
975353:: 212E[]  2022-12-12 08:01:07build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.
[:9753852022-12-12 08:01:07178: .[] E975399[2022-12-12 08:01:07v100x8, slow pcie : 2022-12-12 08:01:07.
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE.[975408: 975423[2022-12-12 08:01:07: 196/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: 2022-12-12 08:01:07.E] :E.[975458 assigning 8 to cpu212 9754762022-12-12 08:01:07[: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: .2022-12-12 08:01:07E:build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8:E975508. 178
213[ : 975549[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] ] 2022-12-12 08:01:07/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE: 2022-12-12 08:01:07:v100x8, slow pcieremote time is 8.68421.: E.178

975599196/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc 975633] : [] [:/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: v100x8, slow pcieE2022-12-12 08:01:07assigning 8 to cpu2022-12-12 08:01:07178:E
 .
.] 178 /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc975703[975706v100x8, slow pcie] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:: 2022-12-12 08:01:07: 
v100x8, slow pcie:212[E.E
213[] 2022-12-12 08:01:07 975774 ] [2022-12-12 08:01:07build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccremote time is 8.684212022-12-12 08:01:07.
975818:E:
.975836: 196 [214975864: [E] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 08:01:07] : E2022-12-12 08:01:07 assigning 8 to cpu:.cpu time is 97.0588E ./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
196975933
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc975943:] : /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:: 212assigning 8 to cpuE:196E] 
[ 196]  build asymm link desc with 8X Tesla V100-SXM2-32GB out of 82022-12-12 08:01:07/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] assigning 8 to cpu/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
[.:assigning 8 to cpu
:2022-12-12 08:01:07976072213[
214.: ] 2022-12-12 08:01:07[] 976109Eremote time is 8.68421.2022-12-12 08:01:07cpu time is 97.0588: [ 
976139.
E2022-12-12 08:01:07/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: [976166 .:E2022-12-12 08:01:07: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc976199212 .E:: ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc976235 212Ebuild asymm link desc with 8X Tesla V100-SXM2-32GB out of 8:: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc]  
213E:build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc]  212
[:remote time is 8.68421/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[] 2022-12-12 08:01:07212
:2022-12-12 08:01:07build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8.] 214.[
976378build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8] 9764072022-12-12 08:01:07: 
cpu time is 97.0588: .[E
E9764312022-12-12 08:01:07[  : .2022-12-12 08:01:07/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE976463.:: : 976482213213/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE: ] ] : Eremote time is 8.68421remote time is 8.68421214/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc 

] :/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cccpu time is 97.0588[213[:
2022-12-12 08:01:07] 2022-12-12 08:01:07213.remote time is 8.68421.] 976610
976613remote time is 8.68421: : 
[EE2022-12-12 08:01:07 [ ./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 08:01:07/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc976676:.:: 214976692214E] : ]  cpu time is 97.0588Ecpu time is 97.0588/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
 
:/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc214:] 214cpu time is 97.0588] 
cpu time is 97.0588
[2022-12-12 08:02:26. 65323: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:118] solver created. now build & solve
[2022-12-12 08:02:26.105111: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:123] solver built. now solve
block 0 storage is 00010001
	access is	0	0	0	0	4	4	4	4	
block 1 storage is 00100010
	access is	1	1	1	1	5	5	5	5	
block 2 storage is 01000100
	access is	2	2	2	2	6	6	6	6	
block 3 storage is 10001000
	access is	3	3	3	3	7	7	7	7	
block 4 storage is 00000000
	access is	8	8	8	8	8	8	8	8	
[2022-12-12 08:02:26.237151: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:127] solver solved
[2022-12-12 08:02:26.237229: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:195] 0 solved master
[2022-12-12 08:02:26.237272: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 0 solved
[2022-12-12 08:02:26.237318: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 0 initing device 0
[2022-12-12 08:02:26.237948: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-12 08:02:26.238010: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 08:02:26.239265: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 08:02:26.240259: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 08:02:26.252659: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 7 solved
[2022-12-12 08:02:26.252715: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 7 initing device 7
[2022-12-12 08:02:26.252756: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 4 solved
[2022-12-12 08:02:26.252832: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 4 initing device 4
[2022-12-12 08:02:26.253082: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 1 solved
[2022-12-12 08:02:26.[2531332022-12-12 08:02:26: .E253140 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE: 1815/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc] :Building Coll Cache with ... num gpu device is 8205
] [worker 0 thread 1 initing device 12022-12-12 08:02:26[
.2022-12-12 08:02:26253167.: 253200E:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc :/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu202:1980] ] 6 solved[eager alloc mem 381.47 MB
2022-12-12 08:02:26
.[2532732022-12-12 08:02:26: .E253308 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE: 1815/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc] :Building Coll Cache with ... num gpu device is 8205
] worker 0 thread 6 initing device 6
[2022-12-12 08:02:26.253373: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 08:02:26.253611: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-12 08:02:26.253657: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 08:02:26.253760: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-12 08:02:26.253807: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 08:02:26.255195: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 2 solved[
2022-12-12 08:02:26.255224: [E2022-12-12 08:02:26.255257 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.ccE: 202/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc] :3 solved205
] worker 0 thread 2 initing device 2
[2022-12-12 08:02:26.255328: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 3 initing device 3
[2022-12-12 08:02:26.255721: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-12 08:02:26.255747: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu[:2022-12-12 08:02:261815.] 255764Building Coll Cache with ... num gpu device is 8: 
E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 08:02:26.255816: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 08:02:26.255894: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 08:02:26.256033: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 08:02:26.256100: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 08:02:26.257535: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 08:02:26.259587: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 08:02:26.259658: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:[19802022-12-12 08:02:26] .eager alloc mem 381.47 MB259682
: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 08:02:26.259799: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 08:02:26.259861: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 08:02:26.259948: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 5 solved
[2022-12-12 08:02:26.260027: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 5 initing device 5
[2022-12-12 08:02:26.260652: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-12 08:02:26.260735: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 08:02:26.261224: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 08:02:26.264066: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 08:02:26.264138: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 08:02:26.265502: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 08:02:26.266884: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 08:02:26.322547: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.00 Bytes
[2022-12-12 08:02:26.333691: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5
[2022-12-12 08:02:26.333818: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 08:02:26.337263: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 114.82 MB
[2022-12-12 08:02:26.338118: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 08:02:26.339217: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 08:02:26.339266: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 95.37 MB
[2022-12-12 08:02:26.343160: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.00 Bytes
[2022-12-12 08:02:26.345553: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.00 Bytes
[2022-12-12 08:02:26.348108: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5
[2022-12-12 08:02:26.348193: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 08:02:26.349222: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 114.82 MB
[2022-12-12 08:02:26.349863: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 08:02:26.350651: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.00 Bytes
[[2022-12-12 08:02:26[2022-12-12 08:02:26.2022-12-12 08:02:26.350720.: 350720350746E: :  EE/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu  :/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu1980::] 19801980eager alloc mem 5.00 Bytes] ] 
eager alloc mem 5.00 Byteseager alloc mem 5.00 Bytes

[2022-12-12 08:02:26.350952: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[[2022-12-12 08:02:262022-12-12 08:02:26..350981351001: : EW  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc::63843] ] eager release cuda mem 5WORKER[0] alloc host memory 95.37 MB

[2022-12-12 08:02:26.351083: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 08:02:26.351887: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 114.82 MB
[2022-12-12 08:02:26.352473: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 08:02:26.353543: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 08:02:26.353588: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 95.37 MB
[2022-12-12 08:02:26.353596: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.00 Bytes
[2022-12-12 08:02:26.356489: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5
[[2022-12-12 08:02:262022-12-12 08:02:26..356582356568: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::638638] ] eager release cuda mem 400000000eager release cuda mem 5

[2022-12-12 08:02:26.356675: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 08:02:26.356756: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5
[2022-12-12 08:02:26.356834: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 08:02:26.356873: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5
[2022-12-12 08:02:26.356990: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 08:02:26.357674: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 114.82 MB
[2022-12-12 08:02:26.358337: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 114.82 MB
[2022-12-12 08:02:26.358517: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5
[2022-12-12 08:02:26.358602: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 08:02:26.359027: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 114.82 MB
[2022-12-12 08:02:26.360237: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 114.82 MB
[2022-12-12 08:02:26.361079: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 08:02:26.361403: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 114.82 MB
[2022-12-12 08:02:26.361768: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 08:02:26.361878: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 08:02:26.362152: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 08:02:26.362198: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 95.37 MB
[2022-12-12 08:02:26.362386: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 08:02:26.362467: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 08:02:26.362860: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 08:02:26.362907: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 95.37 MB
[2022-12-12 08:02:26.362960: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 08:02:26.363005: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 95.37 MB
[2022-12-12 08:02:26.363508: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc[:2022-12-12 08:02:26638.] 363540eager release cuda mem 625663: 
E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 08:02:26[.2022-12-12 08:02:26363613.: 363625W:  W/hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc :/hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc43:] 43WORKER[0] alloc host memory 95.37 MB] 
WORKER[0] alloc host memory 95.37 MB
[2022-12-12 08:02:26.403121: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 08:02:26.403758: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 08:02:26.403801: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 11.93 GB
[2022-12-12 08:02:26.411773: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 08:02:26.412384: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 08:02:26.412428: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 11.93 GB
[2022-12-12 08:02:26.416200: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 08:02:26.416812: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 08:02:26.416854: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 11.93 GB
[2022-12-12 08:02:26.424702: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 08:02:26.425327: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 08:02:26.425371: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 11.93 GB
[2022-12-12 08:02:26.426656: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 08:02:26.427141: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 08:02:26.427279: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 08:02:26.427322: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 11.93 GB
[2022-12-12 08:02:26.427748: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 08:02:26.427791: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 11.93 GB
[2022-12-12 08:02:26.428114: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 08:02:26.428162: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 08:02:26.428730: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[[2022-12-12 08:02:262022-12-12 08:02:26..428768428775: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::6381980] ] eager release cuda mem 25855eager alloc mem 11.93 GB

[2022-12-12 08:02:26.428866: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 11.93 GB
[[[[[[[[2022-12-12 08:02:302022-12-12 08:02:302022-12-12 08:02:302022-12-12 08:02:302022-12-12 08:02:302022-12-12 08:02:302022-12-12 08:02:302022-12-12 08:02:30........806250806250806250806250806250806254806250806250: : : : : : : : EEEEEEEE        /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::::::::19261926192619261926192619261926] ] ] ] ] ] ] ] Device 5 init p2p of link 6Device 3 init p2p of link 2Device 0 init p2p of link 3Device 7 init p2p of link 4Device 6 init p2p of link 0Device 2 init p2p of link 1Device 1 init p2p of link 7Device 4 init p2p of link 5







[[[2022-12-12 08:02:30[[[2022-12-12 08:02:30[2022-12-12 08:02:30.2022-12-12 08:02:302022-12-12 08:02:302022-12-12 08:02:30.2022-12-12 08:02:30.806819.[..806819.806819: 8068212022-12-12 08:02:30806821806821: 806821: E: .: : E: E E806867EE E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu :   /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980: :19801980:1980] 1980/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu1980] ] 1980] eager alloc mem 611.00 KB] :] eager alloc mem 611.00 KBeager alloc mem 611.00 KB] eager alloc mem 611.00 KB
eager alloc mem 611.00 KB1980eager alloc mem 611.00 KB

eager alloc mem 611.00 KB

] 

eager alloc mem 611.00 KB
[2022-12-12 08:02:30.807931: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 08:02:30.808013: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[[2022-12-12 08:02:302022-12-12 08:02:30..808115[808116: 2022-12-12 08:02:30[: E[.[2022-12-12 08:02:30E 2022-12-12 08:02:308081272022-12-12 08:02:30. /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc.: .808132/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:808136E808141: :638:  : E638] E/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccE ] eager release cuda mem 625663 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cceager release cuda mem 625663
/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc638/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:
:] :638638eager release cuda mem 625663638] ] 
] eager release cuda mem 625663eager release cuda mem 625663eager release cuda mem 625663


[2022-12-12 08:02:30.822106: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 6 init p2p of link 5
[2022-12-12 08:02:30.822274: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 08:02:30.822492: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 2
[2022-12-12 08:02:30.822711: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 08:02:30.823232: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 08:02:30.823734: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 08:02:30.831405: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 4 init p2p of link 7
[2022-12-12 08:02:30.831576: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[[2022-12-12 08:02:302022-12-12 08:02:30..832136832138: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::19261926] ] Device 5 init p2p of link 4Device 7 init p2p of link 1

[2022-12-12 08:02:30.832316: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] [Device 0 init p2p of link 6[2022-12-12 08:02:30
2022-12-12 08:02:30[..2022-12-12 08:02:30832355832344.: : 832372EE:   E/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu ::/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu19801926:] ] 1980eager alloc mem 611.00 KBDevice 2 init p2p of link 3] 

eager alloc mem 611.00 KB
[2022-12-12 08:02:30.832508: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 08:02:30.832532: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 08:02:30.832552: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 3 init p2p of link 0
[2022-12-12 08:02:30.832618: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 08:02:30.832724: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 08:02:30.833259: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 08:02:30.833284: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 08:02:30.833440: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 08:02:30.833590: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 08:02:30.833665: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 08:02:30.836897: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 6 init p2p of link 4
[2022-12-12 08:02:30.837025: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 08:02:30.837270: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 3
[2022-12-12 08:02:30.837449: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 08:02:30.837957: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 08:02:30.838422: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 08:02:30.845840: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 7 init p2p of link 6
[2022-12-12 08:02:30.845972: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 08:02:30.846206: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 0 init p2p of link 1
[2022-12-12 08:02:30.846356: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 08:02:30.846937: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 08:02:30.847299: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 08:02:30.852443: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 4 init p2p of link 2
[2022-12-12 08:02:30.852581: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 08:02:30.853375: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 08:02:30.855178: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 5 init p2p of link 7
[2022-12-12 08:02:30.855311: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 08:02:30.855421: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 2 init p2p of link 0
[2022-12-12 08:02:30.855547: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[[2022-12-12 08:02:302022-12-12 08:02:30..855975855982: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::19261926] ] Device 3 init p2p of link 5Device 4 init p2p of link 6

[2022-12-12 08:02:30.856161: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:[19802022-12-12 08:02:30] .eager alloc mem 611.00 KB856176
: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 08:02:30.856237: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 08:02:30.856475: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 08:02:30.856977: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 08:02:30.857135: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 08:02:30.864442: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 3 init p2p of link 1
[2022-12-12 08:02:30.864569: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 08:02:30.865523: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 08:02:30.867602: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 6 init p2p of link 7
[2022-12-12 08:02:30.867723: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 08:02:30.867734: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 0
[2022-12-12 08:02:30.867902: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 08:02:30.868684: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 08:02:30.868901: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 08:02:30.869956: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 7 init p2p of link 5
[2022-12-12 08:02:30.870083: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 08:02:30.870347: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 0 init p2p of link 2
[2022-12-12 08:02:30.870478: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 08:02:30.871027: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 08:02:30.871427: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 08:02:30.881531: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 5 init p2p of link 3
[2022-12-12 08:02:30.881654: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 08:02:30.881989: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 2 init p2p of link 4
[2022-12-12 08:02:30.882111: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 08:02:30.882428: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 08:02:30.882586: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 120400004
[2022-12-12 08:02:30.882891: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 08:02:30.884378: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 25000000 / 100000000 nodes ( 25.00 %~30.00 %) | remote 75000000 / 100000000 nodes ( 75.00 %) | cpu 0 / 100000000 nodes ( 0.00 %) | 11.93 GB | 4.63102 secs 
[2022-12-12 08:02:30.885658: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 120400004
[2022-12-12 08:02:30.887192: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 25000000 / 100000000 nodes ( 25.00 %~30.00 %) | remote 75000000 / 100000000 nodes ( 75.00 %) | cpu 0 / 100000000 nodes ( 0.00 %) | 11.93 GB | 4.63138 secs 
[2022-12-12 08:02:30.887531: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 120400004
[2022-12-12 08:02:30.888099: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 120400004
[2022-12-12 08:02:30.889670: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 25000000 / 100000000 nodes ( 25.00 %~30.00 %) | remote 75000000 / 100000000 nodes ( 75.00 %) | cpu 0 / 100000000 nodes ( 0.00 %) | 11.93 GB | 4.62895 secs 
[2022-12-12 08:02:30.890112: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 25000000 / 100000000 nodes ( 25.00 %~30.00 %) | remote 75000000 / 100000000 nodes ( 75.00 %) | cpu 0 / 100000000 nodes ( 0.00 %) | 11.93 GB | 4.63436 secs 
[2022-12-12 08:02:30.893477: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 120400004
[2022-12-12 08:02:30.894095: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 120400004
[2022-12-12 08:02:30.895706: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 120400004
[2022-12-12 08:02:30.895803: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 25000000 / 100000000 nodes ( 25.00 %~30.00 %) | remote 75000000 / 100000000 nodes ( 75.00 %) | cpu 0 / 100000000 nodes ( 0.00 %) | 11.93 GB | 4.64201 secs 
[2022-12-12 08:02:30.896112: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 120400004
[2022-12-12 08:02:30.896229: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 25000000 / 100000000 nodes ( 25.00 %~30.00 %) | remote 75000000 / 100000000 nodes ( 75.00 %) | cpu 0 / 100000000 nodes ( 0.00 %) | 11.93 GB | 4.64258 secs 
[2022-12-12 08:02:30.897700: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 25000000 / 100000000 nodes ( 25.00 %~30.00 %) | remote 75000000 / 100000000 nodes ( 75.00 %) | cpu 0 / 100000000 nodes ( 0.00 %) | 11.93 GB | 4.64451 secs 
[2022-12-12 08:02:30.898812: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 25000000 / 100000000 nodes ( 25.00 %~30.00 %) | remote 75000000 / 100000000 nodes ( 75.00 %) | cpu 0 / 100000000 nodes ( 0.00 %) | 11.93 GB | 4.66081 secs 
[2022-12-12 08:02:30.899778: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 20.24 GB
[2022-12-12 08:02:32.227261: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 20.51 GB
[2022-12-12 08:02:32.227674: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 20.51 GB
[2022-12-12 08:02:32.228073: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 20.51 GB
[2022-12-12 08:02:33.606842: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 20.77 GB
[2022-12-12 08:02:33.630027: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 20.77 GB
[2022-12-12 08:02:33.638514: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 20.77 GB
[2022-12-12 08:02:34.959032: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 20.99 GB
[2022-12-12 08:02:34.959632: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 20.99 GB
[2022-12-12 08:02:34.961236: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 20.99 GB
[2022-12-12 08:02:36.154076: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 21.20 GB
[2022-12-12 08:02:36.154858: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 21.20 GB
[2022-12-12 08:02:36.155752: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 21.20 GB
[2022-12-12 08:02:37.309889: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 21.66 GB
[2022-12-12 08:02:37.310795: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 21.66 GB
[2022-12-12 08:02:37.311985: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 21.66 GB
[2022-12-12 08:02:38.679769: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 21.86 GB
[2022-12-12 08:02:38.680948: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 21.86 GB
[HCTR][08:02:39.883][ERROR][RK0][main]: replica 5 calling init per replica done, doing barrier
[HCTR][08:02:39.883][ERROR][RK0][main]: replica 7 calling init per replica done, doing barrier
[HCTR][08:02:39.883][ERROR][RK0][tid #140592424134400]: replica 0 calling init per replica done, doing barrier
[HCTR][08:02:39.883][ERROR][RK0][tid #140592617068288]: replica 2 calling init per replica done, doing barrier
[HCTR][08:02:39.883][ERROR][RK0][tid #140592818394880]: replica 3 calling init per replica done, doing barrier
[HCTR][08:02:39.883][ERROR][RK0][tid #140592424134400]: replica 4 calling init per replica done, doing barrier
[HCTR][08:02:39.883][ERROR][RK0][tid #140592063444736]: replica 1 calling init per replica done, doing barrier
[HCTR][08:02:39.883][ERROR][RK0][tid #140591996335872]: replica 6 calling init per replica done, doing barrier
[HCTR][08:02:39.883][ERROR][RK0][main]: replica 7 calling init per replica done, doing barrier done
[HCTR][08:02:39.883][ERROR][RK0][tid #140592063444736]: replica 1 calling init per replica done, doing barrier done
[HCTR][08:02:39.883][ERROR][RK0][tid #140592424134400]: replica 4 calling init per replica done, doing barrier done
[HCTR][08:02:39.883][ERROR][RK0][tid #140592617068288]: replica 2 calling init per replica done, doing barrier done
[HCTR][08:02:39.883][ERROR][RK0][tid #140592424134400]: replica 0 calling init per replica done, doing barrier done
[HCTR][08:02:39.883][ERROR][RK0][main]: replica 5 calling init per replica done, doing barrier done
[HCTR][08:02:39.883][ERROR][RK0][tid #140592818394880]: replica 3 calling init per replica done, doing barrier done
[HCTR][08:02:39.883][ERROR][RK0][tid #140591996335872]: replica 6 calling init per replica done, doing barrier done
[HCTR][08:02:39.883][ERROR][RK0][main]: init per replica done
[HCTR][08:02:39.883][ERROR][RK0][tid #140592063444736]: init per replica done
[HCTR][08:02:39.883][ERROR][RK0][tid #140592424134400]: init per replica done
[HCTR][08:02:39.883][ERROR][RK0][tid #140592617068288]: init per replica done
[HCTR][08:02:39.883][ERROR][RK0][main]: init per replica done
[HCTR][08:02:39.883][ERROR][RK0][tid #140592818394880]: init per replica done
[HCTR][08:02:39.883][ERROR][RK0][tid #140591996335872]: init per replica done
[HCTR][08:02:39.886][ERROR][RK0][tid #140592424134400]: init per replica done
[HCTR][08:02:39.922][ERROR][RK0][tid #140592617068288]: 2 allocated 3276800 at 0x7fbea4238400
[HCTR][08:02:39.922][ERROR][RK0][tid #140592617068288]: 2 allocated 6553600 at 0x7fbea4558400
[HCTR][08:02:39.922][ERROR][RK0][tid #140592617068288]: 2 allocated 3276800 at 0x7fbea4b98400
[HCTR][08:02:39.922][ERROR][RK0][tid #140592617068288]: 2 allocated 6553600 at 0x7fbea4eb8400
[HCTR][08:02:39.922][ERROR][RK0][main]: 5 allocated 3276800 at 0x7fbf44238400
[HCTR][08:02:39.922][ERROR][RK0][tid #140592424134400]: 7 allocated 3276800 at 0x7fbf18238400
[HCTR][08:02:39.922][ERROR][RK0][main]: 5 allocated 6553600 at 0x7fbf44558400
[HCTR][08:02:39.922][ERROR][RK0][tid #140592424134400]: 7 allocated 6553600 at 0x7fbf18558400
[HCTR][08:02:39.922][ERROR][RK0][main]: 5 allocated 3276800 at 0x7fbf44b98400
[HCTR][08:02:39.922][ERROR][RK0][tid #140592424134400]: 7 allocated 3276800 at 0x7fbf18b98400
[HCTR][08:02:39.922][ERROR][RK0][main]: 5 allocated 6553600 at 0x7fbf44eb8400
[HCTR][08:02:39.922][ERROR][RK0][tid #140592424134400]: 7 allocated 6553600 at 0x7fbf18eb8400
[HCTR][08:02:39.922][ERROR][RK0][main]: 4 allocated 3276800 at 0x7fbe60238400
[HCTR][08:02:39.922][ERROR][RK0][tid #140592063444736]: 1 allocated 3276800 at 0x7fbe18238400
[HCTR][08:02:39.922][ERROR][RK0][main]: 4 allocated 6553600 at 0x7fbe60558400
[HCTR][08:02:39.922][ERROR][RK0][tid #140592063444736]: 1 allocated 6553600 at 0x7fbe18558400
[HCTR][08:02:39.922][ERROR][RK0][main]: 4 allocated 3276800 at 0x7fbe60b98400
[HCTR][08:02:39.922][ERROR][RK0][tid #140592063444736]: 1 allocated 3276800 at 0x7fbe18b98400
[HCTR][08:02:39.922][ERROR][RK0][main]: 4 allocated 6553600 at 0x7fbe60eb8400
[HCTR][08:02:39.922][ERROR][RK0][tid #140592063444736]: 1 allocated 6553600 at 0x7fbe18eb8400
[HCTR][08:02:39.922][ERROR][RK0][main]: 6 allocated 3276800 at 0x7fbe34238400
[HCTR][08:02:39.922][ERROR][RK0][main]: 6 allocated 6553600 at 0x7fbe34558400
[HCTR][08:02:39.922][ERROR][RK0][main]: 6 allocated 3276800 at 0x7fbe34b98400
[HCTR][08:02:39.922][ERROR][RK0][main]: 6 allocated 6553600 at 0x7fbe34eb8400
[HCTR][08:02:39.922][ERROR][RK0][main]: 3 allocated 3276800 at 0x7fbee0238400
[HCTR][08:02:39.922][ERROR][RK0][main]: 3 allocated 6553600 at 0x7fbee0558400
[HCTR][08:02:39.922][ERROR][RK0][main]: 3 allocated 3276800 at 0x7fbee0b98400
[HCTR][08:02:39.922][ERROR][RK0][main]: 3 allocated 6553600 at 0x7fbee0eb8400
[HCTR][08:02:39.924][ERROR][RK0][main]: 0 allocated 3276800 at 0x7fbef8320000
[HCTR][08:02:39.924][ERROR][RK0][main]: 0 allocated 6553600 at 0x7fbef8640000
[HCTR][08:02:39.924][ERROR][RK0][main]: 0 allocated 3276800 at 0x7fbef8c80000
[HCTR][08:02:39.924][ERROR][RK0][main]: 0 allocated 6553600 at 0x7fbef8fa0000
