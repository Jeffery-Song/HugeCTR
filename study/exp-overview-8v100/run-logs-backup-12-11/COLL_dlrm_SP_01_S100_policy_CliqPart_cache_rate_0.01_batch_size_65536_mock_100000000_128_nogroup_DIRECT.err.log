2022-12-11 20:11:07.171917: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.177840: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.185472: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.191280: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.197657: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.209105: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.218437: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.230149: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.280423: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.289815: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.293198: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.294136: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.295020: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.296000: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.296945: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.297958: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.299174: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.300911: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.301857: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.302178: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.303582: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.303616: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.305339: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.305381: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.306746: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.307014: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.308395: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.308643: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.310103: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.310454: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.311962: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.312349: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.313685: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.314044: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.315549: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.316588: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.317624: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.318892: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.320607: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.321198: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.321910: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.322792: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.324142: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.325137: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.326120: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.327104: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.327792: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 20:11:07.328191: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.329263: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.333306: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.334541: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.335617: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.336650: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.336681: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.338155: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.338425: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.339522: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.340062: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.342109: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.344253: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.344378: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.346897: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.349291: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.349743: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.351266: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.351928: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.353275: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.354241: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.355517: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.356721: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.357856: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.359333: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.359682: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.360599: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.360633: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.362201: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.374125: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.375886: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.378453: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.378942: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.379739: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.379950: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.379973: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.381482: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.382283: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.382974: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.383062: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.383213: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.384912: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.390145: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.417243: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.417422: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.417518: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.419437: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.419583: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.420753: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.420848: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.420943: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.421729: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.423508: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.423839: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.424622: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.425678: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.425970: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.426428: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.428669: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.429586: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.430060: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.430301: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.430791: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.431115: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.432628: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.433781: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.433913: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.434150: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.435658: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.435714: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.437110: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.438411: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.438707: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.440219: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.440426: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.441510: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.442594: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.442877: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.443536: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.444810: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.445703: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.445825: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.446634: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.447678: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.448704: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.448744: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.449520: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.450414: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.451620: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.451662: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.452241: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.453397: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.454781: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.454999: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.455396: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.456552: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.457947: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.458009: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.458492: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.459877: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.460871: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.461387: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.461805: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.461866: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.462596: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.464549: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.464628: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.465081: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.465090: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.465759: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.467799: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.467921: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.468364: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.468444: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.469018: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.471486: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.471527: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.472120: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.472327: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.474132: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.474790: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.474840: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.475340: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.475470: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.477840: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.478383: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.478846: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.479094: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.479192: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.481357: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.481897: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.482101: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.482291: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.482665: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.482793: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.485291: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.485761: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.486322: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.486392: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.486799: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.486944: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.487318: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.489904: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.490642: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.491374: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.491409: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.492243: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.492558: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.494498: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.495218: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.495327: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.495998: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.496281: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.496361: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 20:11:07.497877: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.498636: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.498817: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.498819: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 20:11:07.499302: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.499587: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.501493: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.502025: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.502212: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.502663: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.502879: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.505544: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.505595: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.505716: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.506285: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.506547: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.507666: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.508429: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 20:11:07.509252: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.509260: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.509492: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.510344: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.511649: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.512863: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.513037: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.513148: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.513866: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.514136: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 20:11:07.515120: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.516332: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.516614: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.516633: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.517581: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.520634: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.521353: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.521759: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.522134: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.522551: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.525162: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.525917: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.526379: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.527018: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.527319: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.529772: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.531123: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.531838: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.532304: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.534741: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.536108: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.536901: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.539461: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.569724: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.570585: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.572602: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.574396: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.575606: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.579270: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.579350: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.581242: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.584288: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.584942: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.587601: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.590625: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.590868: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.595670: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 20:11:07.603180: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.603519: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.604649: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.615782: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.616145: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.616802: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.650391: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.650805: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.651151: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.683471: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.683714: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.691652: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.692016: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.700849: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 20:11:07.709378: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.724065: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.730661: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.735039: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 20:11:07.743591: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.762998: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.764354: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:07.770508: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:08.661326: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:08.661942: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:08.662884: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:08.663603: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 20:11:08.663665: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-11 20:11:08.681855: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:08.682517: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:08.687004: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:08.687768: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:08.688310: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:08.688796: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-11 20:11:08.733924: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 20:11:08.734144: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 20:11:08.779808: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12340
2022-12-11 20:11:08.899091: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:08.899200: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:08.900171: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:08.900249: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:08.901115: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:08.901191: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:08.901992: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 20:11:08.902051: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-11 20:11:08.902072: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 20:11:08.902132: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-11 20:11:08.920251: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:08.920251: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:08.921344: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:08.921390: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:08.922625: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:08.922805: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:08.923603: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:08.923774: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:08.924629: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:08.924899: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:08.925781: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:2/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-11 20:11:08.925858: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:1/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-11 20:11:08.937472: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:08.938086: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:08.938603: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:08.939277: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 20:11:08.939334: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-11 20:11:08.952300: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:08.952917: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:08.953465: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:08.953936: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 20:11:08.953992: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-11 20:11:08.956449: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:08.957071: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:08.957570: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:08.958254: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:08.958766: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:08.959247: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:7/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-11 20:11:08.972576: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:08.973230: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:08.973731: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:08.974314: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:08.975032: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:08.975517: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:4/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-11 20:11:08.979597: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:08.980216: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:08.980731: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:08.981209: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 20:11:08.981254: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-11 20:11:08.991040: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:08.991735: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:08.992284: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:08.992757: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 20:11:08.992809: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-11 20:11:08.999071: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:08.999775: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:09.000365: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:09.000944: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:09.001470: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:09.001937: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:3/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-11 20:11:09.004820: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 20:11:09.005071: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 20:11:09.006920: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12347
2022-12-11 20:11:09.007168: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 20:11:09.007319: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 20:11:09.009361: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12342
2022-12-11 20:11:09.010014: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 20:11:09.010151: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 20:11:09.011617: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:09.011986: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12341
2022-12-11 20:11:09.012274: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:09.012522: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:09.013149: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:09.013570: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:09.014035: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:09.014456: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:09.014916: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:09.015350: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 20:11:09.015405: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-11 20:11:09.015687: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:5/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-11 20:11:09.022593: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 20:11:09.022760: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 20:11:09.023763: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12344
2022-12-11 20:11:09.033938: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:09.034601: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:09.035112: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:09.035708: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:09.036226: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:11:09.036695: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:6/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-11 20:11:09.047564: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 20:11:09.047761: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 20:11:09.051755: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12343
2022-12-11 20:11:09.061234: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 20:11:09.061399: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 20:11:09.063384: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12345
2022-12-11 20:11:09.081112: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 20:11:09.081312: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 20:11:09.083684: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12346
[HCTR][20:11:10.350][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][20:11:10.350][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][20:11:10.350][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][20:11:10.350][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][20:11:10.350][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][20:11:10.354][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][20:11:10.374][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][20:11:10.374][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 1it [00:01,  1.54s/it]warmup run: 1it [00:01,  1.49s/it]warmup run: 1it [00:01,  1.51s/it]warmup run: 97it [00:01, 81.84it/s]warmup run: 1it [00:01,  1.55s/it]warmup run: 1it [00:01,  1.54s/it]warmup run: 1it [00:01,  1.51s/it]warmup run: 99it [00:01, 86.17it/s]warmup run: 82it [00:01, 70.41it/s]warmup run: 193it [00:01, 176.50it/s]warmup run: 98it [00:01, 82.64it/s]warmup run: 93it [00:01, 78.71it/s]warmup run: 1it [00:01,  1.47s/it]warmup run: 1it [00:01,  1.49s/it]warmup run: 98it [00:01, 84.34it/s]warmup run: 199it [00:01, 187.34it/s]warmup run: 174it [00:01, 163.51it/s]warmup run: 290it [00:01, 282.20it/s]warmup run: 195it [00:01, 178.25it/s]warmup run: 185it [00:01, 169.68it/s]warmup run: 100it [00:01, 88.35it/s]warmup run: 100it [00:01, 87.24it/s]warmup run: 196it [00:01, 182.50it/s]warmup run: 299it [00:01, 298.11it/s]warmup run: 267it [00:01, 267.09it/s]warmup run: 388it [00:01, 393.51it/s]warmup run: 294it [00:01, 286.22it/s]warmup run: 278it [00:01, 271.18it/s]warmup run: 200it [00:01, 190.69it/s]warmup run: 201it [00:01, 189.73it/s]warmup run: 294it [00:01, 290.29it/s]warmup run: 399it [00:01, 412.07it/s]warmup run: 360it [00:01, 374.31it/s]warmup run: 486it [00:02, 502.49it/s]warmup run: 371it [00:01, 376.66it/s]warmup run: 393it [00:01, 398.22it/s]warmup run: 301it [00:01, 303.56it/s]warmup run: 301it [00:01, 300.40it/s]warmup run: 391it [00:01, 400.36it/s]warmup run: 499it [00:01, 522.73it/s]warmup run: 446it [00:02, 464.63it/s]warmup run: 586it [00:02, 605.96it/s]warmup run: 464it [00:02, 479.39it/s]warmup run: 489it [00:02, 502.32it/s]warmup run: 402it [00:01, 419.30it/s]warmup run: 402it [00:01, 416.06it/s]warmup run: 489it [00:02, 508.48it/s]warmup run: 599it [00:02, 623.98it/s]warmup run: 539it [00:02, 562.46it/s]warmup run: 684it [00:02, 690.41it/s]warmup run: 588it [00:02, 604.48it/s]warmup run: 558it [00:02, 575.44it/s]warmup run: 501it [00:01, 527.30it/s]warmup run: 504it [00:01, 530.10it/s]warmup run: 589it [00:02, 611.99it/s]warmup run: 700it [00:02, 713.09it/s]warmup run: 633it [00:02, 649.58it/s]warmup run: 688it [00:02, 694.95it/s]warmup run: 783it [00:02, 762.72it/s]warmup run: 655it [00:02, 665.25it/s]warmup run: 602it [00:02, 629.40it/s]warmup run: 607it [00:02, 635.75it/s]warmup run: 689it [00:02, 701.89it/s]warmup run: 801it [00:02, 786.84it/s]warmup run: 727it [00:02, 721.01it/s]warmup run: 787it [00:02, 767.51it/s]warmup run: 750it [00:02, 735.40it/s]warmup run: 882it [00:02, 820.39it/s]warmup run: 702it [00:02, 715.88it/s]warmup run: 706it [00:02, 716.84it/s]warmup run: 787it [00:02, 770.94it/s]warmup run: 902it [00:02, 843.36it/s]warmup run: 823it [00:02, 782.54it/s]warmup run: 887it [00:02, 827.71it/s]warmup run: 982it [00:02, 868.97it/s]warmup run: 845it [00:02, 789.69it/s]warmup run: 802it [00:02, 786.71it/s]warmup run: 809it [00:02, 793.32it/s]warmup run: 884it [00:02, 819.54it/s]warmup run: 1002it [00:02, 885.45it/s]warmup run: 924it [00:02, 843.82it/s]warmup run: 1084it [00:02, 910.93it/s]warmup run: 987it [00:02, 872.78it/s]warmup run: 939it [00:02, 830.10it/s]warmup run: 902it [00:02, 840.93it/s]warmup run: 911it [00:02, 851.00it/s]warmup run: 1102it [00:02, 917.15it/s]warmup run: 1025it [00:02, 888.54it/s]warmup run: 981it [00:02, 807.91it/s]warmup run: 1086it [00:02, 905.04it/s]warmup run: 1186it [00:02, 940.19it/s]warmup run: 1037it [00:02, 871.60it/s]warmup run: 1001it [00:02, 877.88it/s]warmup run: 1012it [00:02, 893.80it/s]warmup run: 1202it [00:02, 936.74it/s]warmup run: 1126it [00:02, 921.17it/s]warmup run: 1077it [00:02, 847.81it/s]warmup run: 1138it [00:02, 909.74it/s]warmup run: 1288it [00:02, 960.35it/s]warmup run: 1185it [00:02, 922.47it/s]warmup run: 1100it [00:02, 890.99it/s]warmup run: 1113it [00:02, 926.26it/s]warmup run: 1302it [00:02, 946.57it/s]warmup run: 1228it [00:02, 949.53it/s]warmup run: 1179it [00:02, 894.24it/s]warmup run: 1390it [00:02, 977.16it/s]warmup run: 1240it [00:02, 938.90it/s]warmup run: 1284it [00:02, 941.55it/s]warmup run: 1197it [00:02, 906.78it/s]warmup run: 1215it [00:02, 950.70it/s]warmup run: 1401it [00:02, 955.00it/s]warmup run: 1327it [00:02, 952.43it/s]warmup run: 1281it [00:02, 927.72it/s]warmup run: 1493it [00:03, 991.93it/s]warmup run: 1338it [00:02, 949.84it/s]warmup run: 1383it [00:02, 949.74it/s]warmup run: 1293it [00:02, 912.56it/s]warmup run: 1318it [00:02, 971.25it/s]warmup run: 1500it [00:03, 945.97it/s]warmup run: 1383it [00:02, 953.88it/s]warmup run: 1425it [00:03, 946.58it/s]warmup run: 1596it [00:03, 1002.25it/s]warmup run: 1436it [00:03, 953.23it/s]warmup run: 1481it [00:03, 957.37it/s]warmup run: 1388it [00:02, 918.98it/s]warmup run: 1420it [00:02, 980.06it/s]warmup run: 1597it [00:03, 950.21it/s]warmup run: 1485it [00:03, 971.55it/s]warmup run: 1522it [00:03, 941.33it/s]warmup run: 1700it [00:03, 1011.52it/s]warmup run: 1580it [00:03, 966.34it/s]warmup run: 1534it [00:03, 957.93it/s]warmup run: 1483it [00:02, 924.89it/s]warmup run: 1522it [00:02, 991.06it/s]warmup run: 1694it [00:03, 955.45it/s]warmup run: 1587it [00:03, 984.85it/s]warmup run: 1618it [00:03, 933.94it/s]warmup run: 1803it [00:03, 1012.09it/s]warmup run: 1679it [00:03, 971.18it/s]warmup run: 1632it [00:03, 961.62it/s]warmup run: 1580it [00:03, 937.29it/s]warmup run: 1624it [00:03, 999.48it/s]warmup run: 1792it [00:03, 960.57it/s]warmup run: 1690it [00:03, 996.13it/s]warmup run: 1713it [00:03, 926.14it/s]warmup run: 1906it [00:03, 1017.11it/s]warmup run: 1778it [00:03, 976.66it/s]warmup run: 1731it [00:03, 968.00it/s]warmup run: 1677it [00:03, 946.89it/s]warmup run: 1726it [00:03, 1001.80it/s]warmup run: 1889it [00:03, 961.90it/s]warmup run: 1793it [00:03, 1003.40it/s]warmup run: 1813it [00:03, 946.30it/s]warmup run: 2012it [00:03, 1028.40it/s]warmup run: 1878it [00:03, 980.79it/s]warmup run: 1829it [00:03, 969.62it/s]warmup run: 1775it [00:03, 953.86it/s]warmup run: 1828it [00:03, 998.34it/s] warmup run: 1986it [00:03, 962.06it/s]warmup run: 1895it [00:03, 1006.42it/s]warmup run: 1916it [00:03, 970.18it/s]warmup run: 2133it [00:03, 1080.59it/s]warmup run: 1979it [00:03, 987.38it/s]warmup run: 1928it [00:03, 973.93it/s]warmup run: 1872it [00:03, 957.70it/s]warmup run: 1929it [00:03, 998.96it/s]warmup run: 2104it [00:03, 1025.75it/s]warmup run: 1997it [00:03, 1007.54it/s]warmup run: 2016it [00:03, 976.54it/s]warmup run: 2253it [00:03, 1114.87it/s]warmup run: 2031it [00:03, 990.22it/s]warmup run: 2095it [00:03, 1036.47it/s]warmup run: 1970it [00:03, 962.43it/s]warmup run: 2034it [00:03, 1013.32it/s]warmup run: 2227it [00:03, 1085.57it/s]warmup run: 2115it [00:03, 1058.81it/s]warmup run: 2136it [00:03, 1042.24it/s]warmup run: 2373it [00:03, 1140.01it/s]warmup run: 2150it [00:03, 1049.34it/s]warmup run: 2214it [00:03, 1081.07it/s]warmup run: 2085it [00:03, 1017.71it/s]warmup run: 2153it [00:03, 1064.96it/s]warmup run: 2350it [00:03, 1127.46it/s]warmup run: 2236it [00:03, 1101.47it/s]warmup run: 2256it [00:03, 1088.25it/s]warmup run: 2495it [00:03, 1161.19it/s]warmup run: 2269it [00:03, 1091.29it/s]warmup run: 2334it [00:03, 1114.78it/s]warmup run: 2209it [00:03, 1083.71it/s]warmup run: 2272it [00:03, 1100.97it/s]warmup run: 2472it [00:03, 1154.17it/s]warmup run: 2357it [00:03, 1133.74it/s]warmup run: 2376it [00:03, 1120.06it/s]warmup run: 2616it [00:04, 1175.61it/s]warmup run: 2389it [00:03, 1122.07it/s]warmup run: 2454it [00:03, 1138.08it/s]warmup run: 2333it [00:03, 1128.85it/s]warmup run: 2391it [00:03, 1125.41it/s]warmup run: 2593it [00:04, 1168.89it/s]warmup run: 2479it [00:03, 1156.72it/s]warmup run: 2496it [00:04, 1143.71it/s]warmup run: 2738it [00:04, 1187.31it/s]warmup run: 2510it [00:04, 1146.75it/s]warmup run: 2574it [00:04, 1154.25it/s]warmup run: 2456it [00:03, 1158.57it/s]warmup run: 2510it [00:03, 1143.59it/s]warmup run: 2713it [00:04, 1176.79it/s]warmup run: 2601it [00:04, 1174.18it/s]warmup run: 2616it [00:04, 1160.08it/s]warmup run: 2861it [00:04, 1197.66it/s]warmup run: 2630it [00:04, 1162.30it/s]warmup run: 2692it [00:04, 1160.89it/s]warmup run: 2578it [00:03, 1174.35it/s]warmup run: 2628it [00:04, 1153.31it/s]warmup run: 2834it [00:04, 1186.29it/s]warmup run: 2724it [00:04, 1187.96it/s]warmup run: 2735it [00:04, 1168.03it/s]warmup run: 2984it [00:04, 1205.50it/s]warmup run: 2752it [00:04, 1176.96it/s]warmup run: 2812it [00:04, 1169.67it/s]warmup run: 2698it [00:04, 1181.14it/s]warmup run: 3000it [00:04, 684.83it/s] warmup run: 2747it [00:04, 1163.55it/s]warmup run: 2954it [00:04, 1188.99it/s]warmup run: 2845it [00:04, 1191.78it/s]warmup run: 2853it [00:04, 1171.25it/s]warmup run: 3000it [00:04, 687.74it/s] warmup run: 2873it [00:04, 1184.31it/s]warmup run: 2933it [00:04, 1179.29it/s]warmup run: 2821it [00:04, 1194.11it/s]warmup run: 2865it [00:04, 1167.84it/s]warmup run: 2967it [00:04, 1198.84it/s]warmup run: 2971it [00:04, 1172.87it/s]warmup run: 3000it [00:04, 678.05it/s] warmup run: 3000it [00:04, 684.28it/s] warmup run: 3000it [00:04, 670.01it/s] warmup run: 2994it [00:04, 1190.30it/s]warmup run: 3000it [00:04, 671.63it/s] warmup run: 2943it [00:04, 1200.39it/s]warmup run: 2983it [00:04, 1169.86it/s]warmup run: 3000it [00:04, 693.16it/s] warmup run: 3000it [00:04, 689.75it/s] 



warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]

warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   6%|         | 165/3000 [00:00<00:01, 1646.65it/s]warmup should be done:   6%|         | 168/3000 [00:00<00:01, 1672.78it/s]warmup should be done:   5%|         | 162/3000 [00:00<00:01, 1614.02it/s]warmup should be done:   5%|         | 163/3000 [00:00<00:01, 1623.60it/s]warmup should be done:   6%|         | 165/3000 [00:00<00:01, 1643.19it/s]warmup should be done:   6%|         | 167/3000 [00:00<00:01, 1660.78it/s]warmup should be done:   5%|         | 162/3000 [00:00<00:01, 1613.40it/s]warmup should be done:   6%|         | 165/3000 [00:00<00:01, 1641.64it/s]warmup should be done:  11%|         | 331/3000 [00:00<00:01, 1652.00it/s]warmup should be done:  11%|         | 328/3000 [00:00<00:01, 1636.64it/s]warmup should be done:  11%|         | 325/3000 [00:00<00:01, 1620.31it/s]warmup should be done:  11%|         | 331/3000 [00:00<00:01, 1650.87it/s]warmup should be done:  11%|         | 325/3000 [00:00<00:01, 1620.02it/s]warmup should be done:  11%|         | 335/3000 [00:00<00:01, 1668.26it/s]warmup should be done:  11%|         | 332/3000 [00:00<00:01, 1655.87it/s]warmup should be done:  11%|         | 336/3000 [00:00<00:01, 1663.22it/s]warmup should be done:  17%|        | 498/3000 [00:00<00:01, 1655.54it/s]warmup should be done:  16%|        | 492/3000 [00:00<00:01, 1633.85it/s]warmup should be done:  17%|        | 502/3000 [00:00<00:01, 1663.93it/s]warmup should be done:  17%|        | 497/3000 [00:00<00:01, 1647.25it/s]warmup should be done:  16%|        | 488/3000 [00:00<00:01, 1615.70it/s]warmup should be done:  16%|        | 488/3000 [00:00<00:01, 1615.14it/s]warmup should be done:  17%|        | 497/3000 [00:00<00:01, 1643.51it/s]warmup should be done:  17%|        | 503/3000 [00:00<00:01, 1653.69it/s]warmup should be done:  22%|       | 664/3000 [00:00<00:01, 1655.61it/s]warmup should be done:  22%|       | 656/3000 [00:00<00:01, 1631.46it/s]warmup should be done:  22%|       | 669/3000 [00:00<00:01, 1663.38it/s]warmup should be done:  22%|       | 662/3000 [00:00<00:01, 1644.83it/s]warmup should be done:  22%|       | 651/3000 [00:00<00:01, 1617.64it/s]warmup should be done:  22%|       | 650/3000 [00:00<00:01, 1614.09it/s]warmup should be done:  22%|       | 662/3000 [00:00<00:01, 1637.77it/s]warmup should be done:  22%|       | 669/3000 [00:00<00:01, 1650.59it/s]warmup should be done:  28%|       | 830/3000 [00:00<00:01, 1653.37it/s]warmup should be done:  27%|       | 814/3000 [00:00<00:01, 1620.04it/s]warmup should be done:  27%|       | 820/3000 [00:00<00:01, 1629.43it/s]warmup should be done:  28%|       | 827/3000 [00:00<00:01, 1641.52it/s]warmup should be done:  27%|       | 812/3000 [00:00<00:01, 1612.90it/s]warmup should be done:  28%|       | 836/3000 [00:00<00:01, 1657.98it/s]warmup should be done:  28%|       | 826/3000 [00:00<00:01, 1636.40it/s]warmup should be done:  28%|       | 835/3000 [00:00<00:01, 1647.12it/s]warmup should be done:  33%|      | 996/3000 [00:00<00:01, 1645.77it/s]warmup should be done:  33%|      | 983/3000 [00:00<00:01, 1623.95it/s]warmup should be done:  33%|      | 977/3000 [00:00<00:01, 1616.99it/s]warmup should be done:  33%|      | 1002/3000 [00:00<00:01, 1654.74it/s]warmup should be done:  32%|      | 974/3000 [00:00<00:01, 1607.36it/s]warmup should be done:  33%|      | 992/3000 [00:00<00:01, 1635.17it/s]warmup should be done:  33%|      | 990/3000 [00:00<00:01, 1630.11it/s]warmup should be done:  33%|      | 1000/3000 [00:00<00:01, 1640.03it/s]warmup should be done:  39%|      | 1161/3000 [00:00<00:01, 1645.47it/s]warmup should be done:  38%|      | 1146/3000 [00:00<00:01, 1623.49it/s]warmup should be done:  38%|      | 1139/3000 [00:00<00:01, 1613.96it/s]warmup should be done:  38%|      | 1135/3000 [00:00<00:01, 1606.64it/s]warmup should be done:  39%|      | 1156/3000 [00:00<00:01, 1634.58it/s]warmup should be done:  39%|      | 1168/3000 [00:00<00:01, 1650.64it/s]warmup should be done:  38%|      | 1154/3000 [00:00<00:01, 1629.95it/s]warmup should be done:  39%|      | 1165/3000 [00:00<00:01, 1638.90it/s]warmup should be done:  44%|     | 1309/3000 [00:00<00:01, 1625.48it/s]warmup should be done:  44%|     | 1326/3000 [00:00<00:01, 1645.61it/s]warmup should be done:  43%|     | 1296/3000 [00:00<00:01, 1606.54it/s]warmup should be done:  44%|     | 1321/3000 [00:00<00:01, 1638.27it/s]warmup should be done:  44%|     | 1334/3000 [00:00<00:01, 1651.22it/s]warmup should be done:  43%|     | 1301/3000 [00:00<00:01, 1611.06it/s]warmup should be done:  44%|     | 1317/3000 [00:00<00:01, 1624.76it/s]warmup should be done:  44%|     | 1329/3000 [00:00<00:01, 1634.40it/s]warmup should be done:  49%|     | 1473/3000 [00:00<00:00, 1629.41it/s]warmup should be done:  50%|     | 1491/3000 [00:00<00:00, 1645.21it/s]warmup should be done:  49%|     | 1457/3000 [00:00<00:00, 1607.15it/s]warmup should be done:  49%|     | 1463/3000 [00:00<00:00, 1609.89it/s]warmup should be done:  49%|     | 1480/3000 [00:00<00:00, 1619.85it/s]warmup should be done:  50%|     | 1493/3000 [00:00<00:00, 1630.21it/s]warmup should be done:  50%|     | 1485/3000 [00:00<00:01, 1453.11it/s]warmup should be done:  50%|     | 1500/3000 [00:00<00:01, 1461.19it/s]warmup should be done:  55%|    | 1637/3000 [00:01<00:00, 1631.89it/s]warmup should be done:  55%|    | 1656/3000 [00:01<00:00, 1644.55it/s]warmup should be done:  54%|    | 1618/3000 [00:01<00:00, 1606.52it/s]warmup should be done:  54%|    | 1624/3000 [00:01<00:00, 1608.14it/s]warmup should be done:  55%|    | 1642/3000 [00:01<00:00, 1616.88it/s]warmup should be done:  55%|    | 1657/3000 [00:01<00:00, 1628.01it/s]warmup should be done:  55%|    | 1648/3000 [00:01<00:00, 1501.22it/s]warmup should be done:  56%|    | 1665/3000 [00:01<00:00, 1511.91it/s]warmup should be done:  60%|    | 1801/3000 [00:01<00:00, 1633.22it/s]warmup should be done:  61%|    | 1821/3000 [00:01<00:00, 1643.07it/s]warmup should be done:  59%|    | 1779/3000 [00:01<00:00, 1605.81it/s]warmup should be done:  60%|    | 1785/3000 [00:01<00:00, 1606.44it/s]warmup should be done:  60%|    | 1804/3000 [00:01<00:00, 1614.17it/s]warmup should be done:  61%|    | 1820/3000 [00:01<00:00, 1625.55it/s]warmup should be done:  60%|    | 1812/3000 [00:01<00:00, 1539.85it/s]warmup should be done:  61%|    | 1820/3000 [00:01<00:00, 1518.79it/s]warmup should be done:  66%|   | 1965/3000 [00:01<00:00, 1631.66it/s]warmup should be done:  65%|   | 1940/3000 [00:01<00:00, 1606.10it/s]warmup should be done:  65%|   | 1946/3000 [00:01<00:00, 1604.09it/s]warmup should be done:  66%|   | 1966/3000 [00:01<00:00, 1611.23it/s]warmup should be done:  66%|   | 1983/3000 [00:01<00:00, 1623.61it/s]warmup should be done:  66%|   | 1986/3000 [00:01<00:00, 1484.27it/s]warmup should be done:  66%|   | 1976/3000 [00:01<00:00, 1568.25it/s]warmup should be done:  66%|   | 1975/3000 [00:01<00:00, 1435.46it/s]warmup should be done:  70%|   | 2101/3000 [00:01<00:00, 1605.83it/s]warmup should be done:  71%|   | 2129/3000 [00:01<00:00, 1626.57it/s]warmup should be done:  70%|   | 2107/3000 [00:01<00:00, 1602.54it/s]warmup should be done:  71%|   | 2128/3000 [00:01<00:00, 1608.83it/s]warmup should be done:  72%|  | 2146/3000 [00:01<00:00, 1620.26it/s]warmup should be done:  72%|  | 2148/3000 [00:01<00:00, 1522.11it/s]warmup should be done:  71%|  | 2141/3000 [00:01<00:00, 1589.82it/s]warmup should be done:  71%|  | 2139/3000 [00:01<00:00, 1492.73it/s]warmup should be done:  75%|  | 2262/3000 [00:01<00:00, 1601.30it/s]warmup should be done:  76%|  | 2292/3000 [00:01<00:00, 1619.33it/s]warmup should be done:  76%|  | 2268/3000 [00:01<00:00, 1599.13it/s]warmup should be done:  76%|  | 2289/3000 [00:01<00:00, 1609.06it/s]warmup should be done:  77%|  | 2309/3000 [00:01<00:00, 1615.07it/s]warmup should be done:  77%|  | 2310/3000 [00:01<00:00, 1549.27it/s]warmup should be done:  77%|  | 2305/3000 [00:01<00:00, 1603.80it/s]warmup should be done:  77%|  | 2304/3000 [00:01<00:00, 1534.97it/s]warmup should be done:  81%|  | 2423/3000 [00:01<00:00, 1602.23it/s]warmup should be done:  82%| | 2454/3000 [00:01<00:00, 1618.37it/s]warmup should be done:  81%|  | 2429/3000 [00:01<00:00, 1599.74it/s]warmup should be done:  82%| | 2454/3000 [00:01<00:00, 1620.00it/s]warmup should be done:  82%| | 2471/3000 [00:01<00:00, 1615.73it/s]warmup should be done:  82%| | 2473/3000 [00:01<00:00, 1572.30it/s]warmup should be done:  82%| | 2471/3000 [00:01<00:00, 1617.58it/s]warmup should be done:  82%| | 2468/3000 [00:01<00:00, 1564.02it/s]warmup should be done:  86%| | 2584/3000 [00:01<00:00, 1603.89it/s]warmup should be done:  87%| | 2616/3000 [00:01<00:00, 1618.63it/s]warmup should be done:  86%| | 2590/3000 [00:01<00:00, 1600.98it/s]warmup should be done:  87%| | 2619/3000 [00:01<00:00, 1627.93it/s]warmup should be done:  88%| | 2634/3000 [00:01<00:00, 1617.02it/s]warmup should be done:  88%| | 2636/3000 [00:01<00:00, 1588.82it/s]warmup should be done:  88%| | 2637/3000 [00:01<00:00, 1627.45it/s]warmup should be done:  88%| | 2633/3000 [00:01<00:00, 1588.16it/s]warmup should be done:  92%|| 2745/3000 [00:01<00:00, 1605.70it/s]warmup should be done:  93%|| 2778/3000 [00:01<00:00, 1618.67it/s]warmup should be done:  92%|| 2751/3000 [00:01<00:00, 1602.58it/s]warmup should be done:  93%|| 2784/3000 [00:01<00:00, 1633.98it/s]warmup should be done:  93%|| 2796/3000 [00:01<00:00, 1617.37it/s]warmup should be done:  93%|| 2800/3000 [00:01<00:00, 1601.43it/s]warmup should be done:  93%|| 2803/3000 [00:01<00:00, 1634.47it/s]warmup should be done:  93%|| 2798/3000 [00:01<00:00, 1605.02it/s]warmup should be done:  97%|| 2907/3000 [00:01<00:00, 1609.26it/s]warmup should be done:  98%|| 2942/3000 [00:01<00:00, 1622.26it/s]warmup should be done:  97%|| 2915/3000 [00:01<00:00, 1613.54it/s]warmup should be done:  98%|| 2949/3000 [00:01<00:00, 1638.21it/s]warmup should be done:  99%|| 2961/3000 [00:01<00:00, 1625.24it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1629.78it/s]warmup should be done:  99%|| 2966/3000 [00:01<00:00, 1618.53it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1627.43it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1625.71it/s]warmup should be done:  99%|| 2970/3000 [00:01<00:00, 1643.08it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1610.08it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1609.71it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1607.70it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1605.51it/s]warmup should be done:  99%|| 2960/3000 [00:01<00:00, 1494.52it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1557.52it/s]






warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   6%|         | 166/3000 [00:00<00:01, 1659.45it/s]warmup should be done:   6%|         | 169/3000 [00:00<00:01, 1689.59it/s]warmup should be done:   6%|         | 170/3000 [00:00<00:01, 1698.46it/s]warmup should be done:   6%|         | 165/3000 [00:00<00:01, 1648.24it/s]warmup should be done:   6%|         | 166/3000 [00:00<00:01, 1658.68it/s]warmup should be done:   6%|         | 166/3000 [00:00<00:01, 1655.72it/s]warmup should be done:   5%|         | 159/3000 [00:00<00:01, 1585.31it/s]warmup should be done:   6%|         | 169/3000 [00:00<00:01, 1681.65it/s]warmup should be done:  11%|         | 332/3000 [00:00<00:01, 1658.80it/s]warmup should be done:  11%|        | 338/3000 [00:00<00:01, 1688.29it/s]warmup should be done:  11%|         | 335/3000 [00:00<00:01, 1674.46it/s]warmup should be done:  11%|         | 328/3000 [00:00<00:01, 1645.79it/s]warmup should be done:  11%|        | 341/3000 [00:00<00:01, 1700.25it/s]warmup should be done:  11%|         | 334/3000 [00:00<00:01, 1666.85it/s]warmup should be done:  11%|        | 340/3000 [00:00<00:01, 1692.92it/s]warmup should be done:  11%|         | 332/3000 [00:00<00:01, 1646.43it/s]warmup should be done:  17%|        | 508/3000 [00:00<00:01, 1690.41it/s]warmup should be done:  17%|        | 498/3000 [00:00<00:01, 1654.50it/s]warmup should be done:  17%|        | 498/3000 [00:00<00:01, 1667.02it/s]warmup should be done:  17%|        | 505/3000 [00:00<00:01, 1681.93it/s]warmup should be done:  17%|        | 503/3000 [00:00<00:01, 1675.84it/s]warmup should be done:  17%|        | 512/3000 [00:00<00:01, 1701.80it/s]warmup should be done:  17%|        | 511/3000 [00:00<00:01, 1698.04it/s]warmup should be done:  17%|        | 497/3000 [00:00<00:01, 1644.03it/s]warmup should be done:  22%|       | 675/3000 [00:00<00:01, 1687.77it/s]warmup should be done:  22%|       | 672/3000 [00:00<00:01, 1679.58it/s]warmup should be done:  22%|       | 665/3000 [00:00<00:01, 1657.55it/s]warmup should be done:  22%|       | 667/3000 [00:00<00:01, 1672.92it/s]warmup should be done:  23%|       | 683/3000 [00:00<00:01, 1701.68it/s]warmup should be done:  23%|       | 678/3000 [00:00<00:01, 1687.58it/s]warmup should be done:  23%|       | 683/3000 [00:00<00:01, 1702.88it/s]warmup should be done:  22%|       | 664/3000 [00:00<00:01, 1652.37it/s]warmup should be done:  28%|       | 840/3000 [00:00<00:01, 1678.36it/s]warmup should be done:  28%|       | 845/3000 [00:00<00:01, 1689.09it/s]warmup should be done:  28%|       | 848/3000 [00:00<00:01, 1691.74it/s]warmup should be done:  28%|       | 836/3000 [00:00<00:01, 1676.29it/s]warmup should be done:  28%|       | 832/3000 [00:00<00:01, 1659.03it/s]warmup should be done:  28%|       | 854/3000 [00:00<00:01, 1699.83it/s]warmup should be done:  28%|       | 855/3000 [00:00<00:01, 1706.09it/s]warmup should be done:  28%|       | 830/3000 [00:00<00:01, 1647.25it/s]warmup should be done:  34%|      | 1014/3000 [00:00<00:01, 1689.15it/s]warmup should be done:  34%|      | 1005/3000 [00:00<00:01, 1680.69it/s]warmup should be done:  33%|      | 998/3000 [00:00<00:01, 1657.72it/s]warmup should be done:  34%|      | 1008/3000 [00:00<00:01, 1674.77it/s]warmup should be done:  34%|      | 1018/3000 [00:00<00:01, 1690.05it/s]warmup should be done:  34%|      | 1026/3000 [00:00<00:01, 1706.31it/s]warmup should be done:  34%|      | 1024/3000 [00:00<00:01, 1696.72it/s]warmup should be done:  33%|      | 996/3000 [00:00<00:01, 1648.79it/s]warmup should be done:  39%|      | 1184/3000 [00:00<00:01, 1689.74it/s]warmup should be done:  39%|      | 1174/3000 [00:00<00:01, 1681.35it/s]warmup should be done:  39%|      | 1165/3000 [00:00<00:01, 1659.21it/s]warmup should be done:  39%|      | 1176/3000 [00:00<00:01, 1671.71it/s]warmup should be done:  40%|      | 1197/3000 [00:00<00:01, 1704.97it/s]warmup should be done:  40%|      | 1188/3000 [00:00<00:01, 1688.04it/s]warmup should be done:  40%|      | 1194/3000 [00:00<00:01, 1693.34it/s]warmup should be done:  39%|      | 1161/3000 [00:00<00:01, 1647.89it/s]warmup should be done:  45%|     | 1343/3000 [00:00<00:00, 1683.51it/s]warmup should be done:  45%|     | 1354/3000 [00:00<00:00, 1691.94it/s]warmup should be done:  44%|     | 1331/3000 [00:00<00:01, 1659.23it/s]warmup should be done:  46%|     | 1369/3000 [00:00<00:00, 1707.86it/s]warmup should be done:  45%|     | 1358/3000 [00:00<00:00, 1691.50it/s]warmup should be done:  45%|     | 1344/3000 [00:00<00:00, 1671.33it/s]warmup should be done:  45%|     | 1364/3000 [00:00<00:00, 1693.95it/s]warmup should be done:  44%|     | 1326/3000 [00:00<00:01, 1644.99it/s]warmup should be done:  51%|     | 1524/3000 [00:00<00:00, 1693.61it/s]warmup should be done:  50%|     | 1512/3000 [00:00<00:00, 1683.56it/s]warmup should be done:  50%|     | 1497/3000 [00:00<00:00, 1657.38it/s]warmup should be done:  51%|    | 1540/3000 [00:00<00:00, 1706.68it/s]warmup should be done:  51%|     | 1528/3000 [00:00<00:00, 1690.63it/s]warmup should be done:  50%|     | 1512/3000 [00:00<00:00, 1670.27it/s]warmup should be done:  50%|     | 1492/3000 [00:00<00:00, 1649.29it/s]warmup should be done:  51%|     | 1534/3000 [00:00<00:00, 1677.57it/s]warmup should be done:  56%|    | 1694/3000 [00:01<00:00, 1694.98it/s]warmup should be done:  56%|    | 1681/3000 [00:01<00:00, 1684.97it/s]warmup should be done:  56%|    | 1665/3000 [00:01<00:00, 1663.00it/s]warmup should be done:  57%|    | 1711/3000 [00:01<00:00, 1706.48it/s]warmup should be done:  56%|    | 1680/3000 [00:01<00:00, 1670.84it/s]warmup should be done:  55%|    | 1659/3000 [00:01<00:00, 1652.66it/s]warmup should be done:  57%|    | 1698/3000 [00:01<00:00, 1684.04it/s]warmup should be done:  57%|    | 1704/3000 [00:01<00:00, 1683.42it/s]warmup should be done:  62%|   | 1850/3000 [00:01<00:00, 1685.73it/s]warmup should be done:  62%|   | 1865/3000 [00:01<00:00, 1696.85it/s]warmup should be done:  63%|   | 1883/3000 [00:01<00:00, 1707.82it/s]warmup should be done:  62%|   | 1848/3000 [00:01<00:00, 1671.40it/s]warmup should be done:  61%|    | 1826/3000 [00:01<00:00, 1656.52it/s]warmup should be done:  62%|   | 1867/3000 [00:01<00:00, 1685.56it/s]warmup should be done:  61%|    | 1832/3000 [00:01<00:00, 1646.40it/s]warmup should be done:  62%|   | 1874/3000 [00:01<00:00, 1686.10it/s]warmup should be done:  67%|   | 2019/3000 [00:01<00:00, 1684.20it/s]warmup should be done:  68%|   | 2035/3000 [00:01<00:00, 1693.71it/s]warmup should be done:  68%|   | 2055/3000 [00:01<00:00, 1710.82it/s]warmup should be done:  67%|   | 2016/3000 [00:01<00:00, 1669.94it/s]warmup should be done:  68%|   | 2036/3000 [00:01<00:00, 1685.89it/s]warmup should be done:  66%|   | 1992/3000 [00:01<00:00, 1652.96it/s]warmup should be done:  68%|   | 2044/3000 [00:01<00:00, 1689.57it/s]warmup should be done:  67%|   | 1997/3000 [00:01<00:00, 1630.50it/s]warmup should be done:  74%|  | 2205/3000 [00:01<00:00, 1692.82it/s]warmup should be done:  74%|  | 2227/3000 [00:01<00:00, 1709.82it/s]warmup should be done:  73%|  | 2183/3000 [00:01<00:00, 1666.82it/s]warmup should be done:  74%|  | 2206/3000 [00:01<00:00, 1687.29it/s]warmup should be done:  73%|  | 2188/3000 [00:01<00:00, 1667.34it/s]warmup should be done:  72%|  | 2159/3000 [00:01<00:00, 1655.32it/s]warmup should be done:  74%|  | 2214/3000 [00:01<00:00, 1690.83it/s]warmup should be done:  72%|  | 2165/3000 [00:01<00:00, 1643.70it/s]warmup should be done:  79%|  | 2375/3000 [00:01<00:00, 1694.45it/s]warmup should be done:  80%|  | 2398/3000 [00:01<00:00, 1707.88it/s]warmup should be done:  78%|  | 2352/3000 [00:01<00:00, 1671.94it/s]warmup should be done:  79%|  | 2376/3000 [00:01<00:00, 1689.46it/s]warmup should be done:  78%|  | 2326/3000 [00:01<00:00, 1659.05it/s]warmup should be done:  79%|  | 2358/3000 [00:01<00:00, 1674.26it/s]warmup should be done:  79%|  | 2384/3000 [00:01<00:00, 1692.61it/s]warmup should be done:  78%|  | 2333/3000 [00:01<00:00, 1653.46it/s]warmup should be done:  85%| | 2545/3000 [00:01<00:00, 1694.35it/s]warmup should be done:  86%| | 2570/3000 [00:01<00:00, 1709.84it/s]warmup should be done:  84%| | 2520/3000 [00:01<00:00, 1674.02it/s]warmup should be done:  85%| | 2546/3000 [00:01<00:00, 1690.95it/s]warmup should be done:  83%| | 2493/3000 [00:01<00:00, 1661.52it/s]warmup should be done:  84%| | 2527/3000 [00:01<00:00, 1677.12it/s]warmup should be done:  85%| | 2554/3000 [00:01<00:00, 1694.57it/s]warmup should be done:  83%| | 2502/3000 [00:01<00:00, 1663.74it/s]warmup should be done:  90%| | 2715/3000 [00:01<00:00, 1693.72it/s]warmup should be done:  91%|| 2742/3000 [00:01<00:00, 1712.38it/s]warmup should be done:  90%| | 2688/3000 [00:01<00:00, 1673.08it/s]warmup should be done:  91%| | 2716/3000 [00:01<00:00, 1689.63it/s]warmup should be done:  90%| | 2695/3000 [00:01<00:00, 1677.75it/s]warmup should be done:  89%| | 2660/3000 [00:01<00:00, 1659.28it/s]warmup should be done:  91%| | 2724/3000 [00:01<00:00, 1685.53it/s]warmup should be done:  89%| | 2671/3000 [00:01<00:00, 1670.06it/s]warmup should be done:  96%|| 2885/3000 [00:01<00:00, 1692.17it/s]warmup should be done:  97%|| 2914/3000 [00:01<00:00, 1713.60it/s]warmup should be done:  95%|| 2856/3000 [00:01<00:00, 1670.61it/s]warmup should be done:  96%|| 2885/3000 [00:01<00:00, 1688.17it/s]warmup should be done:  95%|| 2863/3000 [00:01<00:00, 1674.87it/s]warmup should be done:  94%|| 2826/3000 [00:01<00:00, 1658.24it/s]warmup should be done:  96%|| 2893/3000 [00:01<00:00, 1686.34it/s]warmup should be done:  95%|| 2840/3000 [00:01<00:00, 1675.33it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1707.65it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1690.13it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1689.82it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1688.25it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1675.33it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1671.51it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1662.20it/s]warmup should be done: 100%|| 2993/3000 [00:01<00:00, 1659.21it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1653.70it/s]2022-12-11 20:12:53.511777: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7ef8eb830840 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 20:12:53.511845: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 20:12:53.522857: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7edd5802f700 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 20:12:53.522916: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 20:12:53.526241: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7edce802ddb0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 20:12:53.526281: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 20:12:53.531611: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7ef8e7832ca0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 20:12:53.531653: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 20:12:53.767391: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7edd58029cc0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 20:12:53.767461: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 20:12:53.771676: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7ef8dbf92080 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 20:12:53.771739: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 20:12:53.856951: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7ef8e782c7c0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 20:12:53.856951: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7ef8e382fce0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 20:12:53.857029: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 20:12:53.857039: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 20:12:55.747608: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 20:12:55.825322: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 20:12:55.837251: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 20:12:55.838066: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 20:12:56.034469: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 20:12:56.089846: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 20:12:56.143656: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 20:12:56.181505: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 20:12:58.571907: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 20:12:58.700728: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 20:12:58.705960: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 20:12:58.748960: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 20:12:58.874530: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 20:12:58.967901: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 20:12:59.024099: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 20:12:59.098600: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
[HCTR][20:13:42.372][ERROR][RK0][tid #139607962277632]: replica 2 reaches 1000, calling init pre replica
[HCTR][20:13:42.372][ERROR][RK0][tid #139607962277632]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][20:13:42.378][ERROR][RK0][tid #139607962277632]: coll ps creation done
[HCTR][20:13:42.378][ERROR][RK0][tid #139607962277632]: replica 2 waits for coll ps creation barrier
[HCTR][20:13:42.390][ERROR][RK0][tid #139608163604224]: replica 3 reaches 1000, calling init pre replica
[HCTR][20:13:42.390][ERROR][RK0][tid #139608163604224]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][20:13:42.392][ERROR][RK0][tid #139607828059904]: replica 7 reaches 1000, calling init pre replica
[HCTR][20:13:42.392][ERROR][RK0][tid #139607828059904]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][20:13:42.396][ERROR][RK0][tid #139608163604224]: coll ps creation done
[HCTR][20:13:42.396][ERROR][RK0][tid #139608163604224]: replica 3 waits for coll ps creation barrier
[HCTR][20:13:42.396][ERROR][RK0][tid #139607828059904]: coll ps creation done
[HCTR][20:13:42.396][ERROR][RK0][tid #139607828059904]: replica 7 waits for coll ps creation barrier
[HCTR][20:13:42.410][ERROR][RK0][tid #139608817907456]: replica 6 reaches 1000, calling init pre replica
[HCTR][20:13:42.410][ERROR][RK0][tid #139608817907456]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][20:13:42.418][ERROR][RK0][tid #139608817907456]: coll ps creation done
[HCTR][20:13:42.418][ERROR][RK0][tid #139608817907456]: replica 6 waits for coll ps creation barrier
[HCTR][20:13:42.429][ERROR][RK0][tid #139607828059904]: replica 1 reaches 1000, calling init pre replica
[HCTR][20:13:42.429][ERROR][RK0][tid #139607828059904]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][20:13:42.436][ERROR][RK0][tid #139607828059904]: coll ps creation done
[HCTR][20:13:42.436][ERROR][RK0][tid #139607828059904]: replica 1 waits for coll ps creation barrier
[HCTR][20:13:42.444][ERROR][RK0][tid #139608020993792]: replica 4 reaches 1000, calling init pre replica
[HCTR][20:13:42.444][ERROR][RK0][tid #139608020993792]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][20:13:42.444][ERROR][RK0][main]: replica 0 reaches 1000, calling init pre replica
[HCTR][20:13:42.445][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][20:13:42.449][ERROR][RK0][main]: coll ps creation done
[HCTR][20:13:42.449][ERROR][RK0][main]: replica 0 waits for coll ps creation barrier
[HCTR][20:13:42.452][ERROR][RK0][tid #139608020993792]: coll ps creation done
[HCTR][20:13:42.452][ERROR][RK0][tid #139608020993792]: replica 4 waits for coll ps creation barrier
[HCTR][20:13:42.461][ERROR][RK0][main]: replica 5 reaches 1000, calling init pre replica
[HCTR][20:13:42.461][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][20:13:42.468][ERROR][RK0][main]: coll ps creation done
[HCTR][20:13:42.468][ERROR][RK0][main]: replica 5 waits for coll ps creation barrier
[HCTR][20:13:42.468][ERROR][RK0][main]: replica 0 preparing frequency
[HCTR][20:13:43.303][ERROR][RK0][main]: replica 0 preparing frequency done
[HCTR][20:13:43.334][ERROR][RK0][main]: replica 0 calling init per replica
[HCTR][20:13:43.334][ERROR][RK0][main]: replica 5 calling init per replica
[HCTR][20:13:43.334][ERROR][RK0][tid #139607828059904]: replica 7 calling init per replica
[HCTR][20:13:43.334][ERROR][RK0][tid #139608020993792]: replica 4 calling init per replica
[HCTR][20:13:43.334][ERROR][RK0][tid #139608163604224]: replica 3 calling init per replica
[HCTR][20:13:43.334][ERROR][RK0][tid #139608817907456]: replica 6 calling init per replica
[HCTR][20:13:43.334][ERROR][RK0][tid #139607962277632]: replica 2 calling init per replica
[HCTR][20:13:43.334][ERROR][RK0][tid #139607828059904]: replica 1 calling init per replica
[HCTR][20:13:43.334][ERROR][RK0][main]: Calling build_v2
[HCTR][20:13:43.334][ERROR][RK0][main]: Calling build_v2
[HCTR][20:13:43.334][ERROR][RK0][tid #139607828059904]: Calling build_v2
[HCTR][20:13:43.334][ERROR][RK0][tid #139608020993792]: Calling build_v2
[HCTR][20:13:43.334][ERROR][RK0][tid #139608163604224]: Calling build_v2
[HCTR][20:13:43.334][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][20:13:43.334][ERROR][RK0][tid #139608817907456]: Calling build_v2
[HCTR][20:13:43.334][ERROR][RK0][tid #139608020993792]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][20:13:43.334][ERROR][RK0][tid #139607962277632]: Calling build_v2
[HCTR][20:13:43.334][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][20:13:43.334][ERROR][RK0][tid #139607828059904]: Calling build_v2
[HCTR][20:13:43.334][ERROR][RK0][tid #139607828059904]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][20:13:43.334][ERROR][RK0][tid #139608163604224]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][20:13:43.334][ERROR][RK0][tid #139608817907456]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][20:13:43.334][ERROR][RK0][tid #139607962277632]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][20:13:43.334][ERROR][RK0][tid #139607828059904]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[2022-12-11 20:13:43.338524: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:178] [v100x8, slow pcie2022-12-11 20:13:43
.338571: E [/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[2022-12-11 20:13:43:.178338617] : v100x8, slow pcieE
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:196] [assigning 0 to cpu2022-12-11 20:13:432022-12-11 20:13:43
..[338654338609: : EE2022-12-11 20:13:43  ./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc338653::: 196178[E] ] 2022-12-11 20:13:43 assigning 0 to cpuv100x8, slow pcie.
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
[[338708:2022-12-11 20:13:43: 178.E] 2022-12-11 20:13:43338739 v100x8, slow pcie[[.: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
2022-12-11 20:13:43338707E:[2022-12-11 20:13:43.: [ 212.3387652022-12-11 20:13:43E2022-12-11 20:13:43/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] 338750: [. .:build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8: E338794/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc3388032022-12-11 20:13:43196
E : :: .]  /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE178E[338835assigning 0 to cpu/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: ]  v100x8, slow pcie2022-12-11 20:13:43: 
:212/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
.E178] 178:338934 [] build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8] 196[: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-11 20:13:43v100x8, slow pcie
v100x8, slow pcie] 2022-12-11 20:13:43E:.

assigning 0 to cpu. [178339035
339049[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[2022-12-11 20:13:43] : : 2022-12-11 20:13:43:2022-12-11 20:13:43.v100x8, slow pcieEE.213.[339103
  339115] 3391182022-12-11 20:13:43: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[: remote time is 8.68421: .E::2022-12-11 20:13:43E
E339185 196212.  : [/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] ] 339224/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE2022-12-11 20:13:43:assigning 0 to cpubuild asymm link desc with 8X Tesla V100-SXM2-32GB out of 8: :: .213

E196196/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc339299]  ] ] :[[: remote time is 8.68421/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccassigning 0 to cpuassigning 0 to cpu2122022-12-11 20:13:432022-12-11 20:13:43E
:

] .. 196[build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8339409339397/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[] 2022-12-11 20:13:43
: : :2022-12-11 20:13:43assigning 0 to cpu[.EE214[.
2022-12-11 20:13:43339456  ] 2022-12-11 20:13:43339475.: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cccpu time is 97.0588.: 339501E::
339518[E:  212213: 2022-12-11 20:13:43 E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] ] E./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc :build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8remote time is 8.68421 339589:/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc214

/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: 212:] :E[] 212[cpu time is 97.0588213 2022-12-11 20:13:43build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8] 2022-12-11 20:13:43
] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.
build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8.remote time is 8.68421:339704
339710
212[: : ] [2022-12-11 20:13:43[EEbuild asymm link desc with 8X Tesla V100-SXM2-32GB out of 82022-12-11 20:13:43.2022-12-11 20:13:43  
.339798./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc339816: 339819[::: E: 2022-12-11 20:13:43214213E E.] ]  /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc 339874cpu time is 97.0588remote time is 8.68421/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: 

:213:E213] [214 ] remote time is 8.684212022-12-11 20:13:43] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccremote time is 8.68421
.cpu time is 97.0588:
339992
[213: [2022-12-11 20:13:43] E2022-12-11 20:13:43.remote time is 8.68421 .340039
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc340050: :: E[214E 2022-12-11 20:13:43]  /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.cpu time is 97.0588/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:340084
:214: 214] E] cpu time is 97.0588 cpu time is 97.0588
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
:214] cpu time is 97.0588
[2022-12-11 20:15:00.549080: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:118] solver created. now build & solve
[2022-12-11 20:15:00.817961: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:123] solver built. now solve
block 0 storage is 00010001
	access is	0	0	0	0	4	4	4	4	
block 1 storage is 00100010
	access is	1	1	1	1	5	5	5	5	
block 2 storage is 01000100
	access is	2	2	2	2	6	6	6	6	
block 3 storage is 10001000
	access is	3	3	3	3	7	7	7	7	
block 4 storage is 00000000
	access is	8	8	8	8	8	8	8	8	
[2022-12-11 20:15:00.941484: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:127] solver solved
[2022-12-11 20:15:00.941549: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:195] 0 solved master
[2022-12-11 20:15:00.941583: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 0 solved
[2022-12-11 20:15:00.941615: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 0 initing device 0
[2022-12-11 20:15:00.942159: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-11 20:15:00.942210: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 20:15:00.943105: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 20:15:00.943847: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 20:15:00.956913: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 1 solved
[2022-12-11 20:15:00.956978: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] [worker 0 thread 1 initing device 12022-12-11 20:15:00
.956974: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 3 solved
[2022-12-11 20:15:00.957067: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 3 initing device 3
[2022-12-11 20:15:00.957227: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 7 solved
[2022-12-11 20:15:00.957291: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 7 initing device 7
[2022-12-11 20:15:00.957428: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-11 20:15:00.957493: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 20:15:00.957538: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-11 20:15:00.957593: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 20:15:00.957732: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-11 20:15:00.957779: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 20:15:00.959750: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 20:15:00.959851: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 20:15:00.959910: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 20:15:00.959969: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 6 solved
[[2022-12-11 20:15:002022-12-11 20:15:00..960003960022: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc::202205] ] 2 solvedworker 0 thread 6 initing device 6

[2022-12-11 20:15:00.960113: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 2 initing device 2
[2022-12-11 20:15:00.960509: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[[2022-12-11 20:15:002022-12-11 20:15:00..960551960556: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::18151980] ] Building Coll Cache with ... num gpu device is 8eager alloc mem 381.47 MB

[2022-12-11 20:15:00.960654: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 20:15:00.962092: [E2022-12-11 20:15:00 ./hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc962110:: 202E]  5 solved/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc
:202] 4 solved[
2022-12-11 20:15:00.962192: E[ 2022-12-11 20:15:00/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc.:962209205: ] Eworker 0 thread 5 initing device 5 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 4 initing device 4
[2022-12-11 20:15:00.962657: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-11 20:15:00.962685: E [/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu[2022-12-11 20:15:00:2022-12-11 20:15:00.1815.962694] 962705: Building Coll Cache with ... num gpu device is 8: E
E  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::19801980] ] eager alloc mem 381.47 MB[eager alloc mem 381.47 MB
2022-12-11 20:15:00
.962788: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 20:15:00.962900: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 20:15:00.962954: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 20:15:00.963037: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 20:15:00.963099: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 20:15:00.967241: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 20:15:00.967297: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 20:15:00.967451: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 20:15:00.967513: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 20:15:00.969830: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 20:15:00.969896: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 20:15:01. 22754: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.00 Bytes
[2022-12-11 20:15:01. 28045: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5
[2022-12-11 20:15:01. 28154: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-11 20:15:01. 29131: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 4.20 MB
[2022-12-11 20:15:01. 29882: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 20:15:01. 30863: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 20:15:01. 31960: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 20:15:01. 32676: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 20:15:01. 32720: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 493.16 MB
[2022-12-11 20:15:01. 47555: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.00 Bytes
[2022-12-11 20:15:01[.2022-12-11 20:15:01 50093.: E 50104 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE: 1980/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu] :eager alloc mem 5.00 Bytes1980
] eager alloc mem 5.00 Bytes
[2022-12-11 20:15:01. 52598: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5
[2022-12-11 20:15:01. 52679: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-11 20:15:01. 53812: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 4.20 MB
[[2022-12-11 20:15:012022-12-11 20:15:01.. 54221 54221: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu[:[:2022-12-11 20:15:0119802022-12-11 20:15:011980.] .]  54282eager alloc mem 5.00 Bytes 54282eager alloc mem 5.00 Bytes: 
: 
EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::19801980] ] eager alloc mem 5.00 Byteseager alloc mem 5.00 Bytes

[2022-12-11 20:15:01. 54784: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 20:15:01. 55564: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5
[2022-12-11 20:15:01. 55634: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-11 20:15:01. 55749: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 20:15:01. 56156: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5
[2022-12-11 20:15:01. 56262: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-11 20:15:01. 56418: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 4.20 MB
[2022-12-11 20:15:01. 57537: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 4.20 MB
[2022-12-11 20:15:01. 59336: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 20:15:01. 60051: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 20:15:01. 60094: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 493.16 MB
[2022-12-11 20:15:01. 63902: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 20:15:01. 64853: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 20:15:01. 65765: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 20:15:01. 66460: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 20:15:01. 66503: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 493.16 MB
[2022-12-11 20:15:01. 66520: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5
[2022-12-11 20:15:01. 66609: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-11 20:15:01. 66843: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5
[2022-12-11 20:15:01. 66924: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-11 20:15:01. 66935: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5
[[2022-12-11 20:15:012022-12-11 20:15:01.. 67000 67017: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::638638] ] eager release cuda mem 5eager release cuda mem 400000000

[2022-12-11 20:15:01. 67100: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-11 20:15:01. 69039: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 20:15:01. 70260: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 20:15:01. 74195: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 4.20 MB
[2022-12-11 20:15:01. 74425: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 20:15:01. 74696: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 4.20 MB
[2022-12-11 20:15:01. 75079: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 20:15:01. 75159: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 493.16 MB
[2022-12-11 20:15:01. 75252: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 4.20 MB
[2022-12-11 20:15:01. 75751: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 4.20 MB
[2022-12-11 20:15:01. 82695: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 20:15:01. 82732: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 20:15:01. 82864: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 20:15:01. 82901: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 20:15:01. 83664: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 20:15:01. 83710: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 20:15:01[.2022-12-11 20:15:01 83823.:  83836E:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc :/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc638:] 638eager release cuda mem 625663] 
eager release cuda mem 625663
[2022-12-11 20:15:01. 87372: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 20:15:01. 87980: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 20:15:01. 88031: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 493.16 MB
[2022-12-11 20:15:01. 88227: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 20:15:01. 88286: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 20:15:01. 88498: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 20:15:01. 88827: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 20:15:01. 88870: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 493.16 MB[
2022-12-11 20:15:01. 88889: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 20:15:01. 88934: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 493.16 MB
[2022-12-11 20:15:01. 89106: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 20:15:01. 89151: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 493.16 MB
[[[[[[[[2022-12-11 20:15:012022-12-11 20:15:012022-12-11 20:15:012022-12-11 20:15:012022-12-11 20:15:012022-12-11 20:15:012022-12-11 20:15:012022-12-11 20:15:01........264094264094264095264094264095264095264097264094: : : : : : : : EEEEEEEE        /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::::::::19261926192619261926192619261926] ] ] ] ] ] ] ] Device 1 init p2p of link 7Device 5 init p2p of link 6Device 0 init p2p of link 3Device 7 init p2p of link 4Device 3 init p2p of link 2Device 2 init p2p of link 1Device 4 init p2p of link 5Device 6 init p2p of link 0







[2022-12-11 20:15:01.264558[[: [[2022-12-11 20:15:01[2022-12-11 20:15:01E2022-12-11 20:15:012022-12-11 20:15:01.2022-12-11 20:15:01. [..264562.264563/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu2022-12-11 20:15:01264562264569[: 264574: :.: : 2022-12-11 20:15:01E: E1980264596EE. E ] :   264628/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cueager alloc mem 611.00 KBE/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu: :/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:
 ::E1980:1980/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu19801980 ] 1980] :] ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cueager alloc mem 611.00 KB] eager alloc mem 611.00 KB1980eager alloc mem 611.00 KBeager alloc mem 611.00 KB:
eager alloc mem 611.00 KB
] 

1980
eager alloc mem 611.00 KB] 
eager alloc mem 611.00 KB
[2022-12-11 20:15:01.265581: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 20:15:01.265661: E[ [2022-12-11 20:15:01/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc[2022-12-11 20:15:01.:[2022-12-11 20:15:01.265683638[2022-12-11 20:15:01.[265684: ] 2022-12-11 20:15:01.2656922022-12-11 20:15:01: Eeager release cuda mem 625663.265705: .E 
265718: E265722 /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc: E : /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccE:638 /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc: 638] /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc] eager release cuda mem 625663:638] :eager release cuda mem 625663
638] eager release cuda mem 625663638
] eager release cuda mem 625663
] eager release cuda mem 625663
eager release cuda mem 625663

[2022-12-11 20:15:01.278424: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 2 init p2p of link 3
[2022-12-11 20:15:01.278567: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 20:15:01.278652: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 3 init p2p of link 0
[2022-12-11 20:15:01.278807: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 20:15:01.278886: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 7 init p2p of link 1
[2022-12-11 20:15:01.278923: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 2
[2022-12-11 20:15:01.279038: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[[2022-12-11 20:15:012022-12-11 20:15:01..279077279090: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::19261980] ] Device 5 init p2p of link 4eager alloc mem 611.00 KB

[2022-12-11 20:15:01.279262: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 20:15:01.279378: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 20:15:01.279491: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926[] 2022-12-11 20:15:01Device 6 init p2p of link 5.
279515: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 4 init p2p of link 7
[2022-12-11 20:15:01.279592: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 20:15:01.279652: E[ 2022-12-11 20:15:01/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu.:2796631980: ] Eeager alloc mem 611.00 KB 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 20:15:01.279708: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 0 init p2p of link 6
[2022-12-11 20:15:01.279842: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 20:15:01.279882: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 20:15:01.279948: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 20:15:01.280078: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 20:15:01[.2022-12-11 20:15:01280487.: 280491E:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc :/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc638:] 638eager release cuda mem 625663] 
eager release cuda mem 625663
[2022-12-11 20:15:01.280687: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 20:15:01.301833: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 2 init p2p of link 0
[2022-12-11 20:15:01.301966: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 20:15:01.302173: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 7 init p2p of link 6
[2022-12-11 20:15:01.302296: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 20:15:01.302372: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 3 init p2p of link 5
[2022-12-11 20:15:01.302495: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 20:15:01.302762: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc[:2022-12-11 20:15:01638.] 302767eager release cuda mem 625663: 
E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 3
[2022-12-11 20:15:01.302929: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 20:15:01.302982: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 5 init p2p of link 7
[2022-12-11 20:15:01.303108[: 2022-12-11 20:15:01E. 303114/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu: :E1980 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cceager alloc mem 611.00 KB:
638] eager release cuda mem 625663
[2022-12-11 20:15:01.303267: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 20:15:01.303359: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 6 init p2p of link 4
[2022-12-11 20:15:01.303439: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 0 init p2p of link 1
[2022-12-11 20:15:01.303493: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 20:15:01.303562: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 20:15:01.303601: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 4 init p2p of link 2
[2022-12-11 20:15:01.303721: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] [eager alloc mem 611.00 KB2022-12-11 20:15:01
.303737: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 20:15:01.303941: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 20:15:01.304274: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 20:15:01.304379: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 20:15:01.304529: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 20:15:01.318387: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 2 init p2p of link 4
[2022-12-11 20:15:01.318506: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 20:15:01.318621: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 3 init p2p of link 1
[2022-12-11 20:15:01.318739: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 20:15:01.318905: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 7 init p2p of link 5
[2022-12-11 20:15:01.319027: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 20:15:01.319274: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 20:15:01.319536: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 20:15:01.319678: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 0
[2022-12-11 20:15:01.319792: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 20:15:01.319842: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 20:15:01.319971: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 5 init p2p of link 3
[2022-12-11 20:15:01.320094: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 20:15:01.320184: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 4 init p2p of link 6
[2022-12-11 20:15:01.320302: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] [eager alloc mem 611.00 KB2022-12-11 20:15:01
.320311: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 6 init p2p of link 7
[2022-12-11 20:15:01.320440: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 20:15:01[.2022-12-11 20:15:01320606.: 320604E:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc :/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu638:] 1926eager release cuda mem 625663] 
Device 0 init p2p of link 2
[2022-12-11 20:15:01.320741: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 20:15:01.320858: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 20:15:01.321121: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 20:15:01.321218: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 20:15:01.321550: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 20:15:01.346320: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 4399996
[2022-12-11 20:15:01.347602: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 4399996
[2022-12-11 20:15:01.347823: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 4399996
[2022-12-11 20:15:01.347902: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 4399996
[2022-12-11 20:15:01.348241: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 4399996
[2022-12-11 20:15:01.348884: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 4399996
[2022-12-11 20:15:01.349221: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 4399996
[[2022-12-11 20:15:012022-12-11 20:15:01..349558349567: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::6381955] ] eager release cuda mem 4399996Asymm Coll cache (policy: clique_part) | local 999999 / 100000000 nodes ( 1.00 %~1.00 %) | remote 2999997 / 100000000 nodes ( 3.00 %) | cpu 96000004 / 100000000 nodes ( 96.00 %) | 493.16 MB | 0.388923 secs 

[2022-12-11 20:15:01.349784: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 999999 / 100000000 nodes ( 1.00 %~1.00 %) | remote 2999997 / 100000000 nodes ( 3.00 %) | cpu 96000004 / 100000000 nodes ( 96.00 %) | 493.16 MB | 0.392209 secs 
[2022-12-11 20:15:01.349929: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 999999 / 100000000 nodes ( 1.00 %~1.00 %) | remote 2999997 / 100000000 nodes ( 3.00 %) | cpu 96000004 / 100000000 nodes ( 96.00 %) | 493.16 MB | 0.387234 secs 
[2022-12-11 20:15:01.350100: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 999999 / 100000000 nodes ( 1.00 %~1.00 %) | remote 2999997 / 100000000 nodes ( 3.00 %) | cpu 96000004 / 100000000 nodes ( 96.00 %) | 493.16 MB | 0.392333 secs 
[2022-12-11 20:15:01.350527: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 999999 / 100000000 nodes ( 1.00 %~1.00 %) | remote 2999997 / 100000000 nodes ( 3.00 %) | cpu 96000004 / 100000000 nodes ( 96.00 %) | 493.16 MB | 0.393048 secs 
[2022-12-11 20:15:01.350795: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 999999 / 100000000 nodes ( 1.00 %~1.00 %) | remote 2999997 / 100000000 nodes ( 3.00 %) | cpu 96000004 / 100000000 nodes ( 96.00 %) | 493.16 MB | 0.388018 secs 
[2022-12-11 20:15:01.351666: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 999999 / 100000000 nodes ( 1.00 %~1.00 %) | remote 2999997 / 100000000 nodes ( 3.00 %) | cpu 96000004 / 100000000 nodes ( 96.00 %) | 493.16 MB | 0.391122 secs 
[2022-12-11 20:15:01.352077: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 999999 / 100000000 nodes ( 1.00 %~1.00 %) | remote 2999997 / 100000000 nodes ( 3.00 %) | cpu 96000004 / 100000000 nodes ( 96.00 %) | 493.16 MB | 0.409878 secs 
[HCTR][20:15:01.352][ERROR][RK0][main]: replica 0 calling init per replica done, doing barrier
[HCTR][20:15:01.352][ERROR][RK0][tid #139607962277632]: replica 2 calling init per replica done, doing barrier
[HCTR][20:15:01.352][ERROR][RK0][tid #139607828059904]: replica 1 calling init per replica done, doing barrier
[HCTR][20:15:01.352][ERROR][RK0][tid #139608163604224]: replica 3 calling init per replica done, doing barrier
[HCTR][20:15:01.352][ERROR][RK0][main]: replica 5 calling init per replica done, doing barrier
[HCTR][20:15:01.352][ERROR][RK0][tid #139607828059904]: replica 7 calling init per replica done, doing barrier
[HCTR][20:15:01.352][ERROR][RK0][tid #139608020993792]: replica 4 calling init per replica done, doing barrier
[HCTR][20:15:01.352][ERROR][RK0][tid #139608817907456]: replica 6 calling init per replica done, doing barrier
[HCTR][20:15:01.352][ERROR][RK0][tid #139608817907456]: replica 6 calling init per replica done, doing barrier done
[HCTR][20:15:01.352][ERROR][RK0][main]: replica 0 calling init per replica done, doing barrier done
[HCTR][20:15:01.352][ERROR][RK0][tid #139607828059904]: replica 1 calling init per replica done, doing barrier done
[HCTR][20:15:01.352][ERROR][RK0][main]: replica 5 calling init per replica done, doing barrier done
[HCTR][20:15:01.352][ERROR][RK0][tid #139607828059904]: replica 7 calling init per replica done, doing barrier done
[HCTR][20:15:01.352][ERROR][RK0][tid #139608163604224]: replica 3 calling init per replica done, doing barrier done
[HCTR][20:15:01.352][ERROR][RK0][tid #139608020993792]: replica 4 calling init per replica done, doing barrier done
[HCTR][20:15:01.352][ERROR][RK0][tid #139607962277632]: replica 2 calling init per replica done, doing barrier done
[HCTR][20:15:01.352][ERROR][RK0][tid #139608817907456]: init per replica done
[HCTR][20:15:01.352][ERROR][RK0][tid #139607828059904]: init per replica done
[HCTR][20:15:01.352][ERROR][RK0][main]: init per replica done
[HCTR][20:15:01.352][ERROR][RK0][tid #139607828059904]: init per replica done
[HCTR][20:15:01.352][ERROR][RK0][tid #139608163604224]: init per replica done
[HCTR][20:15:01.352][ERROR][RK0][tid #139608020993792]: init per replica done
[HCTR][20:15:01.352][ERROR][RK0][tid #139607962277632]: init per replica done
[HCTR][20:15:01.354][ERROR][RK0][main]: init per replica done








