2022-12-12 08:10:48.125988: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.134694: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.140444: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.145737: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.149463: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.162733: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.178468: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.187811: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.240800: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.241905: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.242996: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.243757: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.244313: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.245349: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.246089: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.246840: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.247888: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.248273: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.249642: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.249794: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.251422: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.251490: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.253149: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.254228: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.255373: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.256470: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.257484: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.258437: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.259385: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.260501: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.261537: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.262504: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.264312: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.265495: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.266763: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.267681: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.268587: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.269623: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.270583: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.271528: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.274637: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.275680: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.276664: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.277136: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 08:10:48.278007: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.279763: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.280309: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.281135: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.282140: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.283070: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.284280: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.285348: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.286237: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.286387: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.286409: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.288970: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.289029: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.289044: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.291792: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.292136: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.292172: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.292495: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.295324: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.295345: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.295845: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.296051: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.298334: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.298548: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.298559: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.299322: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.299609: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.300237: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.301905: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.302331: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.302995: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.303202: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.303847: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.305112: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.305650: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.306367: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.306475: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.307233: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.308101: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.308977: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.309704: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.309845: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.310540: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.311256: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.312938: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.313078: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.313644: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.314028: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.315658: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.315843: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.316509: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.316573: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.318649: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.319115: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.319176: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.320787: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.320863: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.322061: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.324535: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.325295: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.326300: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.332070: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.346758: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.356231: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.362368: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.362454: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.363573: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.363589: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.363683: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.364130: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.366151: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.366271: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.367091: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.368219: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.368361: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.368456: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.369077: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.371574: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.371687: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.372676: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.373582: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.373722: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.373812: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.375318: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.377159: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.378154: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.378186: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.378972: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.379448: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.379493: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.380134: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.381756: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.383268: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.383328: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.384057: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.384326: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.384371: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.385018: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.386607: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.388141: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.388533: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.389201: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.389457: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.389544: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.390082: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.391880: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.392895: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.393063: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.393977: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.394300: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.394345: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.395138: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.396520: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.397638: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.397847: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.398756: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.398948: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.399047: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.399823: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.401399: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.402332: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.402692: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.403277: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.403456: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.403548: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.404192: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.405803: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.406921: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.407318: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.407848: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.408143: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.408227: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.409029: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.410704: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.412370: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.412555: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.413122: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.413486: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.413558: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.414244: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.415928: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.417664: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.418184: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.418401: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.418478: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.419038: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.420349: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.421609: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.421947: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 08:10:48.422085: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.422132: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.422589: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.423166: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.424497: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.425546: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.426194: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.426363: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.426582: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.426952: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.428443: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.429676: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.430188: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.430441: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.430552: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.430978: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.431049: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.432856: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.434262: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.435747: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.435792: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.435876: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.435998: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.436079: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.438427: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.438543: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.440145: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.440231: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.440425: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.440521: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.440633: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.443335: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.444419: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.444616: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.444983: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.445021: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.445274: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.448291: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.449537: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.449645: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.450509: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.457184: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.457627: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.459861: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.461276: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.461357: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.461648: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.461951: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.462232: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.464745: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.466067: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.466215: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.466524: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.466790: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.467037: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.469643: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.471011: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.471630: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.471889: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.471938: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.472138: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.474078: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.475928: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.476632: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.476838: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.476912: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.477178: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.479120: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.480523: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.481699: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.483751: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.484916: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 08:10:48.484916: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 08:10:48.485120: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 08:10:48.486149: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 08:10:48.488305: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 08:10:48.489067: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 08:10:48.494568: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.494692: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.494837: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.496009: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.497906: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.498652: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.528850: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.529046: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.529203: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.529238: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.529275: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.529327: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.562731: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.562766: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.562782: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.562828: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.563066: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:48.563137: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:49.648189: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:49.649206: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:49.650220: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:49.650706: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 08:10:49.650763: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-12 08:10:49.668562: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:49.669194: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:49.669711: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:49.670285: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:49.670810: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:49.671395: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-12 08:10:49.718181: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 08:10:49.718399: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 08:10:49.747759: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12340
2022-12-12 08:10:49.812616: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:49.813217: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:49.813750: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:49.814339: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 08:10:49.814396: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-12 08:10:49.832263: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:49.833175: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:49.833701: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:49.834270: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:49.834808: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:49.835293: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:1/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-12 08:10:49.910213: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 08:10:49.910432: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 08:10:49.912281: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12341
2022-12-12 08:10:49.928325: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:49.928947: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:49.929478: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:49.929947: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 08:10:49.929999: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-12 08:10:49.930659: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:49.931238: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:49.931768: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:49.932231: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 08:10:49.932272: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-12 08:10:49.948751: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:49.949414: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:49.950279: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:49.950336: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:49.951366: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:49.951452: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:49.952037: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:49.952374: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:49.952879: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:49.953840: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:49.954061: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:2/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-12 08:10:49.954425: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:49.954942: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:49.955381: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:49.955915: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 08:10:49.955985: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-12 08:10:49.956147: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:4/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-12 08:10:49.957096: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:49.957669: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:49.958176: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:49.958647: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 08:10:49.958698: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-12 08:10:49.960052: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:49.960653: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:49.961172: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:49.961644: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 08:10:49.961687: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-12 08:10:49.974003: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:49.974078: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:49.975180: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:49.975258: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:49.975491: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:49.976363: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:49.976682: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:49.977018: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:49.977851: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:49.977955: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 08:10:49.978002: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-12 08:10:49.978420: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:49.979035: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:49.979099: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:49.987588: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:49.988652: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:5/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-12 08:10:49.988776: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:49.988894: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:49.989790: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:49.989874: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:7/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-12 08:10:49.990485: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:49.991024: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:49.991525: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:6/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-12 08:10:49.994257: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:49.994855: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:49.995388: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:49.995942: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:49.996461: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:10:49.996922: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:3/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-12 08:10:50.000601: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 08:10:50.000787: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 08:10:50.002588: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12344
2022-12-12 08:10:50.032416: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 08:10:50.032623: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 08:10:50.034457: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12342
2022-12-12 08:10:50.035934: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 08:10:50.036103: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 08:10:50.036127: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 08:10:50.036257: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 08:10:50.037735: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12345
2022-12-12 08:10:50.037825: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12347
2022-12-12 08:10:50.037978: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 08:10:50.038112: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 08:10:50.038900: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12346
2022-12-12 08:10:50.043380: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 08:10:50.043537: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 08:10:50.044361: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12343
[HCTR][08:10:51.277][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][08:10:51.277][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][08:10:51.277][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][08:10:51.299][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][08:10:51.300][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][08:10:51.300][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][08:10:51.300][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][08:10:51.300][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 1it [00:01,  1.54s/it]warmup run: 1it [00:01,  1.61s/it]warmup run: 1it [00:01,  1.52s/it]warmup run: 95it [00:01, 80.42it/s]warmup run: 1it [00:01,  1.52s/it]warmup run: 1it [00:01,  1.50s/it]warmup run: 93it [00:01, 75.60it/s]warmup run: 99it [00:01, 84.82it/s]warmup run: 1it [00:01,  1.56s/it]warmup run: 190it [00:01, 174.46it/s]warmup run: 101it [00:01, 86.66it/s]warmup run: 100it [00:01, 86.62it/s]warmup run: 188it [00:01, 166.78it/s]warmup run: 198it [00:01, 183.71it/s]warmup run: 100it [00:01, 83.66it/s]warmup run: 1it [00:01,  1.49s/it]warmup run: 1it [00:01,  1.50s/it]warmup run: 265it [00:01, 251.32it/s]warmup run: 202it [00:01, 187.72it/s]warmup run: 199it [00:01, 186.25it/s]warmup run: 285it [00:01, 270.58it/s]warmup run: 294it [00:01, 288.49it/s]warmup run: 201it [00:01, 182.69it/s]warmup run: 73it [00:01, 63.57it/s]warmup run: 101it [00:01, 87.42it/s]warmup run: 364it [00:01, 369.30it/s]warmup run: 304it [00:01, 300.05it/s]warmup run: 298it [00:01, 295.33it/s]warmup run: 382it [00:02, 379.63it/s]warmup run: 391it [00:01, 398.35it/s]warmup run: 303it [00:01, 293.60it/s]warmup run: 157it [00:01, 149.79it/s]warmup run: 193it [00:01, 179.23it/s]warmup run: 465it [00:02, 488.18it/s]warmup run: 406it [00:01, 416.28it/s]warmup run: 397it [00:01, 408.19it/s]warmup run: 479it [00:02, 487.25it/s]warmup run: 492it [00:02, 512.34it/s]warmup run: 405it [00:01, 408.67it/s]warmup run: 256it [00:01, 264.52it/s]warmup run: 282it [00:01, 275.60it/s]warmup run: 566it [00:02, 596.99it/s]warmup run: 505it [00:02, 523.51it/s]warmup run: 494it [00:02, 513.47it/s]warmup run: 576it [00:02, 586.60it/s]warmup run: 593it [00:02, 616.61it/s]warmup run: 507it [00:02, 521.11it/s]warmup run: 355it [00:01, 382.47it/s]warmup run: 371it [00:01, 375.03it/s]warmup run: 669it [00:02, 695.54it/s]warmup run: 609it [00:02, 631.80it/s]warmup run: 595it [00:02, 617.97it/s]warmup run: 675it [00:02, 677.98it/s]warmup run: 692it [00:02, 702.63it/s]warmup run: 608it [00:02, 622.26it/s]warmup run: 457it [00:01, 502.52it/s]warmup run: 466it [00:02, 482.33it/s]warmup run: 775it [00:02, 783.83it/s]warmup run: 710it [00:02, 718.39it/s]warmup run: 695it [00:02, 705.55it/s]warmup run: 775it [00:02, 755.47it/s]warmup run: 792it [00:02, 775.61it/s]warmup run: 708it [00:02, 708.63it/s]warmup run: 559it [00:02, 612.18it/s]warmup run: 562it [00:02, 583.16it/s]warmup run: 880it [00:02, 851.96it/s]warmup run: 809it [00:02, 770.60it/s]warmup run: 793it [00:02, 773.35it/s]warmup run: 874it [00:02, 814.37it/s]warmup run: 890it [00:02, 827.36it/s]warmup run: 807it [00:02, 777.94it/s]warmup run: 663it [00:02, 710.52it/s]warmup run: 654it [00:02, 660.99it/s]warmup run: 985it [00:02, 904.61it/s]warmup run: 910it [00:02, 830.67it/s]warmup run: 892it [00:02, 829.79it/s]warmup run: 973it [00:02, 859.92it/s]warmup run: 988it [00:02, 863.88it/s]warmup run: 907it [00:02, 835.40it/s]warmup run: 766it [00:02, 789.49it/s]warmup run: 752it [00:02, 738.90it/s]warmup run: 1090it [00:02, 943.58it/s]warmup run: 1008it [00:02, 864.57it/s]warmup run: 992it [00:02, 876.12it/s]warmup run: 1072it [00:02, 894.96it/s]warmup run: 1085it [00:02, 888.47it/s]warmup run: 1008it [00:02, 880.72it/s]warmup run: 869it [00:02, 850.63it/s]warmup run: 849it [00:02, 798.72it/s]warmup run: 1195it [00:02, 972.80it/s]warmup run: 1105it [00:02, 887.12it/s]warmup run: 1093it [00:02, 912.73it/s]warmup run: 1171it [00:02, 921.59it/s]warmup run: 1183it [00:02, 912.43it/s]warmup run: 1109it [00:02, 916.61it/s]warmup run: 971it [00:02, 896.46it/s]warmup run: 946it [00:02, 845.16it/s]warmup run: 1299it [00:02, 991.52it/s]warmup run: 1194it [00:02, 939.06it/s]warmup run: 1202it [00:02, 905.05it/s]warmup run: 1269it [00:02, 938.38it/s]warmup run: 1281it [00:02, 930.01it/s]warmup run: 1212it [00:02, 947.60it/s]warmup run: 1074it [00:02, 933.25it/s]warmup run: 1044it [00:02, 882.65it/s]warmup run: 1403it [00:02, 1003.35it/s]warmup run: 1294it [00:02, 956.47it/s]warmup run: 1298it [00:02, 919.11it/s]warmup run: 1369it [00:03, 953.68it/s]warmup run: 1379it [00:02, 942.90it/s]warmup run: 1314it [00:02, 967.67it/s]warmup run: 1178it [00:02, 962.19it/s]warmup run: 1140it [00:02, 904.60it/s]warmup run: 1507it [00:03, 1011.01it/s]warmup run: 1395it [00:02, 971.95it/s]warmup run: 1394it [00:02, 928.71it/s]warmup run: 1469it [00:03, 967.09it/s]warmup run: 1477it [00:03, 949.33it/s]warmup run: 1417it [00:02, 984.94it/s]warmup run: 1281it [00:02, 978.97it/s]warmup run: 1238it [00:02, 925.55it/s]warmup run: 1611it [00:03, 1018.42it/s]warmup run: 1496it [00:03, 982.75it/s]warmup run: 1490it [00:03, 934.83it/s]warmup run: 1570it [00:03, 978.59it/s]warmup run: 1574it [00:03, 948.07it/s]warmup run: 1520it [00:03, 997.65it/s]warmup run: 1386it [00:02, 998.37it/s]warmup run: 1335it [00:02, 937.54it/s]warmup run: 1715it [00:03, 1022.17it/s]warmup run: 1599it [00:03, 994.72it/s]warmup run: 1586it [00:03, 940.80it/s]warmup run: 1672it [00:03, 988.63it/s]warmup run: 1671it [00:03, 952.33it/s]warmup run: 1622it [00:03, 1004.18it/s]warmup run: 1489it [00:02, 1004.59it/s]warmup run: 1819it [00:03, 1022.56it/s]warmup run: 1432it [00:03, 927.95it/s]warmup run: 1702it [00:03, 1003.13it/s]warmup run: 1682it [00:03, 943.36it/s]warmup run: 1773it [00:03, 994.79it/s]warmup run: 1768it [00:03, 953.19it/s]warmup run: 1724it [00:03, 1001.66it/s]warmup run: 1592it [00:03, 1011.25it/s]warmup run: 1922it [00:03, 1020.82it/s]warmup run: 1527it [00:03, 915.79it/s]warmup run: 1804it [00:03, 1001.99it/s]warmup run: 1778it [00:03, 943.06it/s]warmup run: 1875it [00:03, 1000.78it/s]warmup run: 1864it [00:03, 951.89it/s]warmup run: 1826it [00:03, 1001.47it/s]warmup run: 1695it [00:03, 1012.45it/s]warmup run: 2025it [00:03, 1021.49it/s]warmup run: 1620it [00:03, 916.87it/s]warmup run: 1905it [00:03, 996.01it/s] warmup run: 1873it [00:03, 941.41it/s]warmup run: 1978it [00:03, 1007.77it/s]warmup run: 1960it [00:03, 950.95it/s]warmup run: 1927it [00:03, 1001.63it/s]warmup run: 1798it [00:03, 1000.04it/s]warmup run: 2146it [00:03, 1076.07it/s]warmup run: 1713it [00:03, 917.93it/s]warmup run: 1968it [00:03, 942.14it/s]warmup run: 2006it [00:03, 991.80it/s]warmup run: 2092it [00:03, 1046.08it/s]warmup run: 2070it [00:03, 995.10it/s]warmup run: 2032it [00:03, 1015.66it/s]warmup run: 1899it [00:03, 992.49it/s] warmup run: 2264it [00:03, 1106.52it/s]warmup run: 1807it [00:03, 924.02it/s]warmup run: 2077it [00:03, 985.09it/s]warmup run: 2129it [00:03, 1059.87it/s]warmup run: 2211it [00:03, 1086.82it/s]warmup run: 2192it [00:03, 1060.55it/s]warmup run: 2149it [00:03, 1060.33it/s]warmup run: 1999it [00:03, 988.12it/s]warmup run: 2385it [00:03, 1136.94it/s]warmup run: 1902it [00:03, 929.09it/s]warmup run: 2194it [00:03, 1038.69it/s]warmup run: 2252it [00:03, 1110.05it/s]warmup run: 2330it [00:03, 1115.28it/s]warmup run: 2314it [00:03, 1105.43it/s]warmup run: 2266it [00:03, 1091.42it/s]warmup run: 2118it [00:03, 1047.16it/s]warmup run: 2506it [00:03, 1157.99it/s]warmup run: 1997it [00:03, 933.36it/s]warmup run: 2375it [00:03, 1145.18it/s]warmup run: 2314it [00:03, 1084.42it/s]warmup run: 2448it [00:04, 1134.29it/s]warmup run: 2436it [00:03, 1136.78it/s]warmup run: 2387it [00:03, 1126.16it/s]warmup run: 2240it [00:03, 1096.08it/s]warmup run: 2627it [00:04, 1171.97it/s]warmup run: 2116it [00:03, 1007.44it/s]warmup run: 2497it [00:03, 1167.19it/s]warmup run: 2435it [00:03, 1120.97it/s]warmup run: 2567it [00:04, 1148.82it/s]warmup run: 2558it [00:04, 1159.06it/s]warmup run: 2509it [00:03, 1153.11it/s]warmup run: 2362it [00:03, 1130.65it/s]warmup run: 2748it [00:04, 1181.28it/s]warmup run: 2237it [00:03, 1065.37it/s]warmup run: 2618it [00:04, 1177.93it/s]warmup run: 2556it [00:04, 1146.55it/s]warmup run: 2686it [00:04, 1159.87it/s]warmup run: 2680it [00:04, 1175.01it/s]warmup run: 2630it [00:04, 1169.91it/s]warmup run: 2484it [00:03, 1154.59it/s]warmup run: 2867it [00:04, 1182.82it/s]warmup run: 2359it [00:03, 1108.96it/s]warmup run: 2739it [00:04, 1186.09it/s]warmup run: 2678it [00:04, 1165.82it/s]warmup run: 2803it [00:04, 1162.24it/s]warmup run: 2798it [00:04, 1174.46it/s]warmup run: 2751it [00:04, 1181.62it/s]warmup run: 2606it [00:04, 1172.10it/s]warmup run: 2988it [00:04, 1190.35it/s]warmup run: 2482it [00:04, 1142.61it/s]warmup run: 3000it [00:04, 686.11it/s] warmup run: 2858it [00:04, 1187.20it/s]warmup run: 2798it [00:04, 1173.99it/s]warmup run: 2921it [00:04, 1166.84it/s]warmup run: 2918it [00:04, 1181.20it/s]warmup run: 2871it [00:04, 1185.35it/s]warmup run: 2728it [00:04, 1184.55it/s]warmup run: 2602it [00:04, 1158.87it/s]warmup run: 3000it [00:04, 667.53it/s] warmup run: 2979it [00:04, 1192.67it/s]warmup run: 2917it [00:04, 1178.67it/s]warmup run: 3000it [00:04, 678.87it/s] warmup run: 3000it [00:04, 690.79it/s] warmup run: 2992it [00:04, 1192.50it/s]warmup run: 3000it [00:04, 684.04it/s] warmup run: 2847it [00:04, 1184.72it/s]warmup run: 2722it [00:04, 1170.15it/s]warmup run: 3000it [00:04, 678.27it/s] warmup run: 2969it [00:04, 1194.23it/s]warmup run: 2840it [00:04, 1172.96it/s]warmup run: 3000it [00:04, 689.92it/s] warmup run: 2962it [00:04, 1185.75it/s]warmup run: 3000it [00:04, 671.43it/s] 
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]



warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]

warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   6%|▌         | 166/3000 [00:00<00:01, 1659.18it/s]warmup should be done:   6%|▌         | 167/3000 [00:00<00:01, 1669.70it/s]warmup should be done:   6%|▌         | 169/3000 [00:00<00:01, 1686.39it/s]warmup should be done:   5%|▌         | 163/3000 [00:00<00:01, 1626.72it/s]warmup should be done:   5%|▌         | 157/3000 [00:00<00:01, 1567.59it/s]warmup should be done:   6%|▌         | 168/3000 [00:00<00:01, 1675.39it/s]warmup should be done:   5%|▌         | 164/3000 [00:00<00:01, 1632.12it/s]warmup should be done:   5%|▌         | 164/3000 [00:00<00:01, 1632.22it/s]warmup should be done:  11%|█         | 334/3000 [00:00<00:01, 1667.78it/s]warmup should be done:  11%|█         | 336/3000 [00:00<00:01, 1676.71it/s]warmup should be done:  11%|█         | 319/3000 [00:00<00:01, 1595.97it/s]warmup should be done:  11%|█         | 337/3000 [00:00<00:01, 1681.82it/s]warmup should be done:  11%|█▏        | 339/3000 [00:00<00:01, 1689.38it/s]warmup should be done:  11%|█         | 328/3000 [00:00<00:01, 1636.45it/s]warmup should be done:  11%|█         | 329/3000 [00:00<00:01, 1637.47it/s]warmup should be done:  11%|█         | 330/3000 [00:00<00:01, 1643.56it/s]warmup should be done:  16%|█▌        | 480/3000 [00:00<00:01, 1602.05it/s]warmup should be done:  17%|█▋        | 501/3000 [00:00<00:01, 1665.59it/s]warmup should be done:  17%|█▋        | 504/3000 [00:00<00:01, 1672.61it/s]warmup should be done:  17%|█▋        | 508/3000 [00:00<00:01, 1684.91it/s]warmup should be done:  16%|█▋        | 492/3000 [00:00<00:01, 1633.47it/s]warmup should be done:  17%|█▋        | 506/3000 [00:00<00:01, 1679.89it/s]warmup should be done:  16%|█▋        | 495/3000 [00:00<00:01, 1643.19it/s]warmup should be done:  16%|█▋        | 493/3000 [00:00<00:01, 1635.01it/s]warmup should be done:  21%|██▏       | 641/3000 [00:00<00:01, 1602.78it/s]warmup should be done:  22%|██▏       | 668/3000 [00:00<00:01, 1664.50it/s]warmup should be done:  22%|██▏       | 674/3000 [00:00<00:01, 1678.82it/s]warmup should be done:  23%|██▎       | 677/3000 [00:00<00:01, 1683.92it/s]warmup should be done:  22%|██▏       | 672/3000 [00:00<00:01, 1672.24it/s]warmup should be done:  22%|██▏       | 660/3000 [00:00<00:01, 1642.79it/s]warmup should be done:  22%|██▏       | 656/3000 [00:00<00:01, 1631.06it/s]warmup should be done:  22%|██▏       | 657/3000 [00:00<00:01, 1632.16it/s]warmup should be done:  28%|██▊       | 835/3000 [00:00<00:01, 1662.68it/s]warmup should be done:  27%|██▋       | 802/3000 [00:00<00:01, 1600.39it/s]warmup should be done:  28%|██▊       | 842/3000 [00:00<00:01, 1676.35it/s]warmup should be done:  28%|██▊       | 840/3000 [00:00<00:01, 1669.99it/s]warmup should be done:  28%|██▊       | 846/3000 [00:00<00:01, 1680.32it/s]warmup should be done:  28%|██▊       | 825/3000 [00:00<00:01, 1641.03it/s]warmup should be done:  27%|██▋       | 820/3000 [00:00<00:01, 1629.61it/s]warmup should be done:  27%|██▋       | 821/3000 [00:00<00:01, 1629.03it/s]warmup should be done:  33%|███▎      | 1002/3000 [00:00<00:01, 1660.09it/s]warmup should be done:  34%|███▎      | 1010/3000 [00:00<00:01, 1673.60it/s]warmup should be done:  34%|███▎      | 1007/3000 [00:00<00:01, 1667.26it/s]warmup should be done:  33%|███▎      | 983/3000 [00:00<00:01, 1627.54it/s]warmup should be done:  32%|███▏      | 963/3000 [00:00<00:01, 1594.84it/s]warmup should be done:  33%|███▎      | 990/3000 [00:00<00:01, 1637.71it/s]warmup should be done:  33%|███▎      | 984/3000 [00:00<00:01, 1624.85it/s]warmup should be done:  34%|███▍      | 1015/3000 [00:00<00:01, 1674.48it/s]warmup should be done:  39%|███▉      | 1178/3000 [00:00<00:01, 1670.70it/s]warmup should be done:  37%|███▋      | 1123/3000 [00:00<00:01, 1593.00it/s]warmup should be done:  39%|███▉      | 1169/3000 [00:00<00:01, 1654.90it/s]warmup should be done:  39%|███▉      | 1174/3000 [00:00<00:01, 1661.60it/s]warmup should be done:  38%|███▊      | 1146/3000 [00:00<00:01, 1623.03it/s]warmup should be done:  38%|███▊      | 1154/3000 [00:00<00:01, 1635.16it/s]warmup should be done:  38%|███▊      | 1147/3000 [00:00<00:01, 1621.23it/s]warmup should be done:  39%|███▉      | 1183/3000 [00:00<00:01, 1662.00it/s]warmup should be done:  45%|████▍     | 1346/3000 [00:00<00:00, 1672.24it/s]warmup should be done:  43%|████▎     | 1283/3000 [00:00<00:01, 1594.61it/s]warmup should be done:  44%|████▍     | 1335/3000 [00:00<00:01, 1654.19it/s]warmup should be done:  45%|████▍     | 1341/3000 [00:00<00:00, 1661.10it/s]warmup should be done:  44%|████▎     | 1309/3000 [00:00<00:01, 1622.01it/s]warmup should be done:  44%|████▍     | 1319/3000 [00:00<00:01, 1637.16it/s]warmup should be done:  44%|████▎     | 1310/3000 [00:00<00:01, 1622.00it/s]warmup should be done:  45%|████▌     | 1350/3000 [00:00<00:00, 1657.87it/s]warmup should be done:  50%|█████     | 1514/3000 [00:00<00:00, 1672.06it/s]warmup should be done:  48%|████▊     | 1443/3000 [00:00<00:00, 1594.64it/s]warmup should be done:  50%|█████     | 1501/3000 [00:00<00:00, 1653.48it/s]warmup should be done:  49%|████▉     | 1483/3000 [00:00<00:00, 1637.23it/s]warmup should be done:  50%|█████     | 1508/3000 [00:00<00:00, 1660.64it/s]warmup should be done:  49%|████▉     | 1472/3000 [00:00<00:00, 1620.77it/s]warmup should be done:  49%|████▉     | 1473/3000 [00:00<00:00, 1620.70it/s]warmup should be done:  51%|█████     | 1516/3000 [00:00<00:00, 1653.56it/s]warmup should be done:  53%|█████▎    | 1603/3000 [00:01<00:00, 1594.76it/s]warmup should be done:  56%|█████▌    | 1667/3000 [00:01<00:00, 1654.15it/s]warmup should be done:  55%|█████▍    | 1648/3000 [00:01<00:00, 1638.35it/s]warmup should be done:  56%|█████▌    | 1675/3000 [00:01<00:00, 1660.98it/s]warmup should be done:  55%|█████▍    | 1635/3000 [00:01<00:00, 1620.11it/s]warmup should be done:  55%|█████▍    | 1637/3000 [00:01<00:00, 1624.43it/s]warmup should be done:  56%|█████▌    | 1682/3000 [00:01<00:00, 1655.38it/s]warmup should be done:  56%|█████▌    | 1682/3000 [00:01<00:00, 1647.19it/s]warmup should be done:  59%|█████▉    | 1763/3000 [00:01<00:00, 1595.34it/s]warmup should be done:  61%|██████    | 1833/3000 [00:01<00:00, 1654.27it/s]warmup should be done:  60%|██████    | 1812/3000 [00:01<00:00, 1637.54it/s]warmup should be done:  61%|██████▏   | 1842/3000 [00:01<00:00, 1660.85it/s]warmup should be done:  60%|█████▉    | 1798/3000 [00:01<00:00, 1620.65it/s]warmup should be done:  60%|██████    | 1803/3000 [00:01<00:00, 1633.82it/s]warmup should be done:  62%|██████▏   | 1848/3000 [00:01<00:00, 1644.39it/s]warmup should be done:  62%|██████▏   | 1847/3000 [00:01<00:00, 1643.41it/s]warmup should be done:  64%|██████▍   | 1923/3000 [00:01<00:00, 1596.56it/s]warmup should be done:  67%|██████▋   | 1999/3000 [00:01<00:00, 1654.36it/s]warmup should be done:  66%|██████▌   | 1976/3000 [00:01<00:00, 1636.82it/s]warmup should be done:  67%|██████▋   | 2009/3000 [00:01<00:00, 1661.30it/s]warmup should be done:  66%|██████▌   | 1969/3000 [00:01<00:00, 1640.23it/s]warmup should be done:  65%|██████▌   | 1961/3000 [00:01<00:00, 1620.75it/s]warmup should be done:  67%|██████▋   | 2012/3000 [00:01<00:00, 1641.36it/s]warmup should be done:  67%|██████▋   | 2013/3000 [00:01<00:00, 1636.09it/s]warmup should be done:  69%|██████▉   | 2083/3000 [00:01<00:00, 1597.26it/s]warmup should be done:  72%|███████▏  | 2165/3000 [00:01<00:00, 1653.86it/s]warmup should be done:  71%|███████▏  | 2140/3000 [00:01<00:00, 1636.79it/s]warmup should be done:  73%|███████▎  | 2176/3000 [00:01<00:00, 1661.22it/s]warmup should be done:  71%|███████   | 2135/3000 [00:01<00:00, 1643.47it/s]warmup should be done:  71%|███████   | 2124/3000 [00:01<00:00, 1621.23it/s]warmup should be done:  73%|███████▎  | 2177/3000 [00:01<00:00, 1638.90it/s]warmup should be done:  73%|███████▎  | 2177/3000 [00:01<00:00, 1630.56it/s]warmup should be done:  75%|███████▍  | 2243/3000 [00:01<00:00, 1596.81it/s]warmup should be done:  77%|███████▋  | 2304/3000 [00:01<00:00, 1635.98it/s]warmup should be done:  78%|███████▊  | 2331/3000 [00:01<00:00, 1653.79it/s]warmup should be done:  77%|███████▋  | 2301/3000 [00:01<00:00, 1646.60it/s]warmup should be done:  76%|███████▌  | 2287/3000 [00:01<00:00, 1621.34it/s]warmup should be done:  78%|███████▊  | 2343/3000 [00:01<00:00, 1658.42it/s]warmup should be done:  78%|███████▊  | 2341/3000 [00:01<00:00, 1635.27it/s]warmup should be done:  78%|███████▊  | 2341/3000 [00:01<00:00, 1621.62it/s]warmup should be done:  80%|████████  | 2403/3000 [00:01<00:00, 1595.31it/s]warmup should be done:  82%|████████▏ | 2468/3000 [00:01<00:00, 1633.12it/s]warmup should be done:  83%|████████▎ | 2497/3000 [00:01<00:00, 1651.29it/s]warmup should be done:  82%|████████▏ | 2466/3000 [00:01<00:00, 1644.41it/s]warmup should be done:  84%|████████▎ | 2510/3000 [00:01<00:00, 1659.76it/s]warmup should be done:  82%|████████▏ | 2450/3000 [00:01<00:00, 1619.29it/s]warmup should be done:  84%|████████▎ | 2505/3000 [00:01<00:00, 1635.21it/s]warmup should be done:  83%|████████▎ | 2504/3000 [00:01<00:00, 1618.98it/s]warmup should be done:  88%|████████▊ | 2632/3000 [00:01<00:00, 1633.71it/s]warmup should be done:  89%|████████▉ | 2663/3000 [00:01<00:00, 1651.80it/s]warmup should be done:  89%|████████▉ | 2677/3000 [00:01<00:00, 1660.84it/s]warmup should be done:  85%|████████▌ | 2563/3000 [00:01<00:00, 1594.13it/s]warmup should be done:  87%|████████▋ | 2612/3000 [00:01<00:00, 1601.40it/s]warmup should be done:  88%|████████▊ | 2631/3000 [00:01<00:00, 1617.54it/s]warmup should be done:  89%|████████▉ | 2669/3000 [00:01<00:00, 1635.15it/s]warmup should be done:  89%|████████▉ | 2667/3000 [00:01<00:00, 1619.33it/s]warmup should be done:  94%|█████████▍| 2831/3000 [00:01<00:00, 1660.04it/s]warmup should be done:  93%|█████████▎| 2796/3000 [00:01<00:00, 1632.88it/s]warmup should be done:  95%|█████████▍| 2845/3000 [00:01<00:00, 1663.56it/s]warmup should be done:  91%|█████████ | 2723/3000 [00:01<00:00, 1577.53it/s]warmup should be done:  94%|█████████▍| 2833/3000 [00:01<00:00, 1629.36it/s]warmup should be done:  92%|█████████▏| 2773/3000 [00:01<00:00, 1582.60it/s]warmup should be done:  94%|█████████▍| 2833/3000 [00:01<00:00, 1630.27it/s]warmup should be done:  93%|█████████▎| 2793/3000 [00:01<00:00, 1594.16it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1664.17it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1668.54it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1658.62it/s]warmup should be done:  99%|█████████▊| 2962/3000 [00:01<00:00, 1639.25it/s]warmup should be done:  96%|█████████▌| 2884/3000 [00:01<00:00, 1586.09it/s]warmup should be done: 100%|█████████▉| 2996/3000 [00:01<00:00, 1628.48it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1650.74it/s]warmup should be done:  98%|█████████▊| 2933/3000 [00:01<00:00, 1586.23it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1648.75it/s]warmup should be done:  98%|█████████▊| 2953/3000 [00:01<00:00, 1594.75it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1637.42it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1621.98it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1614.48it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1593.21it/s]



warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]



warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   6%|▌         | 167/3000 [00:00<00:01, 1668.49it/s]warmup should be done:   6%|▌         | 167/3000 [00:00<00:01, 1667.37it/s]warmup should be done:   6%|▌         | 171/3000 [00:00<00:01, 1706.32it/s]warmup should be done:   6%|▌         | 167/3000 [00:00<00:01, 1664.91it/s]warmup should be done:   6%|▌         | 171/3000 [00:00<00:01, 1704.07it/s]warmup should be done:   6%|▌         | 170/3000 [00:00<00:01, 1693.49it/s]warmup should be done:   6%|▌         | 165/3000 [00:00<00:01, 1641.60it/s]warmup should be done:   6%|▌         | 165/3000 [00:00<00:01, 1640.92it/s]warmup should be done:  11%|█▏        | 342/3000 [00:00<00:01, 1706.97it/s]warmup should be done:  11%|█▏        | 343/3000 [00:00<00:01, 1712.22it/s]warmup should be done:  11%|█         | 336/3000 [00:00<00:01, 1676.79it/s]warmup should be done:  11%|█         | 334/3000 [00:00<00:01, 1664.04it/s]warmup should be done:  11%|█         | 336/3000 [00:00<00:01, 1679.88it/s]warmup should be done:  11%|█         | 335/3000 [00:00<00:01, 1668.27it/s]warmup should be done:  11%|█         | 330/3000 [00:00<00:01, 1641.42it/s]warmup should be done:  11%|█▏        | 340/3000 [00:00<00:01, 1686.10it/s]warmup should be done:  17%|█▋        | 513/3000 [00:00<00:01, 1705.74it/s]warmup should be done:  17%|█▋        | 501/3000 [00:00<00:01, 1665.76it/s]warmup should be done:  17%|█▋        | 505/3000 [00:00<00:01, 1680.94it/s]warmup should be done:  17%|█▋        | 507/3000 [00:00<00:01, 1692.48it/s]warmup should be done:  17%|█▋        | 516/3000 [00:00<00:01, 1716.47it/s]warmup should be done:  16%|█▋        | 495/3000 [00:00<00:01, 1643.74it/s]warmup should be done:  17%|█▋        | 503/3000 [00:00<00:01, 1670.44it/s]warmup should be done:  17%|█▋        | 511/3000 [00:00<00:01, 1693.83it/s]warmup should be done:  22%|██▏       | 674/3000 [00:00<00:01, 1682.01it/s]warmup should be done:  22%|██▏       | 670/3000 [00:00<00:01, 1672.05it/s]warmup should be done:  22%|██▏       | 671/3000 [00:00<00:01, 1673.95it/s]warmup should be done:  23%|██▎       | 679/3000 [00:00<00:01, 1699.46it/s]warmup should be done:  22%|██▏       | 661/3000 [00:00<00:01, 1646.31it/s]warmup should be done:  23%|██▎       | 684/3000 [00:00<00:01, 1699.24it/s]warmup should be done:  23%|██▎       | 682/3000 [00:00<00:01, 1699.21it/s]warmup should be done:  23%|██▎       | 688/3000 [00:00<00:01, 1710.03it/s]warmup should be done:  28%|██▊       | 839/3000 [00:00<00:01, 1677.90it/s]warmup should be done:  28%|██▊       | 839/3000 [00:00<00:01, 1673.12it/s]warmup should be done:  28%|██▊       | 850/3000 [00:00<00:01, 1701.06it/s]warmup should be done:  28%|██▊       | 843/3000 [00:00<00:01, 1680.71it/s]warmup should be done:  28%|██▊       | 826/3000 [00:00<00:01, 1645.95it/s]warmup should be done:  28%|██▊       | 854/3000 [00:00<00:01, 1703.14it/s]warmup should be done:  28%|██▊       | 854/3000 [00:00<00:01, 1692.70it/s]warmup should be done:  29%|██▊       | 860/3000 [00:00<00:01, 1703.93it/s]warmup should be done:  34%|███▎      | 1009/3000 [00:00<00:01, 1683.91it/s]warmup should be done:  34%|███▎      | 1008/3000 [00:00<00:01, 1678.76it/s]warmup should be done:  34%|███▎      | 1012/3000 [00:00<00:01, 1681.76it/s]warmup should be done:  33%|███▎      | 991/3000 [00:00<00:01, 1645.07it/s]warmup should be done:  34%|███▍      | 1025/3000 [00:00<00:01, 1704.55it/s]warmup should be done:  34%|███▍      | 1032/3000 [00:00<00:01, 1707.84it/s]warmup should be done:  34%|███▍      | 1021/3000 [00:00<00:01, 1689.39it/s]warmup should be done:  34%|███▍      | 1024/3000 [00:00<00:01, 1686.42it/s]warmup should be done:  39%|███▉      | 1179/3000 [00:00<00:01, 1688.53it/s]warmup should be done:  39%|███▉      | 1178/3000 [00:00<00:01, 1683.65it/s]warmup should be done:  39%|███▊      | 1156/3000 [00:00<00:01, 1645.24it/s]warmup should be done:  39%|███▉      | 1181/3000 [00:00<00:01, 1681.52it/s]warmup should be done:  40%|███▉      | 1196/3000 [00:00<00:01, 1703.51it/s]warmup should be done:  40%|████      | 1203/3000 [00:00<00:01, 1704.72it/s]warmup should be done:  40%|███▉      | 1193/3000 [00:00<00:01, 1696.24it/s]warmup should be done:  40%|███▉      | 1193/3000 [00:00<00:01, 1679.19it/s]warmup should be done:  45%|████▌     | 1350/3000 [00:00<00:00, 1694.79it/s]warmup should be done:  45%|████▍     | 1349/3000 [00:00<00:00, 1689.74it/s]warmup should be done:  45%|████▌     | 1350/3000 [00:00<00:00, 1683.48it/s]warmup should be done:  44%|████▍     | 1321/3000 [00:00<00:01, 1643.96it/s]warmup should be done:  46%|████▌     | 1368/3000 [00:00<00:00, 1707.11it/s]warmup should be done:  46%|████▌     | 1365/3000 [00:00<00:00, 1702.03it/s]warmup should be done:  46%|████▌     | 1374/3000 [00:00<00:00, 1698.67it/s]warmup should be done:  45%|████▌     | 1362/3000 [00:00<00:00, 1680.28it/s]warmup should be done:  51%|█████     | 1520/3000 [00:00<00:00, 1696.19it/s]warmup should be done:  51%|█████     | 1519/3000 [00:00<00:00, 1692.82it/s]warmup should be done:  51%|█████     | 1519/3000 [00:00<00:00, 1683.67it/s]warmup should be done:  50%|████▉     | 1487/3000 [00:00<00:00, 1645.96it/s]warmup should be done:  51%|█████▏    | 1539/3000 [00:00<00:00, 1700.54it/s]warmup should be done:  51%|█████     | 1536/3000 [00:00<00:00, 1704.31it/s]warmup should be done:  51%|█████▏    | 1544/3000 [00:00<00:00, 1693.38it/s]warmup should be done:  51%|█████     | 1531/3000 [00:00<00:00, 1677.64it/s]warmup should be done:  56%|█████▋    | 1691/3000 [00:01<00:00, 1697.89it/s]warmup should be done:  56%|█████▋    | 1690/3000 [00:01<00:00, 1695.49it/s]warmup should be done:  56%|█████▋    | 1688/3000 [00:01<00:00, 1684.08it/s]warmup should be done:  55%|█████▌    | 1652/3000 [00:01<00:00, 1647.14it/s]warmup should be done:  57%|█████▋    | 1708/3000 [00:01<00:00, 1706.25it/s]warmup should be done:  57%|█████▋    | 1710/3000 [00:01<00:00, 1697.64it/s]warmup should be done:  57%|█████▋    | 1699/3000 [00:01<00:00, 1677.09it/s]warmup should be done:  57%|█████▋    | 1714/3000 [00:01<00:00, 1689.41it/s]warmup should be done:  62%|██████▏   | 1862/3000 [00:01<00:00, 1700.68it/s]warmup should be done:  62%|██████▏   | 1861/3000 [00:01<00:00, 1698.00it/s]warmup should be done:  62%|██████▏   | 1857/3000 [00:01<00:00, 1685.71it/s]warmup should be done:  61%|██████    | 1818/3000 [00:01<00:00, 1648.17it/s]warmup should be done:  63%|██████▎   | 1880/3000 [00:01<00:00, 1709.04it/s]warmup should be done:  63%|██████▎   | 1880/3000 [00:01<00:00, 1697.26it/s]warmup should be done:  62%|██████▏   | 1869/3000 [00:01<00:00, 1682.78it/s]warmup should be done:  63%|██████▎   | 1883/3000 [00:01<00:00, 1689.19it/s]warmup should be done:  68%|██████▊   | 2033/3000 [00:01<00:00, 1702.00it/s]warmup should be done:  68%|██████▊   | 2026/3000 [00:01<00:00, 1685.86it/s]warmup should be done:  68%|██████▊   | 2032/3000 [00:01<00:00, 1699.41it/s]warmup should be done:  66%|██████▌   | 1983/3000 [00:01<00:00, 1647.12it/s]warmup should be done:  68%|██████▊   | 2050/3000 [00:01<00:00, 1697.11it/s]warmup should be done:  68%|██████▊   | 2052/3000 [00:01<00:00, 1710.21it/s]warmup should be done:  68%|██████▊   | 2039/3000 [00:01<00:00, 1686.66it/s]warmup should be done:  68%|██████▊   | 2052/3000 [00:01<00:00, 1689.03it/s]warmup should be done:  73%|███████▎  | 2202/3000 [00:01<00:00, 1699.33it/s]warmup should be done:  73%|███████▎  | 2195/3000 [00:01<00:00, 1684.43it/s]warmup should be done:  73%|███████▎  | 2204/3000 [00:01<00:00, 1694.72it/s]warmup should be done:  72%|███████▏  | 2148/3000 [00:01<00:00, 1646.52it/s]warmup should be done:  74%|███████▍  | 2220/3000 [00:01<00:00, 1694.88it/s]warmup should be done:  74%|███████▍  | 2224/3000 [00:01<00:00, 1709.91it/s]warmup should be done:  74%|███████▎  | 2208/3000 [00:01<00:00, 1685.52it/s]warmup should be done:  74%|███████▍  | 2221/3000 [00:01<00:00, 1686.07it/s]warmup should be done:  79%|███████▉  | 2372/3000 [00:01<00:00, 1699.50it/s]warmup should be done:  79%|███████▉  | 2364/3000 [00:01<00:00, 1683.80it/s]warmup should be done:  77%|███████▋  | 2314/3000 [00:01<00:00, 1649.27it/s]warmup should be done:  79%|███████▉  | 2374/3000 [00:01<00:00, 1690.92it/s]warmup should be done:  80%|███████▉  | 2395/3000 [00:01<00:00, 1709.66it/s]warmup should be done:  80%|███████▉  | 2390/3000 [00:01<00:00, 1695.42it/s]warmup should be done:  79%|███████▉  | 2377/3000 [00:01<00:00, 1683.00it/s]warmup should be done:  80%|███████▉  | 2390/3000 [00:01<00:00, 1684.32it/s]warmup should be done:  85%|████████▍ | 2543/3000 [00:01<00:00, 1700.10it/s]warmup should be done:  84%|████████▍ | 2533/3000 [00:01<00:00, 1684.42it/s]warmup should be done:  83%|████████▎ | 2479/3000 [00:01<00:00, 1647.49it/s]warmup should be done:  85%|████████▍ | 2544/3000 [00:01<00:00, 1690.23it/s]warmup should be done:  86%|████████▌ | 2567/3000 [00:01<00:00, 1710.35it/s]warmup should be done:  85%|████████▌ | 2561/3000 [00:01<00:00, 1696.95it/s]warmup should be done:  85%|████████▌ | 2559/3000 [00:01<00:00, 1685.71it/s]warmup should be done:  85%|████████▍ | 2546/3000 [00:01<00:00, 1682.68it/s]warmup should be done:  90%|█████████ | 2714/3000 [00:01<00:00, 1697.12it/s]warmup should be done:  90%|█████████ | 2703/3000 [00:01<00:00, 1686.13it/s]warmup should be done:  90%|█████████ | 2714/3000 [00:01<00:00, 1690.72it/s]warmup should be done:  91%|█████████▏| 2739/3000 [00:01<00:00, 1711.84it/s]warmup should be done:  88%|████████▊ | 2644/3000 [00:01<00:00, 1630.37it/s]warmup should be done:  91%|█████████ | 2731/3000 [00:01<00:00, 1690.33it/s]warmup should be done:  90%|█████████ | 2715/3000 [00:01<00:00, 1684.59it/s]warmup should be done:  91%|█████████ | 2729/3000 [00:01<00:00, 1686.89it/s]warmup should be done:  96%|█████████▌| 2872/3000 [00:01<00:00, 1684.21it/s]warmup should be done:  96%|█████████▌| 2884/3000 [00:01<00:00, 1692.53it/s]warmup should be done:  96%|█████████▌| 2884/3000 [00:01<00:00, 1686.50it/s]warmup should be done:  97%|█████████▋| 2911/3000 [00:01<00:00, 1707.87it/s]warmup should be done:  96%|█████████▌| 2886/3000 [00:01<00:00, 1690.53it/s]warmup should be done:  97%|█████████▋| 2898/3000 [00:01<00:00, 1685.99it/s]warmup should be done:  94%|█████████▎| 2808/3000 [00:01<00:00, 1606.24it/s]warmup should be done:  97%|█████████▋| 2901/3000 [00:01<00:00, 1603.04it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1703.15it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1693.48it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1689.51it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1688.19it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1687.90it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1683.29it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1668.58it/s]warmup should be done:  99%|█████████▉| 2973/3000 [00:01<00:00, 1618.64it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1637.61it/s]
WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f14e07a5190>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f14e0ab7e80>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f14e07b21c0>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f14e07b32b0>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f14e0abad30>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f14e07a51f0>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f14e07a40d0>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f14e0ab8730>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
2022-12-12 08:12:21.775709: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f1012833830 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 08:12:21.775766: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 08:12:21.783904: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 08:12:21.829133: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f101682b180 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 08:12:21.829186: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 08:12:21.831931: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f10128302c0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 08:12:21.831987: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 08:12:21.836548: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 08:12:21.839667: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 08:12:22.238013: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f1016833d80 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 08:12:22.238081: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 08:12:22.245949: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 08:12:22.565964: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f1016830250 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 08:12:22.566032: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 08:12:22.573629: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f101682c190 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 08:12:22.573695: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 08:12:22.574139: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 08:12:22.583675: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 08:12:22.612962: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f101b030ca0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 08:12:22.613024: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 08:12:22.614010: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f100af938e0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 08:12:22.614055: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 08:12:22.620629: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 08:12:22.623683: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 08:12:28.953510: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 08:12:28.964277: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 08:12:29.126679: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 08:12:29.201833: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 08:12:29.245883: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 08:12:29.472807: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 08:12:29.609699: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 08:12:29.659609: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
[HCTR][08:13:18.475][ERROR][RK0][main]: replica 5 reaches 1000, calling init pre replica
[HCTR][08:13:18.475][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][08:13:18.483][ERROR][RK0][main]: coll ps creation done
[HCTR][08:13:18.483][ERROR][RK0][main]: replica 5 waits for coll ps creation barrier
[HCTR][08:13:18.529][ERROR][RK0][tid #139708139026176]: replica 7 reaches 1000, calling init pre replica
[HCTR][08:13:18.529][ERROR][RK0][tid #139708139026176]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][08:13:18.534][ERROR][RK0][tid #139708139026176]: coll ps creation done
[HCTR][08:13:18.534][ERROR][RK0][tid #139708139026176]: replica 7 waits for coll ps creation barrier
[HCTR][08:13:18.609][ERROR][RK0][main]: replica 4 reaches 1000, calling init pre replica
[HCTR][08:13:18.609][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][08:13:18.616][ERROR][RK0][main]: coll ps creation done
[HCTR][08:13:18.616][ERROR][RK0][main]: replica 4 waits for coll ps creation barrier
[HCTR][08:13:18.705][ERROR][RK0][tid #139707526653696]: replica 1 reaches 1000, calling init pre replica
[HCTR][08:13:18.705][ERROR][RK0][tid #139707526653696]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][08:13:18.712][ERROR][RK0][tid #139707526653696]: coll ps creation done
[HCTR][08:13:18.712][ERROR][RK0][tid #139707526653696]: replica 1 waits for coll ps creation barrier
[HCTR][08:13:18.756][ERROR][RK0][tid #139707602155264]: replica 6 reaches 1000, calling init pre replica
[HCTR][08:13:18.756][ERROR][RK0][tid #139707602155264]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][08:13:18.761][ERROR][RK0][tid #139707602155264]: coll ps creation done
[HCTR][08:13:18.761][ERROR][RK0][tid #139707602155264]: replica 6 waits for coll ps creation barrier
[HCTR][08:13:18.769][ERROR][RK0][main]: replica 3 reaches 1000, calling init pre replica
[HCTR][08:13:18.769][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][08:13:18.776][ERROR][RK0][main]: coll ps creation done
[HCTR][08:13:18.776][ERROR][RK0][main]: replica 3 waits for coll ps creation barrier
[HCTR][08:13:18.878][ERROR][RK0][main]: replica 0 reaches 1000, calling init pre replica
[HCTR][08:13:18.878][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][08:13:18.883][ERROR][RK0][main]: coll ps creation done
[HCTR][08:13:18.883][ERROR][RK0][main]: replica 0 waits for coll ps creation barrier
[HCTR][08:13:18.923][ERROR][RK0][tid #139707652478720]: replica 2 reaches 1000, calling init pre replica
[HCTR][08:13:18.923][ERROR][RK0][tid #139707652478720]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][08:13:18.928][ERROR][RK0][tid #139707652478720]: coll ps creation done
[HCTR][08:13:18.928][ERROR][RK0][tid #139707652478720]: replica 2 waits for coll ps creation barrier
[HCTR][08:13:18.928][ERROR][RK0][main]: replica 0 preparing frequency
[HCTR][08:13:19.807][ERROR][RK0][main]: replica 0 preparing frequency done
[HCTR][08:13:19.865][ERROR][RK0][tid #139707526653696]: replica 1 calling init per replica
[HCTR][08:13:19.865][ERROR][RK0][main]: replica 4 calling init per replica
[HCTR][08:13:19.865][ERROR][RK0][tid #139707652478720]: replica 2 calling init per replica
[HCTR][08:13:19.865][ERROR][RK0][tid #139707602155264]: replica 6 calling init per replica
[HCTR][08:13:19.865][ERROR][RK0][main]: replica 0 calling init per replica
[HCTR][08:13:19.865][ERROR][RK0][tid #139708139026176]: replica 7 calling init per replica
[HCTR][08:13:19.865][ERROR][RK0][main]: replica 3 calling init per replica
[HCTR][08:13:19.865][ERROR][RK0][main]: replica 5 calling init per replica
[HCTR][08:13:19.865][ERROR][RK0][tid #139707526653696]: Calling build_v2
[HCTR][08:13:19.865][ERROR][RK0][main]: Calling build_v2
[HCTR][08:13:19.865][ERROR][RK0][tid #139707652478720]: Calling build_v2
[HCTR][08:13:19.865][ERROR][RK0][tid #139707602155264]: Calling build_v2
[HCTR][08:13:19.865][ERROR][RK0][tid #139707526653696]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][08:13:19.865][ERROR][RK0][main]: Calling build_v2
[HCTR][08:13:19.865][ERROR][RK0][tid #139708139026176]: Calling build_v2
[HCTR][08:13:19.865][ERROR][RK0][main]: Calling build_v2
[HCTR][08:13:19.865][ERROR][RK0][main]: Calling build_v2
[HCTR][08:13:19.865][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][08:13:19.865][ERROR][RK0][tid #139707652478720]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][08:13:19.865][ERROR][RK0][tid #139707602155264]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][08:13:19.865][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][08:13:19.865][ERROR][RK0][tid #139708139026176]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][08:13:19.865][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][08:13:19.865][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[[[[[[[2022-12-12 08:13:192022-12-12 08:13:192022-12-12 08:13:192022-12-12 08:13:19.2022-12-12 08:13:19.2022-12-12 08:13:192022-12-12 08:13:19..866020.866024..866031866034: 866037: 866024866037: : E: [E: : EE E EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc /hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc  /hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc:/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc:/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc2022-12-12 08:13:19/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc::136:136:.:136136] 136] 136866087136] ] using concurrent impl MPS] using concurrent impl MPS] : ] using concurrent impl MPSusing concurrent impl MPS
using concurrent impl MPS
using concurrent impl MPSEusing concurrent impl MPS



 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc:136] using concurrent impl MPS
[2022-12-12 08:13:19.870305: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:178] v100x8, slow pcie
[2022-12-12 08:13:19.870344: E [/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 08:13:19:.196870350] : assigning 8 to cpuE
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:178] v100x8, slow pcie
[[2022-12-12 08:13:192022-12-12 08:13:19..870398870394: : EE[  2022-12-12 08:13:19/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.::870419196178: [] ] E2022-12-12 08:13:19assigning 8 to cpuv100x8, slow pcie .

/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc870438:: 212[E] 2022-12-12 08:13:19 build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8.[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[
8704752022-12-12 08:13:19:2022-12-12 08:13:19: .178.E870485] 870483[ : v100x8, slow pcie: 2022-12-12 08:13:19/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE
E[.:  2022-12-12 08:13:19[870517196/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.2022-12-12 08:13:19: ] ::870529.E[assigning 8 to cpu212178: 870553 2022-12-12 08:13:19
] ] E: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8v100x8, slow pcie E:870574

/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc 213: :/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] [E178:remote time is 8.68421[[2022-12-12 08:13:19 ] 196[
2022-12-12 08:13:192022-12-12 08:13:19./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccv100x8, slow pcie] 2022-12-12 08:13:19..870643[:
assigning 8 to cpu.870648870642870648: 2022-12-12 08:13:19178
[: : : E.] 2022-12-12 08:13:19EEE 870691v100x8, slow pcie.   /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[: 
870721/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:2022-12-12 08:13:19E: :[::196. E2132022-12-12 08:13:19212178] 870781/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc ] .] ] assigning 8 to cpu: :/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccremote time is 8.68421870811build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8v100x8, slow pcie
E214:
: 

 ] 196E[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[cpu time is 97.0588]  2022-12-12 08:13:19:[2022-12-12 08:13:19[
assigning 8 to cpu/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.2122022-12-12 08:13:19.2022-12-12 08:13:19
:870917] .870926870929.196: build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8: : 870938] E
[E: Eassigning 8 to cpu 2022-12-12 08:13:19 E[ 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc 2022-12-12 08:13:19/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:871016:/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.:214: 213:871046[196] E] 212: 2022-12-12 08:13:19] cpu time is 97.0588 remote time is 8.68421] E.assigning 8 to cpu
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8 871107
:
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: [212:E2022-12-12 08:13:19] 213[ .build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8] 2022-12-12 08:13:19/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc871194
remote time is 8.68421.:: 
871215212E: []  [E2022-12-12 08:13:19build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 08:13:19[ .
:.2022-12-12 08:13:19/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc871249214871254[.:: ] : 2022-12-12 08:13:19871257213Ecpu time is 97.0588E.: ]  
 871297Eremote time is 8.68421/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:  
::E[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc213214 2022-12-12 08:13:19:] ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.212remote time is 8.68421cpu time is 97.0588:871398] 

213: build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8] [E
remote time is 8.684212022-12-12 08:13:19 
./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc871464:: [[214E2022-12-12 08:13:192022-12-12 08:13:19]  ..cpu time is 97.0588/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc871507871504
:: : 214EE]   cpu time is 97.0588/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
::214213] ] cpu time is 97.0588remote time is 8.68421

[2022-12-12 08:13:19.871665: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:214] cpu time is 97.0588
[2022-12-12 08:14:39.204663: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:118] solver created. now build & solve
[2022-12-12 08:14:39.244871: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:123] solver built. now solve
[2022-12-12 08:14:39.244940: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:919] num_cached_nodes = 30000001
[2022-12-12 08:14:39.359063: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:127] solver solved
[2022-12-12 08:14:39.359162: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:195] 0 solved master
[2022-12-12 08:14:39.503538: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 0 solved
[2022-12-12 08:14:39.503578: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 0 initing device 0
[2022-12-12 08:14:39.504011: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 08:14:39.504947: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 08:14:39.505790: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 08:14:39.518696: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 2 solved
[2022-12-12 08:14:39.518758: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 2 initing device 2
[2022-12-12 08:14:39.519029: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 6 solved
[2022-12-12 08:14:39.519086: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 6 initing device 6
[2022-12-12 08:14:39.519139: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 08:14:39.519400: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 5 solved
[2022-12-12 08:14:39.519457: [E[2022-12-12 08:14:39 [2022-12-12 08:14:39./hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc2022-12-12 08:14:39[.2022-12-12 08:14:39519464:.519448.: 205519481: 519493E] : E:  worker 0 thread 5 initing device 5E E/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc :/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc1980:202:] 202] 202eager alloc mem 381.47 MB] 3 solved7 solved] 


4 solved
[2022-12-12 08:14:39[.[2022-12-12 08:14:395197012022-12-12 08:14:39.: .519704E519710:  : E/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.ccE : /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc205/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:] :205worker 0 thread 3 initing device 3205] 
] worker 0 thread 7 initing device 7worker 0 thread 4 initing device 4

[2022-12-12 08:14:39.519954: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 08:14:39.520199: E [[/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu2022-12-12 08:14:392022-12-12 08:14:39:..1980520212520215] : : eager alloc mem 381.47 MBEE
  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::19801980] ] eager alloc mem 381.47 MBeager alloc mem 381.47 MB

[2022-12-12 08:14:39.522757: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 08:14:39.522879: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 1 solved
[2022-12-12 08:14:39.522930: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 1 initing device 1
[2022-12-12 08:14:39.523068: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 08:14:39.523324: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 08:14:39.523876: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 08:14:39.523934: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 08:14:39.524984: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 08:14:39.525045: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 08:14:39.527307: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 08:14:39.527587: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 08:14:39.527714: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 08:14:39.528304: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 08:14:39.528359: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 08:14:39.528907: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 08:14:39.529455: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 08:14:39.532198: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 08:14:39.584742: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 2.00 Bytes
[2022-12-12 08:14:39.585117: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1024.00 Bytes
[2022-12-12 08:14:39.602544: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-12 08:14:39.602648: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 2
[2022-12-12 08:14:39.602694: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 08:14:39.603628: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 114.82 MB
[2022-12-12 08:14:39.604562: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 08:14:39.605702: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 08:14:39.605792: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 08:14:39.606470: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 08:14:39.606513: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 14.31 GB
[2022-12-12 08:14:39.613173: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980[[] 2022-12-12 08:14:39[[2022-12-12 08:14:39eager alloc mem 2.00 Bytes.2022-12-12 08:14:392022-12-12 08:14:39.
613219..613219: 613233613234: E: : E EE /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980::1980] 19801980] eager alloc mem 2.00 Bytes] ] eager alloc mem 2.00 Bytes
eager alloc mem 2.00 Byteseager alloc mem 2.00 Bytes


[2022-12-12 08:14:39.613529: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1024.00 Bytes
[2022-12-12 08:14:39.[[6136352022-12-12 08:14:39[2022-12-12 08:14:39: .2022-12-12 08:14:39.E613640.613639 : 613646: /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE: E: E 1980/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu] :/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:eager alloc mem 1024.00 Bytes1980:1980
] 1980] eager alloc mem 1024.00 Bytes] eager alloc mem 1024.00 Bytes
eager alloc mem 1024.00 Bytes

[[2022-12-12 08:14:392022-12-12 08:14:39..616365616365: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::19801980] ] eager alloc mem 2.00 Byteseager alloc mem 2.00 Bytes

[[2022-12-12 08:14:392022-12-12 08:14:39..616711616711: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::19801980] ] eager alloc mem 1024.00 Byteseager alloc mem 1024.00 Bytes

[2022-12-12 08:14:39.659856: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-12 08:14:39.659946: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc[:2022-12-12 08:14:39638.] 659937eager release cuda mem 2: 
E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-12 08:14:39.660004[: 2022-12-12 08:14:39E[. 2022-12-12 08:14:39659996/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc.: :660023E638:  ] E/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cceager release cuda mem 400000000 :
/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc638:] 638eager release cuda mem 1024] 
eager release cuda mem 2
[[2022-12-12 08:14:392022-12-12 08:14:39[..2022-12-12 08:14:39660079660100.: : 660103EE:   E/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc ::/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc638638:] ] 638eager release cuda mem 1024eager release cuda mem 2] 

eager release cuda mem 400000000
[[2022-12-12 08:14:39[2022-12-12 08:14:39.2022-12-12 08:14:39.660186.660186: 660174: E: E E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:[638:6382022-12-12 08:14:39] 638] .eager release cuda mem 2] eager release cuda mem 400000000660222
eager release cuda mem 1024
: 
E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc[:2022-12-12 08:14:39638[.[] 2022-12-12 08:14:396603152022-12-12 08:14:39eager release cuda mem 1024.: .
660311E660329:  : E/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc[E :2022-12-12 08:14:39 /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc638./hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] 660395:] eager release cuda mem 400000000: 638eager release cuda mem 1024
E] 
 eager release cuda mem 2/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc
:638] [eager release cuda mem 22022-12-12 08:14:39
.660531: E [/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc2022-12-12 08:14:39[:.2022-12-12 08:14:39638660547.] : 660558eager release cuda mem 2E: 
 E/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc :/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc[638:2022-12-12 08:14:39] 638.eager release cuda mem 400000000] 660632
eager release cuda mem 400000000: 
E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 08:14:39.660860: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 114.82 MB
[2022-12-12 08:14:39.662028: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 114.82 MB
[2022-12-12 08:14:39.662975: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 114.82 MB
[2022-12-12 08:14:39.663665: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 114.82 MB
[2022-12-12 08:14:39.664174: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 114.82 MB
[2022-12-12 08:14:39.664710: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 114.82 MB
[2022-12-12 08:14:39.665220: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 114.82 MB
[2022-12-12 08:14:39.666365: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 08:14:39.666764: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 08:14:39.667069: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 08:14:39.667123: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 08:14:39.667176: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980[] 2022-12-12 08:14:39eager alloc mem 611.00 KB.
667196: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:[19802022-12-12 08:14:39] .eager alloc mem 611.00 KB667233
: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 08:14:39.667457: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 08:14:39.667550: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 08:14:39.667854: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 08:14:39.667938: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 08:14:39.668175: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 08:14:39.668219[: 2022-12-12 08:14:39E. 668224/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc: :E638 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cceager release cuda mem 25855:
[6382022-12-12 08:14:39] .eager release cuda mem 625663668258
: [E2022-12-12 08:14:39 ./hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu668275:: 1980E]  [eager alloc mem 25.25 KB/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu2022-12-12 08:14:39
:.[19806682962022-12-12 08:14:39] [: [.eager alloc mem 14.31 GB2022-12-12 08:14:39E2022-12-12 08:14:39668308
. .: 668324/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc668332E: ::  E638E/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc ]  :/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cceager release cuda mem 625663/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:638:
1980] 638] eager release cuda mem 625663] eager alloc mem 25.25 KB
eager release cuda mem 625663

[2022-12-12 08:14:39.668529: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 08:14:39.668559[: 2022-12-12 08:14:39E. 668566/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu: :E1980 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cueager alloc mem 25.25 KB:
1980] eager alloc mem 25.25 KB[
2022-12-12 08:14:39.668608: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 08:14:39.668663: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 14.31 GB
[2022-12-12 08:14:39.668955: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 08:14:39.668995: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 14.31 GB
[2022-12-12 08:14:39.669157: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 08:14:39.669196[: 2022-12-12 08:14:39E. 669201/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu: :E1980 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cceager alloc mem 14.31 GB:
638] eager release cuda mem 25855
[2022-12-12 08:14:39.669244: E [/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc2022-12-12 08:14:39:[.6382022-12-12 08:14:39669260] .: eager release cuda mem 25855669270E
:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu :[/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc19802022-12-12 08:14:39:] .638eager alloc mem 14.31 GB669334] 
: eager release cuda mem 25855E
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 14.31 GB
[2022-12-12 08:14:39.669408: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 14.31 GB
[[[[[[[[2022-12-12 08:14:422022-12-12 08:14:422022-12-12 08:14:422022-12-12 08:14:422022-12-12 08:14:422022-12-12 08:14:422022-12-12 08:14:422022-12-12 08:14:42........933414933414933414933414933414933415933415933441: : : : : : : : EEEEEEEE        /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::::::::19801980198019801980198019801980] ] ] ] ] ] ] ] eager alloc mem 611.00 KBeager alloc mem 611.00 KBeager alloc mem 611.00 KBeager alloc mem 611.00 KBeager alloc mem 611.00 KBeager alloc mem 611.00 KBeager alloc mem 611.00 KBeager alloc mem 611.00 KB







[[2022-12-12 08:14:422022-12-12 08:14:42[.[[.2022-12-12 08:14:429345502022-12-12 08:14:42[2022-12-12 08:14:42[934550.: .2022-12-12 08:14:42.[2022-12-12 08:14:42: 934560E934556.9345572022-12-12 08:14:42.E:  : 934573: .934575 E/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccE: E934597: /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc : E : E:/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc638/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccE 638:] :/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc: /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc] 638eager release cuda mem 625663638:638/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:eager release cuda mem 625663] 
] 638] :638
eager release cuda mem 625663eager release cuda mem 625663] eager release cuda mem 625663638] 

eager release cuda mem 625663
] eager release cuda mem 625663
eager release cuda mem 625663
[
2022-12-12 08:14:42.934901[: 2022-12-12 08:14:42E. 934920/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu: :E[[1980 2022-12-12 08:14:42[2022-12-12 08:14:42] [/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu.2022-12-12 08:14:42[.eager alloc mem 611.00 KB[2022-12-12 08:14:42:934940.2022-12-12 08:14:42934939
2022-12-12 08:14:42.1980: 934949.: .934957] E: 934965E934972: eager alloc mem 611.00 KB E:  : E
/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu E/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE :/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu : /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu1980:/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu1980/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:] 1980:] :1980eager alloc mem 611.00 KB] 1980eager alloc mem 611.00 KB1980] 
eager alloc mem 611.00 KB] 
] eager alloc mem 611.00 KB
eager alloc mem 611.00 KBeager alloc mem 611.00 KB


[2022-12-12 08:14:42.935753: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 08:14:42.935825: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 08:14:42.935863: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 08:14:42.935935: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu[:2022-12-12 08:14:421980[.[] 2022-12-12 08:14:42935946[2022-12-12 08:14:42eager alloc mem 611.00 KB.[: [2022-12-12 08:14:42.
9359582022-12-12 08:14:42E2022-12-12 08:14:42.935964: . .935974: E935980/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc935981: E : :: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccE638E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc: ]  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cceager release cuda mem 625663/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] :
:638] eager release cuda mem 625663638638] eager release cuda mem 625663
] ] eager release cuda mem 625663
eager release cuda mem 625663eager release cuda mem 625663


[2022-12-12 08:14:42.936185: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980[] 2022-12-12 08:14:42eager alloc mem 611.00 KB.
936209[: 2022-12-12 08:14:42E.[ 936222[2022-12-12 08:14:42/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu[: 2022-12-12 08:14:42.:2022-12-12 08:14:42E.9362301980. 936234: ] 936238/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu: Eeager alloc mem 611.00 KB: :E 
E1980 /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cueager alloc mem 611.00 KB:1980:
1980] 1980] eager alloc mem 611.00 KB] eager alloc mem 611.00 KB
eager alloc mem 611.00 KB

[2022-12-12 08:14:42.936578: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 08:14:42.936648: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 08:14:42.936742: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 08:14:42.936810: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 08:14:42.936950: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 08:14:42.937020: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 08:14:42.937050: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 08:14:42.937098: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] [eager release cuda mem 6256632022-12-12 08:14:42[
.2022-12-12 08:14:42937121[.[: 2022-12-12 08:14:429371242022-12-12 08:14:42E.: . 937132E937132/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu[:  : :2022-12-12 08:14:42E/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccE1980. : ] 937178/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc638/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cceager alloc mem 611.00 KB: :] :
E638eager release cuda mem 625663638 ] 
] /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cueager release cuda mem 625663eager release cuda mem 625663:

1980] eager alloc mem 611.00 KB
[2022-12-12 08:14:42.937349: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB[
2022-12-12 08:14:42[.2022-12-12 08:14:42937370.: 937373E[:  2022-12-12 08:14:42E/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu. :937398/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu1980: :] E1980eager alloc mem 611.00 KB ] 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cceager alloc mem 611.00 KB:
638] eager release cuda mem 625663
[2022-12-12 08:14:42.937541: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980[] 2022-12-12 08:14:42eager alloc mem 611.00 KB.
937556: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 08:14:42.937630: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 08:14:42.937769: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 08:14:42.937839: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 08:14:42.938011: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 08:14:42.938065: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc[:2022-12-12 08:14:42638.] 938079eager release cuda mem 625663: 
E[ 2022-12-12 08:14:42/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu.:9381021980: ] Eeager alloc mem 611.00 KB 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 08:14:42.938145: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 08:14:42.938185: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:[19802022-12-12 08:14:42] .eager alloc mem 611.00 KB938201
: [E2022-12-12 08:14:42 ./hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc938215:: 638E]  eager release cuda mem 625663/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc
:638] eager release cuda mem 625663
[2022-12-12 08:14:42.938300: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638[] 2022-12-12 08:14:42[eager release cuda mem 625663.2022-12-12 08:14:42
938317.: 938327E:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu [:/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu2022-12-12 08:14:421980:[.] 19802022-12-12 08:14:42938375eager alloc mem 611.00 KB] .: 
eager alloc mem 611.00 KB938393E
:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc :/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu638:] 1980eager release cuda mem 625663] 
eager alloc mem 611.00 KB
[2022-12-12 08:14:42.938515: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 08:14:42.938587: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 08:14:42.938656: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 08:14:42.938859: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 08:14:42.938898: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 08:14:42.938927: E [/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu2022-12-12 08:14:42:.1980938939] : eager alloc mem 611.00 KBE
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc[:2022-12-12 08:14:42638.] 938967eager release cuda mem 625663: 
E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 08:14:42.939043: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[[2022-12-12 08:14:422022-12-12 08:14:42..939168939169: : EE[  2022-12-12 08:14:42/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc.::939203638638: ] ] Eeager release cuda mem 625663eager release cuda mem 625663[ 

2022-12-12 08:14:42/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc.:939261638: ] Eeager release cuda mem 625663 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[[2022-12-12 08:14:422022-12-12 08:14:42..939329939332[: : 2022-12-12 08:14:42E[E. 2022-12-12 08:14:42 939355/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu./hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu[: :939365:2022-12-12 08:14:42E1980: 1980. ] E] 939402/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cueager alloc mem 611.00 KB eager alloc mem 611.00 KB: :
/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu
E1980: ] 1980/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cceager alloc mem 611.00 KB] :
eager alloc mem 611.00 KB638
] eager release cuda mem 625663
[2022-12-12 08:14:42.939595: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 08:14:42.939689: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 08:14:42.939739: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:[6382022-12-12 08:14:42] .eager release cuda mem 625663939756
: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 08:14:42.939792: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 08:14:42.939817: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 08:14:42.939861: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 08:14:42.940215[: 2022-12-12 08:14:42E. 940220/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc: :E638 ] [/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cceager release cuda mem 625663[2022-12-12 08:14:42:
2022-12-12 08:14:42.638.940262] 940268: eager release cuda mem 625663: E
E  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::638638] ] [eager release cuda mem 625663eager release cuda mem 625663[2022-12-12 08:14:42

2022-12-12 08:14:42.[.940337[2022-12-12 08:14:42[940340: 2022-12-12 08:14:42.2022-12-12 08:14:42: E.940362.E 940376: 940379 /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu: E: /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:E E:1980 /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu 638] /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc] eager alloc mem 611.00 KB:1980:eager release cuda mem 625663
638] 638
] eager alloc mem 611.00 KB[] eager release cuda mem 120400004
2022-12-12 08:14:42[eager release cuda mem 120400004
.2022-12-12 08:14:42
940517.[: 9405382022-12-12 08:14:42E: . E940569/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc : :/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccE638: ] 638[/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cceager release cuda mem 625663] 2022-12-12 08:14:42:
eager release cuda mem 120400004.638
940609] : eager release cuda mem 625663[E
2022-12-12 08:14:42 ./hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc940641:: [638E2022-12-12 08:14:42]  .eager release cuda mem 625663/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc940663
:: 638E]  eager release cuda mem 120400004/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc[
:2022-12-12 08:14:42638.] 940707eager release cuda mem 120400004: 
E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 120400004
[2022-12-12 08:14:42.941238: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 08:14:42.[9412662022-12-12 08:14:42.: 941275E:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc :/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc638:] 638eager release cuda mem 625663] 
eager release cuda mem 120400004
[2022-12-12 08:14:42.941354: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 120400004
[2022-12-12 08:14:42.941502: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 30000001 / 100000000 nodes ( 30.00 %~30.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 69999999 / 100000000 nodes ( 70.00 %) | 14.31 GB | 3.42129 secs 
[2022-12-12 08:14:42.941809: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 30000001 / 100000000 nodes ( 30.00 %~30.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 69999999 / 100000000 nodes ( 70.00 %) | 14.31 GB | 3.42161 secs 
[2022-12-12 08:14:42.942211: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 30000001 / 100000000 nodes ( 30.00 %~30.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 69999999 / 100000000 nodes ( 70.00 %) | 14.31 GB | 3.42202 secs 
[2022-12-12 08:14:42.942543: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 30000001 / 100000000 nodes ( 30.00 %~30.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 69999999 / 100000000 nodes ( 70.00 %) | 14.31 GB | 3.42309 secs 
[2022-12-12 08:14:42.942745: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 30000001 / 100000000 nodes ( 30.00 %~30.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 69999999 / 100000000 nodes ( 70.00 %) | 14.31 GB | 3.42362 secs 
[2022-12-12 08:14:42.942963: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 30000001 / 100000000 nodes ( 30.00 %~30.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 69999999 / 100000000 nodes ( 70.00 %) | 14.31 GB | 3.42301 secs 
[2022-12-12 08:14:42.943426: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 30000001 / 100000000 nodes ( 30.00 %~30.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 69999999 / 100000000 nodes ( 70.00 %) | 14.31 GB | 3.42011 secs 
[2022-12-12 08:14:42.943655: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 30000001 / 100000000 nodes ( 30.00 %~30.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 69999999 / 100000000 nodes ( 70.00 %) | 14.31 GB | 3.43965 secs 
[2022-12-12 08:14:42.945052: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 22.63 GB
[2022-12-12 08:14:44.323268: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 22.89 GB
[2022-12-12 08:14:44.323800: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 22.89 GB
[2022-12-12 08:14:44.324074: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 22.89 GB
[2022-12-12 08:14:45.755015: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 23.15 GB
[2022-12-12 08:14:45.755190: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 23.15 GB
[2022-12-12 08:14:45.755505: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 23.15 GB
[2022-12-12 08:14:47.117842: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 23.37 GB
[2022-12-12 08:14:47.117975: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 23.37 GB
[2022-12-12 08:14:47.118296: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 23.37 GB
[2022-12-12 08:14:48.431267: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 23.58 GB
[2022-12-12 08:14:48.432149: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 23.58 GB
[2022-12-12 08:14:48.433088: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 23.58 GB
[2022-12-12 08:14:49.825834: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 24.04 GB
[2022-12-12 08:14:49.826487: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 24.04 GB
[2022-12-12 08:14:49.827313: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 24.04 GB
[2022-12-12 08:14:51.429275: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 24.24 GB
[2022-12-12 08:14:51.430374: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 24.24 GB
[HCTR][08:14:52.536][ERROR][RK0][main]: replica 4 calling init per replica done, doing barrier
[HCTR][08:14:52.536][ERROR][RK0][main]: replica 5 calling init per replica done, doing barrier
[HCTR][08:14:52.536][ERROR][RK0][main]: replica 0 calling init per replica done, doing barrier
[HCTR][08:14:52.536][ERROR][RK0][tid #139707526653696]: replica 1 calling init per replica done, doing barrier
[HCTR][08:14:52.536][ERROR][RK0][main]: replica 3 calling init per replica done, doing barrier
[HCTR][08:14:52.536][ERROR][RK0][tid #139707602155264]: replica 6 calling init per replica done, doing barrier
[HCTR][08:14:52.536][ERROR][RK0][tid #139707652478720]: replica 2 calling init per replica done, doing barrier
[HCTR][08:14:52.536][ERROR][RK0][tid #139708139026176]: replica 7 calling init per replica done, doing barrier
[HCTR][08:14:52.536][ERROR][RK0][tid #139707526653696]: replica 1 calling init per replica done, doing barrier done
[HCTR][08:14:52.536][ERROR][RK0][tid #139707602155264]: replica 6 calling init per replica done, doing barrier done
[HCTR][08:14:52.536][ERROR][RK0][main]: replica 5 calling init per replica done, doing barrier done
[HCTR][08:14:52.536][ERROR][RK0][main]: replica 3 calling init per replica done, doing barrier done
[HCTR][08:14:52.536][ERROR][RK0][tid #139707526653696]: init per replica done
[HCTR][08:14:52.536][ERROR][RK0][main]: replica 0 calling init per replica done, doing barrier done
[HCTR][08:14:52.536][ERROR][RK0][tid #139707602155264]: init per replica done
[HCTR][08:14:52.536][ERROR][RK0][tid #139707652478720]: replica 2 calling init per replica done, doing barrier done
[HCTR][08:14:52.536][ERROR][RK0][main]: replica 4 calling init per replica done, doing barrier done
[HCTR][08:14:52.536][ERROR][RK0][tid #139708139026176]: replica 7 calling init per replica done, doing barrier done
[HCTR][08:14:52.536][ERROR][RK0][main]: init per replica done
[HCTR][08:14:52.536][ERROR][RK0][main]: init per replica done
[HCTR][08:14:52.536][ERROR][RK0][main]: init per replica done
[HCTR][08:14:52.536][ERROR][RK0][tid #139707652478720]: init per replica done
[HCTR][08:14:52.536][ERROR][RK0][tid #139708139026176]: init per replica done
[HCTR][08:14:52.539][ERROR][RK0][main]: init per replica done
[HCTR][08:14:52.575][ERROR][RK0][tid #139707526653696]: 1 allocated 3276800 at 0x7ef144238400
[HCTR][08:14:52.575][ERROR][RK0][tid #139707526653696]: 1 allocated 6553600 at 0x7ef144558400
[HCTR][08:14:52.575][ERROR][RK0][tid #139707526653696]: 1 allocated 3276800 at 0x7ef144b98400
[HCTR][08:14:52.575][ERROR][RK0][tid #139707526653696]: 1 allocated 6553600 at 0x7ef144eb8400
[HCTR][08:14:52.575][ERROR][RK0][tid #139708139026176]: 7 allocated 3276800 at 0x7ef05c238400
[HCTR][08:14:52.575][ERROR][RK0][tid #139708139026176]: 7 allocated 6553600 at 0x7ef05c558400
[HCTR][08:14:52.575][ERROR][RK0][tid #139708139026176]: 7 allocated 3276800 at 0x7ef05cb98400
[HCTR][08:14:52.575][ERROR][RK0][tid #139708139026176]: 7 allocated 6553600 at 0x7ef05ceb8400
[HCTR][08:14:52.575][ERROR][RK0][main]: 5 allocated 3276800 at 0x7ef146238400
[HCTR][08:14:52.575][ERROR][RK0][tid #139707652478720]: 2 allocated 3276800 at 0x7ef144238400
[HCTR][08:14:52.575][ERROR][RK0][tid #139707602155264]: 6 allocated 3276800 at 0x7ef146238400
[HCTR][08:14:52.575][ERROR][RK0][main]: 5 allocated 6553600 at 0x7ef146558400
[HCTR][08:14:52.575][ERROR][RK0][tid #139707652478720]: 2 allocated 6553600 at 0x7ef144558400
[HCTR][08:14:52.575][ERROR][RK0][tid #139707602155264]: 6 allocated 6553600 at 0x7ef146558400
[HCTR][08:14:52.575][ERROR][RK0][main]: 5 allocated 3276800 at 0x7ef146b98400
[HCTR][08:14:52.575][ERROR][RK0][tid #139707652478720]: 2 allocated 3276800 at 0x7ef144b98400
[HCTR][08:14:52.575][ERROR][RK0][tid #139707602155264]: 6 allocated 3276800 at 0x7ef146b98400
[HCTR][08:14:52.575][ERROR][RK0][main]: 5 allocated 6553600 at 0x7ef146eb8400
[HCTR][08:14:52.575][ERROR][RK0][tid #139707602155264]: 6 allocated 6553600 at 0x7ef146eb8400
[HCTR][08:14:52.575][ERROR][RK0][tid #139707652478720]: 2 allocated 6553600 at 0x7ef144eb8400
[HCTR][08:14:52.575][ERROR][RK0][tid #139707837052672]: 4 allocated 3276800 at 0x7ef144238400
[HCTR][08:14:52.575][ERROR][RK0][tid #139707837052672]: 4 allocated 6553600 at 0x7ef144558400
[HCTR][08:14:52.575][ERROR][RK0][main]: 3 allocated 3276800 at 0x7ef13c238400
[HCTR][08:14:52.575][ERROR][RK0][tid #139707837052672]: 4 allocated 3276800 at 0x7ef144b98400
[HCTR][08:14:52.575][ERROR][RK0][main]: 3 allocated 6553600 at 0x7ef13c558400
[HCTR][08:14:52.575][ERROR][RK0][tid #139707837052672]: 4 allocated 6553600 at 0x7ef144eb8400
[HCTR][08:14:52.575][ERROR][RK0][main]: 3 allocated 3276800 at 0x7ef13cb98400
[HCTR][08:14:52.575][ERROR][RK0][main]: 3 allocated 6553600 at 0x7ef13ceb8400
[HCTR][08:14:52.578][ERROR][RK0][main]: 0 allocated 3276800 at 0x7ef000320000
[HCTR][08:14:52.578][ERROR][RK0][main]: 0 allocated 6553600 at 0x7ef000640000
[HCTR][08:14:52.578][ERROR][RK0][main]: 0 allocated 3276800 at 0x7ef000c80000
[HCTR][08:14:52.578][ERROR][RK0][main]: 0 allocated 6553600 at 0x7ef000fa0000
