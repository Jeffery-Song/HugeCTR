2022-12-12 00:15:12.813568: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:12.824982: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:12.831980: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:12.839390: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:12.850848: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:12.865341: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:12.872007: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:12.875611: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:12.925778: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:12.927376: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:12.928952: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:12.930374: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:12.931916: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:12.933441: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:12.935056: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:12.936615: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:12.936783: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:12.938834: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:12.940562: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:12.942127: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:12.943618: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:12.945221: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:12.946764: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:12.948402: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:12.949990: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:12.951906: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:12.954484: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:12.956941: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:12.958469: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:12.959439: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:12.961081: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:12.962891: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:12.965169: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:12.966691: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:12.968633: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:12.969963: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:12.970179: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:12.971296: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:12.971869: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:12.972934: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:12.973517: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:12.974625: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:12.975143: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:12.976363: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:12.976760: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:12.978560: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:12.980519: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:12.981353: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:12.982426: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 00:15:12.982591: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:12.982931: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:12.983609: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:12.983723: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:12.985444: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:12.986516: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:12.986708: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:12.987958: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:12.989576: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:12.989832: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:12.990210: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:12.990373: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:12.991880: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:12.992056: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:12.992830: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:12.993332: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:12.993605: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:12.995588: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:12.995888: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:12.996737: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:12.996987: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:12.997784: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:12.997850: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:12.999306: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:12.999862: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:13.000630: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:13.000812: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:13.001872: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:13.002065: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:13.002941: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:13.003910: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:13.004421: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:13.004836: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:13.006109: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:13.006243: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:13.006975: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:13.008137: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:13.008703: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:13.009827: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:13.009943: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:13.010764: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:13.011606: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:13.014187: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:13.014303: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:13.014897: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:13.015226: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:13.018288: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:13.018502: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:13.019096: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:13.019116: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:13.022400: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:13.023417: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:13.023570: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:13.026375: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:13.026567: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:13.026930: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:13.029172: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:13.029340: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:13.029726: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:13.031694: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:13.031906: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:13.033752: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:13.034443: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:13.041936: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:13.042488: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:13.043772: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:13.044331: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:13.046078: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:13.052220: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:13.056656: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:13.066962: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:13.072900: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:13.075863: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:13.081174: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:13.084603: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:13.084664: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:13.085091: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:13.085297: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:13.085346: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:13.085392: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:13.086156: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:13.090278: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:13.090374: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:13.090425: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:13.090692: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:13.090713: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:13.090752: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:13.091485: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:13.096669: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:13.096926: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:13.097002: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:13.097087: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:13.097338: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:13.097398: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:13.098214: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:13.101586: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:13.101710: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:13.101964: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:13.102061: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:13.102297: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:13.102396: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:13.103198: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:13.106607: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:13.106885: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:13.107006: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:13.107200: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:13.107385: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:13.107654: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:13.112021: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:13.112219: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:13.112310: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:13.112513: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:13.112590: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:13.112819: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:13.112898: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 00:15:13.116529: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:13.116717: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:13.116901: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:13.117148: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:13.117201: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:13.117363: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:13.120950: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:13.121182: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:13.121311: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:13.121537: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:13.121595: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:13.121763: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:13.121941: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:13.126166: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:13.126312: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:13.126557: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:13.126691: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:13.126899: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:13.127042: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:13.127573: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:13.131466: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:13.131853: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:13.132085: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:13.132221: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:13.132472: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:13.132670: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:13.132991: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:13.137017: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:13.137274: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:13.137318: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:13.137479: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:13.137571: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:13.137715: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:13.142438: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:13.142635: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:13.142678: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:13.142938: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:13.142974: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:13.143172: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:13.148120: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:13.148307: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:13.148352: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:13.148626: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:13.148693: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:13.148879: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:13.153332: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:13.153415: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:13.153595: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:13.153773: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:13.154129: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:13.154187: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:13.158707: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:13.158981: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:13.159025: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:13.159359: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:13.159585: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:13.159691: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:13.164966: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:13.165118: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:13.165279: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:13.165775: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:13.166103: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:13.166449: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:13.169480: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:13.169854: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:13.169970: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:13.170643: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:13.171325: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:13.171691: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:13.174594: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:13.175224: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:13.175322: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:13.175809: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:13.176762: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:13.176980: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:13.179849: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:13.180462: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:13.180801: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:13.181730: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:13.181911: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:13.184323: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:13.184337: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 00:15:13.184925: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:13.185283: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:13.186170: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:13.186398: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:13.191697: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:13.192948: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:13.193022: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:13.193487: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:13.194519: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:13.194841: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:13.198048: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:13.198178: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:13.198212: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:13.198688: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:13.199605: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:13.199866: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:13.204717: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:13.204974: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:13.205097: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:13.205500: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:13.234738: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:13.234756: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:13.240768: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 00:15:13.241676: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 00:15:13.242023: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 00:15:13.242909: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 00:15:13.243521: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 00:15:13.250131: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:13.251206: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:13.251452: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:13.252349: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:13.253316: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:13.255220: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:13.256682: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:13.257094: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:13.257983: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:13.259454: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:13.261411: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:13.262327: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:13.262988: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:13.263774: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:13.265223: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:14.371060: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:14.372134: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:14.373290: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:14.374805: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 00:15:14.374871: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-12 00:15:14.393528: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:14.395067: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:14.396155: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:14.397610: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:14.399200: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:14.400335: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-12 00:15:14.445908: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 00:15:14.446109: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 00:15:14.480473: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12340
2022-12-12 00:15:14.520889: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:14.521978: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:14.524024: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:14.524985: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 00:15:14.525043: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-12 00:15:14.542022: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:14.543050: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:14.543941: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:14.547259: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:14.549750: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:14.550313: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:1/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-12 00:15:14.601161: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:14.602323: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:14.603794: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:14.604780: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 00:15:14.604834: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-12 00:15:14.621787: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:14.623137: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:14.624365: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:14.625826: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:14.626878: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:14.628000: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:2/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-12 00:15:14.637502: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 00:15:14.637686: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 00:15:14.639515: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12341
2022-12-12 00:15:14.656557: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:14.657170: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:14.657872: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:14.658120: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:14.658752: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:14.659048: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:14.659625: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 00:15:14.659680: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-12 00:15:14.659823: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 00:15:14.659870: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-12 00:15:14.664347: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:14.664347: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:14.665492: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:14.665524: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:14.666453: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:14.666552: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:14.667318: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 00:15:14.667372: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-12 00:15:14.667434: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 00:15:14.667482: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-12 00:15:14.676578: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:14.677265: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:14.677789: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:14.678369: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:14.678368: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:14.679572: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:14.679590: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:14.681002: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:14.681048: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:7/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-12 00:15:14.681571: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:14.682090: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:14.682553: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:5/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-12 00:15:14.685330: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:14.685944: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:14.686436: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:14.686447: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:14.687547: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:14.687582: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:14.688140: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:14.688701: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:14.688849: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:14.689719: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:14.690248: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:6/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-12 00:15:14.690520: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:14.690929: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:14.691491: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:14.691985: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 00:15:14.692050: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-12 00:15:14.692334: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:4/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-12 00:15:14.706725: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 00:15:14.706924: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 00:15:14.708810: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12342
2022-12-12 00:15:14.709223: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:14.709876: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:14.710380: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:14.710963: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:14.711516: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:15:14.712005: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:3/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-12 00:15:14.726532: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 00:15:14.726707: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 00:15:14.727591: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 00:15:14.727789: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 00:15:14.728522: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12347
2022-12-12 00:15:14.729508: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12345
2022-12-12 00:15:14.735718: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 00:15:14.735892: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 00:15:14.737206: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 00:15:14.737343: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 00:15:14.737817: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12346
2022-12-12 00:15:14.739220: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12344
2022-12-12 00:15:14.758166: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 00:15:14.758362: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 00:15:14.760239: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12343
[HCTR][00:15:16.029][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][00:15:16.029][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][00:15:16.029][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][00:15:16.029][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][00:15:16.029][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][00:15:16.029][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][00:15:16.030][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][00:15:16.030][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 1it [00:01,  1.53s/it]warmup run: 1it [00:01,  1.54s/it]warmup run: 1it [00:01,  1.52s/it]warmup run: 1it [00:01,  1.55s/it]warmup run: 1it [00:01,  1.51s/it]warmup run: 1it [00:01,  1.49s/it]warmup run: 1it [00:01,  1.50s/it]warmup run: 100it [00:01, 84.38it/s]warmup run: 101it [00:01, 85.87it/s]warmup run: 98it [00:01, 83.67it/s]warmup run: 97it [00:01, 81.79it/s]warmup run: 96it [00:01, 82.67it/s]warmup run: 100it [00:01, 86.91it/s]warmup run: 100it [00:01, 86.68it/s]warmup run: 197it [00:01, 179.90it/s]warmup run: 201it [00:01, 184.97it/s]warmup run: 197it [00:01, 182.40it/s]warmup run: 1it [00:01,  1.48s/it]warmup run: 195it [00:01, 178.43it/s]warmup run: 192it [00:01, 179.03it/s]warmup run: 199it [00:01, 186.38it/s]warmup run: 201it [00:01, 188.95it/s]warmup run: 295it [00:01, 286.60it/s]warmup run: 297it [00:01, 288.96it/s]warmup run: 297it [00:01, 292.16it/s]warmup run: 94it [00:01, 82.62it/s]warmup run: 292it [00:01, 283.96it/s]warmup run: 288it [00:01, 284.75it/s]warmup run: 297it [00:01, 294.36it/s]warmup run: 298it [00:01, 295.37it/s]warmup run: 395it [00:01, 400.37it/s]warmup run: 392it [00:01, 394.97it/s]warmup run: 395it [00:01, 402.52it/s]warmup run: 193it [00:01, 183.97it/s]warmup run: 390it [00:01, 395.01it/s]warmup run: 384it [00:01, 394.11it/s]warmup run: 395it [00:01, 405.89it/s]warmup run: 395it [00:01, 404.97it/s]warmup run: 494it [00:02, 509.54it/s]warmup run: 490it [00:02, 503.99it/s]warmup run: 492it [00:02, 507.73it/s]warmup run: 293it [00:01, 296.02it/s]warmup run: 485it [00:02, 498.13it/s]warmup run: 480it [00:02, 499.93it/s]warmup run: 494it [00:02, 516.09it/s]warmup run: 492it [00:01, 510.68it/s]warmup run: 594it [00:02, 611.99it/s]warmup run: 591it [00:02, 609.53it/s]warmup run: 593it [00:02, 612.34it/s]warmup run: 393it [00:01, 411.20it/s]warmup run: 582it [00:02, 596.51it/s]warmup run: 578it [00:02, 600.91it/s]warmup run: 596it [00:02, 621.94it/s]warmup run: 591it [00:02, 611.54it/s]warmup run: 695it [00:02, 703.03it/s]warmup run: 692it [00:02, 700.94it/s]warmup run: 695it [00:02, 705.09it/s]warmup run: 492it [00:01, 520.30it/s]warmup run: 681it [00:02, 685.87it/s]warmup run: 676it [00:02, 687.59it/s]warmup run: 696it [00:02, 709.50it/s]warmup run: 690it [00:02, 698.03it/s]warmup run: 796it [00:02, 778.83it/s]warmup run: 794it [00:02, 779.56it/s]warmup run: 795it [00:02, 776.71it/s]warmup run: 593it [00:02, 624.54it/s]warmup run: 780it [00:02, 760.31it/s]warmup run: 772it [00:02, 754.28it/s]warmup run: 796it [00:02, 780.84it/s]warmup run: 789it [00:02, 769.89it/s]warmup run: 898it [00:02, 840.16it/s]warmup run: 897it [00:02, 842.96it/s]warmup run: 895it [00:02, 834.41it/s]warmup run: 694it [00:02, 713.47it/s]warmup run: 879it [00:02, 818.42it/s]warmup run: 867it [00:02, 805.27it/s]warmup run: 898it [00:02, 841.37it/s]warmup run: 887it [00:02, 824.84it/s]warmup run: 999it [00:02, 884.46it/s]warmup run: 999it [00:02, 889.98it/s]warmup run: 796it [00:02, 788.68it/s]warmup run: 994it [00:02, 853.80it/s]warmup run: 977it [00:02, 862.06it/s]warmup run: 963it [00:02, 845.00it/s]warmup run: 999it [00:02, 886.72it/s]warmup run: 985it [00:02, 865.89it/s]warmup run: 1101it [00:02, 920.05it/s]warmup run: 1099it [00:02, 920.26it/s]warmup run: 898it [00:02, 847.84it/s]warmup run: 1090it [00:02, 853.25it/s]warmup run: 1076it [00:02, 897.02it/s]warmup run: 1059it [00:02, 875.81it/s]warmup run: 1102it [00:02, 925.31it/s]warmup run: 1084it [00:02, 898.41it/s]warmup run: 1201it [00:02, 947.27it/s]warmup run: 1201it [00:02, 915.41it/s]warmup run: 998it [00:02, 875.89it/s]warmup run: 1193it [00:02, 900.66it/s]warmup run: 1174it [00:02, 919.91it/s]warmup run: 1156it [00:02, 902.52it/s]warmup run: 1204it [00:02, 950.14it/s]warmup run: 1183it [00:02, 923.70it/s]warmup run: 1305it [00:02, 971.58it/s]warmup run: 1299it [00:02, 897.56it/s]warmup run: 1096it [00:02, 893.28it/s]warmup run: 1297it [00:02, 938.69it/s]warmup run: 1273it [00:02, 939.14it/s]warmup run: 1255it [00:02, 926.54it/s]warmup run: 1307it [00:02, 970.51it/s]warmup run: 1283it [00:02, 945.29it/s]warmup run: 1407it [00:02, 983.20it/s]warmup run: 1402it [00:02, 933.78it/s]warmup run: 1193it [00:02, 907.38it/s]warmup run: 1400it [00:02, 964.59it/s]warmup run: 1372it [00:02, 952.03it/s]warmup run: 1354it [00:02, 943.11it/s]warmup run: 1408it [00:02, 980.91it/s]warmup run: 1383it [00:02, 959.73it/s]warmup run: 1511it [00:03, 998.34it/s]warmup run: 1504it [00:03, 957.60it/s]warmup run: 1289it [00:02, 921.50it/s]warmup run: 1504it [00:03, 983.68it/s]warmup run: 1470it [00:03, 958.89it/s]warmup run: 1452it [00:03, 943.75it/s]warmup run: 1482it [00:03, 967.79it/s]warmup run: 1509it [00:03, 973.89it/s]warmup run: 1613it [00:03, 1002.43it/s]warmup run: 1606it [00:03, 972.48it/s]warmup run: 1386it [00:02, 933.06it/s]warmup run: 1606it [00:03, 992.36it/s]warmup run: 1568it [00:03, 964.14it/s]warmup run: 1553it [00:03, 961.68it/s]warmup run: 1582it [00:03, 975.80it/s]warmup run: 1609it [00:03, 978.06it/s]warmup run: 1715it [00:03, 1002.89it/s]warmup run: 1706it [00:03, 979.42it/s]warmup run: 1482it [00:02, 933.27it/s]warmup run: 1707it [00:03, 996.94it/s]warmup run: 1667it [00:03, 969.59it/s]warmup run: 1655it [00:03, 977.01it/s]warmup run: 1683it [00:03, 983.25it/s]warmup run: 1711it [00:03, 989.97it/s]warmup run: 1817it [00:03, 1001.26it/s]warmup run: 1808it [00:03, 990.99it/s]warmup run: 1581it [00:03, 949.43it/s]warmup run: 1808it [00:03, 1000.43it/s]warmup run: 1766it [00:03, 973.83it/s]warmup run: 1757it [00:03, 986.84it/s]warmup run: 1787it [00:03, 998.71it/s]warmup run: 1811it [00:03, 991.48it/s]warmup run: 1918it [00:03, 999.82it/s] warmup run: 1912it [00:03, 1003.19it/s]warmup run: 1683it [00:03, 969.71it/s]warmup run: 1909it [00:03, 994.93it/s] warmup run: 1865it [00:03, 978.45it/s]warmup run: 1859it [00:03, 996.07it/s]warmup run: 1891it [00:03, 1009.85it/s]warmup run: 1911it [00:03, 988.40it/s]warmup run: 2021it [00:03, 1007.18it/s]warmup run: 2018it [00:03, 1018.11it/s]warmup run: 1785it [00:03, 982.90it/s]warmup run: 2014it [00:03, 1010.35it/s]warmup run: 1965it [00:03, 983.25it/s]warmup run: 1994it [00:03, 1015.16it/s]warmup run: 1960it [00:03, 996.11it/s]warmup run: 2011it [00:03, 991.28it/s]warmup run: 2136it [00:03, 1049.13it/s]warmup run: 2140it [00:03, 1076.41it/s]warmup run: 1886it [00:03, 989.64it/s]warmup run: 2133it [00:03, 1063.05it/s]warmup run: 2078it [00:03, 1025.17it/s]warmup run: 2066it [00:03, 1014.76it/s]warmup run: 2114it [00:03, 1069.43it/s]warmup run: 2131it [00:03, 1052.01it/s]warmup run: 2250it [00:03, 1074.03it/s]warmup run: 2261it [00:03, 1114.49it/s]warmup run: 1988it [00:03, 997.38it/s]warmup run: 2252it [00:03, 1099.99it/s]warmup run: 2198it [00:03, 1074.98it/s]warmup run: 2186it [00:03, 1068.66it/s]warmup run: 2236it [00:03, 1111.63it/s]warmup run: 2251it [00:03, 1094.63it/s]warmup run: 2368it [00:03, 1105.31it/s]warmup run: 2383it [00:03, 1144.80it/s]warmup run: 2106it [00:03, 1050.05it/s]warmup run: 2372it [00:03, 1128.32it/s]warmup run: 2318it [00:03, 1110.94it/s]warmup run: 2304it [00:03, 1101.24it/s]warmup run: 2357it [00:03, 1140.87it/s]warmup run: 2371it [00:03, 1124.97it/s]warmup run: 2483it [00:03, 1116.22it/s]warmup run: 2505it [00:03, 1166.84it/s]warmup run: 2226it [00:03, 1094.03it/s]warmup run: 2491it [00:03, 1146.11it/s]warmup run: 2438it [00:03, 1135.46it/s]warmup run: 2416it [00:03, 1105.11it/s]warmup run: 2478it [00:03, 1160.70it/s]warmup run: 2491it [00:03, 1145.60it/s]warmup run: 2600it [00:04, 1132.19it/s]warmup run: 2628it [00:04, 1183.25it/s]warmup run: 2346it [00:03, 1125.35it/s]warmup run: 2611it [00:04, 1160.71it/s]warmup run: 2559it [00:04, 1154.96it/s]warmup run: 2528it [00:04, 1107.74it/s]warmup run: 2600it [00:04, 1175.57it/s]warmup run: 2610it [00:04, 1158.17it/s]warmup run: 2717it [00:04, 1142.98it/s]warmup run: 2750it [00:04, 1191.61it/s]warmup run: 2466it [00:03, 1147.46it/s]warmup run: 2730it [00:04, 1167.47it/s]warmup run: 2679it [00:04, 1166.40it/s]warmup run: 2639it [00:04, 1108.07it/s]warmup run: 2720it [00:04, 1182.60it/s]warmup run: 2729it [00:04, 1165.70it/s]warmup run: 2833it [00:04, 1146.34it/s]warmup run: 2872it [00:04, 1199.86it/s]warmup run: 2587it [00:04, 1163.90it/s]warmup run: 2850it [00:04, 1175.49it/s]warmup run: 2799it [00:04, 1175.83it/s]warmup run: 2757it [00:04, 1128.03it/s]warmup run: 2842it [00:04, 1191.68it/s]warmup run: 2850it [00:04, 1176.19it/s]warmup run: 2950it [00:04, 1153.23it/s]warmup run: 2994it [00:04, 1205.38it/s]warmup run: 2707it [00:04, 1171.88it/s]warmup run: 2969it [00:04, 1179.33it/s]warmup run: 3000it [00:04, 682.91it/s] warmup run: 3000it [00:04, 682.15it/s] warmup run: 3000it [00:04, 682.66it/s] warmup run: 2918it [00:04, 1179.72it/s]warmup run: 2873it [00:04, 1136.35it/s]warmup run: 2962it [00:04, 1193.95it/s]warmup run: 2971it [00:04, 1183.49it/s]warmup run: 3000it [00:04, 689.54it/s] warmup run: 3000it [00:04, 690.16it/s] warmup run: 2826it [00:04, 1174.88it/s]warmup run: 3000it [00:04, 676.56it/s] warmup run: 2991it [00:04, 1147.03it/s]warmup run: 3000it [00:04, 675.14it/s] warmup run: 2944it [00:04, 1173.11it/s]warmup run: 3000it [00:04, 688.33it/s] 
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]



warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]


warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   5%|▌         | 163/3000 [00:00<00:01, 1629.40it/s]warmup should be done:   5%|▌         | 163/3000 [00:00<00:01, 1626.67it/s]warmup should be done:   6%|▌         | 166/3000 [00:00<00:01, 1656.57it/s]warmup should be done:   5%|▌         | 164/3000 [00:00<00:01, 1633.26it/s]warmup should be done:   6%|▌         | 165/3000 [00:00<00:01, 1648.90it/s]warmup should be done:   6%|▌         | 165/3000 [00:00<00:01, 1648.25it/s]warmup should be done:   5%|▌         | 164/3000 [00:00<00:01, 1630.89it/s]warmup should be done:   5%|▌         | 155/3000 [00:00<00:01, 1541.01it/s]warmup should be done:  11%|█         | 333/3000 [00:00<00:01, 1663.51it/s]warmup should be done:  11%|█         | 328/3000 [00:00<00:01, 1637.82it/s]warmup should be done:  11%|█         | 331/3000 [00:00<00:01, 1653.87it/s]warmup should be done:  11%|█         | 329/3000 [00:00<00:01, 1640.67it/s]warmup should be done:  11%|█         | 327/3000 [00:00<00:01, 1631.14it/s]warmup should be done:  11%|█         | 333/3000 [00:00<00:01, 1663.81it/s]warmup should be done:  11%|█         | 331/3000 [00:00<00:01, 1649.12it/s]warmup should be done:  10%|█         | 310/3000 [00:00<00:01, 1503.75it/s]warmup should be done:  17%|█▋        | 500/3000 [00:00<00:01, 1660.02it/s]warmup should be done:  16%|█▋        | 492/3000 [00:00<00:01, 1631.64it/s]warmup should be done:  17%|█▋        | 500/3000 [00:00<00:01, 1659.77it/s]warmup should be done:  17%|█▋        | 496/3000 [00:00<00:01, 1646.06it/s]warmup should be done:  16%|█▋        | 494/3000 [00:00<00:01, 1633.61it/s]warmup should be done:  16%|█▋        | 491/3000 [00:00<00:01, 1623.34it/s]warmup should be done:  17%|█▋        | 497/3000 [00:00<00:01, 1635.01it/s]warmup should be done:  16%|█▌        | 468/3000 [00:00<00:01, 1537.33it/s]warmup should be done:  22%|██▏       | 661/3000 [00:00<00:01, 1646.30it/s]warmup should be done:  22%|██▏       | 667/3000 [00:00<00:01, 1658.29it/s]warmup should be done:  22%|██▏       | 656/3000 [00:00<00:01, 1630.24it/s]warmup should be done:  22%|██▏       | 666/3000 [00:00<00:01, 1653.39it/s]warmup should be done:  22%|██▏       | 658/3000 [00:00<00:01, 1632.28it/s]warmup should be done:  22%|██▏       | 654/3000 [00:00<00:01, 1619.19it/s]warmup should be done:  22%|██▏       | 661/3000 [00:00<00:01, 1624.80it/s]warmup should be done:  21%|██        | 628/3000 [00:00<00:01, 1561.12it/s]warmup should be done:  28%|██▊       | 826/3000 [00:00<00:01, 1647.10it/s]warmup should be done:  28%|██▊       | 833/3000 [00:00<00:01, 1655.93it/s]warmup should be done:  27%|██▋       | 820/3000 [00:00<00:01, 1629.23it/s]warmup should be done:  28%|██▊       | 832/3000 [00:00<00:01, 1650.11it/s]warmup should be done:  27%|██▋       | 816/3000 [00:00<00:01, 1616.67it/s]warmup should be done:  27%|██▋       | 822/3000 [00:00<00:01, 1627.05it/s]warmup should be done:  27%|██▋       | 824/3000 [00:00<00:01, 1619.70it/s]warmup should be done:  26%|██▌       | 785/3000 [00:00<00:01, 1553.98it/s]warmup should be done:  33%|███▎      | 991/3000 [00:00<00:01, 1641.74it/s]warmup should be done:  33%|███▎      | 999/3000 [00:00<00:01, 1651.23it/s]warmup should be done:  33%|███▎      | 983/3000 [00:00<00:01, 1624.73it/s]warmup should be done:  33%|███▎      | 978/3000 [00:00<00:01, 1613.06it/s]warmup should be done:  33%|███▎      | 998/3000 [00:00<00:01, 1644.80it/s]warmup should be done:  33%|███▎      | 985/3000 [00:00<00:01, 1621.22it/s]warmup should be done:  33%|███▎      | 986/3000 [00:00<00:01, 1614.68it/s]warmup should be done:  31%|███▏      | 944/3000 [00:00<00:01, 1563.99it/s]warmup should be done:  38%|███▊      | 1146/3000 [00:00<00:01, 1624.91it/s]warmup should be done:  39%|███▉      | 1165/3000 [00:00<00:01, 1651.41it/s]warmup should be done:  39%|███▊      | 1156/3000 [00:00<00:01, 1635.31it/s]warmup should be done:  39%|███▉      | 1163/3000 [00:00<00:01, 1644.61it/s]warmup should be done:  38%|███▊      | 1148/3000 [00:00<00:01, 1622.69it/s]warmup should be done:  38%|███▊      | 1140/3000 [00:00<00:01, 1607.91it/s]warmup should be done:  38%|███▊      | 1148/3000 [00:00<00:01, 1614.98it/s]warmup should be done:  37%|███▋      | 1101/3000 [00:00<00:01, 1554.03it/s]warmup should be done:  44%|████▍     | 1331/3000 [00:00<00:01, 1652.45it/s]warmup should be done:  44%|████▎     | 1309/3000 [00:00<00:01, 1623.27it/s]warmup should be done:  44%|████▎     | 1311/3000 [00:00<00:01, 1623.55it/s]warmup should be done:  44%|████▍     | 1320/3000 [00:00<00:01, 1630.21it/s]warmup should be done:  43%|████▎     | 1301/3000 [00:00<00:01, 1606.73it/s]warmup should be done:  44%|████▎     | 1310/3000 [00:00<00:01, 1614.47it/s]warmup should be done:  42%|████▏     | 1259/3000 [00:00<00:01, 1560.89it/s]warmup should be done:  44%|████▍     | 1328/3000 [00:00<00:01, 1610.47it/s]warmup should be done:  49%|████▉     | 1472/3000 [00:00<00:00, 1624.00it/s]warmup should be done:  50%|████▉     | 1497/3000 [00:00<00:00, 1651.75it/s]warmup should be done:  49%|████▉     | 1474/3000 [00:00<00:00, 1623.92it/s]warmup should be done:  49%|████▊     | 1462/3000 [00:00<00:00, 1607.17it/s]warmup should be done:  49%|████▉     | 1484/3000 [00:00<00:00, 1625.74it/s]warmup should be done:  49%|████▉     | 1472/3000 [00:00<00:00, 1611.82it/s]warmup should be done:  47%|████▋     | 1416/3000 [00:00<00:01, 1562.82it/s]warmup should be done:  50%|████▉     | 1492/3000 [00:00<00:00, 1617.97it/s]warmup should be done:  55%|█████▍    | 1635/3000 [00:01<00:00, 1624.37it/s]warmup should be done:  55%|█████▌    | 1663/3000 [00:01<00:00, 1651.41it/s]warmup should be done:  55%|█████▍    | 1637/3000 [00:01<00:00, 1623.39it/s]warmup should be done:  54%|█████▍    | 1623/3000 [00:01<00:00, 1606.48it/s]warmup should be done:  55%|█████▍    | 1647/3000 [00:01<00:00, 1622.45it/s]warmup should be done:  54%|█████▍    | 1634/3000 [00:01<00:00, 1612.72it/s]warmup should be done:  52%|█████▏    | 1574/3000 [00:01<00:00, 1565.34it/s]warmup should be done:  55%|█████▌    | 1656/3000 [00:01<00:00, 1623.51it/s]warmup should be done:  60%|█████▉    | 1798/3000 [00:01<00:00, 1624.84it/s]warmup should be done:  61%|██████    | 1829/3000 [00:01<00:00, 1649.05it/s]warmup should be done:  60%|██████    | 1800/3000 [00:01<00:00, 1623.02it/s]warmup should be done:  60%|█████▉    | 1785/3000 [00:01<00:00, 1607.86it/s]warmup should be done:  60%|█████▉    | 1796/3000 [00:01<00:00, 1609.98it/s]warmup should be done:  61%|██████    | 1820/3000 [00:01<00:00, 1627.11it/s]warmup should be done:  58%|█████▊    | 1731/3000 [00:01<00:00, 1549.04it/s]warmup should be done:  60%|██████    | 1810/3000 [00:01<00:00, 1583.96it/s]warmup should be done:  65%|██████▌   | 1961/3000 [00:01<00:00, 1625.60it/s]warmup should be done:  65%|██████▌   | 1963/3000 [00:01<00:00, 1625.05it/s]warmup should be done:  65%|██████▍   | 1948/3000 [00:01<00:00, 1612.94it/s]warmup should be done:  66%|██████▋   | 1994/3000 [00:01<00:00, 1642.37it/s]warmup should be done:  65%|██████▌   | 1958/3000 [00:01<00:00, 1610.16it/s]warmup should be done:  66%|██████▌   | 1985/3000 [00:01<00:00, 1633.35it/s]warmup should be done:  63%|██████▎   | 1893/3000 [00:01<00:00, 1568.41it/s]warmup should be done:  66%|██████▌   | 1974/3000 [00:01<00:00, 1598.49it/s]warmup should be done:  71%|███████   | 2124/3000 [00:01<00:00, 1625.85it/s]warmup should be done:  71%|███████   | 2126/3000 [00:01<00:00, 1625.61it/s]warmup should be done:  72%|███████▏  | 2159/3000 [00:01<00:00, 1637.59it/s]warmup should be done:  70%|███████   | 2110/3000 [00:01<00:00, 1605.08it/s]warmup should be done:  71%|███████   | 2121/3000 [00:01<00:00, 1615.32it/s]warmup should be done:  72%|███████▏  | 2151/3000 [00:01<00:00, 1638.75it/s]warmup should be done:  69%|██████▊   | 2058/3000 [00:01<00:00, 1590.44it/s]warmup should be done:  71%|███████▏  | 2138/3000 [00:01<00:00, 1609.82it/s]warmup should be done:  76%|███████▌  | 2287/3000 [00:01<00:00, 1622.85it/s]warmup should be done:  76%|███████▋  | 2289/3000 [00:01<00:00, 1622.00it/s]warmup should be done:  76%|███████▌  | 2275/3000 [00:01<00:00, 1616.09it/s]warmup should be done:  77%|███████▋  | 2323/3000 [00:01<00:00, 1631.12it/s]warmup should be done:  76%|███████▌  | 2285/3000 [00:01<00:00, 1620.44it/s]warmup should be done:  77%|███████▋  | 2315/3000 [00:01<00:00, 1638.47it/s]warmup should be done:  74%|███████▍  | 2221/3000 [00:01<00:00, 1601.75it/s]warmup should be done:  77%|███████▋  | 2302/3000 [00:01<00:00, 1616.64it/s]warmup should be done:  82%|████████▏ | 2450/3000 [00:01<00:00, 1622.84it/s]warmup should be done:  82%|████████▏ | 2452/3000 [00:01<00:00, 1618.41it/s]warmup should be done:  81%|████████  | 2437/3000 [00:01<00:00, 1613.96it/s]warmup should be done:  82%|████████▏ | 2450/3000 [00:01<00:00, 1628.10it/s]warmup should be done:  83%|████████▎ | 2487/3000 [00:01<00:00, 1624.70it/s]warmup should be done:  83%|████████▎ | 2480/3000 [00:01<00:00, 1640.36it/s]warmup should be done:  79%|███████▉  | 2384/3000 [00:01<00:00, 1607.40it/s]warmup should be done:  82%|████████▏ | 2466/3000 [00:01<00:00, 1623.54it/s]warmup should be done:  87%|████████▋ | 2613/3000 [00:01<00:00, 1622.83it/s]warmup should be done:  87%|████████▋ | 2614/3000 [00:01<00:00, 1615.83it/s]warmup should be done:  87%|████████▋ | 2599/3000 [00:01<00:00, 1614.72it/s]warmup should be done:  87%|████████▋ | 2615/3000 [00:01<00:00, 1634.30it/s]warmup should be done:  88%|████████▊ | 2650/3000 [00:01<00:00, 1620.26it/s]warmup should be done:  88%|████████▊ | 2645/3000 [00:01<00:00, 1641.96it/s]warmup should be done:  85%|████████▍ | 2547/3000 [00:01<00:00, 1613.62it/s]warmup should be done:  88%|████████▊ | 2631/3000 [00:01<00:00, 1628.84it/s]warmup should be done:  93%|█████████▎| 2776/3000 [00:01<00:00, 1622.89it/s]warmup should be done:  93%|█████████▎| 2776/3000 [00:01<00:00, 1611.84it/s]warmup should be done:  92%|█████████▏| 2762/3000 [00:01<00:00, 1617.02it/s]warmup should be done:  93%|█████████▎| 2780/3000 [00:01<00:00, 1638.67it/s]warmup should be done:  94%|█████████▎| 2810/3000 [00:01<00:00, 1643.47it/s]warmup should be done:  94%|█████████▍| 2813/3000 [00:01<00:00, 1618.55it/s]warmup should be done:  90%|█████████ | 2711/3000 [00:01<00:00, 1618.69it/s]warmup should be done:  93%|█████████▎| 2796/3000 [00:01<00:00, 1632.79it/s]warmup should be done:  98%|█████████▊| 2941/3000 [00:01<00:00, 1628.14it/s]warmup should be done:  98%|█████████▊| 2939/3000 [00:01<00:00, 1615.00it/s]warmup should be done:  98%|█████████▊| 2927/3000 [00:01<00:00, 1624.57it/s]warmup should be done:  98%|█████████▊| 2947/3000 [00:01<00:00, 1646.59it/s]warmup should be done:  99%|█████████▉| 2977/3000 [00:01<00:00, 1648.94it/s]warmup should be done:  99%|█████████▉| 2977/3000 [00:01<00:00, 1623.18it/s]warmup should be done:  96%|█████████▌| 2875/3000 [00:01<00:00, 1624.37it/s]warmup should be done:  99%|█████████▊| 2962/3000 [00:01<00:00, 1640.00it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1639.90it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1639.65it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1628.01it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1626.83it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1626.10it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1621.96it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1615.40it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1584.30it/s]




warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]


warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   5%|▌         | 162/3000 [00:00<00:01, 1618.54it/s]warmup should be done:   6%|▌         | 168/3000 [00:00<00:01, 1679.38it/s]warmup should be done:   6%|▌         | 165/3000 [00:00<00:01, 1647.03it/s]warmup should be done:   6%|▌         | 168/3000 [00:00<00:01, 1677.41it/s]warmup should be done:   6%|▌         | 167/3000 [00:00<00:01, 1664.59it/s]warmup should be done:   6%|▌         | 170/3000 [00:00<00:01, 1693.39it/s]warmup should be done:   6%|▌         | 165/3000 [00:00<00:01, 1641.72it/s]warmup should be done:   6%|▌         | 168/3000 [00:00<00:01, 1671.08it/s]warmup should be done:  11%|█         | 336/3000 [00:00<00:01, 1679.44it/s]warmup should be done:  11%|█         | 337/3000 [00:00<00:01, 1683.68it/s]warmup should be done:  11%|█         | 334/3000 [00:00<00:01, 1667.34it/s]warmup should be done:  11%|█         | 330/3000 [00:00<00:01, 1650.43it/s]warmup should be done:  11%|█         | 332/3000 [00:00<00:01, 1655.31it/s]warmup should be done:  11%|█▏        | 340/3000 [00:00<00:01, 1691.25it/s]warmup should be done:  11%|█         | 331/3000 [00:00<00:01, 1646.69it/s]warmup should be done:  11%|█         | 336/3000 [00:00<00:01, 1660.81it/s]warmup should be done:  17%|█▋        | 505/3000 [00:00<00:01, 1682.78it/s]warmup should be done:  17%|█▋        | 498/3000 [00:00<00:01, 1663.57it/s]warmup should be done:  17%|█▋        | 507/3000 [00:00<00:01, 1688.56it/s]warmup should be done:  17%|█▋        | 502/3000 [00:00<00:01, 1670.46it/s]warmup should be done:  17%|█▋        | 497/3000 [00:00<00:01, 1652.21it/s]warmup should be done:  17%|█▋        | 501/3000 [00:00<00:01, 1666.57it/s]warmup should be done:  17%|█▋        | 510/3000 [00:00<00:01, 1690.46it/s]warmup should be done:  17%|█▋        | 504/3000 [00:00<00:01, 1666.31it/s]warmup should be done:  23%|██▎       | 677/3000 [00:00<00:01, 1692.79it/s]warmup should be done:  22%|██▎       | 675/3000 [00:00<00:01, 1686.85it/s]warmup should be done:  22%|██▏       | 665/3000 [00:00<00:01, 1663.75it/s]warmup should be done:  22%|██▏       | 670/3000 [00:00<00:01, 1670.55it/s]warmup should be done:  22%|██▏       | 663/3000 [00:00<00:01, 1653.03it/s]warmup should be done:  22%|██▏       | 670/3000 [00:00<00:01, 1672.75it/s]warmup should be done:  23%|██▎       | 680/3000 [00:00<00:01, 1692.41it/s]warmup should be done:  22%|██▏       | 672/3000 [00:00<00:01, 1671.25it/s]warmup should be done:  28%|██▊       | 845/3000 [00:00<00:01, 1689.71it/s]warmup should be done:  28%|██▊       | 847/3000 [00:00<00:01, 1692.02it/s]warmup should be done:  28%|██▊       | 832/3000 [00:00<00:01, 1663.31it/s]warmup should be done:  28%|██▊       | 850/3000 [00:00<00:01, 1693.69it/s]warmup should be done:  28%|██▊       | 838/3000 [00:00<00:01, 1671.76it/s]warmup should be done:  28%|██▊       | 829/3000 [00:00<00:01, 1649.17it/s]warmup should be done:  28%|██▊       | 840/3000 [00:00<00:01, 1673.13it/s]warmup should be done:  28%|██▊       | 838/3000 [00:00<00:01, 1664.13it/s]warmup should be done:  34%|███▍      | 1015/3000 [00:00<00:01, 1690.60it/s]warmup should be done:  34%|███▍      | 1017/3000 [00:00<00:01, 1692.90it/s]warmup should be done:  33%|███▎      | 1000/3000 [00:00<00:01, 1665.84it/s]warmup should be done:  34%|███▍      | 1020/3000 [00:00<00:01, 1690.96it/s]warmup should be done:  33%|███▎      | 994/3000 [00:00<00:01, 1646.99it/s]warmup should be done:  34%|███▎      | 1005/3000 [00:00<00:01, 1664.63it/s]warmup should be done:  34%|███▎      | 1006/3000 [00:00<00:01, 1667.13it/s]warmup should be done:  34%|███▎      | 1008/3000 [00:00<00:01, 1669.92it/s]warmup should be done:  40%|███▉      | 1187/3000 [00:00<00:01, 1693.49it/s]warmup should be done:  39%|███▉      | 1167/3000 [00:00<00:01, 1665.44it/s]warmup should be done:  40%|███▉      | 1185/3000 [00:00<00:01, 1688.01it/s]warmup should be done:  39%|███▊      | 1159/3000 [00:00<00:01, 1647.24it/s]warmup should be done:  39%|███▉      | 1173/3000 [00:00<00:01, 1666.75it/s]warmup should be done:  39%|███▉      | 1174/3000 [00:00<00:01, 1669.61it/s]warmup should be done:  40%|███▉      | 1190/3000 [00:00<00:01, 1685.50it/s]warmup should be done:  39%|███▉      | 1175/3000 [00:00<00:01, 1662.73it/s]warmup should be done:  45%|████▌     | 1357/3000 [00:00<00:00, 1695.18it/s]warmup should be done:  44%|████▍     | 1334/3000 [00:00<00:01, 1665.85it/s]warmup should be done:  45%|████▌     | 1355/3000 [00:00<00:00, 1690.77it/s]warmup should be done:  45%|████▍     | 1341/3000 [00:00<00:00, 1669.25it/s]warmup should be done:  44%|████▍     | 1324/3000 [00:00<00:01, 1642.69it/s]warmup should be done:  45%|████▍     | 1343/3000 [00:00<00:00, 1672.96it/s]warmup should be done:  45%|████▌     | 1359/3000 [00:00<00:00, 1686.30it/s]warmup should be done:  45%|████▍     | 1342/3000 [00:00<00:00, 1663.38it/s]warmup should be done:  51%|█████     | 1527/3000 [00:00<00:00, 1694.53it/s]warmup should be done:  50%|█████     | 1501/3000 [00:00<00:00, 1665.43it/s]warmup should be done:  51%|█████     | 1525/3000 [00:00<00:00, 1690.92it/s]warmup should be done:  50%|████▉     | 1491/3000 [00:00<00:00, 1649.90it/s]warmup should be done:  50%|█████     | 1509/3000 [00:00<00:00, 1670.12it/s]warmup should be done:  50%|█████     | 1511/3000 [00:00<00:00, 1674.11it/s]warmup should be done:  51%|█████     | 1528/3000 [00:00<00:00, 1684.92it/s]warmup should be done:  50%|█████     | 1509/3000 [00:00<00:00, 1662.93it/s]warmup should be done:  57%|█████▋    | 1697/3000 [00:01<00:00, 1691.75it/s]warmup should be done:  56%|█████▌    | 1669/3000 [00:01<00:00, 1667.50it/s]warmup should be done:  56%|█████▋    | 1695/3000 [00:01<00:00, 1692.24it/s]warmup should be done:  55%|█████▌    | 1658/3000 [00:01<00:00, 1656.00it/s]warmup should be done:  56%|█████▌    | 1677/3000 [00:01<00:00, 1672.30it/s]warmup should be done:  56%|█████▌    | 1680/3000 [00:01<00:00, 1676.60it/s]warmup should be done:  57%|█████▋    | 1698/3000 [00:01<00:00, 1688.08it/s]warmup should be done:  56%|█████▌    | 1676/3000 [00:01<00:00, 1664.06it/s]warmup should be done:  61%|██████    | 1837/3000 [00:01<00:00, 1668.34it/s]warmup should be done:  62%|██████▏   | 1866/3000 [00:01<00:00, 1695.45it/s]warmup should be done:  62%|██████▏   | 1845/3000 [00:01<00:00, 1669.96it/s]warmup should be done:  62%|██████▏   | 1868/3000 [00:01<00:00, 1689.69it/s]warmup should be done:  62%|██████▏   | 1848/3000 [00:01<00:00, 1674.36it/s]warmup should be done:  61%|██████▏   | 1843/3000 [00:01<00:00, 1663.31it/s]warmup should be done:  61%|██████    | 1824/3000 [00:01<00:00, 1645.26it/s]warmup should be done:  62%|██████▏   | 1867/3000 [00:01<00:00, 1673.00it/s]warmup should be done:  68%|██████▊   | 2036/3000 [00:01<00:00, 1696.21it/s]warmup should be done:  67%|██████▋   | 2004/3000 [00:01<00:00, 1665.06it/s]warmup should be done:  67%|██████▋   | 2012/3000 [00:01<00:00, 1668.47it/s]warmup should be done:  68%|██████▊   | 2038/3000 [00:01<00:00, 1691.98it/s]warmup should be done:  67%|██████▋   | 2016/3000 [00:01<00:00, 1672.38it/s]warmup should be done:  67%|██████▋   | 2010/3000 [00:01<00:00, 1661.89it/s]warmup should be done:  66%|██████▋   | 1992/3000 [00:01<00:00, 1653.23it/s]warmup should be done:  68%|██████▊   | 2035/3000 [00:01<00:00, 1670.78it/s]warmup should be done:  74%|███████▎  | 2206/3000 [00:01<00:00, 1694.77it/s]warmup should be done:  72%|███████▏  | 2171/3000 [00:01<00:00, 1661.90it/s]warmup should be done:  73%|███████▎  | 2179/3000 [00:01<00:00, 1668.23it/s]warmup should be done:  74%|███████▎  | 2208/3000 [00:01<00:00, 1690.53it/s]warmup should be done:  73%|███████▎  | 2185/3000 [00:01<00:00, 1675.25it/s]warmup should be done:  72%|███████▏  | 2160/3000 [00:01<00:00, 1661.14it/s]warmup should be done:  73%|███████▎  | 2177/3000 [00:01<00:00, 1661.25it/s]warmup should be done:  73%|███████▎  | 2203/3000 [00:01<00:00, 1665.86it/s]warmup should be done:  79%|███████▉  | 2376/3000 [00:01<00:00, 1694.56it/s]warmup should be done:  78%|███████▊  | 2338/3000 [00:01<00:00, 1662.10it/s]warmup should be done:  78%|███████▊  | 2347/3000 [00:01<00:00, 1671.22it/s]warmup should be done:  78%|███████▊  | 2355/3000 [00:01<00:00, 1682.26it/s]warmup should be done:  79%|███████▉  | 2378/3000 [00:01<00:00, 1690.48it/s]warmup should be done:  78%|███████▊  | 2328/3000 [00:01<00:00, 1666.72it/s]warmup should be done:  78%|███████▊  | 2344/3000 [00:01<00:00, 1663.44it/s]warmup should be done:  79%|███████▉  | 2370/3000 [00:01<00:00, 1663.39it/s]warmup should be done:  85%|████████▍ | 2546/3000 [00:01<00:00, 1695.86it/s]warmup should be done:  84%|████████▎ | 2506/3000 [00:01<00:00, 1666.67it/s]warmup should be done:  84%|████████▍ | 2516/3000 [00:01<00:00, 1673.89it/s]warmup should be done:  84%|████████▍ | 2524/3000 [00:01<00:00, 1684.25it/s]warmup should be done:  85%|████████▍ | 2548/3000 [00:01<00:00, 1692.13it/s]warmup should be done:  83%|████████▎ | 2496/3000 [00:01<00:00, 1670.37it/s]warmup should be done:  84%|████████▎ | 2511/3000 [00:01<00:00, 1664.92it/s]warmup should be done:  85%|████████▍ | 2541/3000 [00:01<00:00, 1674.33it/s]warmup should be done:  91%|█████████ | 2716/3000 [00:01<00:00, 1696.52it/s]warmup should be done:  89%|████████▉ | 2674/3000 [00:01<00:00, 1670.27it/s]warmup should be done:  89%|████████▉ | 2684/3000 [00:01<00:00, 1673.68it/s]warmup should be done:  89%|████████▉ | 2664/3000 [00:01<00:00, 1671.25it/s]warmup should be done:  90%|████████▉ | 2693/3000 [00:01<00:00, 1681.42it/s]warmup should be done:  91%|█████████ | 2718/3000 [00:01<00:00, 1690.61it/s]warmup should be done:  89%|████████▉ | 2678/3000 [00:01<00:00, 1664.03it/s]warmup should be done:  90%|█████████ | 2712/3000 [00:01<00:00, 1682.41it/s]warmup should be done:  96%|█████████▌| 2886/3000 [00:01<00:00, 1695.02it/s]warmup should be done:  95%|█████████▍| 2842/3000 [00:01<00:00, 1672.20it/s]warmup should be done:  95%|█████████▌| 2852/3000 [00:01<00:00, 1671.95it/s]warmup should be done:  94%|█████████▍| 2833/3000 [00:01<00:00, 1674.48it/s]warmup should be done:  96%|█████████▋| 2888/3000 [00:01<00:00, 1687.01it/s]warmup should be done:  95%|█████████▌| 2862/3000 [00:01<00:00, 1674.64it/s]warmup should be done:  95%|█████████▍| 2845/3000 [00:01<00:00, 1657.90it/s]warmup should be done:  96%|█████████▌| 2882/3000 [00:01<00:00, 1685.28it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1692.10it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1689.37it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1683.75it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1673.43it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1670.29it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1666.36it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1662.93it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1660.11it/s]2022-12-12 00:16:50.785850: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f132402c8e0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 00:16:50.785913: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 00:16:51.801475: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f13d402c5f0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 00:16:51.801539: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 00:16:51.802061: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f12f80284a0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 00:16:51.802104: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 00:16:51.815240: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f2fc3f925d0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 00:16:51.815310: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 00:16:51.827551: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f140c02fbe0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 00:16:51.827613: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 00:16:52.026804: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f2fc782f980 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 00:16:52.026870: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 00:16:52.165172: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f2fc382bad0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 00:16:52.165238: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 00:16:52.211279: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f139402fa10 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 00:16:52.211355: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 00:16:53.048505: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 00:16:54.085477: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 00:16:54.087086: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 00:16:54.109451: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 00:16:54.110670: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 00:16:54.307874: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 00:16:54.418462: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 00:16:54.467251: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 00:16:55.976584: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 00:16:56.957387: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 00:16:56.997031: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 00:16:57.020618: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 00:16:57.042166: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 00:16:57.160137: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 00:16:57.314352: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 00:16:57.324659: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
[HCTR][00:17:32.806][ERROR][RK0][main]: replica 5 reaches 1000, calling init pre replica
[HCTR][00:17:32.806][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][00:17:32.816][ERROR][RK0][main]: coll ps creation done
[HCTR][00:17:32.816][ERROR][RK0][main]: replica 5 waits for coll ps creation barrier
[HCTR][00:17:32.820][ERROR][RK0][main]: replica 4 reaches 1000, calling init pre replica
[HCTR][00:17:32.820][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][00:17:32.825][ERROR][RK0][main]: coll ps creation done
[HCTR][00:17:32.825][ERROR][RK0][main]: replica 4 waits for coll ps creation barrier
[HCTR][00:17:32.954][ERROR][RK0][main]: replica 3 reaches 1000, calling init pre replica
[HCTR][00:17:32.955][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][00:17:32.960][ERROR][RK0][main]: coll ps creation done
[HCTR][00:17:32.960][ERROR][RK0][main]: replica 3 waits for coll ps creation barrier
[HCTR][00:17:33.009][ERROR][RK0][tid #139843732502272]: replica 2 reaches 1000, calling init pre replica
[HCTR][00:17:33.009][ERROR][RK0][tid #139843732502272]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][00:17:33.017][ERROR][RK0][tid #139843732502272]: coll ps creation done
[HCTR][00:17:33.017][ERROR][RK0][tid #139843732502272]: replica 2 waits for coll ps creation barrier
[HCTR][00:17:33.136][ERROR][RK0][tid #139843195631360]: replica 1 reaches 1000, calling init pre replica
[HCTR][00:17:33.136][ERROR][RK0][tid #139843195631360]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][00:17:33.145][ERROR][RK0][tid #139843195631360]: coll ps creation done
[HCTR][00:17:33.145][ERROR][RK0][tid #139843195631360]: replica 1 waits for coll ps creation barrier
[HCTR][00:17:33.239][ERROR][RK0][tid #139843657000704]: replica 0 reaches 1000, calling init pre replica
[HCTR][00:17:33.240][ERROR][RK0][tid #139843657000704]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][00:17:33.248][ERROR][RK0][tid #139843657000704]: coll ps creation done
[HCTR][00:17:33.248][ERROR][RK0][tid #139843657000704]: replica 0 waits for coll ps creation barrier
[HCTR][00:17:33.255][ERROR][RK0][tid #139843262740224]: replica 7 reaches 1000, calling init pre replica
[HCTR][00:17:33.255][ERROR][RK0][tid #139843262740224]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][00:17:33.263][ERROR][RK0][tid #139843262740224]: coll ps creation done
[HCTR][00:17:33.263][ERROR][RK0][tid #139843262740224]: replica 7 waits for coll ps creation barrier
[HCTR][00:17:33.272][ERROR][RK0][main]: replica 6 reaches 1000, calling init pre replica
[HCTR][00:17:33.273][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][00:17:33.281][ERROR][RK0][main]: coll ps creation done
[HCTR][00:17:33.281][ERROR][RK0][main]: replica 6 waits for coll ps creation barrier
[HCTR][00:17:33.281][ERROR][RK0][tid #139843657000704]: replica 0 preparing frequency
[HCTR][00:17:34.138][ERROR][RK0][tid #139843657000704]: replica 0 preparing frequency done
[HCTR][00:17:34.169][ERROR][RK0][tid #139843657000704]: replica 0 calling init per replica
[HCTR][00:17:34.169][ERROR][RK0][main]: replica 5 calling init per replica
[HCTR][00:17:34.169][ERROR][RK0][tid #139843195631360]: replica 1 calling init per replica
[HCTR][00:17:34.169][ERROR][RK0][main]: replica 4 calling init per replica
[HCTR][00:17:34.169][ERROR][RK0][main]: replica 6 calling init per replica
[HCTR][00:17:34.169][ERROR][RK0][tid #139843262740224]: replica 7 calling init per replica
[HCTR][00:17:34.169][ERROR][RK0][main]: replica 3 calling init per replica
[HCTR][00:17:34.169][ERROR][RK0][tid #139843732502272]: replica 2 calling init per replica
[HCTR][00:17:34.169][ERROR][RK0][tid #139843657000704]: Calling build_v2
[HCTR][00:17:34.170][ERROR][RK0][main]: Calling build_v2
[HCTR][00:17:34.170][ERROR][RK0][tid #139843195631360]: Calling build_v2
[HCTR][00:17:34.170][ERROR][RK0][main]: Calling build_v2
[HCTR][00:17:34.170][ERROR][RK0][main]: Calling build_v2
[HCTR][00:17:34.170][ERROR][RK0][tid #139843732502272]: Calling build_v2
[HCTR][00:17:34.170][ERROR][RK0][tid #139843262740224]: Calling build_v2
[HCTR][00:17:34.170][ERROR][RK0][main]: Calling build_v2
[HCTR][00:17:34.170][ERROR][RK0][tid #139843657000704]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][00:17:34.170][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][00:17:34.170][ERROR][RK0][tid #139843195631360]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][00:17:34.170][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][00:17:34.170][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][00:17:34.170][ERROR][RK0][tid #139843732502272]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][00:17:34.170][ERROR][RK0][tid #139843262740224]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][00:17:34.170][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[[[[[[[2022-12-12 00:17:342022-12-12 00:17:342022-12-12 00:17:342022-12-12 00:17:342022-12-12 00:17:34[.2022-12-12 00:17:34..2022-12-12 00:17:34..170135.170137170142.170144170135: 170135: 2022-12-12 00:17:34: 170151: : E: E.E: EE E 170176 E  /hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc /hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc: /hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc /hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc:/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc:E:/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc::136:136 136:136136] 136] /hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc] 136] ] using concurrent impl MPSPhase] using concurrent impl MPSPhase:using concurrent impl MPSPhase] using concurrent impl MPSPhaseusing concurrent impl MPSPhase
using concurrent impl MPSPhase
136
using concurrent impl MPSPhase


] 
using concurrent impl MPSPhase
[2022-12-12 00:17:34.174249: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:178] v100x8, slow pcie
[2022-12-12 00:17:34.174288: E [/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 00:17:34:.196174293] : assigning 8 to cpuE
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:178] v100x8, slow pcie
[[2022-12-12 00:17:342022-12-12 00:17:34..174342174337: : EE[  2022-12-12 00:17:34/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.::174363196178: ] [] Eassigning 8 to cpu2022-12-12 00:17:34v100x8, slow pcie 
.
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc174381:: 212[E] 2022-12-12 00:17:34 build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[
174415:2022-12-12 00:17:34: 178.E] 174423 [v100x8, slow pcie: [/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 00:17:34
E2022-12-12 00:17:34:. .196[174452/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc174456] 2022-12-12 00:17:34: :: [assigning 8 to cpu2022-12-12 00:17:34.E178E
.174475 ] [ 174475: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccv100x8, slow pcie2022-12-12 00:17:34/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: E:
.:E[ 212174513213 [2022-12-12 00:17:34/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] : ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[2022-12-12 00:17:34.:build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8Eremote time is 8.68421:2022-12-12 00:17:34.174545196
 
178.174556: ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] 174561[: [Eassigning 8 to cpu:v100x8, slow pcie: 2022-12-12 00:17:34E2022-12-12 00:17:34 
178
E. ./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc]  174630[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc174633:v100x8, slow pcie/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: 2022-12-12 00:17:34:: 212
:E.196E] [178[ 174689]  build asymm link desc with 8X Tesla V100-SXM2-32GB out of 82022-12-12 00:17:34] 2022-12-12 00:17:34/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: assigning 8 to cpu/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
.v100x8, slow pcie.:E
:174733
174739[214 213: : 2022-12-12 00:17:34] [/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] EE.cpu time is 97.05882022-12-12 00:17:34[:remote time is 8.68421  174798
.2022-12-12 00:17:34196
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: 174817.] ::[E: 174836assigning 8 to cpu2121962022-12-12 00:17:34 E: 
] ] ./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc Ebuild asymm link desc with 8X Tesla V100-SXM2-32GB out of 8assigning 8 to cpu174887:/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc 

: 213:[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE] [1962022-12-12 00:17:34: remote time is 8.684212022-12-12 00:17:34[] .212/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
.2022-12-12 00:17:34assigning 8 to cpu174975] :174990.[
: build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8214: 1750062022-12-12 00:17:34E
] E: . cpu time is 97.0588 E175042/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc : :[2022-12-12 00:17:34:/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE 2022-12-12 00:17:34..212213:/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc175089175095] ] 212:: : build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8remote time is 8.68421] 214EE

build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8]   
cpu time is 97.0588/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
:2022-12-12 00:17:342022-12-12 00:17:34[:213..2022-12-12 00:17:34212] 175214175218.] remote time is 8.68421: : 175233build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8
EE: 
  E[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc 2022-12-12 00:17:34::/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.[214213:1753052022-12-12 00:17:34] ] 213: .cpu time is 97.0588remote time is 8.68421] E175325

remote time is 8.68421 : 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[E:2022-12-12 00:17:34[ 214.2022-12-12 00:17:34/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] 175396.:cpu time is 97.0588: 175408213
E: ]  Eremote time is 8.68421/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc 
:/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc214:] [214cpu time is 97.05882022-12-12 00:17:34] 
.cpu time is 97.0588175506
: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:214] cpu time is 97.0588
[2022-12-12 00:18:51.196477: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:118] solver created. now build & solve
[2022-12-12 00:18:51.236788: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 381.47 MB
[2022-12-12 00:18:51.236873: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 381.47 MB
[2022-12-12 00:18:51.237852: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:74] mapping nid to rank...
[2022-12-12 00:18:51.318556: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:91] counting slots...
[2022-12-12 00:18:51.695780: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:105] Final num slot is 49
[2022-12-12 00:18:51.695874: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:109] counting blocks...
[2022-12-12 00:18:59.165324: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:118] Final num block is 1023
[2022-12-12 00:18:59.165418: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:123] counting freq and density...
[2022-12-12 00:19:00.947722: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:154] averaging freq and density...
[2022-12-12 00:19:00.947821: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:155] 1023
[2022-12-12 00:19:00.950812: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:123] solver built. now solve
[2022-12-12 00:19:00.950884: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:354] constructing optimal solver, device=8, stream=1
1023 blocks, 8 devices
[2022-12-12 00:19:01.253141: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:527] Add Var...
[2022-12-12 00:19:01.283988: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:545] Capacity...
[2022-12-12 00:19:01.285435: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:548] Connect CPU...
[2022-12-12 00:19:01.308144: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:550] Connect Access To Storage...
[2022-12-12 00:19:01.918166: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:554] Time...
[2022-12-12 00:19:01.920443: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:516] dst 0, total sm is 80
[2022-12-12 00:19:01.923465: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:516] dst 1, total sm is 80
[2022-12-12 00:19:01.926350: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:516] dst 2, total sm is 80
[2022-12-12 00:19:01.929223: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:516] dst 3, total sm is 80
[2022-12-12 00:19:01.932093: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:516] dst 4, total sm is 80
[2022-12-12 00:19:01.934951: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:516] dst 5, total sm is 80
[2022-12-12 00:19:01.937818: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:516] dst 6, total sm is 80
[2022-12-12 00:19:01.940670: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:516] dst 7, total sm is 80
[2022-12-12 00:19:28.772369: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:569] Coll Cache init block placement array
[2022-12-12 00:19:28.780443: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:645] Coll Cache init block placement array done
[2022-12-12 00:19:28.782952: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:647] Coll Cache model reset done
[2022-12-12 00:19:28.828845: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:127] solver solved
[2022-12-12 00:19:28.828939: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:195] 0 solved master
[2022-12-12 00:19:28.828972: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 0 solved
[2022-12-12 00:19:28.829003: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 0 initing device 0
[2022-12-12 00:19:28.829529: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-12 00:19:28.829579: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 00:19:28.830464: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 00:19:28.831124: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 00:19:28.844356: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 7 solved
[2022-12-12 00:19:28.844431: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 7 initing device 7
[2022-12-12 00:19:28.844872: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-12 00:19:28.844923: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 00:19:28.845476: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] [1 solved2022-12-12 00:19:28
.845514: E[ 2022-12-12 00:19:28/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc.:845546202: ] E4 solved 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 1 initing device 1[
2022-12-12 00:19:28.845580: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 4 initing device 4
[2022-12-12 00:19:28.845707: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 5 solved
[2022-12-12 00:19:28.845759: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 5 initing device 5
[[2022-12-12 00:19:282022-12-12 00:19:28..845781845767: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc::1980202] ] eager alloc mem 381.47 MB2 solved

[2022-12-12 00:19:28.845890: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 2 initing device 2
[2022-12-12 00:19:28.846001[: 2022-12-12 00:19:28E. 846012/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu: :E1815 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuBuilding Coll Cache with ... num gpu device is 8:
1815] Building Coll Cache with ... num gpu device is 8
[2022-12-12 00:19:28.[8460672022-12-12 00:19:28: .E846072 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE: 1980/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu] :eager alloc mem 381.47 MB1980
] eager alloc mem 381.47 MB
[2022-12-12 00:19:28.846175: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-12 00:19:28.846216: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 00:19:28.846342: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-12 00:19:28.846392: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 00:19:28.847119: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 6 solved
[2022-12-12 00:19:28.847178: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 6 initing device 6
[2022-12-12 00:19:28.847586: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-12 00:19:28.847626: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 00:19:28.847686: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 3 solved
[2022-12-12 00:19:28.847753: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 3 initing device 3
[2022-12-12 00:19:28.848182: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-12 00:19:28.848224: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 00:19:28.849935: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 00:19:28.850186: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 00:19:28.850232: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 00:19:28.850352: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 00:19:28.850942: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 00:19:28.851462: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 00:19:28.852472: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 00:19:28.854507: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 00:19:28.854599: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 00:19:28.854649: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 00:19:28.854781: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 00:19:28.854844: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 00:19:28.856239: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 00:19:28.910460: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1023.00 Bytes
[2022-12-12 00:19:28.915811: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1023
[2022-12-12 00:19:28.915921: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 00:19:28.916723: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 15.64 MB
[2022-12-12 00:19:28.917287: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:19:28.918286: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:19:28.918331: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 15.22 MB
[2022-12-12 00:19:28.921356: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 00:19:28.922108: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 00:19:28.922154: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1.91 GB
[[2022-12-12 00:19:282022-12-12 00:19:28..931664931664: : EE[ [[[ 2022-12-12 00:19:28/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu2022-12-12 00:19:282022-12-12 00:19:282022-12-12 00:19:28/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu.:...:93168519809316859316859316851980: ] : : : ] Eeager alloc mem 1023.00 BytesEEEeager alloc mem 1023.00 Bytes 
   
/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::::1980198019801980] ] ] ] eager alloc mem 1023.00 Byteseager alloc mem 1023.00 Byteseager alloc mem 1023.00 Byteseager alloc mem 1023.00 Bytes



[2022-12-12 00:19:28.938053: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1023
[2022-12-12 00:19:28.938128: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 00:19:28.938140: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1023
[2022-12-12 00:19:28.938215: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 00:19:28.938245: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1023
[[2022-12-12 00:19:282022-12-12 00:19:28..938334938355: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::638638] ] eager release cuda mem 1023eager release cuda mem 400000000

[2022-12-12 00:19:28.938412: E[ 2022-12-12 00:19:28/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc.:938451638: ] Eeager release cuda mem 1023 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 00:19:28.938491: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc[:2022-12-12 00:19:28638.] 938524eager release cuda mem 1023: 
E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 00:19:28.938587: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 00:19:28.939035: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 15.64 MB
[2022-12-12 00:19:28.939612: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 15.64 MB
[2022-12-12 00:19:28.940911: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 15.64 MB
[2022-12-12 00:19:28.941471: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 15.64 MB
[2022-12-12 00:19:28.941500: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1023.00 Bytes
[2022-12-12 00:19:28.941982: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 15.64 MB
[2022-12-12 00:19:28.942500: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 15.64 MB
[2022-12-12 00:19:28.943065: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:19:28.943183: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:19:28.943682: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:19:28.943724: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:19:28.943772: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:19:28.943818: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:19:28.944041: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:19:28.944088: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 15.20 MB
[2022-12-12 00:19:28.944156: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:19:28.944201: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 15.17 MB
[2022-12-12 00:19:28.944669: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663[
2022-12-12 00:19:28.944697: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663[
2022-12-12 00:19:28.944725: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 15.25 MB[
2022-12-12 00:19:28.944749: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] [WORKER[0] alloc host memory 15.19 MB2022-12-12 00:19:28
.944769: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc[:2022-12-12 00:19:28638.] 944787eager release cuda mem 625663: 
E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:19:28.944827: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc[:2022-12-12 00:19:2843.] 944842WORKER[0] alloc host memory 15.22 MB: 
W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 15.24 MB
[2022-12-12 00:19:28.946241: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1023
[2022-12-12 00:19:28.946337: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 00:19:28.951191: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 15.64 MB
[2022-12-12 00:19:28.951735: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:19:28.952722: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:19:28.952769: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 15.21 MB
[2022-12-12 00:19:28.954771: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 00:19:28.955299: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 00:19:28.955381: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 00:19:28.955426: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1.90 GB
[2022-12-12 00:19:28.955573[: 2022-12-12 00:19:28E. 955586/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu: :E1980 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cueager alloc mem 25.25 KB:
1980] eager alloc mem 25.25 KB
[2022-12-12 00:19:28.955750: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 00:19:28.955910: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 00:19:28.955956: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1.90 GB
[2022-12-12 00:19:28.[9562042022-12-12 00:19:28: .E956210 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccE: 638/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc] :eager release cuda mem 25855638
] eager release cuda mem 25855
[[2022-12-12 00:19:28[2022-12-12 00:19:28.2022-12-12 00:19:28.956267.956256: 956272: E: E E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980:1980] 1980] eager alloc mem 1.91 GB] eager alloc mem 25.25 KB
eager alloc mem 1.91 GB

[2022-12-12 00:19:28.956356: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 00:19:28.956402: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1.90 GB
[2022-12-12 00:19:28.956907: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 00:19:28.956953: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1.91 GB
[2022-12-12 00:19:28.963373: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 00:19:28.963984: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 00:19:28.964029: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1.91 GB
[[[[[[[[2022-12-12 00:19:292022-12-12 00:19:292022-12-12 00:19:292022-12-12 00:19:292022-12-12 00:19:292022-12-12 00:19:292022-12-12 00:19:292022-12-12 00:19:29........580640580640580640580640580641580644580640580641: : : : : : : : EEEEEEEE        /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::::::::19261926192619261926192619261926] ] ] ] ] ] ] Device 3 init p2p of link 2] Device 5 init p2p of link 6Device 4 init p2p of link 5Device 6 init p2p of link 0Device 7 init p2p of link 4Device 1 init p2p of link 7Device 2 init p2p of link 1
Device 0 init p2p of link 3






[[[2022-12-12 00:19:29[2022-12-12 00:19:29[2022-12-12 00:19:29.2022-12-12 00:19:29.2022-12-12 00:19:29.[581176.581176.[[5811792022-12-12 00:19:29: 581183: 5811852022-12-12 00:19:292022-12-12 00:19:29: .E: E: ..E581202 E E581202581206 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu : : /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE:/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuEE: 1980:1980:  1980/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu] 1980] 1980/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu] :eager alloc mem 611.00 KB] eager alloc mem 611.00 KB] ::eager alloc mem 611.00 KB1980
eager alloc mem 611.00 KB
eager alloc mem 611.00 KB19801980
] 

] ] eager alloc mem 611.00 KBeager alloc mem 611.00 KBeager alloc mem 611.00 KB


[2022-12-12 00:19:29.582218: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:19:29[.[2022-12-12 00:19:295822462022-12-12 00:19:29.: [.582251E2022-12-12 00:19:29582253:  .[: [E/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc[5822642022-12-12 00:19:29E2022-12-12 00:19:29 :2022-12-12 00:19:29: . ./hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc638.E582279/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc582282:] 582285 : :: 638eager release cuda mem 625663: /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccE638E] 
E: ]  eager release cuda mem 625663 638/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cceager release cuda mem 625663/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc
/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc] :
::eager release cuda mem 625663638638638
] ] ] eager release cuda mem 625663eager release cuda mem 625663eager release cuda mem 625663


[2022-12-12 00:19:29.595477: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 6 init p2p of link 5
[2022-12-12 00:19:29.595622: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:19:29.596068: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 4 init p2p of link 7
[2022-12-12 00:19:29.596115: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 5 init p2p of link 4
[2022-12-12 00:19:29.596203: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:19:29.596269: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:19:29.596316: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 7 init p2p of link 1
[2022-12-12 00:19:29.596391: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 2
[2022-12-12 00:19:29.596441: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:19:29.596473: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:19:29.596553: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:19:29.596975: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu[:2022-12-12 00:19:291926.] 597008Device 2 init p2p of link 3: 
E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:19:29.597075: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[[2022-12-12 00:19:292022-12-12 00:19:29..597185597204: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::19261980] ] Device 3 init p2p of link 0eager alloc mem 611.00 KB

[2022-12-12 00:19:29.597265: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:19:29.597281: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 0 init p2p of link 6
[2022-12-12 00:19:29.597371: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:[6382022-12-12 00:19:29] .eager release cuda mem 625663597391
: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:19:29.597453: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:19:29.598038: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:19:29.598216: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:19:29.598246: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:19:29.610032: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 6 init p2p of link 4
[2022-12-12 00:19:29.610157: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:19:29.610356: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 4 init p2p of link 2
[2022-12-12 00:19:29.610472: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:19:29.610609: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 7 init p2p of link 6
[2022-12-12 00:19:29.610666: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 5 init p2p of link 7
[2022-12-12 00:19:29.610728: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:19:29.610791: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:19:29.610964: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:19:29.611126: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 3
[2022-12-12 00:19:29.611266[: 2022-12-12 00:19:29E. 611271/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu: :E1980 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cceager alloc mem 611.00 KB:
638] eager release cuda mem 625663
[2022-12-12 00:19:29.611385: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 0 init p2p of link 1
[2022-12-12 00:19:29.611464: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 2 init p2p of link 0
[[2022-12-12 00:19:292022-12-12 00:19:29..611526611526: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::1980638] [] eager alloc mem 611.00 KB2022-12-12 00:19:29eager release cuda mem 625663
.
[6115892022-12-12 00:19:29: .E611614 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccE: 638/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu] :eager release cuda mem 6256631980
] eager alloc mem 611.00 KB
[2022-12-12 00:19:29.611709: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 3 init p2p of link 5
[2022-12-12 00:19:29.611878: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:19:29.612090: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:19:29.612395: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:19:29.612472: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:19:29.612673: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:19:29.626542: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 6 init p2p of link 7
[2022-12-12 00:19:29.626665: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:19:29.626919: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 4 init p2p of link 6
[2022-12-12 00:19:29.627036: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:19:29.627471: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:19:29.627836: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:19:29.627882: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 7 init p2p of link 5
[2022-12-12 00:19:29.627997: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:19:29.628169: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 5 init p2p of link 3
[2022-12-12 00:19:29.628286: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:19:29.628313: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 3 init p2p of link 1
[2022-12-12 00:19:29.628433: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:19:29.628491: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 0
[2022-12-12 00:19:29.628616: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:19:29.628765: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:[19262022-12-12 00:19:29] .Device 0 init p2p of link 2628794
: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:19:29.628908: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:19:29.629069: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:19:29.629233: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:19:29.629418[: 2022-12-12 00:19:29E. 629419/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc: :E638 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cueager release cuda mem 625663:
1926] Device 2 init p2p of link 4
[2022-12-12 00:19:29.629586: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:19:29.629714: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:19:29.630364: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:19:29.642174: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 16399996
[2022-12-12 00:19:29.642917: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 16399996
[2022-12-12 00:19:29.643399: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 16399996
[2022-12-12 00:19:29.643899: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 16399996
[2022-12-12 00:19:29.644111: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 3995469 / 100000000 nodes ( 4.00 %~4.00 %) | remote 11942005 / 100000000 nodes ( 11.94 %) | cpu 84062526 / 100000000 nodes ( 84.06 %) | 1.91 GB | 0.796489 secs 
[2022-12-12 00:19:29.644203: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 3997129 / 100000000 nodes ( 4.00 %~4.00 %) | remote 11940345 / 100000000 nodes ( 11.94 %) | cpu 84062526 / 100000000 nodes ( 84.06 %) | 1.91 GB | 0.798136 secs 
[2022-12-12 00:19:29.644276: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 16399996
[2022-12-12 00:19:29.644342: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 3989662 / 100000000 nodes ( 3.99 %~4.00 %) | remote 11947812 / 100000000 nodes ( 11.95 %) | cpu 84062526 / 100000000 nodes ( 84.06 %) | 1.91 GB | 0.798132 secs 
[2022-12-12 00:19:29.644579: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 3982748 / 100000000 nodes ( 3.98 %~4.00 %) | remote 11954726 / 100000000 nodes ( 11.95 %) | cpu 84062526 / 100000000 nodes ( 84.06 %) | 1.90 GB | 0.799669 secs 
[2022-12-12 00:19:29.644820: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 3984930 / 100000000 nodes ( 3.98 %~4.00 %) | remote 11952544 / 100000000 nodes ( 11.95 %) | cpu 84062526 / 100000000 nodes ( 84.06 %) | 1.90 GB | 0.798441 secs 
[2022-12-12 00:19:29.644844: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 16399996
[2022-12-12 00:19:29.645147: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 16399996
[2022-12-12 00:19:29.645185: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 3977275 / 100000000 nodes ( 3.98 %~4.00 %) | remote 11960199 / 100000000 nodes ( 11.96 %) | cpu 84062526 / 100000000 nodes ( 84.06 %) | 1.90 GB | 0.796969 secs 
[2022-12-12 00:19:29.645474: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 16399996
[2022-12-12 00:19:29.648373: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 3986686 / 100000000 nodes ( 3.99 %~4.00 %) | remote 11950788 / 100000000 nodes ( 11.95 %) | cpu 84062526 / 100000000 nodes ( 84.06 %) | 1.91 GB | 0.802315 secs 
[2022-12-12 00:19:29.649962: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 3988583 / 100000000 nodes ( 3.99 %~4.00 %) | remote 11948891 / 100000000 nodes ( 11.95 %) | cpu 84062526 / 100000000 nodes ( 84.06 %) | 1.91 GB | 0.820395 secs 
[2022-12-12 00:19:29.650547: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2213] before create ctx, mem is 8.55 GB
[2022-12-12 00:19:30.917288: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2219] after create ctx, mem is 8.81 GB
[2022-12-12 00:19:30.917439: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2226] after create stream, mem is 8.81 GB
[2022-12-12 00:19:30.917678: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2213] before create ctx, mem is 8.81 GB
[2022-12-12 00:19:32.175837: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2219] after create ctx, mem is 9.07 GB
[2022-12-12 00:19:32.175977: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2226] after create stream, mem is 9.07 GB
[2022-12-12 00:19:32.176338: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2213] before create ctx, mem is 9.07 GB
[2022-12-12 00:19:33.339656: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2219] after create ctx, mem is 9.29 GB
[2022-12-12 00:19:33.339783: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2226] after create stream, mem is 9.29 GB
[2022-12-12 00:19:33.340091: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2213] before create ctx, mem is 9.29 GB
[2022-12-12 00:19:34.204242: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2219] after create ctx, mem is 9.50 GB
[2022-12-12 00:19:34.205159: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2226] after create stream, mem is 9.50 GB
[2022-12-12 00:19:34.206163: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2243] before create stream, mem is 9.50 GB
[2022-12-12 00:19:34.207064: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2249] after create stream, mem is 9.50 GB
[2022-12-12 00:19:34.207764: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2213] before create ctx, mem is 9.50 GB
[2022-12-12 00:19:35.371440: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2219] after create ctx, mem is 9.70 GB
[2022-12-12 00:19:35.371616: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2226] after create stream, mem is 9.70 GB
[HCTR][00:19:36.633][ERROR][RK0][tid #139843195631360]: replica 1 calling init per replica done, doing barrier
[HCTR][00:19:36.633][ERROR][RK0][main]: replica 4 calling init per replica done, doing barrier
[HCTR][00:19:36.633][ERROR][RK0][tid #139843657000704]: replica 0 calling init per replica done, doing barrier
[HCTR][00:19:36.633][ERROR][RK0][main]: replica 6 calling init per replica done, doing barrier
[HCTR][00:19:36.633][ERROR][RK0][tid #139843262740224]: replica 7 calling init per replica done, doing barrier
[HCTR][00:19:36.633][ERROR][RK0][main]: replica 5 calling init per replica done, doing barrier
[HCTR][00:19:36.633][ERROR][RK0][tid #139843732502272]: replica 2 calling init per replica done, doing barrier
[HCTR][00:19:36.633][ERROR][RK0][main]: replica 3 calling init per replica done, doing barrier
[HCTR][00:19:36.633][ERROR][RK0][main]: replica 3 calling init per replica done, doing barrier done
[HCTR][00:19:36.633][ERROR][RK0][tid #139843195631360]: replica 1 calling init per replica done, doing barrier done
[HCTR][00:19:36.633][ERROR][RK0][main]: replica 4 calling init per replica done, doing barrier done
[HCTR][00:19:36.633][ERROR][RK0][main]: replica 6 calling init per replica done, doing barrier done
[HCTR][00:19:36.633][ERROR][RK0][tid #139843262740224]: replica 7 calling init per replica done, doing barrier done
[HCTR][00:19:36.633][ERROR][RK0][main]: replica 5 calling init per replica done, doing barrier done
[HCTR][00:19:36.633][ERROR][RK0][tid #139843732502272]: replica 2 calling init per replica done, doing barrier done
[HCTR][00:19:36.633][ERROR][RK0][main]: init per replica done
[HCTR][00:19:36.633][ERROR][RK0][tid #139843657000704]: replica 0 calling init per replica done, doing barrier done
[HCTR][00:19:36.633][ERROR][RK0][tid #139843195631360]: init per replica done
[HCTR][00:19:36.633][ERROR][RK0][main]: init per replica done
[HCTR][00:19:36.633][ERROR][RK0][main]: init per replica done
[HCTR][00:19:36.633][ERROR][RK0][tid #139843262740224]: init per replica done
[HCTR][00:19:36.633][ERROR][RK0][main]: init per replica done
[HCTR][00:19:36.633][ERROR][RK0][tid #139843732502272]: init per replica done
[HCTR][00:19:36.636][ERROR][RK0][tid #139843657000704]: init per replica done
[HCTR][00:19:36.639][ERROR][RK0][tid #139843262740224]: 7 allocated 3276800 at 0x7f1cdb520000
[HCTR][00:19:36.639][ERROR][RK0][tid #139843262740224]: 7 allocated 6553600 at 0x7f31ad000000
[HCTR][00:19:36.639][ERROR][RK0][tid #139843262740224]: 7 allocated 3276800 at 0x7f31ad640000
[HCTR][00:19:36.639][ERROR][RK0][tid #139843262740224]: 7 allocated 6553600 at 0x7f31ad960000
[HCTR][00:19:36.639][ERROR][RK0][main]: 3 allocated 3276800 at 0x7f1cdb520000
[HCTR][00:19:36.639][ERROR][RK0][main]: 3 allocated 6553600 at 0x7f31b1000000
[HCTR][00:19:36.639][ERROR][RK0][main]: 3 allocated 3276800 at 0x7f31b1640000
[HCTR][00:19:36.639][ERROR][RK0][main]: 3 allocated 6553600 at 0x7f31b1960000
[HCTR][00:19:36.639][ERROR][RK0][main]: 6 allocated 3276800 at 0x7f1cdb520000
[HCTR][00:19:36.639][ERROR][RK0][main]: 5 allocated 3276800 at 0x7f1cdb520000
[HCTR][00:19:36.639][ERROR][RK0][main]: 6 allocated 6553600 at 0x7f31b1000000
[HCTR][00:19:36.639][ERROR][RK0][main]: 6 allocated 3276800 at 0x7f31b1640000
[HCTR][00:19:36.639][ERROR][RK0][main]: 5 allocated 6553600 at 0x7f31ab000000
[HCTR][00:19:36.639][ERROR][RK0][main]: 6 allocated 6553600 at 0x7f31b1960000
[HCTR][00:19:36.639][ERROR][RK0][main]: 5 allocated 3276800 at 0x7f31ab640000
[HCTR][00:19:36.639][ERROR][RK0][main]: 5 allocated 6553600 at 0x7f31ab960000
[HCTR][00:19:36.639][ERROR][RK0][tid #139843732502272]: 2 allocated 3276800 at 0x7f1cdb520000
[HCTR][00:19:36.639][ERROR][RK0][tid #139843321456384]: 4 allocated 3276800 at 0x7f1cdf520000
[HCTR][00:19:36.639][ERROR][RK0][tid #139843732502272]: 2 allocated 6553600 at 0x7f31b1000000
[HCTR][00:19:36.640][ERROR][RK0][tid #139843732502272]: 2 allocated 3276800 at 0x7f31b1640000
[HCTR][00:19:36.640][ERROR][RK0][tid #139843321456384]: 4 allocated 6553600 at 0x7f31ab000000
[HCTR][00:19:36.640][ERROR][RK0][tid #139843732502272]: 2 allocated 6553600 at 0x7f31b1960000
[HCTR][00:19:36.640][ERROR][RK0][tid #139843321456384]: 4 allocated 3276800 at 0x7f31ab640000
[HCTR][00:19:36.640][ERROR][RK0][tid #139843321456384]: 4 allocated 6553600 at 0x7f31ab960000
[HCTR][00:19:36.640][ERROR][RK0][main]: 1 allocated 3276800 at 0x7f1ce3520000
[HCTR][00:19:36.640][ERROR][RK0][main]: 1 allocated 6553600 at 0x7f31b3000000
[HCTR][00:19:36.640][ERROR][RK0][main]: 1 allocated 3276800 at 0x7f31b3640000
[HCTR][00:19:36.640][ERROR][RK0][main]: 1 allocated 6553600 at 0x7f31b3960000
[HCTR][00:19:36.642][ERROR][RK0][tid #139843657000704]: 0 allocated 3276800 at 0x7f31b2d20000
[HCTR][00:19:36.642][ERROR][RK0][tid #139843657000704]: 0 allocated 6553600 at 0x7f31b3200000
[HCTR][00:19:36.642][ERROR][RK0][tid #139843657000704]: 0 allocated 3276800 at 0x7f31b3f0e800
[HCTR][00:19:36.642][ERROR][RK0][tid #139843657000704]: 0 allocated 6553600 at 0x7f31b422e800








