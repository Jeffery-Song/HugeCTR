2022-12-12 05:02:53.433270: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.440244: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.446326: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.451554: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.457152: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.468210: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.475964: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.489054: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.540684: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.542847: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.544282: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.545760: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.546169: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.547062: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.547995: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.548630: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.549645: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.550270: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.551305: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.551736: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.552914: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.553386: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.554517: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.554911: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.556290: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.556394: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.557977: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.558061: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.559561: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.560648: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.561713: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.562651: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.565066: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.566307: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.567388: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.568418: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.569532: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.570825: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.572529: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.573164: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.573879: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.574794: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.576094: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.577159: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.578237: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.579302: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.579734: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 05:02:53.580374: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.581375: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.586901: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.587960: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.588946: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.589074: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.590500: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.590637: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.592502: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.592598: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.594808: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.594864: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.597467: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.597608: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.597766: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.600244: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.600500: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.600833: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.603592: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.603963: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.604612: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.606267: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.606639: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.607442: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.607839: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.607967: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.609051: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.609505: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.610550: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.611027: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.611147: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.612369: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.612681: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.613799: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.613979: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.614490: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.615555: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.615829: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.617031: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.617127: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.617813: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.618878: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.619538: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.619916: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.620506: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.622994: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.623512: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.624371: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.625053: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.625480: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.626744: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.626858: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.627196: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.648152: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.663109: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.669729: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.670039: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.670103: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.670175: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.671217: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.671298: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.672419: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.673616: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.674237: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.674371: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.676028: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.676070: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.677065: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.678034: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.679640: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.679656: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.680863: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.680966: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.682846: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.683829: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.684496: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.684616: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.685533: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.685809: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.687064: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.688155: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.688651: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.688815: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.690216: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.690393: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.691331: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.692470: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.693282: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.693566: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.694832: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.695051: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.695821: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.697211: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.697704: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.697948: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.699022: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.699250: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.700131: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.701290: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.701776: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.702309: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.702919: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.703100: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.704018: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.705505: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.706366: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.706630: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.706689: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.706890: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.707843: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.709297: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.710671: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.711018: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.711045: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.711585: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.712975: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.714609: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.715572: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.715586: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.715832: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.716049: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.717233: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.718762: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.719999: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.720187: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.720197: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.720340: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.721517: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.723113: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.723343: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.723988: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.724196: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.724327: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.724611: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.725844: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.727586: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.727938: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.728732: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.728972: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.729109: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.729236: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.730467: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.732495: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.732745: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.733170: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.733559: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.733726: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.733743: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.735009: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.737145: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.737783: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.738257: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.738272: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.738423: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.739827: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.740224: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.742801: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.743187: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.743553: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.744792: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.745125: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.745543: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.746691: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.747219: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.747451: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.748141: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 05:02:53.748604: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.749044: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.749781: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.750715: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.751151: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.751458: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.753071: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.753350: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.753758: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.754676: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.755336: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.755364: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.757069: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.757109: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.757531: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.757828: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.759268: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.760505: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.760636: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.762545: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.762696: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.762718: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.762878: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.763982: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.765010: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.765506: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.766880: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.766980: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.767178: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.767250: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.768046: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.769634: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.769825: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.771100: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.771434: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.771510: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.772878: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.775794: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.775831: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.776410: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.778172: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 05:02:53.778430: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 05:02:53.778759: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 05:02:53.780315: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.782240: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.782782: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 05:02:53.782885: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 05:02:53.784028: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.786642: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.787651: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.787891: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.788127: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.790769: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.791827: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.791997: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.792541: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.792677: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.792687: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.795023: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.796248: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.796333: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.796962: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.797033: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.797176: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.799586: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.800255: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.800384: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.803527: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.838215: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.842811: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.877565: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.886925: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 05:02:53.896050: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.900627: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:53.904969: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:54.940953: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:54.941587: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:54.942118: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:54.942754: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 05:02:54.942809: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-12 05:02:54.962156: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:54.962798: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:54.963315: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:54.963897: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:54.964402: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:54.965083: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-12 05:02:55.011474: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 05:02:55.011689: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 05:02:55.063879: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12340
2022-12-12 05:02:55.198655: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:55.199495: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:55.200033: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:55.200498: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 05:02:55.200548: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-12 05:02:55.219391: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:55.223698: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:55.224205: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:55.224979: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:55.225850: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:55.226319: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:1/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-12 05:02:55.249441: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:55.250079: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:55.250839: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:55.251321: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 05:02:55.251375: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-12 05:02:55.254743: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:55.255943: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:55.256474: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:55.257390: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 05:02:55.257461: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-12 05:02:55.269278: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:55.269917: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:55.270428: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:55.271021: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:55.271551: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:55.272034: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:2/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-12 05:02:55.274281: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:55.274895: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:55.275622: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:55.276196: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:55.276722: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:55.277185: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:5/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-12 05:02:55.279818: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:55.280417: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:55.281289: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:55.281810: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 05:02:55.281862: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-12 05:02:55.287879: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:55.288453: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:55.288980: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:55.289443: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 05:02:55.289484: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-12 05:02:55.298280: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:55.298910: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:55.299772: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:55.300379: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:55.300920: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:55.301393: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:4/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-12 05:02:55.305973: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:55.306621: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:55.307148: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:55.307745: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:55.308256: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:55.308743: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:7/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-12 05:02:55.312292: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:55.312920: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:55.313439: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:55.314163: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 05:02:55.314234: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-12 05:02:55.322473: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 05:02:55.322665: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 05:02:55.324490: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12345
2022-12-12 05:02:55.325913: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 05:02:55.326093: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 05:02:55.327822: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12341
2022-12-12 05:02:55.330925: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:55.331631: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:55.332147: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:55.332755: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:55.338849: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:55.339384: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:6/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-12 05:02:55.345468: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:55.346095: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:55.346623: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:55.347091: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 05:02:55.347156: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-12 05:02:55.347238: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 05:02:55.347390: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 05:02:55.349343: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12344
2022-12-12 05:02:55.354143: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 05:02:55.354347: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 05:02:55.356125: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12347
2022-12-12 05:02:55.360590: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 05:02:55.360739: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 05:02:55.362565: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12342
2022-12-12 05:02:55.364197: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:55.364870: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:55.365374: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:55.365960: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:55.366468: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:02:55.366940: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:3/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-12 05:02:55.385086: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 05:02:55.385269: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 05:02:55.387141: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12346
2022-12-12 05:02:55.411596: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 05:02:55.411789: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 05:02:55.413459: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12343
[HCTR][05:02:56.679][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][05:02:56.679][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][05:02:56.679][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][05:02:56.679][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][05:02:56.679][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][05:02:56.679][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][05:02:56.685][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][05:02:56.685][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 1it [00:01,  1.60s/it]warmup run: 1it [00:01,  1.58s/it]warmup run: 99it [00:01, 81.05it/s]warmup run: 1it [00:01,  1.58s/it]warmup run: 1it [00:01,  1.52s/it]warmup run: 1it [00:01,  1.50s/it]warmup run: 98it [00:01, 81.11it/s]warmup run: 196it [00:01, 174.24it/s]warmup run: 96it [00:01, 79.33it/s]warmup run: 1it [00:01,  1.48s/it]warmup run: 99it [00:01, 84.73it/s]warmup run: 1it [00:01,  1.48s/it]warmup run: 1it [00:01,  1.50s/it]warmup run: 95it [00:01, 82.19it/s]warmup run: 196it [00:01, 176.21it/s]warmup run: 294it [00:01, 279.08it/s]warmup run: 178it [00:01, 157.52it/s]warmup run: 96it [00:01, 84.10it/s]warmup run: 197it [00:01, 182.50it/s]warmup run: 95it [00:01, 83.40it/s]warmup run: 75it [00:01, 64.95it/s]warmup run: 189it [00:01, 176.78it/s]warmup run: 294it [00:01, 281.45it/s]warmup run: 392it [00:01, 388.52it/s]warmup run: 266it [00:01, 252.07it/s]warmup run: 191it [00:01, 180.68it/s]warmup run: 295it [00:01, 289.88it/s]warmup run: 189it [00:01, 178.93it/s]warmup run: 174it [00:01, 167.07it/s]warmup run: 284it [00:01, 281.99it/s]warmup run: 392it [00:01, 391.25it/s]warmup run: 490it [00:02, 496.40it/s]warmup run: 365it [00:01, 368.21it/s]warmup run: 287it [00:01, 287.72it/s]warmup run: 394it [00:01, 402.72it/s]warmup run: 286it [00:01, 287.57it/s]warmup run: 276it [00:01, 283.05it/s]warmup run: 380it [00:01, 391.91it/s]warmup run: 490it [00:02, 499.38it/s]warmup run: 590it [00:02, 600.12it/s]warmup run: 464it [00:02, 482.07it/s]warmup run: 384it [00:01, 399.43it/s]warmup run: 493it [00:02, 511.88it/s]warmup run: 384it [00:01, 400.77it/s]warmup run: 377it [00:01, 401.05it/s]warmup run: 477it [00:02, 500.08it/s]warmup run: 591it [00:02, 605.06it/s]warmup run: 690it [00:02, 690.36it/s]warmup run: 565it [00:02, 590.73it/s]warmup run: 481it [00:01, 507.33it/s]warmup run: 593it [00:02, 614.15it/s]warmup run: 481it [00:01, 508.63it/s]warmup run: 477it [00:02, 513.82it/s]warmup run: 575it [00:02, 600.94it/s]warmup run: 694it [00:02, 702.01it/s]warmup run: 790it [00:02, 764.52it/s]warmup run: 666it [00:02, 685.54it/s]warmup run: 579it [00:02, 606.99it/s]warmup run: 693it [00:02, 702.72it/s]warmup run: 580it [00:02, 610.64it/s]warmup run: 577it [00:02, 616.93it/s]warmup run: 674it [00:02, 689.85it/s]warmup run: 797it [00:02, 782.61it/s]warmup run: 889it [00:02, 821.68it/s]warmup run: 767it [00:02, 763.57it/s]warmup run: 676it [00:02, 690.24it/s]warmup run: 793it [00:02, 775.70it/s]warmup run: 677it [00:02, 694.03it/s]warmup run: 678it [00:02, 707.10it/s]warmup run: 771it [00:02, 757.78it/s]warmup run: 900it [00:02, 845.10it/s]warmup run: 988it [00:02, 865.91it/s]warmup run: 868it [00:02, 826.34it/s]warmup run: 774it [00:02, 761.47it/s]warmup run: 893it [00:02, 833.33it/s]warmup run: 772it [00:02, 757.90it/s]warmup run: 779it [00:02, 781.53it/s]warmup run: 866it [00:02, 807.53it/s]warmup run: 1003it [00:02, 893.98it/s]warmup run: 1088it [00:02, 902.02it/s]warmup run: 968it [00:02, 872.42it/s]warmup run: 871it [00:02, 814.22it/s]warmup run: 995it [00:02, 883.88it/s]warmup run: 870it [00:02, 814.59it/s]warmup run: 879it [00:02, 837.43it/s]warmup run: 1106it [00:02, 930.19it/s]warmup run: 961it [00:02, 838.70it/s]warmup run: 1187it [00:02, 919.95it/s]warmup run: 1068it [00:02, 906.38it/s]warmup run: 967it [00:02, 851.97it/s]warmup run: 1097it [00:02, 921.45it/s]warmup run: 967it [00:02, 856.79it/s]warmup run: 980it [00:02, 882.53it/s]warmup run: 1056it [00:02, 869.55it/s]warmup run: 1208it [00:02, 947.74it/s]warmup run: 1286it [00:02, 939.76it/s]warmup run: 1167it [00:02, 927.22it/s]warmup run: 1064it [00:02, 883.22it/s]warmup run: 1197it [00:02, 941.26it/s]warmup run: 1066it [00:02, 893.93it/s]warmup run: 1081it [00:02, 915.82it/s]warmup run: 1154it [00:02, 898.42it/s]warmup run: 1309it [00:02, 962.15it/s]warmup run: 1385it [00:03, 953.85it/s]warmup run: 1266it [00:02, 937.85it/s]warmup run: 1161it [00:02, 907.87it/s]warmup run: 1300it [00:02, 965.24it/s]warmup run: 1165it [00:02, 919.96it/s]warmup run: 1181it [00:02, 938.99it/s]warmup run: 1252it [00:02, 920.62it/s]warmup run: 1411it [00:02, 978.72it/s]warmup run: 1484it [00:03, 964.06it/s]warmup run: 1366it [00:02, 953.51it/s]warmup run: 1260it [00:02, 928.99it/s]warmup run: 1403it [00:02, 984.01it/s]warmup run: 1263it [00:02, 934.27it/s]warmup run: 1281it [00:02, 955.48it/s]warmup run: 1514it [00:03, 992.94it/s]warmup run: 1349it [00:02, 932.43it/s]warmup run: 1583it [00:03, 966.75it/s]warmup run: 1466it [00:03, 964.40it/s]warmup run: 1358it [00:02, 941.50it/s]warmup run: 1507it [00:03, 999.67it/s]warmup run: 1361it [00:02, 946.67it/s]warmup run: 1381it [00:02, 964.43it/s]warmup run: 1445it [00:03, 939.88it/s]warmup run: 1618it [00:03, 1005.80it/s]warmup run: 1682it [00:03, 966.47it/s]warmup run: 1565it [00:03, 968.28it/s]warmup run: 1455it [00:02, 942.26it/s]warmup run: 1611it [00:03, 1009.59it/s]warmup run: 1461it [00:02, 962.33it/s]warmup run: 1481it [00:03, 972.14it/s]warmup run: 1541it [00:03, 944.81it/s]warmup run: 1723it [00:03, 1016.64it/s]warmup run: 1781it [00:03, 972.44it/s]warmup run: 1664it [00:03, 972.53it/s]warmup run: 1553it [00:03, 953.03it/s]warmup run: 1715it [00:03, 1018.10it/s]warmup run: 1565it [00:03, 982.87it/s]warmup run: 1583it [00:03, 983.42it/s]warmup run: 1637it [00:03, 946.69it/s]warmup run: 1828it [00:03, 1023.68it/s]warmup run: 1880it [00:03, 975.43it/s]warmup run: 1763it [00:03, 970.75it/s]warmup run: 1652it [00:03, 963.04it/s]warmup run: 1820it [00:03, 1025.52it/s]warmup run: 1670it [00:03, 1001.59it/s]warmup run: 1684it [00:03, 989.89it/s]warmup run: 1932it [00:03, 1028.09it/s]warmup run: 1733it [00:03, 947.07it/s]warmup run: 1980it [00:03, 982.01it/s]warmup run: 1861it [00:03, 972.13it/s]warmup run: 1751it [00:03, 968.91it/s]warmup run: 1924it [00:03, 1026.93it/s]warmup run: 1774it [00:03, 1011.72it/s]warmup run: 1785it [00:03, 995.78it/s]warmup run: 2042it [00:03, 1048.48it/s]warmup run: 1829it [00:03, 948.46it/s]warmup run: 2094it [00:03, 1029.04it/s]warmup run: 1959it [00:03, 973.27it/s]warmup run: 1849it [00:03, 968.70it/s]warmup run: 2032it [00:03, 1040.13it/s]warmup run: 1879it [00:03, 1022.90it/s]warmup run: 1886it [00:03, 998.54it/s]warmup run: 2164it [00:03, 1099.00it/s]warmup run: 1925it [00:03, 948.52it/s]warmup run: 2213it [00:03, 1077.04it/s]warmup run: 2070it [00:03, 1012.27it/s]warmup run: 1947it [00:03, 968.07it/s]warmup run: 2153it [00:03, 1088.50it/s]warmup run: 1984it [00:03, 1030.10it/s]warmup run: 1987it [00:03, 997.28it/s]warmup run: 2285it [00:03, 1131.15it/s]warmup run: 2026it [00:03, 964.52it/s]warmup run: 2333it [00:03, 1111.87it/s]warmup run: 2189it [00:03, 1064.06it/s]warmup run: 2056it [00:03, 1003.07it/s]warmup run: 2274it [00:03, 1122.28it/s]warmup run: 2104it [00:03, 1080.16it/s]warmup run: 2103it [00:03, 1043.85it/s]warmup run: 2406it [00:03, 1154.15it/s]warmup run: 2146it [00:03, 1033.84it/s]warmup run: 2453it [00:04, 1136.41it/s]warmup run: 2308it [00:03, 1100.54it/s]warmup run: 2177it [00:03, 1063.17it/s]warmup run: 2395it [00:03, 1146.06it/s]warmup run: 2228it [00:03, 1126.40it/s]warmup run: 2223it [00:03, 1087.81it/s]warmup run: 2527it [00:03, 1170.37it/s]warmup run: 2266it [00:03, 1083.06it/s]warmup run: 2573it [00:04, 1154.40it/s]warmup run: 2427it [00:04, 1126.02it/s]warmup run: 2298it [00:03, 1105.60it/s]warmup run: 2516it [00:03, 1163.74it/s]warmup run: 2352it [00:03, 1158.51it/s]warmup run: 2343it [00:03, 1118.93it/s]warmup run: 2647it [00:04, 1177.76it/s]warmup run: 2387it [00:03, 1118.64it/s]warmup run: 2693it [00:04, 1166.55it/s]warmup run: 2546it [00:04, 1143.78it/s]warmup run: 2419it [00:03, 1134.63it/s]warmup run: 2636it [00:04, 1172.46it/s]warmup run: 2476it [00:03, 1181.02it/s]warmup run: 2463it [00:03, 1140.43it/s]warmup run: 2768it [00:04, 1185.78it/s]warmup run: 2509it [00:04, 1146.56it/s]warmup run: 2815it [00:04, 1181.36it/s]warmup run: 2664it [00:04, 1153.82it/s]warmup run: 2539it [00:03, 1154.11it/s]warmup run: 2757it [00:04, 1182.26it/s]warmup run: 2600it [00:03, 1196.43it/s]warmup run: 2582it [00:04, 1155.09it/s]warmup run: 2889it [00:04, 1191.16it/s]warmup run: 2632it [00:04, 1169.17it/s]warmup run: 2937it [00:04, 1192.36it/s]warmup run: 2783it [00:04, 1164.17it/s]warmup run: 2659it [00:04, 1164.93it/s]warmup run: 2878it [00:04, 1188.05it/s]warmup run: 2724it [00:04, 1206.98it/s]warmup run: 3000it [00:04, 670.65it/s] warmup run: 2700it [00:04, 1162.28it/s]warmup run: 3000it [00:04, 683.08it/s] warmup run: 2751it [00:04, 1175.05it/s]warmup run: 2902it [00:04, 1170.18it/s]warmup run: 2780it [00:04, 1178.04it/s]warmup run: 2998it [00:04, 1189.87it/s]warmup run: 3000it [00:04, 690.60it/s] warmup run: 2845it [00:04, 1207.66it/s]warmup run: 2819it [00:04, 1170.02it/s]warmup run: 2871it [00:04, 1180.51it/s]warmup run: 3000it [00:04, 664.45it/s] warmup run: 2903it [00:04, 1192.07it/s]warmup run: 2968it [00:04, 1212.34it/s]warmup run: 2938it [00:04, 1174.58it/s]warmup run: 3000it [00:04, 693.71it/s] warmup run: 2991it [00:04, 1185.24it/s]warmup run: 3000it [00:04, 676.18it/s] warmup run: 3000it [00:04, 684.00it/s] warmup run: 3000it [00:04, 685.93it/s] 

warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]





warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   5%|▌         | 164/3000 [00:00<00:01, 1638.38it/s]warmup should be done:   6%|▌         | 166/3000 [00:00<00:01, 1655.40it/s]warmup should be done:   5%|▌         | 159/3000 [00:00<00:01, 1581.98it/s]warmup should be done:   6%|▌         | 165/3000 [00:00<00:01, 1644.26it/s]warmup should be done:   6%|▌         | 167/3000 [00:00<00:01, 1662.99it/s]warmup should be done:   5%|▌         | 156/3000 [00:00<00:01, 1555.00it/s]warmup should be done:   6%|▌         | 169/3000 [00:00<00:01, 1682.01it/s]warmup should be done:   6%|▌         | 166/3000 [00:00<00:01, 1650.54it/s]warmup should be done:  11%|█         | 333/3000 [00:00<00:01, 1660.35it/s]warmup should be done:  11%|█         | 331/3000 [00:00<00:01, 1651.21it/s]warmup should be done:  11%|█         | 330/3000 [00:00<00:01, 1646.00it/s]warmup should be done:  11%|█         | 321/3000 [00:00<00:01, 1600.92it/s]warmup should be done:  11%|█         | 334/3000 [00:00<00:01, 1662.78it/s]warmup should be done:  11%|█▏        | 338/3000 [00:00<00:01, 1682.35it/s]warmup should be done:  10%|█         | 314/3000 [00:00<00:01, 1564.89it/s]warmup should be done:  11%|█         | 332/3000 [00:00<00:01, 1645.39it/s]warmup should be done:  16%|█▌        | 478/3000 [00:00<00:01, 1598.69it/s]warmup should be done:  16%|█▌        | 482/3000 [00:00<00:01, 1600.47it/s]warmup should be done:  16%|█▋        | 495/3000 [00:00<00:01, 1641.87it/s]warmup should be done:  17%|█▋        | 497/3000 [00:00<00:01, 1648.21it/s]warmup should be done:  17%|█▋        | 501/3000 [00:00<00:01, 1658.05it/s]warmup should be done:  17%|█▋        | 507/3000 [00:00<00:01, 1677.99it/s]warmup should be done:  17%|█▋        | 500/3000 [00:00<00:01, 1654.08it/s]warmup should be done:  17%|█▋        | 497/3000 [00:00<00:01, 1640.36it/s]warmup should be done:  21%|██▏       | 641/3000 [00:00<00:01, 1610.78it/s]warmup should be done:  22%|██▏       | 662/3000 [00:00<00:01, 1646.57it/s]warmup should be done:  22%|██▏       | 667/3000 [00:00<00:01, 1658.52it/s]warmup should be done:  22%|██▎       | 675/3000 [00:00<00:01, 1676.54it/s]warmup should be done:  21%|██▏       | 643/3000 [00:00<00:01, 1598.28it/s]warmup should be done:  22%|██▏       | 660/3000 [00:00<00:01, 1636.42it/s]warmup should be done:  22%|██▏       | 666/3000 [00:00<00:01, 1650.84it/s]warmup should be done:  22%|██▏       | 662/3000 [00:00<00:01, 1637.11it/s]warmup should be done:  27%|██▋       | 805/3000 [00:00<00:01, 1618.31it/s]warmup should be done:  28%|██▊       | 828/3000 [00:00<00:01, 1650.16it/s]warmup should be done:  28%|██▊       | 833/3000 [00:00<00:01, 1656.10it/s]warmup should be done:  27%|██▋       | 803/3000 [00:00<00:01, 1595.73it/s]warmup should be done:  28%|██▊       | 843/3000 [00:00<00:01, 1671.12it/s]warmup should be done:  27%|██▋       | 824/3000 [00:00<00:01, 1631.75it/s]warmup should be done:  28%|██▊       | 832/3000 [00:00<00:01, 1645.46it/s]warmup should be done:  28%|██▊       | 826/3000 [00:00<00:01, 1633.91it/s]warmup should be done:  32%|███▏      | 968/3000 [00:00<00:01, 1619.13it/s]warmup should be done:  33%|███▎      | 994/3000 [00:00<00:01, 1650.15it/s]warmup should be done:  32%|███▏      | 964/3000 [00:00<00:01, 1599.56it/s]warmup should be done:  33%|███▎      | 999/3000 [00:00<00:01, 1650.48it/s]warmup should be done:  34%|███▎      | 1011/3000 [00:00<00:01, 1666.88it/s]warmup should be done:  33%|███▎      | 997/3000 [00:00<00:01, 1645.04it/s]warmup should be done:  33%|███▎      | 988/3000 [00:00<00:01, 1625.46it/s]warmup should be done:  33%|███▎      | 990/3000 [00:00<00:01, 1625.72it/s]warmup should be done:  38%|███▊      | 1132/3000 [00:00<00:01, 1623.17it/s]warmup should be done:  39%|███▊      | 1161/3000 [00:00<00:01, 1653.88it/s]warmup should be done:  38%|███▊      | 1126/3000 [00:00<00:01, 1603.14it/s]warmup should be done:  39%|███▉      | 1165/3000 [00:00<00:01, 1649.53it/s]warmup should be done:  39%|███▉      | 1178/3000 [00:00<00:01, 1664.59it/s]warmup should be done:  39%|███▊      | 1162/3000 [00:00<00:01, 1643.35it/s]warmup should be done:  38%|███▊      | 1151/3000 [00:00<00:01, 1625.79it/s]warmup should be done:  38%|███▊      | 1153/3000 [00:00<00:01, 1626.73it/s]warmup should be done:  43%|████▎     | 1296/3000 [00:00<00:01, 1625.83it/s]warmup should be done:  43%|████▎     | 1288/3000 [00:00<00:01, 1607.81it/s]warmup should be done:  44%|████▍     | 1328/3000 [00:00<00:01, 1655.82it/s]warmup should be done:  44%|████▍     | 1330/3000 [00:00<00:01, 1644.45it/s]warmup should be done:  44%|████▍     | 1327/3000 [00:00<00:01, 1643.87it/s]warmup should be done:  44%|████▍     | 1314/3000 [00:00<00:01, 1625.82it/s]warmup should be done:  44%|████▍     | 1316/3000 [00:00<00:01, 1627.43it/s]warmup should be done:  45%|████▍     | 1345/3000 [00:00<00:01, 1629.64it/s]warmup should be done:  49%|████▊     | 1460/3000 [00:00<00:00, 1628.43it/s]warmup should be done:  48%|████▊     | 1450/3000 [00:00<00:00, 1610.63it/s]warmup should be done:  50%|████▉     | 1494/3000 [00:00<00:00, 1655.81it/s]warmup should be done:  49%|████▉     | 1477/3000 [00:00<00:00, 1626.27it/s]warmup should be done:  49%|████▉     | 1479/3000 [00:00<00:00, 1627.58it/s]warmup should be done:  50%|████▉     | 1495/3000 [00:00<00:00, 1640.12it/s]warmup should be done:  50%|████▉     | 1492/3000 [00:00<00:00, 1627.28it/s]warmup should be done:  50%|█████     | 1509/3000 [00:00<00:00, 1601.53it/s]warmup should be done:  54%|█████▍    | 1624/3000 [00:01<00:00, 1630.23it/s]warmup should be done:  54%|█████▎    | 1612/3000 [00:01<00:00, 1613.10it/s]warmup should be done:  55%|█████▌    | 1660/3000 [00:01<00:00, 1656.28it/s]warmup should be done:  55%|█████▍    | 1640/3000 [00:01<00:00, 1626.67it/s]warmup should be done:  55%|█████▍    | 1643/3000 [00:01<00:00, 1628.73it/s]warmup should be done:  55%|█████▌    | 1660/3000 [00:01<00:00, 1633.56it/s]warmup should be done:  55%|█████▌    | 1657/3000 [00:01<00:00, 1634.04it/s]warmup should be done:  56%|█████▌    | 1670/3000 [00:01<00:00, 1584.25it/s]warmup should be done:  59%|█████▉    | 1774/3000 [00:01<00:00, 1614.12it/s]warmup should be done:  61%|██████    | 1826/3000 [00:01<00:00, 1657.02it/s]warmup should be done:  60%|█████▉    | 1788/3000 [00:01<00:00, 1630.77it/s]warmup should be done:  60%|██████    | 1803/3000 [00:01<00:00, 1627.30it/s]warmup should be done:  60%|██████    | 1807/3000 [00:01<00:00, 1629.62it/s]warmup should be done:  61%|██████    | 1823/3000 [00:01<00:00, 1639.07it/s]warmup should be done:  61%|██████    | 1824/3000 [00:01<00:00, 1622.82it/s]warmup should be done:  61%|██████    | 1829/3000 [00:01<00:00, 1572.52it/s]warmup should be done:  66%|██████▋   | 1992/3000 [00:01<00:00, 1657.75it/s]warmup should be done:  65%|██████▍   | 1936/3000 [00:01<00:00, 1614.63it/s]warmup should be done:  65%|██████▌   | 1952/3000 [00:01<00:00, 1631.11it/s]warmup should be done:  66%|██████▌   | 1966/3000 [00:01<00:00, 1626.85it/s]warmup should be done:  66%|██████▌   | 1970/3000 [00:01<00:00, 1629.57it/s]warmup should be done:  66%|██████▋   | 1988/3000 [00:01<00:00, 1642.25it/s]warmup should be done:  66%|██████▌   | 1987/3000 [00:01<00:00, 1617.01it/s]warmup should be done:  66%|██████▌   | 1987/3000 [00:01<00:00, 1568.15it/s]warmup should be done:  70%|██████▉   | 2098/3000 [00:01<00:00, 1616.07it/s]warmup should be done:  72%|███████▏  | 2158/3000 [00:01<00:00, 1656.57it/s]warmup should be done:  71%|███████   | 2116/3000 [00:01<00:00, 1631.48it/s]warmup should be done:  71%|███████   | 2129/3000 [00:01<00:00, 1626.73it/s]warmup should be done:  71%|███████   | 2133/3000 [00:01<00:00, 1629.32it/s]warmup should be done:  72%|███████▏  | 2153/3000 [00:01<00:00, 1643.23it/s]warmup should be done:  72%|███████▏  | 2149/3000 [00:01<00:00, 1613.54it/s]warmup should be done:  72%|███████▏  | 2145/3000 [00:01<00:00, 1570.61it/s]warmup should be done:  77%|███████▋  | 2324/3000 [00:01<00:00, 1654.82it/s]warmup should be done:  75%|███████▌  | 2260/3000 [00:01<00:00, 1612.81it/s]warmup should be done:  76%|███████▌  | 2280/3000 [00:01<00:00, 1627.30it/s]warmup should be done:  76%|███████▋  | 2292/3000 [00:01<00:00, 1623.17it/s]warmup should be done:  77%|███████▋  | 2296/3000 [00:01<00:00, 1625.74it/s]warmup should be done:  77%|███████▋  | 2318/3000 [00:01<00:00, 1645.08it/s]warmup should be done:  77%|███████▋  | 2311/3000 [00:01<00:00, 1608.71it/s]warmup should be done:  77%|███████▋  | 2303/3000 [00:01<00:00, 1569.69it/s]warmup should be done:  81%|████████  | 2422/3000 [00:01<00:00, 1614.26it/s]warmup should be done:  83%|████████▎ | 2490/3000 [00:01<00:00, 1650.54it/s]warmup should be done:  81%|████████▏ | 2443/3000 [00:01<00:00, 1626.23it/s]warmup should be done:  82%|████████▏ | 2455/3000 [00:01<00:00, 1622.16it/s]warmup should be done:  82%|████████▏ | 2459/3000 [00:01<00:00, 1624.63it/s]warmup should be done:  83%|████████▎ | 2483/3000 [00:01<00:00, 1644.14it/s]warmup should be done:  82%|████████▏ | 2472/3000 [00:01<00:00, 1606.84it/s]warmup should be done:  82%|████████▏ | 2466/3000 [00:01<00:00, 1586.26it/s]warmup should be done:  86%|████████▌ | 2584/3000 [00:01<00:00, 1614.27it/s]warmup should be done:  89%|████████▊ | 2657/3000 [00:01<00:00, 1653.30it/s]warmup should be done:  87%|████████▋ | 2606/3000 [00:01<00:00, 1625.90it/s]warmup should be done:  87%|████████▋ | 2618/3000 [00:01<00:00, 1623.78it/s]warmup should be done:  87%|████████▋ | 2622/3000 [00:01<00:00, 1623.86it/s]warmup should be done:  88%|████████▊ | 2648/3000 [00:01<00:00, 1642.78it/s]warmup should be done:  88%|████████▊ | 2633/3000 [00:01<00:00, 1606.28it/s]warmup should be done:  88%|████████▊ | 2630/3000 [00:01<00:00, 1600.60it/s]warmup should be done:  92%|█████████▏| 2747/3000 [00:01<00:00, 1616.11it/s]warmup should be done:  94%|█████████▍| 2824/3000 [00:01<00:00, 1657.20it/s]warmup should be done:  92%|█████████▏| 2769/3000 [00:01<00:00, 1624.86it/s]warmup should be done:  93%|█████████▎| 2783/3000 [00:01<00:00, 1629.17it/s]warmup should be done:  93%|█████████▎| 2785/3000 [00:01<00:00, 1623.35it/s]warmup should be done:  94%|█████████▍| 2813/3000 [00:01<00:00, 1642.13it/s]warmup should be done:  93%|█████████▎| 2794/3000 [00:01<00:00, 1606.14it/s]warmup should be done:  93%|█████████▎| 2794/3000 [00:01<00:00, 1612.11it/s]warmup should be done:  97%|█████████▋| 2913/3000 [00:01<00:00, 1627.37it/s]warmup should be done: 100%|█████████▉| 2992/3000 [00:01<00:00, 1661.57it/s]warmup should be done:  98%|█████████▊| 2932/3000 [00:01<00:00, 1623.71it/s]warmup should be done:  98%|█████████▊| 2950/3000 [00:01<00:00, 1638.75it/s]warmup should be done:  98%|█████████▊| 2949/3000 [00:01<00:00, 1626.23it/s]warmup should be done:  99%|█████████▉| 2981/3000 [00:01<00:00, 1650.70it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1654.92it/s]warmup should be done:  99%|█████████▊| 2957/3000 [00:01<00:00, 1611.19it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1644.52it/s]warmup should be done:  99%|█████████▊| 2962/3000 [00:01<00:00, 1630.85it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1630.55it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1628.53it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1627.62it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1620.90it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1619.68it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1612.80it/s]




warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]


warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   6%|▌         | 171/3000 [00:00<00:01, 1706.20it/s]warmup should be done:   6%|▌         | 170/3000 [00:00<00:01, 1695.64it/s]warmup should be done:   6%|▌         | 165/3000 [00:00<00:01, 1647.74it/s]warmup should be done:   6%|▌         | 168/3000 [00:00<00:01, 1673.98it/s]warmup should be done:   6%|▌         | 166/3000 [00:00<00:01, 1652.14it/s]warmup should be done:   6%|▌         | 171/3000 [00:00<00:01, 1701.19it/s]warmup should be done:   6%|▌         | 167/3000 [00:00<00:01, 1661.50it/s]warmup should be done:   6%|▌         | 171/3000 [00:00<00:01, 1700.39it/s]warmup should be done:  11%|█         | 331/3000 [00:00<00:01, 1653.25it/s]warmup should be done:  11%|█         | 336/3000 [00:00<00:01, 1675.41it/s]warmup should be done:  11%|█         | 334/3000 [00:00<00:01, 1665.80it/s]warmup should be done:  11%|█▏        | 340/3000 [00:00<00:01, 1692.35it/s]warmup should be done:  11%|█         | 333/3000 [00:00<00:01, 1657.68it/s]warmup should be done:  11%|█▏        | 342/3000 [00:00<00:01, 1701.69it/s]warmup should be done:  11%|█▏        | 342/3000 [00:00<00:01, 1695.55it/s]warmup should be done:  11%|█▏        | 342/3000 [00:00<00:01, 1691.81it/s]warmup should be done:  17%|█▋        | 497/3000 [00:00<00:01, 1654.53it/s]warmup should be done:  17%|█▋        | 502/3000 [00:00<00:01, 1668.81it/s]warmup should be done:  17%|█▋        | 505/3000 [00:00<00:01, 1678.09it/s]warmup should be done:  17%|█▋        | 513/3000 [00:00<00:01, 1703.78it/s]warmup should be done:  17%|█▋        | 500/3000 [00:00<00:01, 1659.96it/s]warmup should be done:  17%|█▋        | 512/3000 [00:00<00:01, 1693.30it/s]warmup should be done:  17%|█▋        | 512/3000 [00:00<00:01, 1693.09it/s]warmup should be done:  17%|█▋        | 510/3000 [00:00<00:01, 1683.61it/s]warmup should be done:  22%|██▏       | 663/3000 [00:00<00:01, 1655.02it/s]warmup should be done:  22%|██▏       | 674/3000 [00:00<00:01, 1679.25it/s]warmup should be done:  22%|██▏       | 666/3000 [00:00<00:01, 1657.36it/s]warmup should be done:  23%|██▎       | 684/3000 [00:00<00:01, 1699.92it/s]warmup should be done:  23%|██▎       | 679/3000 [00:00<00:01, 1684.55it/s]warmup should be done:  23%|██▎       | 683/3000 [00:00<00:01, 1697.07it/s]warmup should be done:  23%|██▎       | 682/3000 [00:00<00:01, 1690.86it/s]warmup should be done:  22%|██▏       | 669/3000 [00:00<00:01, 1649.82it/s]warmup should be done:  28%|██▊       | 829/3000 [00:00<00:01, 1655.58it/s]warmup should be done:  28%|██▊       | 842/3000 [00:00<00:01, 1677.54it/s]warmup should be done:  28%|██▊       | 832/3000 [00:00<00:01, 1656.35it/s]warmup should be done:  28%|██▊       | 848/3000 [00:00<00:01, 1685.76it/s]warmup should be done:  28%|██▊       | 854/3000 [00:00<00:01, 1694.86it/s]warmup should be done:  28%|██▊       | 854/3000 [00:00<00:01, 1699.15it/s]warmup should be done:  28%|██▊       | 852/3000 [00:00<00:01, 1690.55it/s]warmup should be done:  28%|██▊       | 835/3000 [00:00<00:01, 1640.82it/s]warmup should be done:  33%|███▎      | 995/3000 [00:00<00:01, 1656.39it/s]warmup should be done:  34%|███▎      | 1011/3000 [00:00<00:01, 1678.24it/s]warmup should be done:  33%|███▎      | 999/3000 [00:00<00:01, 1657.99it/s]warmup should be done:  34%|███▍      | 1024/3000 [00:00<00:01, 1693.54it/s]warmup should be done:  34%|███▍      | 1017/3000 [00:00<00:01, 1681.89it/s]warmup should be done:  34%|███▍      | 1024/3000 [00:00<00:01, 1695.77it/s]warmup should be done:  34%|███▍      | 1022/3000 [00:00<00:01, 1686.31it/s]warmup should be done:  33%|███▎      | 1000/3000 [00:00<00:01, 1641.12it/s]warmup should be done:  39%|███▊      | 1161/3000 [00:00<00:01, 1656.64it/s]warmup should be done:  39%|███▉      | 1180/3000 [00:00<00:01, 1679.48it/s]warmup should be done:  39%|███▉      | 1166/3000 [00:00<00:01, 1659.22it/s]warmup should be done:  40%|███▉      | 1195/3000 [00:00<00:01, 1695.72it/s]warmup should be done:  40%|███▉      | 1194/3000 [00:00<00:01, 1694.19it/s]warmup should be done:  40%|███▉      | 1186/3000 [00:00<00:01, 1675.75it/s]warmup should be done:  40%|███▉      | 1191/3000 [00:00<00:01, 1681.03it/s]warmup should be done:  39%|███▉      | 1165/3000 [00:00<00:01, 1642.89it/s]warmup should be done:  44%|████▍     | 1327/3000 [00:00<00:01, 1654.28it/s]warmup should be done:  45%|████▍     | 1349/3000 [00:00<00:00, 1679.80it/s]warmup should be done:  44%|████▍     | 1332/3000 [00:00<00:01, 1655.30it/s]warmup should be done:  46%|████▌     | 1365/3000 [00:00<00:00, 1696.89it/s]warmup should be done:  46%|████▌     | 1367/3000 [00:00<00:00, 1704.04it/s]warmup should be done:  45%|████▌     | 1354/3000 [00:00<00:00, 1674.39it/s]warmup should be done:  45%|████▌     | 1360/3000 [00:00<00:00, 1683.21it/s]warmup should be done:  44%|████▍     | 1330/3000 [00:00<00:01, 1643.27it/s]warmup should be done:  50%|████▉     | 1493/3000 [00:00<00:00, 1648.18it/s]warmup should be done:  51%|█████     | 1517/3000 [00:00<00:00, 1679.16it/s]warmup should be done:  50%|████▉     | 1498/3000 [00:00<00:00, 1656.32it/s]warmup should be done:  51%|█████▏    | 1539/3000 [00:00<00:00, 1708.52it/s]warmup should be done:  51%|█████     | 1535/3000 [00:00<00:00, 1692.72it/s]warmup should be done:  51%|█████     | 1532/3000 [00:00<00:00, 1692.55it/s]warmup should be done:  51%|█████     | 1522/3000 [00:00<00:00, 1670.30it/s]warmup should be done:  50%|████▉     | 1495/3000 [00:00<00:00, 1644.19it/s]warmup should be done:  56%|█████▌    | 1686/3000 [00:01<00:00, 1680.95it/s]warmup should be done:  56%|█████▌    | 1665/3000 [00:01<00:00, 1657.84it/s]warmup should be done:  57%|█████▋    | 1711/3000 [00:01<00:00, 1711.31it/s]warmup should be done:  57%|█████▋    | 1705/3000 [00:01<00:00, 1687.77it/s]warmup should be done:  57%|█████▋    | 1704/3000 [00:01<00:00, 1699.69it/s]warmup should be done:  56%|█████▋    | 1690/3000 [00:01<00:00, 1672.22it/s]warmup should be done:  55%|█████▌    | 1663/3000 [00:01<00:00, 1653.57it/s]warmup should be done:  55%|█████▌    | 1658/3000 [00:01<00:00, 1560.41it/s]warmup should be done:  62%|██████▏   | 1855/3000 [00:01<00:00, 1682.49it/s]warmup should be done:  61%|██████    | 1832/3000 [00:01<00:00, 1660.06it/s]warmup should be done:  63%|██████▎   | 1884/3000 [00:01<00:00, 1713.97it/s]warmup should be done:  62%|██████▏   | 1874/3000 [00:01<00:00, 1686.99it/s]warmup should be done:  63%|██████▎   | 1876/3000 [00:01<00:00, 1704.38it/s]warmup should be done:  62%|██████▏   | 1859/3000 [00:01<00:00, 1675.93it/s]warmup should be done:  61%|██████    | 1833/3000 [00:01<00:00, 1667.55it/s]warmup should be done:  61%|██████    | 1826/3000 [00:01<00:00, 1594.51it/s]warmup should be done:  67%|██████▋   | 2024/3000 [00:01<00:00, 1681.87it/s]warmup should be done:  67%|██████▋   | 1999/3000 [00:01<00:00, 1661.49it/s]warmup should be done:  69%|██████▊   | 2057/3000 [00:01<00:00, 1716.28it/s]warmup should be done:  68%|██████▊   | 2048/3000 [00:01<00:00, 1707.58it/s]warmup should be done:  68%|██████▊   | 2043/3000 [00:01<00:00, 1685.66it/s]warmup should be done:  68%|██████▊   | 2030/3000 [00:01<00:00, 1683.78it/s]warmup should be done:  67%|██████▋   | 2003/3000 [00:01<00:00, 1675.82it/s]warmup should be done:  66%|██████▌   | 1987/3000 [00:01<00:00, 1475.23it/s]warmup should be done:  72%|███████▏  | 2166/3000 [00:01<00:00, 1663.26it/s]warmup should be done:  74%|███████▍  | 2229/3000 [00:01<00:00, 1715.77it/s]warmup should be done:  74%|███████▍  | 2219/3000 [00:01<00:00, 1707.39it/s]warmup should be done:  73%|███████▎  | 2193/3000 [00:01<00:00, 1670.60it/s]warmup should be done:  73%|███████▎  | 2199/3000 [00:01<00:00, 1685.44it/s]warmup should be done:  74%|███████▎  | 2212/3000 [00:01<00:00, 1681.23it/s]warmup should be done:  72%|███████▏  | 2171/3000 [00:01<00:00, 1657.18it/s]warmup should be done:  72%|███████▏  | 2153/3000 [00:01<00:00, 1525.89it/s]warmup should be done:  78%|███████▊  | 2334/3000 [00:01<00:00, 1666.36it/s]warmup should be done:  80%|████████  | 2401/3000 [00:01<00:00, 1715.04it/s]warmup should be done:  80%|███████▉  | 2390/3000 [00:01<00:00, 1707.99it/s]warmup should be done:  79%|███████▉  | 2368/3000 [00:01<00:00, 1684.20it/s]warmup should be done:  79%|███████▊  | 2361/3000 [00:01<00:00, 1665.47it/s]warmup should be done:  79%|███████▉  | 2381/3000 [00:01<00:00, 1678.21it/s]warmup should be done:  78%|███████▊  | 2339/3000 [00:01<00:00, 1663.70it/s]warmup should be done:  77%|███████▋  | 2322/3000 [00:01<00:00, 1570.61it/s]warmup should be done:  83%|████████▎ | 2504/3000 [00:01<00:00, 1673.85it/s]warmup should be done:  86%|████████▌ | 2574/3000 [00:01<00:00, 1716.48it/s]warmup should be done:  85%|████████▌ | 2562/3000 [00:01<00:00, 1710.48it/s]warmup should be done:  85%|████████▍ | 2539/3000 [00:01<00:00, 1689.47it/s]warmup should be done:  84%|████████▍ | 2528/3000 [00:01<00:00, 1664.59it/s]warmup should be done:  85%|████████▍ | 2549/3000 [00:01<00:00, 1676.36it/s]warmup should be done:  84%|████████▎ | 2507/3000 [00:01<00:00, 1666.77it/s]warmup should be done:  83%|████████▎ | 2491/3000 [00:01<00:00, 1602.74it/s]warmup should be done:  89%|████████▉ | 2674/3000 [00:01<00:00, 1680.03it/s]warmup should be done:  92%|█████████▏| 2747/3000 [00:01<00:00, 1718.90it/s]warmup should be done:  91%|█████████ | 2734/3000 [00:01<00:00, 1711.91it/s]warmup should be done:  90%|█████████ | 2711/3000 [00:01<00:00, 1695.75it/s]warmup should be done:  91%|█████████ | 2718/3000 [00:01<00:00, 1680.09it/s]warmup should be done:  90%|████████▉ | 2695/3000 [00:01<00:00, 1661.93it/s]warmup should be done:  89%|████████▉ | 2675/3000 [00:01<00:00, 1669.30it/s]warmup should be done:  89%|████████▊ | 2658/3000 [00:01<00:00, 1620.63it/s]warmup should be done:  95%|█████████▍| 2844/3000 [00:01<00:00, 1683.70it/s]warmup should be done:  97%|█████████▋| 2919/3000 [00:01<00:00, 1717.54it/s]warmup should be done:  97%|█████████▋| 2906/3000 [00:01<00:00, 1709.56it/s]warmup should be done:  96%|█████████▌| 2882/3000 [00:01<00:00, 1698.74it/s]warmup should be done:  96%|█████████▌| 2887/3000 [00:01<00:00, 1681.05it/s]warmup should be done:  95%|█████████▌| 2862/3000 [00:01<00:00, 1655.02it/s]warmup should be done:  95%|█████████▍| 2842/3000 [00:01<00:00, 1668.81it/s]warmup should be done:  94%|█████████▍| 2827/3000 [00:01<00:00, 1638.52it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1708.97it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1700.47it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1688.62it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1686.25it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1671.54it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1667.62it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1660.25it/s]warmup should be done: 100%|█████████▉| 2996/3000 [00:01<00:00, 1653.69it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1616.89it/s]2022-12-12 05:04:33.591525: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f3b2f830eb0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 05:04:33.591594: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 05:04:33.601332: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f3b27795350 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 05:04:33.601392: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 05:04:33.667068: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f1d0002e830 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 05:04:33.667148: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 05:04:33.963573: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f3b37834ff0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 05:04:33.963640: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 05:04:33.971178: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f1d4c028c80 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 05:04:33.971237: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 05:04:34.018095: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f3b27fffe80 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 05:04:34.018163: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 05:04:34.074542: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f3b33830670 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 05:04:34.074553: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f3b3382c9b0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 05:04:34.074619: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 05:04:34.074628: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 05:04:35.863334: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 05:04:35.865216: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 05:04:36.008649: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 05:04:36.213669: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 05:04:36.248720: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 05:04:36.330128: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 05:04:36.380494: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 05:04:36.384500: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 05:04:38.773767: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 05:04:38.821338: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 05:04:38.900434: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 05:04:39.096063: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 05:04:39.144897: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 05:04:39.190528: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 05:04:39.306061: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 05:04:39.317972: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
[HCTR][05:05:03.833][ERROR][RK0][main]: replica 6 reaches 1000, calling init pre replica
[HCTR][05:05:03.834][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][05:05:03.840][ERROR][RK0][main]: coll ps creation done
[HCTR][05:05:03.840][ERROR][RK0][main]: replica 6 waits for coll ps creation barrier
[HCTR][05:05:03.855][ERROR][RK0][tid #139892378035968]: replica 7 reaches 1000, calling init pre replica
[HCTR][05:05:03.856][ERROR][RK0][tid #139892378035968]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][05:05:03.861][ERROR][RK0][tid #139892378035968]: coll ps creation done
[HCTR][05:05:03.861][ERROR][RK0][tid #139892378035968]: replica 7 waits for coll ps creation barrier
[HCTR][05:05:03.903][ERROR][RK0][tid #139892721972992]: replica 2 reaches 1000, calling init pre replica
[HCTR][05:05:03.903][ERROR][RK0][tid #139892721972992]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][05:05:03.903][ERROR][RK0][tid #139892436752128]: replica 0 reaches 1000, calling init pre replica
[HCTR][05:05:03.903][ERROR][RK0][tid #139892436752128]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][05:05:03.903][ERROR][RK0][tid #139892386428672]: replica 4 reaches 1000, calling init pre replica
[HCTR][05:05:03.904][ERROR][RK0][tid #139892386428672]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][05:05:03.907][ERROR][RK0][tid #139893510493952]: replica 1 reaches 1000, calling init pre replica
[HCTR][05:05:03.907][ERROR][RK0][tid #139893510493952]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][05:05:03.908][ERROR][RK0][tid #139892721972992]: coll ps creation done
[HCTR][05:05:03.908][ERROR][RK0][tid #139892721972992]: replica 2 waits for coll ps creation barrier
[HCTR][05:05:03.908][ERROR][RK0][tid #139892436752128]: coll ps creation done
[HCTR][05:05:03.908][ERROR][RK0][tid #139892436752128]: replica 0 waits for coll ps creation barrier
[HCTR][05:05:03.912][ERROR][RK0][tid #139892386428672]: coll ps creation done
[HCTR][05:05:03.912][ERROR][RK0][tid #139892386428672]: replica 4 waits for coll ps creation barrier
[HCTR][05:05:03.914][ERROR][RK0][tid #139893510493952]: coll ps creation done
[HCTR][05:05:03.914][ERROR][RK0][tid #139893510493952]: replica 1 waits for coll ps creation barrier
[HCTR][05:05:03.961][ERROR][RK0][main]: replica 3 reaches 1000, calling init pre replica
[HCTR][05:05:03.961][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][05:05:03.968][ERROR][RK0][main]: coll ps creation done
[HCTR][05:05:03.968][ERROR][RK0][main]: replica 3 waits for coll ps creation barrier
[HCTR][05:05:03.998][ERROR][RK0][tid #139892386428672]: replica 5 reaches 1000, calling init pre replica
[HCTR][05:05:03.998][ERROR][RK0][tid #139892386428672]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][05:05:04.002][ERROR][RK0][tid #139892386428672]: coll ps creation done
[HCTR][05:05:04.002][ERROR][RK0][tid #139892386428672]: replica 5 waits for coll ps creation barrier
[HCTR][05:05:04.002][ERROR][RK0][tid #139892436752128]: replica 0 preparing frequency
[HCTR][05:05:04.828][ERROR][RK0][tid #139892436752128]: replica 0 preparing frequency done
[HCTR][05:05:04.870][ERROR][RK0][tid #139892436752128]: replica 0 calling init per replica
[HCTR][05:05:04.870][ERROR][RK0][main]: replica 3 calling init per replica
[HCTR][05:05:04.870][ERROR][RK0][tid #139893510493952]: replica 1 calling init per replica
[HCTR][05:05:04.870][ERROR][RK0][main]: replica 6 calling init per replica
[HCTR][05:05:04.870][ERROR][RK0][tid #139892721972992]: replica 2 calling init per replica
[HCTR][05:05:04.870][ERROR][RK0][tid #139892386428672]: replica 4 calling init per replica
[HCTR][05:05:04.870][ERROR][RK0][tid #139892386428672]: replica 5 calling init per replica
[HCTR][05:05:04.870][ERROR][RK0][tid #139892378035968]: replica 7 calling init per replica
[HCTR][05:05:04.870][ERROR][RK0][tid #139892436752128]: Calling build_v2
[HCTR][05:05:04.870][ERROR][RK0][main]: Calling build_v2
[HCTR][05:05:04.870][ERROR][RK0][tid #139893510493952]: Calling build_v2
[HCTR][05:05:04.870][ERROR][RK0][main]: Calling build_v2
[HCTR][05:05:04.870][ERROR][RK0][tid #139892721972992]: Calling build_v2
[HCTR][05:05:04.870][ERROR][RK0][tid #139892386428672]: Calling build_v2
[HCTR][05:05:04.870][ERROR][RK0][tid #139892386428672]: Calling build_v2
[HCTR][05:05:04.870][ERROR][RK0][tid #139892436752128]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][05:05:04.870][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][05:05:04.870][ERROR][RK0][tid #139893510493952]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][05:05:04.870][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][05:05:04.870][ERROR][RK0][tid #139892721972992]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][05:05:04.870][ERROR][RK0][tid #139892378035968]: Calling build_v2
[HCTR][05:05:04.870][ERROR][RK0][tid #139892386428672]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][05:05:04.870][ERROR][RK0][tid #139892386428672]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][05:05:04.870][ERROR][RK0][tid #139892378035968]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[[[[[[[2022-12-12 05:05:042022-12-12 05:05:042022-12-12 05:05:04.2022-12-12 05:05:04.2022-12-12 05:05:04.2022-12-12 05:05:04[2022-12-12 05:05:04870713.870722.870713..: 870713: 8707132022-12-12 05:05:04: 870724870724E: E: .E: :  E E870757 EE/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc /hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc /hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc::   :/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc:/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc136E/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc136:136:]  ::] 136] 136using concurrent impl MPS/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc136136using concurrent impl MPS] using concurrent impl MPS] 
:] ] 
using concurrent impl MPS
using concurrent impl MPS136using concurrent impl MPSusing concurrent impl MPS

] 
using concurrent impl MPS

[2022-12-12 05:05:04.875054: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:178] v100x8, slow pcie
[2022-12-12 05:05:04.875092: E[ 2022-12-12 05:05:04/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.:875097196: ] Eassigning 8 to cpu 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:178] v100x8, slow pcie
[2022-12-12 05:05:04[.2022-12-12 05:05:04875150.: 875159E:  E[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc 2022-12-12 05:05:04:/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.178:875178] 196[: v100x8, slow pcie] 2022-12-12 05:05:04E
assigning 8 to cpu. 
875196/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[: :2022-12-12 05:05:04E212. ] 875225/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8: :2022-12-12 05:05:04[
E178.2022-12-12 05:05:04 ] 875242./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccv100x8, slow pcie: 875254:[
E: 1962022-12-12 05:05:04[ E] [.2022-12-12 05:05:04/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc assigning 8 to cpu2022-12-12 05:05:04875283.:/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
.: 875298178:875291E: ] 212: [ Ev100x8, slow pcie] E2022-12-12 05:05:04/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc 
[build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8 .:/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 05:05:04
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[875336213:.:2022-12-12 05:05:04: ] 196875371178[.[Eremote time is 8.68421] : ] 2022-12-12 05:05:048753782022-12-12 05:05:04 
assigning 8 to cpuEv100x8, slow pcie.: ./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
 
[875400E875393:/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 05:05:04: [ : 178:[.E2022-12-12 05:05:04/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE] 2122022-12-12 05:05:04875456 .: v100x8, slow pcie] .: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc875481196/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8875494E:: ] :
:  [213Eassigning 8 to cpu178E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 05:05:04]  [
]  :.remote time is 8.68421/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 05:05:04v100x8, slow pcie/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc214875579
:.
:] : 196[875605[212cpu time is 97.0588E] [2022-12-12 05:05:04: 2022-12-12 05:05:04] 
 assigning 8 to cpu2022-12-12 05:05:04.E.build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
.875668 875672
:875679: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: 196: [E[:E] E2022-12-12 05:05:04 2022-12-12 05:05:04213 assigning 8 to cpu ./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc875771:875790remote time is 8.68421::: 214: 
212196E] E] []  cpu time is 97.0588 build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8[2022-12-12 05:05:04assigning 8 to cpu/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
2022-12-12 05:05:04.
::.875875[213212875885: 2022-12-12 05:05:04] ] : E.remote time is 8.68421build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8E 875939

 /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: [/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[:E2022-12-12 05:05:04[:2022-12-12 05:05:04214 .2022-12-12 05:05:04212.] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc875986.] 875982cpu time is 97.0588:: 875993build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8: 
213E: 
E]  E remote time is 8.68421/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc [/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
:/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 05:05:04[:214:.2022-12-12 05:05:04212] 213876095.] cpu time is 97.0588] : 876127build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8
remote time is 8.68421E: 

 E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc [:/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 05:05:04213:[.] 2142022-12-12 05:05:04876209remote time is 8.68421] .: 
cpu time is 97.0588876217E
: [ E2022-12-12 05:05:04/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc .:/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc876275214:: ] 213Ecpu time is 97.0588]  
remote time is 8.68421/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
:214] cpu time is 97.0588
[2022-12-12 05:05:04.876386: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:214] cpu time is 97.0588
[2022-12-12 05:06:22.291069: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:118] solver created. now build & solve
[2022-12-12 05:06:22.331065: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:123] solver built. now solve
[2022-12-12 05:06:22.331137: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:919] num_cached_nodes = 20000000
[2022-12-12 05:06:22.440758: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:127] solver solved
[2022-12-12 05:06:22.440850: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:195] 0 solved master
[2022-12-12 05:06:22.440883: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 0 solved
[2022-12-12 05:06:22.440913: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 0 initing device 0
[2022-12-12 05:06:22.441387: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 05:06:22.442294: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 05:06:22.442960: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 05:06:22.456128: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 2 solved
[2022-12-12 05:06:22.456206: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 2 initing device 2
[2022-12-12 05:06:22.456379: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 4 solved
[[2022-12-12 05:06:222022-12-12 05:06:22..456423456441: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc::202205] ] 6 solvedworker 0 thread 4 initing device 4

[2022-12-12 05:06:22.456537: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 6 initing device 6
[2022-12-12 05:06:22.456636: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 05:06:22.456902: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 05:06:22.456959: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 05:06:22.458677: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 05:06:22.458734: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 05:06:22.458796: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[[2022-12-12 05:06:222022-12-12 05:06:22..459169459172: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc::202202] ] 7 solved1 solved

[[2022-12-12 05:06:222022-12-12 05:06:22..459278459281: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc::205205] ] worker 0 thread 7 initing device 7worker 0 thread 1 initing device 1

[2022-12-12 05:06:22.459732[: 2022-12-12 05:06:22E. 459739/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu: :E1980 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cueager alloc mem 381.47 MB:
1980] eager alloc mem 381.47 MB
[[2022-12-12 05:06:222022-12-12 05:06:22..460245460253: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc::202202] ] 3 solved5 solved

[[2022-12-12 05:06:222022-12-12 05:06:22..460356460358: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc::205205] ] worker 0 thread 3 initing device 3worker 0 thread 5 initing device 5

[[2022-12-12 05:06:222022-12-12 05:06:22..460806460808: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::19801980] ] eager alloc mem 381.47 MBeager alloc mem 381.47 MB

[2022-12-12 05:06:22.462522: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 05:06:22.462580: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 05:06:22.462962: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 05:06:22.463982: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 05:06:22.464181: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 05:06:22.464827: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 05:06:22.465330: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 05:06:22.467478: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 05:06:22.467597: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 05:06:22.467653: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 05:06:22.467709: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 05:06:22.524255: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 2.00 Bytes
[2022-12-12 05:06:22.524635: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1024.00 Bytes
[2022-12-12 05:06:22.529932: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-12 05:06:22.530024: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 2
[2022-12-12 05:06:22.530070: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 05:06:22.530978: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 76.68 MB
[2022-12-12 05:06:22.532224: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 05:06:22.533308: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 05:06:22.533402: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 05:06:22.534069: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 05:06:22.534112: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 9.54 GB
[[2022-12-12 05:06:222022-12-12 05:06:22..546691546691: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::19801980] ] eager alloc mem 2.00 Byteseager alloc mem 2.00 Bytes

[2022-12-12 05:06:22.547047: [E2022-12-12 05:06:22 ./hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu547058:: 1980E]  eager alloc mem 1024.00 Bytes/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu
:1980] eager alloc mem 1024.00 Bytes
[[2022-12-12 05:06:222022-12-12 05:06:22..552557552557: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu[:[:2022-12-12 05:06:2219802022-12-12 05:06:221980.] .] 552620eager alloc mem 2.00 Byteseager alloc mem 2.00 Bytes552620: 

: EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::19801980] ] eager alloc mem 2.00 Byteseager alloc mem 2.00 Bytes

[[2022-12-12 05:06:222022-12-12 05:06:22..552942552943: : EE[ [ 2022-12-12 05:06:22/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu2022-12-12 05:06:22/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu.:.:55296419805529661980: ] : ] Eeager alloc mem 1024.00 BytesEeager alloc mem 1024.00 Bytes 
 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::19801980] ] eager alloc mem 1024.00 Byteseager alloc mem 1024.00 Bytes

[2022-12-12 05:06:22.554430: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 2.00 Bytes
[2022-12-12 05:06:22.554727: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1024.00 Bytes
[2022-12-12 05:06:22.566769: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-12 05:06:22.566836: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 2
[2022-12-12 05:06:22.566853[: 2022-12-12 05:06:22E. 566879/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc: :E638 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cceager release cuda mem 1024:
638] eager release cuda mem 400000000
[2022-12-12 05:06:22.566937: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 2
[2022-12-12 05:06:22.566993: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 05:06:22.572716: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 76.68 MB
[2022-12-12 05:06:22.573222: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:[19802022-12-12 05:06:22] .eager alloc mem 76.68 MB573228
: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-12 05:06:22.[5732982022-12-12 05:06:22: .E573324 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccE: 638/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc] :eager release cuda mem 1024638
] eager release cuda mem 2
[2022-12-12 05:06:22.[5733832022-12-12 05:06:22[: .2022-12-12 05:06:22E573387. : 573379/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccE: : E638/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc ] :/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cceager release cuda mem 2638:
] 638eager release cuda mem 400000000] 
eager release cuda mem 1024
[2022-12-12 05:06:22.573469: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:[6382022-12-12 05:06:22] .eager release cuda mem 400000000573483
: E[ 2022-12-12 05:06:22/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc.:573487638: ] Eeager release cuda mem 2 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024[
2022-12-12 05:06:22[.2022-12-12 05:06:22573533.: [573554E2022-12-12 05:06:22:  .E/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc573574 :: /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc638E:]  638eager release cuda mem 1024/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc] 
:eager release cuda mem 400000000638
] eager release cuda mem 2
[2022-12-12 05:06:22.573682: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc[:2022-12-12 05:06:22638.] 573699eager release cuda mem 2: 
E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 05:06:22.573748: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 05:06:22.574527: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 05:06:22.575000: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 76.68 MB
[2022-12-12 05:06:22.575519: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 76.68 MB
[2022-12-12 05:06:22.575577: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 05:06:22.575666: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 05:06:22.575900: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 05:06:22.576289: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 76.68 MB
[2022-12-12 05:06:22.576340: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 05:06:22.576381: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 9.54 GB
[2022-12-12 05:06:22.576947: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 05:06:22.577031: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 05:06:22.577071: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 76.68 MB
[2022-12-12 05:06:22.577606: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 76.68 MB
[2022-12-12 05:06:22.577696: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 05:06:22.577742: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 9.54 GB
[2022-12-12 05:06:22.603105: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 05:06:22.603243: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 05:06:22.603485: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 05:06:22.603589: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 05:06:22.603780: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 05:06:22.604150: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 05:06:22.604239: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 05:06:22.604268: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 05:06:22.604349: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 05:06:22.604568: E[ 2022-12-12 05:06:22/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc.:604572638: ] Eeager release cuda mem 625663 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 05:06:22.604674: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 05:06:22.604722: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 05:06:22.604780: E[ 2022-12-12 05:06:22/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc.:604794638: ] Eeager release cuda mem 625663 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 05:06:22.604864: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 9.54 GB
[2022-12-12 05:06:22.604890: E[ 2022-12-12 05:06:22./hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu604902:: 1980E]  eager alloc mem 25.25 KB/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc
:638] eager release cuda mem 25855
[2022-12-12 05:06:22.604983: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 9.54 GB
[2022-12-12 05:06:22.605228: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 05:06:22.605269: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 9.54 GB
[2022-12-12 05:06:22.605311: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 05:06:22.605357: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 9.54 GB
[2022-12-12 05:06:22.605489: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 05:06:22.605529: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 9.54 GB
[[[[[[[[2022-12-12 05:06:242022-12-12 05:06:242022-12-12 05:06:242022-12-12 05:06:242022-12-12 05:06:242022-12-12 05:06:242022-12-12 05:06:242022-12-12 05:06:24........612690612690612681612677612681612686612686612696: : : : : : : : EEEEEEEE        /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::::::::19801980198019801980198019801980] ] ] ] ] ] ] ] eager alloc mem 611.00 KBeager alloc mem 611.00 KBeager alloc mem 611.00 KBeager alloc mem 611.00 KBeager alloc mem 611.00 KBeager alloc mem 611.00 KBeager alloc mem 611.00 KBeager alloc mem 611.00 KB







[[[[[2022-12-12 05:06:24[2022-12-12 05:06:242022-12-12 05:06:242022-12-12 05:06:24[2022-12-12 05:06:24[.2022-12-12 05:06:24...2022-12-12 05:06:24.2022-12-12 05:06:24613836.613837613837613848.613839.: 613843: : : 613852: 613857E: EEE: E:  E   E E/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc :/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:::/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc638:638638638:638:] 638] ] ] 638] 638eager release cuda mem 625663] eager release cuda mem 625663eager release cuda mem 625663eager release cuda mem 625663] eager release cuda mem 625663] 
eager release cuda mem 625663


eager release cuda mem 625663
eager release cuda mem 625663


[2022-12-12 05:06:24.614178[: [[2022-12-12 05:06:24E[2022-12-12 05:06:242022-12-12 05:06:24. [2022-12-12 05:06:24[[..614184/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu2022-12-12 05:06:24.2022-12-12 05:06:242022-12-12 05:06:24614187614189: :.614197..: : E1980614209: 614210614210EE ] : E: :   /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cueager alloc mem 611.00 KBE EE/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu  ::1980/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu19801980] :1980::] ] eager alloc mem 611.00 KB1980] 19801980eager alloc mem 611.00 KBeager alloc mem 611.00 KB
] eager alloc mem 611.00 KB] ] 

eager alloc mem 611.00 KB
eager alloc mem 611.00 KBeager alloc mem 611.00 KB


[2022-12-12 05:06:24.615099: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[[2022-12-12 05:06:242022-12-12 05:06:24..[[615176[6151802022-12-12 05:06:242022-12-12 05:06:24: 2022-12-12 05:06:24[[: [..E.2022-12-12 05:06:242022-12-12 05:06:24E2022-12-12 05:06:24615187615192 615195.. .: : /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc: 615210615210/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu615214EE:E: : ::   638 EE1980E/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc] /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc  ]  ::eager release cuda mem 625663:/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cceager alloc mem 611.00 KB/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc638638
638::
:] ] ] 638638638eager release cuda mem 625663eager release cuda mem 625663eager release cuda mem 625663] ] ] [


eager release cuda mem 625663eager release cuda mem 625663eager release cuda mem 6256632022-12-12 05:06:24


.615437: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[[2022-12-12 05:06:24[2022-12-12 05:06:24.2022-12-12 05:06:24.615491.615493[: [615490[: 2022-12-12 05:06:24E2022-12-12 05:06:24: 2022-12-12 05:06:24E. .E. 615510/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu615512 615515/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu: :: /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu: :E1980E:E1980 ]  1980 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cueager alloc mem 611.00 KB/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu] /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cueager alloc mem 611.00 KB:
:eager alloc mem 611.00 KB:
19801980
1980] ] ] eager alloc mem 611.00 KBeager alloc mem 611.00 KBeager alloc mem 611.00 KB


[2022-12-12 05:06:24.616128: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 05:06:24.616195: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 05:06:24.616215: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 05:06:24.616284: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 05:06:24.616426: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638[] 2022-12-12 05:06:24eager release cuda mem 625663.
616443[: 2022-12-12 05:06:24E. 616452/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc: :E638[ ] [[[2022-12-12 05:06:24/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cceager release cuda mem 6256632022-12-12 05:06:242022-12-12 05:06:242022-12-12 05:06:24.:
...616478638616482616484616499: ] : : : Eeager release cuda mem 625663EE[E 
  2022-12-12 05:06:24 /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc./hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:::616577:638638638[: 1980] ] ] 2022-12-12 05:06:24E] eager release cuda mem 625663eager release cuda mem 625663eager release cuda mem 625663. eager alloc mem 611.00 KB


616649/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu
: :E1980 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cueager alloc mem 611.00 KB:
1980] eager alloc mem 611.00 KB
[2022-12-12 05:06:24.616760: [E[2022-12-12 05:06:24 2022-12-12 05:06:24./hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu.616767:1980616767: : ] EEeager alloc mem 611.00 KB  
/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::19801980] ] eager alloc mem 611.00 KBeager alloc mem 611.00 KB

[2022-12-12 05:06:24.616939: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 05:06:24.617004: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 05:06:24.617032: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 05:06:24.617098: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 05:06:24.617455: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 05:06:24.617477: [E2022-12-12 05:06:24 ./hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc617483:: 638E]  eager release cuda mem 625663/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc
:[6382022-12-12 05:06:24] .eager release cuda mem 625663617520
: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 05:06:24.617563: E[ 2022-12-12 05:06:24/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu[.:2022-12-12 05:06:246175721980.: [] [617579E2022-12-12 05:06:24eager alloc mem 611.00 KB2022-12-12 05:06:24:  .
.E/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc617606617610 :: : /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu638EE:]   1980eager release cuda mem 625663/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc] 
::eager alloc mem 611.00 KB638638
] [] eager release cuda mem 6256632022-12-12 05:06:24eager release cuda mem 625663
.
[6177502022-12-12 05:06:24: .E617781 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccE: 638/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu] :eager release cuda mem 6256631980[
] 2022-12-12 05:06:24[eager alloc mem 611.00 KB.2022-12-12 05:06:24[
617825.2022-12-12 05:06:24: 617834.E[: 617843 2022-12-12 05:06:24E: /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu. E:617874/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu 1980: :/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc] E1980:eager alloc mem 611.00 KB ] 638
/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cueager alloc mem 611.00 KB] :
eager release cuda mem 6256631980
] eager alloc mem 611.00 KB
[2022-12-12 05:06:24.618033: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 05:06:24.618278: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 05:06:24.618342: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 05:06:24.618377: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 05:06:24.618445: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 05:06:24.618479: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 05:06:24.618545: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 05:06:24.618589: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 05:06:24.618655: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 05:06:24.618690: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] [eager release cuda mem 6256632022-12-12 05:06:24[
.2022-12-12 05:06:24618710.: 618717E:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc :/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc638[:] 2022-12-12 05:06:24638[eager release cuda mem 625663.] 2022-12-12 05:06:24
618768eager release cuda mem 625663.: 
618783E:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu :/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc1980[:] 2022-12-12 05:06:24[638eager alloc mem 611.00 KB.2022-12-12 05:06:24] 
618857.eager release cuda mem 625663: 618876
E:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu :/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu1980:] 1980eager alloc mem 611.00 KB] 
eager alloc mem 611.00 KB[
2022-12-12 05:06:24.618965: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 05:06:24.619084: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 05:06:24.619157: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 05:06:24.619192: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 05:06:24.619259: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 05:06:24.619293: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 05:06:24.619368: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 05:06:24.619401: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 05:06:24.619481: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 05:06:24.619629: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 05:06:24[.2022-12-12 05:06:24619696[.: 2022-12-12 05:06:24619697[E.: 2022-12-12 05:06:24 619710E./hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:  619723:E/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc: 1980 :E] /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638 eager alloc mem 611.00 KB638] /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc
] eager release cuda mem 625663:eager release cuda mem 625663
638
] eager release cuda mem 625663
[2022-12-12 05:06:24.[6198662022-12-12 05:06:24: .E619872[ : 2022-12-12 05:06:24[/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccE.2022-12-12 05:06:24: 619893.638/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc: 619902] :E: eager release cuda mem 80400000638 E
] /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu eager release cuda mem 80400000:/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc
1980:] 638eager alloc mem 611.00 KB] 
eager release cuda mem 625663[
2022-12-12 05:06:24.620009: E [/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc2022-12-12 05:06:24:.638620028] : eager release cuda mem 625663E
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 80400000[
2022-12-12 05:06:24.620061: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 80400000
[2022-12-12 05:06:24.620122: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 05:06:24.620167: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 80400000
[2022-12-12 05:06:24.620228: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 05:06:24.620266: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 80400000
[2022-12-12 05:06:24.620558: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 05:06:24.620595: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 80400000
[2022-12-12 05:06:24.620731: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 05:06:24.620768: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 80400000
[2022-12-12 05:06:24.621049: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 20000000 / 100000000 nodes ( 20.00 %~20.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 80000000 / 100000000 nodes ( 80.00 %) | 9.54 GB | 2.16416 secs 
[2022-12-12 05:06:24.621220: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 20000000 / 100000000 nodes ( 20.00 %~20.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 80000000 / 100000000 nodes ( 80.00 %) | 9.54 GB | 2.16042 secs 
[2022-12-12 05:06:24.621543: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 20000000 / 100000000 nodes ( 20.00 %~20.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 80000000 / 100000000 nodes ( 80.00 %) | 9.54 GB | 2.16182 secs 
[2022-12-12 05:06:24.621706: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 20000000 / 100000000 nodes ( 20.00 %~20.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 80000000 / 100000000 nodes ( 80.00 %) | 9.54 GB | 2.16091 secs 
[2022-12-12 05:06:24.622358: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 20000000 / 100000000 nodes ( 20.00 %~20.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 80000000 / 100000000 nodes ( 80.00 %) | 9.54 GB | 2.18098 secs 
[2022-12-12 05:06:24.622769: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 20000000 / 100000000 nodes ( 20.00 %~20.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 80000000 / 100000000 nodes ( 80.00 %) | 9.54 GB | 2.16615 secs 
[2022-12-12 05:06:24.622968: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 20000000 / 100000000 nodes ( 20.00 %~20.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 80000000 / 100000000 nodes ( 80.00 %) | 9.54 GB | 2.16324 secs 
[2022-12-12 05:06:24.623484: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 20000000 / 100000000 nodes ( 20.00 %~20.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 80000000 / 100000000 nodes ( 80.00 %) | 9.54 GB | 2.16654 secs 
[2022-12-12 05:06:24.623525: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 16.17 GB
[2022-12-12 05:06:26. 24056: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 16.44 GB
[2022-12-12 05:06:26. 24404: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 16.44 GB
[2022-12-12 05:06:26. 26249: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 16.44 GB
[2022-12-12 05:06:27.719609: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 16.70 GB
[2022-12-12 05:06:27.723218: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 16.70 GB
[2022-12-12 05:06:27.726589: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 16.70 GB
[2022-12-12 05:06:29. 40479: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 16.92 GB
[2022-12-12 05:06:29. 40616: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 16.92 GB
[2022-12-12 05:06:29. 40945: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 16.92 GB
[2022-12-12 05:06:31.111076: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 17.13 GB
[2022-12-12 05:06:31.111870: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 17.13 GB
[2022-12-12 05:06:31.113174: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 17.13 GB
[2022-12-12 05:06:32.778051: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 17.59 GB
[2022-12-12 05:06:32.778418: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 17.59 GB
[2022-12-12 05:06:32.779122: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 17.59 GB
[2022-12-12 05:06:34.241461: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 17.79 GB
[2022-12-12 05:06:34.241614: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 17.79 GB
[HCTR][05:06:34.244][ERROR][RK0][tid #139892378035968]: replica 7 calling init per replica done, doing barrier
[HCTR][05:06:34.244][ERROR][RK0][tid #139892386428672]: replica 4 calling init per replica done, doing barrier
[HCTR][05:06:34.244][ERROR][RK0][tid #139892721972992]: replica 2 calling init per replica done, doing barrier
[HCTR][05:06:34.244][ERROR][RK0][tid #139892386428672]: replica 5 calling init per replica done, doing barrier
[HCTR][05:06:34.244][ERROR][RK0][main]: replica 6 calling init per replica done, doing barrier
[HCTR][05:06:34.244][ERROR][RK0][tid #139892436752128]: replica 0 calling init per replica done, doing barrier
[HCTR][05:06:34.244][ERROR][RK0][tid #139893510493952]: replica 1 calling init per replica done, doing barrier
[HCTR][05:06:34.244][ERROR][RK0][main]: replica 3 calling init per replica done, doing barrier
[HCTR][05:06:34.244][ERROR][RK0][main]: replica 3 calling init per replica done, doing barrier done
[HCTR][05:06:34.244][ERROR][RK0][tid #139892378035968]: replica 7 calling init per replica done, doing barrier done
[HCTR][05:06:34.244][ERROR][RK0][tid #139892436752128]: replica 0 calling init per replica done, doing barrier done
[HCTR][05:06:34.244][ERROR][RK0][tid #139892721972992]: replica 2 calling init per replica done, doing barrier done
[HCTR][05:06:34.244][ERROR][RK0][tid #139892386428672]: replica 4 calling init per replica done, doing barrier done
[HCTR][05:06:34.244][ERROR][RK0][main]: replica 6 calling init per replica done, doing barrier done
[HCTR][05:06:34.244][ERROR][RK0][tid #139893510493952]: replica 1 calling init per replica done, doing barrier done
[HCTR][05:06:34.244][ERROR][RK0][tid #139892386428672]: replica 5 calling init per replica done, doing barrier done
[HCTR][05:06:34.244][ERROR][RK0][main]: init per replica done
[HCTR][05:06:34.244][ERROR][RK0][tid #139892378035968]: init per replica done
[HCTR][05:06:34.244][ERROR][RK0][tid #139892721972992]: init per replica done
[HCTR][05:06:34.244][ERROR][RK0][tid #139892386428672]: init per replica done
[HCTR][05:06:34.244][ERROR][RK0][main]: init per replica done
[HCTR][05:06:34.244][ERROR][RK0][tid #139893510493952]: init per replica done
[HCTR][05:06:34.244][ERROR][RK0][tid #139892386428672]: init per replica done
[HCTR][05:06:34.247][ERROR][RK0][tid #139892436752128]: init per replica done
[HCTR][05:06:34.250][ERROR][RK0][tid #139893510493952]: 1 allocated 3276800 at 0x7f284f320000
[HCTR][05:06:34.250][ERROR][RK0][tid #139893510493952]: 1 allocated 6553600 at 0x7f3d22a00000
[HCTR][05:06:34.250][ERROR][RK0][tid #139893510493952]: 1 allocated 3276800 at 0x7f3d23040000
[HCTR][05:06:34.250][ERROR][RK0][tid #139893510493952]: 1 allocated 6553600 at 0x7f3d23360000
[HCTR][05:06:34.250][ERROR][RK0][tid #139892386428672]: 4 allocated 3276800 at 0x7f284b320000
[HCTR][05:06:34.250][ERROR][RK0][tid #139892386428672]: 4 allocated 6553600 at 0x7f3d24a00000
[HCTR][05:06:34.250][ERROR][RK0][tid #139892386428672]: 4 allocated 3276800 at 0x7f3d25040000
[HCTR][05:06:34.250][ERROR][RK0][tid #139892386428672]: 4 allocated 6553600 at 0x7f3d25360000
[HCTR][05:06:34.250][ERROR][RK0][main]: 2 allocated 3276800 at 0x7f284f320000
[HCTR][05:06:34.250][ERROR][RK0][tid #139892386428672]: 3 allocated 3276800 at 0x7f284f320000
[HCTR][05:06:34.250][ERROR][RK0][main]: 2 allocated 6553600 at 0x7f3d24a00000
[HCTR][05:06:34.250][ERROR][RK0][main]: 2 allocated 3276800 at 0x7f3d25040000
[HCTR][05:06:34.250][ERROR][RK0][tid #139892386428672]: 3 allocated 6553600 at 0x7f3d26a00000
[HCTR][05:06:34.250][ERROR][RK0][main]: 2 allocated 6553600 at 0x7f3d25360000
[HCTR][05:06:34.250][ERROR][RK0][tid #139892386428672]: 3 allocated 3276800 at 0x7f3d27040000
[HCTR][05:06:34.250][ERROR][RK0][tid #139892386428672]: 3 allocated 6553600 at 0x7f3d27360000
[HCTR][05:06:34.250][ERROR][RK0][tid #139892386428672]: 5 allocated 3276800 at 0x7f284f320000
[HCTR][05:06:34.250][ERROR][RK0][tid #139892378035968]: 7 allocated 3276800 at 0x7f2847320000
[HCTR][05:06:34.250][ERROR][RK0][tid #139892386428672]: 5 allocated 6553600 at 0x7f3d26a00000
[HCTR][05:06:34.250][ERROR][RK0][tid #139892378035968]: 7 allocated 6553600 at 0x7f3d26a00000
[HCTR][05:06:34.250][ERROR][RK0][tid #139892386428672]: 5 allocated 3276800 at 0x7f3d27040000
[HCTR][05:06:34.250][ERROR][RK0][main]: 6 allocated 3276800 at 0x7f284b320000
[HCTR][05:06:34.250][ERROR][RK0][tid #139892378035968]: 7 allocated 3276800 at 0x7f3d27040000
[HCTR][05:06:34.250][ERROR][RK0][tid #139892386428672]: 5 allocated 6553600 at 0x7f3d27360000
[HCTR][05:06:34.250][ERROR][RK0][main]: 6 allocated 6553600 at 0x7f3d24a00000
[HCTR][05:06:34.250][ERROR][RK0][tid #139892378035968]: 7 allocated 6553600 at 0x7f3d27360000
[HCTR][05:06:34.250][ERROR][RK0][main]: 6 allocated 3276800 at 0x7f3d25040000
[HCTR][05:06:34.251][ERROR][RK0][main]: 6 allocated 6553600 at 0x7f3d25360000
[HCTR][05:06:34.253][ERROR][RK0][tid #139892436752128]: 0 allocated 3276800 at 0x7f3d24920000
[HCTR][05:06:34.253][ERROR][RK0][tid #139892436752128]: 0 allocated 6553600 at 0x7f3d24e00000
[HCTR][05:06:34.253][ERROR][RK0][tid #139892436752128]: 0 allocated 3276800 at 0x7f3d25b0e800
[HCTR][05:06:34.253][ERROR][RK0][tid #139892436752128]: 0 allocated 6553600 at 0x7f3d25e2e800








