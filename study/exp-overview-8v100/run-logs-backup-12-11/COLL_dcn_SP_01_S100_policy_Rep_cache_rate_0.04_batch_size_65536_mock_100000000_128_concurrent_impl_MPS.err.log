2022-12-12 00:39:41.611645: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.619442: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.623855: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.628842: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.640830: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.647773: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.653193: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.664151: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.715212: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.718011: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.720641: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.722417: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.723159: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.723732: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.724791: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.725333: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.726412: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.727085: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.728112: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.728615: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.729754: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.730136: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.731478: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.731748: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.733282: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.733389: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.738739: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.738912: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.740601: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.741556: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.742618: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.743719: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.745591: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.746769: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.747797: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.748752: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.749792: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.750849: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.752000: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.753120: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.756674: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.757706: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.758574: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 00:39:41.758636: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.759586: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.760654: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.761624: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.762597: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.763683: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.768099: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.769232: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.769971: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.771261: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.771277: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.772084: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.773363: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.773599: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.774381: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.775969: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.776592: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.778647: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.779454: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.781821: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.782077: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.782584: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.784521: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.785246: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.785824: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.786025: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.786257: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.788182: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.789193: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.789887: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.790149: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.790872: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.791435: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.792550: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.793359: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.793604: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.794194: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.794625: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.796590: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.796830: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.797354: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.797472: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.799309: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.799630: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.799994: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.800139: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.801981: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.802403: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.802459: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.802777: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.808642: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.812336: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.812827: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.812916: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.813514: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.814359: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.815890: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.816273: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.816544: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.817355: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.818684: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.834365: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.839567: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.853789: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.854113: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.855474: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.855717: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.855901: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.857181: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.858853: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.859115: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.859244: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.859391: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.860245: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.862594: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.863054: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.863086: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.863206: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.864836: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.866525: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.866976: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.867021: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.867063: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.868228: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.870698: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.871143: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.871194: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.871281: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.872650: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.875209: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.875910: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.875985: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.876022: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.876676: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.879059: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.879476: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.879496: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.879539: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.880287: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.882829: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.883178: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.883285: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.883320: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.884011: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.886289: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.886616: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.886725: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.886771: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.887538: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.889916: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.890252: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.890315: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.890558: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.891354: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.893214: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.893527: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.893643: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.893948: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.894649: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.896709: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.897056: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.897145: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.897245: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.897398: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.898309: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.899792: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.900684: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.900850: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.901093: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.901157: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.901605: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.903455: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.904156: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.905199: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.905438: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.905758: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.905762: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.905875: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.908695: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.909556: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.910398: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.910642: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.910934: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.911027: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.913221: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.913576: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.914608: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.914827: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.915341: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 00:39:41.915744: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.916328: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.917773: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.918109: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.918958: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.919212: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.919864: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.920526: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.921767: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.922347: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.923359: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.924023: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.924403: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.924434: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.925687: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.926061: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.926302: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.927716: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.928771: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.929383: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.929390: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.930945: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.931117: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.931125: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.932725: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.933650: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.934353: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.934466: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.936355: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.936515: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.936681: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.937841: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.938911: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.939468: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.939542: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.941501: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.942384: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.943697: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.944023: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.944063: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.945313: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 00:39:41.945435: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.946335: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.947593: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.947841: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.947953: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.949713: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.950409: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.952013: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.953185: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.953613: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.955385: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.955534: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 00:39:41.955699: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 00:39:41.955875: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.956506: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.956874: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.959538: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.959998: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.960299: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.962398: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.962527: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 00:39:41.962797: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.963817: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.965976: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.966038: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.966163: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.966587: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.969923: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.970022: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.970244: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.970618: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.972544: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.973414: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.973536: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.973811: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.974438: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.978336: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.979722: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.980004: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.983316: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.984535: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:41.984656: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:42.044961: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:42.044961: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:42.050326: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:42.050368: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:42.056204: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:42.059821: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 00:39:42.066669: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 00:39:42.070121: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:42.076618: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:42.077020: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:42.081068: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:42.081714: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:42.117390: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:43.038709: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:43.040121: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:43.041639: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:43.044245: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 00:39:43.044314: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-12 00:39:43.062127: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:43.063626: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:43.065087: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:43.066212: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:43.067211: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:43.068314: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-12 00:39:43.113330: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 00:39:43.113531: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 00:39:43.159884: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12340
2022-12-12 00:39:43.264732: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:43.266309: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:43.267444: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:43.268485: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 00:39:43.268551: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-12 00:39:43.285810: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:43.287035: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:43.288231: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:43.289647: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:43.291070: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:43.292134: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:1/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-12 00:39:43.320842: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:43.321836: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:43.322924: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:43.323732: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 00:39:43.323794: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-12 00:39:43.340814: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:43.341606: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:43.342120: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:43.342734: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:43.343400: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:43.343885: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:2/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-12 00:39:43.352616: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:43.353236: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:43.353887: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:43.354365: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 00:39:43.354422: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-12 00:39:43.361270: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:43.361870: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:43.362397: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:43.362859: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 00:39:43.362913: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-12 00:39:43.372764: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:43.373263: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:43.374342: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:43.374783: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:43.375813: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:43.376263: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:43.377410: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:43.377646: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 00:39:43.377701: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-12 00:39:43.378654: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:43.379608: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:5/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-12 00:39:43.380234: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:43.381707: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:43.382685: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:43.383773: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:43.384750: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:43.385063: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 00:39:43.385251: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 00:39:43.385708: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:3/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-12 00:39:43.386894: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12341
2022-12-12 00:39:43.386952: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:43.388106: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:43.389144: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:43.390075: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:43.390816: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:43.391578: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:43.392367: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 00:39:43.392425: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-12 00:39:43.393049: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 00:39:43.393108: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-12 00:39:43.395648: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:43.396986: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:43.398012: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:43.399064: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:43.400103: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:43.401022: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:4/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-12 00:39:43.410082: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:43.411163: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:43.411411: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:43.412866: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:43.413095: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:43.414969: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:43.415423: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:43.416694: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:43.417006: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:43.418254: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:39:43.418501: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:7/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-12 00:39:43.419366: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:6/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-12 00:39:43.425600: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 00:39:43.425779: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 00:39:43.427600: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12345
2022-12-12 00:39:43.432625: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 00:39:43.432791: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 00:39:43.433805: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12343
2022-12-12 00:39:43.439151: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 00:39:43.439329: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 00:39:43.441067: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12342
2022-12-12 00:39:43.447953: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 00:39:43.448138: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 00:39:43.449933: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12344
2022-12-12 00:39:43.463863: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 00:39:43.464049: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 00:39:43.465746: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12347
2022-12-12 00:39:43.465779: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 00:39:43.465948: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 00:39:43.467716: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12346
[HCTR][00:39:44.721][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][00:39:44.721][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][00:39:44.721][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][00:39:44.723][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][00:39:44.725][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][00:39:44.727][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][00:39:44.727][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][00:39:44.727][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 1it [00:01,  1.53s/it]warmup run: 1it [00:01,  1.54s/it]warmup run: 1it [00:01,  1.58s/it]warmup run: 97it [00:01, 82.45it/s]warmup run: 102it [00:01, 86.13it/s]warmup run: 99it [00:01, 81.61it/s]warmup run: 1it [00:01,  1.53s/it]warmup run: 1it [00:01,  1.53s/it]warmup run: 1it [00:01,  1.53s/it]warmup run: 1it [00:01,  1.53s/it]warmup run: 192it [00:01, 176.48it/s]warmup run: 1it [00:01,  1.53s/it]warmup run: 205it [00:01, 187.83it/s]warmup run: 198it [00:01, 177.38it/s]warmup run: 96it [00:01, 81.57it/s]warmup run: 97it [00:01, 82.80it/s]warmup run: 86it [00:01, 73.26it/s]warmup run: 102it [00:01, 87.01it/s]warmup run: 290it [00:01, 284.08it/s]warmup run: 94it [00:01, 80.00it/s]warmup run: 309it [00:01, 301.22it/s]warmup run: 297it [00:01, 283.65it/s]warmup run: 198it [00:01, 183.27it/s]warmup run: 196it [00:01, 181.58it/s]warmup run: 181it [00:01, 168.56it/s]warmup run: 204it [00:01, 188.53it/s]warmup run: 388it [00:01, 395.86it/s]warmup run: 191it [00:01, 176.58it/s]warmup run: 412it [00:01, 417.68it/s]warmup run: 396it [00:01, 394.73it/s]warmup run: 300it [00:01, 295.34it/s]warmup run: 294it [00:01, 288.87it/s]warmup run: 277it [00:01, 274.75it/s]warmup run: 307it [00:01, 301.33it/s]warmup run: 487it [00:02, 505.97it/s]warmup run: 289it [00:01, 284.23it/s]warmup run: 515it [00:02, 531.31it/s]warmup run: 495it [00:02, 503.65it/s]warmup run: 400it [00:01, 408.05it/s]warmup run: 392it [00:01, 400.24it/s]warmup run: 372it [00:01, 382.99it/s]warmup run: 406it [00:01, 411.95it/s]warmup run: 588it [00:02, 611.28it/s]warmup run: 386it [00:01, 394.50it/s]warmup run: 619it [00:02, 637.58it/s]warmup run: 596it [00:02, 607.78it/s]warmup run: 503it [00:02, 523.15it/s]warmup run: 490it [00:02, 508.10it/s]warmup run: 468it [00:02, 489.88it/s]warmup run: 507it [00:02, 523.34it/s]warmup run: 689it [00:02, 702.53it/s]warmup run: 483it [00:02, 501.70it/s]warmup run: 724it [00:02, 731.71it/s]warmup run: 696it [00:02, 696.82it/s]warmup run: 601it [00:02, 617.84it/s]warmup run: 589it [00:02, 609.11it/s]warmup run: 565it [00:02, 589.71it/s]warmup run: 610it [00:02, 629.96it/s]warmup run: 789it [00:02, 776.05it/s]warmup run: 582it [00:02, 604.01it/s]warmup run: 828it [00:02, 807.35it/s]warmup run: 796it [00:02, 770.01it/s]warmup run: 697it [00:02, 695.90it/s]warmup run: 688it [00:02, 696.81it/s]warmup run: 662it [00:02, 677.09it/s]warmup run: 710it [00:02, 715.18it/s]warmup run: 889it [00:02, 833.65it/s]warmup run: 681it [00:02, 693.08it/s]warmup run: 932it [00:02, 866.75it/s]warmup run: 896it [00:02, 828.76it/s]warmup run: 793it [00:02, 760.89it/s]warmup run: 787it [00:02, 768.90it/s]warmup run: 757it [00:02, 743.49it/s]warmup run: 809it [00:02, 781.54it/s]warmup run: 988it [00:02, 876.06it/s]warmup run: 780it [00:02, 765.04it/s]warmup run: 1035it [00:02, 910.69it/s]warmup run: 995it [00:02, 872.26it/s]warmup run: 889it [00:02, 811.96it/s]warmup run: 886it [00:02, 825.28it/s]warmup run: 851it [00:02, 792.80it/s]warmup run: 908it [00:02, 832.03it/s]warmup run: 1088it [00:02, 908.26it/s]warmup run: 877it [00:02, 815.71it/s]warmup run: 1140it [00:02, 948.41it/s]warmup run: 1095it [00:02, 905.77it/s]warmup run: 986it [00:02, 852.61it/s]warmup run: 985it [00:02, 868.76it/s]warmup run: 945it [00:02, 831.13it/s]warmup run: 1007it [00:02, 872.73it/s]warmup run: 1188it [00:02, 933.71it/s]warmup run: 975it [00:02, 858.65it/s]warmup run: 1244it [00:02, 972.00it/s]warmup run: 1195it [00:02, 929.84it/s]warmup run: 1083it [00:02, 883.59it/s]warmup run: 1083it [00:02, 899.12it/s]warmup run: 1039it [00:02, 859.74it/s]warmup run: 1105it [00:02, 899.49it/s]warmup run: 1288it [00:02, 951.01it/s]warmup run: 1072it [00:02, 889.62it/s]warmup run: 1348it [00:02, 989.95it/s]warmup run: 1294it [00:02, 944.67it/s]warmup run: 1180it [00:02, 905.57it/s]warmup run: 1181it [00:02, 921.50it/s]warmup run: 1133it [00:02, 881.05it/s]warmup run: 1203it [00:02, 921.20it/s]warmup run: 1388it [00:02, 964.06it/s]warmup run: 1169it [00:02, 836.29it/s]warmup run: 1452it [00:02, 1003.68it/s]warmup run: 1393it [00:02, 956.55it/s]warmup run: 1276it [00:02, 920.84it/s]warmup run: 1279it [00:02, 937.23it/s]warmup run: 1227it [00:02, 895.76it/s]warmup run: 1301it [00:02, 934.85it/s]warmup run: 1493it [00:03, 988.55it/s]warmup run: 1259it [00:02, 838.99it/s]warmup run: 1556it [00:03, 1001.25it/s]warmup run: 1374it [00:02, 937.90it/s]warmup run: 1492it [00:03, 953.54it/s]warmup run: 1379it [00:02, 953.49it/s]warmup run: 1321it [00:02, 902.64it/s]warmup run: 1399it [00:02, 947.49it/s]warmup run: 1595it [00:03, 997.11it/s]warmup run: 1359it [00:02, 883.03it/s]warmup run: 1659it [00:03, 994.53it/s] warmup run: 1473it [00:03, 952.45it/s]warmup run: 1478it [00:03, 963.94it/s]warmup run: 1590it [00:03, 947.46it/s]warmup run: 1414it [00:03, 910.00it/s]warmup run: 1498it [00:03, 957.98it/s]warmup run: 1697it [00:03, 998.28it/s]warmup run: 1461it [00:03, 919.94it/s]warmup run: 1760it [00:03, 986.27it/s]warmup run: 1571it [00:03, 957.20it/s]warmup run: 1577it [00:03, 970.12it/s]warmup run: 1687it [00:03, 945.59it/s]warmup run: 1507it [00:03, 915.25it/s]warmup run: 1598it [00:03, 968.72it/s]warmup run: 1798it [00:03, 1000.26it/s]warmup run: 1564it [00:03, 950.39it/s]warmup run: 1860it [00:03, 980.20it/s]warmup run: 1669it [00:03, 960.46it/s]warmup run: 1676it [00:03, 975.06it/s]warmup run: 1783it [00:03, 947.48it/s]warmup run: 1601it [00:03, 922.55it/s]warmup run: 1697it [00:03, 971.41it/s]warmup run: 1899it [00:03, 999.04it/s] warmup run: 1667it [00:03, 971.06it/s]warmup run: 1962it [00:03, 989.71it/s]warmup run: 1766it [00:03, 963.05it/s]warmup run: 1776it [00:03, 982.39it/s]warmup run: 1883it [00:03, 960.57it/s]warmup run: 1696it [00:03, 929.13it/s]warmup run: 1796it [00:03, 971.90it/s]warmup run: 2000it [00:03, 1000.88it/s]warmup run: 1769it [00:03, 984.73it/s]warmup run: 2074it [00:03, 1026.48it/s]warmup run: 1864it [00:03, 967.43it/s]warmup run: 1878it [00:03, 992.78it/s]warmup run: 1981it [00:03, 963.68it/s]warmup run: 1791it [00:03, 932.37it/s]warmup run: 1894it [00:03, 974.24it/s]warmup run: 2120it [00:03, 1059.27it/s]warmup run: 1871it [00:03, 992.48it/s]warmup run: 2192it [00:03, 1071.15it/s]warmup run: 1962it [00:03, 970.90it/s]warmup run: 1980it [00:03, 999.78it/s]warmup run: 2093it [00:03, 1007.74it/s]warmup run: 1885it [00:03, 934.54it/s]warmup run: 1999it [00:03, 994.36it/s]warmup run: 2240it [00:03, 1098.95it/s]warmup run: 1972it [00:03, 997.27it/s]warmup run: 2310it [00:03, 1102.86it/s]warmup run: 2073it [00:03, 1012.07it/s]warmup run: 2097it [00:03, 1048.51it/s]warmup run: 2211it [00:03, 1056.64it/s]warmup run: 1979it [00:03, 935.74it/s]warmup run: 2119it [00:03, 1053.68it/s]warmup run: 2360it [00:03, 1126.62it/s]warmup run: 2087it [00:03, 1041.31it/s]warmup run: 2428it [00:03, 1125.74it/s]warmup run: 2194it [00:03, 1069.02it/s]warmup run: 2217it [00:03, 1092.86it/s]warmup run: 2329it [00:03, 1092.92it/s]warmup run: 2091it [00:03, 990.59it/s]warmup run: 2240it [00:03, 1100.02it/s]warmup run: 2479it [00:03, 1144.71it/s]warmup run: 2209it [00:03, 1093.95it/s]warmup run: 2547it [00:03, 1142.78it/s]warmup run: 2315it [00:03, 1109.80it/s]warmup run: 2335it [00:03, 1117.83it/s]warmup run: 2447it [00:04, 1118.43it/s]warmup run: 2210it [00:03, 1049.15it/s]warmup run: 2361it [00:03, 1131.75it/s]warmup run: 2598it [00:04, 1157.31it/s]warmup run: 2331it [00:03, 1131.50it/s]warmup run: 2662it [00:04, 1139.60it/s]warmup run: 2436it [00:03, 1137.94it/s]warmup run: 2454it [00:03, 1138.27it/s]warmup run: 2566it [00:04, 1137.59it/s]warmup run: 2330it [00:03, 1092.64it/s]warmup run: 2481it [00:03, 1151.91it/s]warmup run: 2717it [00:04, 1166.64it/s]warmup run: 2454it [00:03, 1158.21it/s]warmup run: 2779it [00:04, 1147.58it/s]warmup run: 2557it [00:04, 1158.07it/s]warmup run: 2574it [00:04, 1154.15it/s]warmup run: 2685it [00:04, 1151.38it/s]warmup run: 2450it [00:04, 1122.83it/s]warmup run: 2602it [00:04, 1166.72it/s]warmup run: 2835it [00:04, 1170.22it/s]warmup run: 2576it [00:04, 1176.05it/s]warmup run: 2898it [00:04, 1157.70it/s]warmup run: 2678it [00:04, 1172.99it/s]warmup run: 2693it [00:04, 1163.87it/s]warmup run: 2802it [00:04, 1155.60it/s]warmup run: 2570it [00:04, 1143.42it/s]warmup run: 2723it [00:04, 1179.25it/s]warmup run: 2955it [00:04, 1176.35it/s]warmup run: 2696it [00:04, 1181.40it/s]warmup run: 3000it [00:04, 683.07it/s] warmup run: 3000it [00:04, 687.21it/s] warmup run: 2797it [00:04, 1176.39it/s]warmup run: 2811it [00:04, 1166.88it/s]warmup run: 2919it [00:04, 1159.14it/s]warmup run: 2689it [00:04, 1155.77it/s]warmup run: 2843it [00:04, 1184.89it/s]warmup run: 2815it [00:04, 1180.83it/s]warmup run: 3000it [00:04, 668.79it/s] warmup run: 2916it [00:04, 1179.05it/s]warmup run: 2932it [00:04, 1177.15it/s]warmup run: 2807it [00:04, 1162.92it/s]warmup run: 2964it [00:04, 1191.33it/s]warmup run: 3000it [00:04, 684.96it/s] warmup run: 3000it [00:04, 681.74it/s] warmup run: 2934it [00:04, 1170.63it/s]warmup run: 3000it [00:04, 677.68it/s] warmup run: 2925it [00:04, 1166.47it/s]warmup run: 3000it [00:04, 674.69it/s] warmup run: 3000it [00:04, 664.36it/s] 
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]




warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   5%|         | 160/3000 [00:00<00:01, 1599.16it/s]warmup should be done:   5%|         | 162/3000 [00:00<00:01, 1616.86it/s]warmup should be done:   6%|         | 165/3000 [00:00<00:01, 1646.61it/s]warmup should be done:   6%|         | 165/3000 [00:00<00:01, 1643.55it/s]warmup should be done:   6%|         | 165/3000 [00:00<00:01, 1643.84it/s]warmup should be done:   5%|         | 161/3000 [00:00<00:01, 1602.09it/s]warmup should be done:   4%|         | 114/3000 [00:00<00:02, 1136.79it/s]warmup should be done:   5%|         | 163/3000 [00:00<00:01, 1621.70it/s]warmup should be done:  11%|         | 325/3000 [00:00<00:01, 1622.87it/s]warmup should be done:  11%|         | 322/3000 [00:00<00:01, 1607.31it/s]warmup should be done:  11%|         | 331/3000 [00:00<00:01, 1649.64it/s]warmup should be done:  11%|         | 332/3000 [00:00<00:01, 1655.63it/s]warmup should be done:  11%|         | 324/3000 [00:00<00:01, 1615.11it/s]warmup should be done:   9%|         | 281/3000 [00:00<00:01, 1448.14it/s]warmup should be done:  11%|         | 331/3000 [00:00<00:01, 1648.92it/s]warmup should be done:  11%|         | 327/3000 [00:00<00:01, 1627.26it/s]warmup should be done:  16%|        | 486/3000 [00:00<00:01, 1616.69it/s]warmup should be done:  15%|        | 448/3000 [00:00<00:01, 1547.14it/s]warmup should be done:  17%|        | 496/3000 [00:00<00:01, 1645.79it/s]warmup should be done:  17%|        | 496/3000 [00:00<00:01, 1645.62it/s]warmup should be done:  17%|        | 498/3000 [00:00<00:01, 1652.62it/s]warmup should be done:  16%|        | 483/3000 [00:00<00:01, 1601.92it/s]warmup should be done:  16%|        | 491/3000 [00:00<00:01, 1630.51it/s]warmup should be done:  16%|        | 488/3000 [00:00<00:01, 1616.74it/s]warmup should be done:  22%|       | 648/3000 [00:00<00:01, 1616.55it/s]warmup should be done:  20%|        | 614/3000 [00:00<00:01, 1590.51it/s]warmup should be done:  22%|       | 664/3000 [00:00<00:01, 1651.88it/s]warmup should be done:  22%|       | 661/3000 [00:00<00:01, 1643.04it/s]warmup should be done:  22%|       | 661/3000 [00:00<00:01, 1642.69it/s]warmup should be done:  22%|       | 655/3000 [00:00<00:01, 1628.12it/s]warmup should be done:  21%|       | 644/3000 [00:00<00:01, 1598.01it/s]warmup should be done:  22%|       | 650/3000 [00:00<00:01, 1611.36it/s]warmup should be done:  27%|       | 810/3000 [00:00<00:01, 1616.84it/s]warmup should be done:  26%|       | 781/3000 [00:00<00:01, 1617.42it/s]warmup should be done:  28%|       | 830/3000 [00:00<00:01, 1650.78it/s]warmup should be done:  28%|       | 826/3000 [00:00<00:01, 1640.86it/s]warmup should be done:  28%|       | 826/3000 [00:00<00:01, 1640.09it/s]warmup should be done:  27%|       | 804/3000 [00:00<00:01, 1595.92it/s]warmup should be done:  27%|       | 812/3000 [00:00<00:01, 1609.05it/s]warmup should be done:  27%|       | 818/3000 [00:00<00:01, 1620.18it/s]warmup should be done:  32%|      | 947/3000 [00:00<00:01, 1631.26it/s]warmup should be done:  32%|      | 972/3000 [00:00<00:01, 1614.88it/s]warmup should be done:  33%|      | 996/3000 [00:00<00:01, 1648.71it/s]warmup should be done:  33%|      | 991/3000 [00:00<00:01, 1636.27it/s]warmup should be done:  33%|      | 991/3000 [00:00<00:01, 1636.59it/s]warmup should be done:  32%|      | 964/3000 [00:00<00:01, 1589.87it/s]warmup should be done:  32%|      | 973/3000 [00:00<00:01, 1604.61it/s]warmup should be done:  33%|      | 981/3000 [00:00<00:01, 1613.97it/s]warmup should be done:  37%|      | 1112/3000 [00:00<00:01, 1636.25it/s]warmup should be done:  38%|      | 1134/3000 [00:00<00:01, 1611.42it/s]warmup should be done:  39%|      | 1161/3000 [00:00<00:01, 1645.58it/s]warmup should be done:  37%|      | 1124/3000 [00:00<00:01, 1590.68it/s]warmup should be done:  38%|      | 1155/3000 [00:00<00:01, 1631.23it/s]warmup should be done:  38%|      | 1134/3000 [00:00<00:01, 1600.61it/s]warmup should be done:  38%|      | 1155/3000 [00:00<00:01, 1629.10it/s]warmup should be done:  38%|      | 1143/3000 [00:00<00:01, 1606.53it/s]warmup should be done:  43%|     | 1279/3000 [00:00<00:01, 1644.08it/s]warmup should be done:  43%|     | 1296/3000 [00:00<00:01, 1606.17it/s]warmup should be done:  44%|     | 1326/3000 [00:00<00:01, 1645.32it/s]warmup should be done:  43%|     | 1287/3000 [00:00<00:01, 1600.66it/s]warmup should be done:  44%|     | 1319/3000 [00:00<00:01, 1629.60it/s]warmup should be done:  43%|     | 1295/3000 [00:00<00:01, 1598.73it/s]warmup should be done:  44%|     | 1318/3000 [00:00<00:01, 1624.81it/s]warmup should be done:  43%|     | 1304/3000 [00:00<00:01, 1603.61it/s]warmup should be done:  48%|     | 1445/3000 [00:00<00:00, 1647.86it/s]warmup should be done:  50%|     | 1491/3000 [00:00<00:00, 1645.21it/s]warmup should be done:  49%|     | 1457/3000 [00:00<00:00, 1603.34it/s]warmup should be done:  48%|     | 1450/3000 [00:00<00:00, 1606.85it/s]warmup should be done:  49%|     | 1481/3000 [00:00<00:00, 1625.90it/s]warmup should be done:  49%|     | 1483/3000 [00:00<00:00, 1629.91it/s]warmup should be done:  49%|     | 1456/3000 [00:00<00:00, 1598.90it/s]warmup should be done:  49%|     | 1466/3000 [00:00<00:00, 1606.01it/s]warmup should be done:  54%|    | 1611/3000 [00:01<00:00, 1650.59it/s]warmup should be done:  55%|    | 1656/3000 [00:01<00:00, 1646.32it/s]warmup should be done:  54%|    | 1618/3000 [00:01<00:00, 1604.13it/s]warmup should be done:  54%|    | 1613/3000 [00:01<00:00, 1611.11it/s]warmup should be done:  55%|    | 1644/3000 [00:01<00:00, 1626.33it/s]warmup should be done:  55%|    | 1646/3000 [00:01<00:00, 1629.56it/s]warmup should be done:  54%|    | 1616/3000 [00:01<00:00, 1598.09it/s]warmup should be done:  54%|    | 1631/3000 [00:01<00:00, 1616.96it/s]warmup should be done:  59%|    | 1777/3000 [00:01<00:00, 1652.20it/s]warmup should be done:  61%|    | 1821/3000 [00:01<00:00, 1646.31it/s]warmup should be done:  59%|    | 1780/3000 [00:01<00:00, 1608.27it/s]warmup should be done:  59%|    | 1775/3000 [00:01<00:00, 1612.92it/s]warmup should be done:  60%|    | 1809/3000 [00:01<00:00, 1629.27it/s]warmup should be done:  60%|    | 1807/3000 [00:01<00:00, 1626.39it/s]warmup should be done:  59%|    | 1776/3000 [00:01<00:00, 1596.58it/s]warmup should be done:  60%|    | 1796/3000 [00:01<00:00, 1624.76it/s]warmup should be done:  65%|   | 1943/3000 [00:01<00:00, 1649.70it/s]warmup should be done:  66%|   | 1986/3000 [00:01<00:00, 1647.01it/s]warmup should be done:  65%|   | 1942/3000 [00:01<00:00, 1611.52it/s]warmup should be done:  65%|   | 1937/3000 [00:01<00:00, 1613.88it/s]warmup should be done:  66%|   | 1972/3000 [00:01<00:00, 1628.61it/s]warmup should be done:  66%|   | 1970/3000 [00:01<00:00, 1626.26it/s]warmup should be done:  65%|   | 1936/3000 [00:01<00:00, 1595.81it/s]warmup should be done:  65%|   | 1960/3000 [00:01<00:00, 1627.41it/s]warmup should be done:  70%|   | 2108/3000 [00:01<00:00, 1649.70it/s]warmup should be done:  70%|   | 2104/3000 [00:01<00:00, 1611.77it/s]warmup should be done:  70%|   | 2099/3000 [00:01<00:00, 1612.81it/s]warmup should be done:  71%|   | 2136/3000 [00:01<00:00, 1629.42it/s]warmup should be done:  71%|   | 2134/3000 [00:01<00:00, 1628.12it/s]warmup should be done:  70%|   | 2097/3000 [00:01<00:00, 1597.80it/s]warmup should be done:  72%|  | 2151/3000 [00:01<00:00, 1628.80it/s]warmup should be done:  71%|   | 2124/3000 [00:01<00:00, 1629.22it/s]warmup should be done:  76%|  | 2274/3000 [00:01<00:00, 1650.04it/s]warmup should be done:  76%|  | 2266/3000 [00:01<00:00, 1607.22it/s]warmup should be done:  75%|  | 2261/3000 [00:01<00:00, 1613.46it/s]warmup should be done:  77%|  | 2300/3000 [00:01<00:00, 1630.07it/s]warmup should be done:  77%|  | 2298/3000 [00:01<00:00, 1629.42it/s]warmup should be done:  75%|  | 2257/3000 [00:01<00:00, 1596.37it/s]warmup should be done:  76%|  | 2287/3000 [00:01<00:00, 1628.75it/s]warmup should be done:  77%|  | 2314/3000 [00:01<00:00, 1511.39it/s]warmup should be done:  81%| | 2440/3000 [00:01<00:00, 1649.64it/s]warmup should be done:  81%|  | 2423/3000 [00:01<00:00, 1612.49it/s]warmup should be done:  82%| | 2461/3000 [00:01<00:00, 1626.67it/s]warmup should be done:  81%|  | 2427/3000 [00:01<00:00, 1600.62it/s]warmup should be done:  82%| | 2464/3000 [00:01<00:00, 1627.44it/s]warmup should be done:  81%|  | 2417/3000 [00:01<00:00, 1593.35it/s]warmup should be done:  82%| | 2450/3000 [00:01<00:00, 1627.84it/s]warmup should be done:  83%| | 2479/3000 [00:01<00:00, 1550.34it/s]warmup should be done:  87%| | 2605/3000 [00:01<00:00, 1649.22it/s]warmup should be done:  86%| | 2585/3000 [00:01<00:00, 1613.54it/s]warmup should be done:  88%| | 2627/3000 [00:01<00:00, 1626.67it/s]warmup should be done:  86%| | 2588/3000 [00:01<00:00, 1599.34it/s]warmup should be done:  87%| | 2624/3000 [00:01<00:00, 1622.59it/s]warmup should be done:  86%| | 2577/3000 [00:01<00:00, 1592.78it/s]warmup should be done:  87%| | 2614/3000 [00:01<00:00, 1631.19it/s]warmup should be done:  88%| | 2645/3000 [00:01<00:00, 1581.77it/s]warmup should be done:  92%|| 2770/3000 [00:01<00:00, 1639.72it/s]warmup should be done:  93%|| 2790/3000 [00:01<00:00, 1626.62it/s]warmup should be done:  92%|| 2748/3000 [00:01<00:00, 1598.78it/s]warmup should be done:  92%|| 2747/3000 [00:01<00:00, 1608.82it/s]warmup should be done:  91%|| 2738/3000 [00:01<00:00, 1595.63it/s]warmup should be done:  93%|| 2780/3000 [00:01<00:00, 1638.85it/s]warmup should be done:  93%|| 2787/3000 [00:01<00:00, 1593.65it/s]warmup should be done:  94%|| 2809/3000 [00:01<00:00, 1598.25it/s]warmup should be done:  98%|| 2936/3000 [00:01<00:00, 1644.80it/s]warmup should be done:  98%|| 2955/3000 [00:01<00:00, 1632.83it/s]warmup should be done:  97%|| 2910/3000 [00:01<00:00, 1603.09it/s]warmup should be done:  97%|| 2909/3000 [00:01<00:00, 1610.17it/s]warmup should be done:  97%|| 2899/3000 [00:01<00:00, 1599.51it/s]warmup should be done:  98%|| 2948/3000 [00:01<00:00, 1649.59it/s]warmup should be done:  98%|| 2951/3000 [00:01<00:00, 1604.96it/s]warmup should be done:  99%|| 2976/3000 [00:01<00:00, 1616.81it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1632.22it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1627.70it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1625.03it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1623.87it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1617.78it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1607.02it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1606.68it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1600.69it/s]




warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]

warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   6%|         | 167/3000 [00:00<00:01, 1669.04it/s]warmup should be done:   6%|         | 165/3000 [00:00<00:01, 1646.03it/s]warmup should be done:   6%|         | 170/3000 [00:00<00:01, 1695.53it/s]warmup should be done:   6%|         | 171/3000 [00:00<00:01, 1704.73it/s]warmup should be done:   6%|         | 170/3000 [00:00<00:01, 1693.47it/s]warmup should be done:   6%|         | 169/3000 [00:00<00:01, 1683.09it/s]warmup should be done:   6%|         | 169/3000 [00:00<00:01, 1680.82it/s]warmup should be done:   5%|         | 164/3000 [00:00<00:01, 1632.25it/s]warmup should be done:  11%|         | 335/3000 [00:00<00:01, 1675.25it/s]warmup should be done:  11%|        | 341/3000 [00:00<00:01, 1702.20it/s]warmup should be done:  11%|        | 338/3000 [00:00<00:01, 1686.49it/s]warmup should be done:  11%|        | 342/3000 [00:00<00:01, 1704.56it/s]warmup should be done:  11%|         | 330/3000 [00:00<00:01, 1644.23it/s]warmup should be done:  11%|         | 329/3000 [00:00<00:01, 1637.42it/s]warmup should be done:  11%|        | 338/3000 [00:00<00:01, 1678.88it/s]warmup should be done:  11%|        | 340/3000 [00:00<00:01, 1660.47it/s]warmup should be done:  17%|        | 503/3000 [00:00<00:01, 1676.52it/s]warmup should be done:  17%|        | 512/3000 [00:00<00:01, 1705.46it/s]warmup should be done:  17%|        | 513/3000 [00:00<00:01, 1706.05it/s]warmup should be done:  17%|        | 496/3000 [00:00<00:01, 1647.48it/s]warmup should be done:  17%|        | 507/3000 [00:00<00:01, 1683.00it/s]warmup should be done:  16%|        | 494/3000 [00:00<00:01, 1638.73it/s]warmup should be done:  17%|        | 507/3000 [00:00<00:01, 1680.67it/s]warmup should be done:  17%|        | 510/3000 [00:00<00:01, 1677.48it/s]warmup should be done:  23%|       | 684/3000 [00:00<00:01, 1709.16it/s]warmup should be done:  23%|       | 685/3000 [00:00<00:01, 1709.48it/s]warmup should be done:  22%|       | 662/3000 [00:00<00:01, 1651.43it/s]warmup should be done:  23%|       | 676/3000 [00:00<00:01, 1682.13it/s]warmup should be done:  23%|       | 677/3000 [00:00<00:01, 1685.44it/s]warmup should be done:  22%|       | 658/3000 [00:00<00:01, 1634.41it/s]warmup should be done:  22%|       | 671/3000 [00:00<00:01, 1663.33it/s]warmup should be done:  23%|       | 682/3000 [00:00<00:01, 1690.80it/s]warmup should be done:  29%|       | 856/3000 [00:00<00:01, 1709.48it/s]warmup should be done:  29%|       | 857/3000 [00:00<00:01, 1714.16it/s]warmup should be done:  28%|       | 847/3000 [00:00<00:01, 1689.83it/s]warmup should be done:  28%|       | 828/3000 [00:00<00:01, 1649.47it/s]warmup should be done:  28%|       | 846/3000 [00:00<00:01, 1686.10it/s]warmup should be done:  27%|       | 824/3000 [00:00<00:01, 1641.21it/s]warmup should be done:  28%|       | 838/3000 [00:00<00:01, 1659.53it/s]warmup should be done:  28%|       | 854/3000 [00:00<00:01, 1697.94it/s]warmup should be done:  34%|      | 1027/3000 [00:00<00:01, 1707.62it/s]warmup should be done:  34%|      | 1030/3000 [00:00<00:01, 1716.52it/s]warmup should be done:  34%|      | 1018/3000 [00:00<00:01, 1696.21it/s]warmup should be done:  33%|      | 993/3000 [00:00<00:01, 1647.63it/s]warmup should be done:  33%|      | 991/3000 [00:00<00:01, 1649.18it/s]warmup should be done:  34%|      | 1015/3000 [00:00<00:01, 1682.82it/s]warmup should be done:  34%|      | 1005/3000 [00:00<00:01, 1661.77it/s]warmup should be done:  34%|      | 1025/3000 [00:00<00:01, 1699.20it/s]warmup should be done:  40%|      | 1202/3000 [00:00<00:01, 1715.34it/s]warmup should be done:  40%|      | 1198/3000 [00:00<00:01, 1704.30it/s]warmup should be done:  40%|      | 1189/3000 [00:00<00:01, 1700.34it/s]warmup should be done:  39%|      | 1158/3000 [00:00<00:01, 1647.23it/s]warmup should be done:  39%|      | 1158/3000 [00:00<00:01, 1654.78it/s]warmup should be done:  39%|      | 1172/3000 [00:00<00:01, 1662.81it/s]warmup should be done:  39%|      | 1184/3000 [00:00<00:01, 1678.81it/s]warmup should be done:  40%|      | 1195/3000 [00:00<00:01, 1696.87it/s]warmup should be done:  46%|     | 1375/3000 [00:00<00:00, 1718.63it/s]warmup should be done:  45%|     | 1361/3000 [00:00<00:00, 1704.48it/s]warmup should be done:  46%|     | 1370/3000 [00:00<00:00, 1706.62it/s]warmup should be done:  44%|     | 1323/3000 [00:00<00:01, 1646.46it/s]warmup should be done:  44%|     | 1324/3000 [00:00<00:01, 1655.00it/s]warmup should be done:  45%|     | 1339/3000 [00:00<00:00, 1663.06it/s]warmup should be done:  45%|     | 1353/3000 [00:00<00:00, 1680.86it/s]warmup should be done:  46%|     | 1365/3000 [00:00<00:00, 1692.21it/s]warmup should be done:  52%|    | 1547/3000 [00:00<00:00, 1718.29it/s]warmup should be done:  51%|    | 1541/3000 [00:00<00:00, 1706.43it/s]warmup should be done:  50%|     | 1488/3000 [00:00<00:00, 1647.41it/s]warmup should be done:  51%|     | 1533/3000 [00:00<00:00, 1706.34it/s]warmup should be done:  50%|     | 1491/3000 [00:00<00:00, 1656.66it/s]warmup should be done:  50%|     | 1506/3000 [00:00<00:00, 1661.85it/s]warmup should be done:  51%|     | 1522/3000 [00:00<00:00, 1680.21it/s]warmup should be done:  51%|     | 1535/3000 [00:00<00:00, 1686.69it/s]warmup should be done:  57%|    | 1719/3000 [00:01<00:00, 1716.42it/s]warmup should be done:  57%|    | 1712/3000 [00:01<00:00, 1706.69it/s]warmup should be done:  57%|    | 1704/3000 [00:01<00:00, 1706.73it/s]warmup should be done:  55%|    | 1658/3000 [00:01<00:00, 1658.09it/s]warmup should be done:  56%|    | 1691/3000 [00:01<00:00, 1681.73it/s]warmup should be done:  56%|    | 1673/3000 [00:01<00:00, 1657.63it/s]warmup should be done:  55%|    | 1653/3000 [00:01<00:00, 1629.32it/s]warmup should be done:  57%|    | 1704/3000 [00:01<00:00, 1683.87it/s]warmup should be done:  63%|   | 1891/3000 [00:01<00:00, 1717.01it/s]warmup should be done:  62%|   | 1875/3000 [00:01<00:00, 1707.62it/s]warmup should be done:  63%|   | 1884/3000 [00:01<00:00, 1708.35it/s]warmup should be done:  61%|    | 1824/3000 [00:01<00:00, 1658.17it/s]warmup should be done:  62%|   | 1860/3000 [00:01<00:00, 1682.51it/s]warmup should be done:  61%|    | 1819/3000 [00:01<00:00, 1637.60it/s]warmup should be done:  61%|   | 1839/3000 [00:01<00:00, 1654.48it/s]warmup should be done:  62%|   | 1873/3000 [00:01<00:00, 1683.52it/s]warmup should be done:  69%|   | 2063/3000 [00:01<00:00, 1716.69it/s]warmup should be done:  68%|   | 2046/3000 [00:01<00:00, 1706.27it/s]warmup should be done:  69%|   | 2056/3000 [00:01<00:00, 1709.53it/s]warmup should be done:  66%|   | 1990/3000 [00:01<00:00, 1655.70it/s]warmup should be done:  68%|   | 2029/3000 [00:01<00:00, 1682.52it/s]warmup should be done:  66%|   | 1984/3000 [00:01<00:00, 1640.51it/s]warmup should be done:  67%|   | 2005/3000 [00:01<00:00, 1648.35it/s]warmup should be done:  68%|   | 2042/3000 [00:01<00:00, 1682.62it/s]warmup should be done:  74%|  | 2227/3000 [00:01<00:00, 1707.81it/s]warmup should be done:  72%|  | 2156/3000 [00:01<00:00, 1654.54it/s]warmup should be done:  74%|  | 2217/3000 [00:01<00:00, 1695.74it/s]warmup should be done:  74%|  | 2235/3000 [00:01<00:00, 1703.37it/s]warmup should be done:  73%|  | 2198/3000 [00:01<00:00, 1680.16it/s]warmup should be done:  72%|  | 2149/3000 [00:01<00:00, 1643.05it/s]warmup should be done:  72%|  | 2170/3000 [00:01<00:00, 1639.49it/s]warmup should be done:  74%|  | 2211/3000 [00:01<00:00, 1675.63it/s]warmup should be done:  80%|  | 2398/3000 [00:01<00:00, 1707.37it/s]warmup should be done:  80%|  | 2406/3000 [00:01<00:00, 1703.64it/s]warmup should be done:  77%|  | 2322/3000 [00:01<00:00, 1652.99it/s]warmup should be done:  80%|  | 2387/3000 [00:01<00:00, 1690.31it/s]warmup should be done:  77%|  | 2315/3000 [00:01<00:00, 1645.68it/s]warmup should be done:  79%|  | 2367/3000 [00:01<00:00, 1679.04it/s]warmup should be done:  78%|  | 2335/3000 [00:01<00:00, 1641.60it/s]warmup should be done:  79%|  | 2379/3000 [00:01<00:00, 1675.68it/s]warmup should be done:  86%| | 2570/3000 [00:01<00:00, 1709.39it/s]warmup should be done:  86%| | 2577/3000 [00:01<00:00, 1702.51it/s]warmup should be done:  83%| | 2488/3000 [00:01<00:00, 1650.63it/s]warmup should be done:  85%| | 2536/3000 [00:01<00:00, 1680.86it/s]warmup should be done:  83%| | 2481/3000 [00:01<00:00, 1647.38it/s]warmup should be done:  85%| | 2557/3000 [00:01<00:00, 1687.90it/s]warmup should be done:  83%| | 2500/3000 [00:01<00:00, 1643.98it/s]warmup should be done:  85%| | 2548/3000 [00:01<00:00, 1678.11it/s]warmup should be done:  91%|| 2742/3000 [00:01<00:00, 1711.33it/s]warmup should be done:  92%|| 2749/3000 [00:01<00:00, 1706.70it/s]warmup should be done:  88%| | 2646/3000 [00:01<00:00, 1648.12it/s]warmup should be done:  90%| | 2705/3000 [00:01<00:00, 1682.08it/s]warmup should be done:  91%| | 2726/3000 [00:01<00:00, 1686.58it/s]warmup should be done:  88%| | 2654/3000 [00:01<00:00, 1645.44it/s]warmup should be done:  89%| | 2665/3000 [00:01<00:00, 1641.58it/s]warmup should be done:  91%| | 2716/3000 [00:01<00:00, 1676.63it/s]warmup should be done:  97%|| 2914/3000 [00:01<00:00, 1710.57it/s]warmup should be done:  97%|| 2922/3000 [00:01<00:00, 1710.82it/s]warmup should be done:  94%|| 2811/3000 [00:01<00:00, 1647.86it/s]warmup should be done:  96%|| 2874/3000 [00:01<00:00, 1681.06it/s]warmup should be done:  94%|| 2819/3000 [00:01<00:00, 1643.64it/s]warmup should be done:  94%|| 2830/3000 [00:01<00:00, 1641.60it/s]warmup should be done:  96%|| 2884/3000 [00:01<00:00, 1673.14it/s]warmup should be done:  96%|| 2895/3000 [00:01<00:00, 1649.79it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1710.69it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1708.13it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1682.28it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1681.69it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1681.50it/s]warmup should be done:  99%|| 2977/3000 [00:01<00:00, 1650.29it/s]warmup should be done:  99%|| 2984/3000 [00:01<00:00, 1644.70it/s]warmup should be done: 100%|| 2995/3000 [00:01<00:00, 1644.03it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1652.39it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1648.37it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1645.63it/s]
WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f7b43d8b0a0>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f7b43d7b100>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f7b43d892b0>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f7b43d7b0a0>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f7b43d8b1c0>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f7b43d7e310>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f7b440bee80>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f7b440c09d0>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
2022-12-12 00:41:14.908695: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f767b028240 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 00:41:14.908757: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 00:41:14.919150: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 00:41:15.033605: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f767b02cda0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 00:41:15.033672: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 00:41:15.036390: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f766e82f460 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 00:41:15.036453: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 00:41:15.043145: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 00:41:15.045802: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 00:41:15.476439: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f7676833d60 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 00:41:15.476502: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 00:41:15.486023: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 00:41:15.537570: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f767b0305a0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 00:41:15.537640: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 00:41:15.545265: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 00:41:15.545832: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f767a799140 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 00:41:15.545890: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 00:41:15.555399: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 00:41:15.618970: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f767282f4f0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 00:41:15.619039: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 00:41:15.628711: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 00:41:15.655017: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f7676833a60 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 00:41:15.655086: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 00:41:15.664537: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 00:41:21.912746: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 00:41:22.036388: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 00:41:22.066279: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 00:41:22.279180: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 00:41:22.345561: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 00:41:22.432063: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 00:41:22.496219: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 00:41:22.622169: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
[HCTR][00:42:20.958][ERROR][RK0][main]: replica 7 reaches 1000, calling init pre replica
[HCTR][00:42:20.958][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][00:42:20.967][ERROR][RK0][main]: coll ps creation done
[HCTR][00:42:20.967][ERROR][RK0][main]: replica 7 waits for coll ps creation barrier
[HCTR][00:42:20.979][ERROR][RK0][tid #140147232323328]: replica 4 reaches 1000, calling init pre replica
[HCTR][00:42:20.980][ERROR][RK0][tid #140147232323328]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][00:42:20.985][ERROR][RK0][tid #140147232323328]: coll ps creation done
[HCTR][00:42:20.985][ERROR][RK0][tid #140147232323328]: replica 4 waits for coll ps creation barrier
[HCTR][00:42:20.990][ERROR][RK0][tid #140147425257216]: replica 2 reaches 1000, calling init pre replica
[HCTR][00:42:20.990][ERROR][RK0][tid #140147425257216]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][00:42:20.994][ERROR][RK0][tid #140147232323328]: replica 3 reaches 1000, calling init pre replica
[HCTR][00:42:20.994][ERROR][RK0][tid #140147232323328]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][00:42:20.995][ERROR][RK0][tid #140147425257216]: coll ps creation done
[HCTR][00:42:20.995][ERROR][RK0][tid #140147425257216]: replica 2 waits for coll ps creation barrier
[HCTR][00:42:21.002][ERROR][RK0][tid #140147232323328]: coll ps creation done
[HCTR][00:42:21.002][ERROR][RK0][tid #140147232323328]: replica 3 waits for coll ps creation barrier
[HCTR][00:42:21.094][ERROR][RK0][main]: replica 5 reaches 1000, calling init pre replica
[HCTR][00:42:21.094][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][00:42:21.102][ERROR][RK0][main]: coll ps creation done
[HCTR][00:42:21.102][ERROR][RK0][main]: replica 5 waits for coll ps creation barrier
[HCTR][00:42:21.115][ERROR][RK0][tid #140147760801536]: replica 6 reaches 1000, calling init pre replica
[HCTR][00:42:21.115][ERROR][RK0][tid #140147760801536]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][00:42:21.122][ERROR][RK0][tid #140147760801536]: coll ps creation done
[HCTR][00:42:21.122][ERROR][RK0][tid #140147760801536]: replica 6 waits for coll ps creation barrier
[HCTR][00:42:21.148][ERROR][RK0][main]: replica 1 reaches 1000, calling init pre replica
[HCTR][00:42:21.148][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][00:42:21.153][ERROR][RK0][main]: coll ps creation done
[HCTR][00:42:21.153][ERROR][RK0][main]: replica 1 waits for coll ps creation barrier
[HCTR][00:42:21.266][ERROR][RK0][main]: replica 0 reaches 1000, calling init pre replica
[HCTR][00:42:21.267][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][00:42:21.274][ERROR][RK0][main]: coll ps creation done
[HCTR][00:42:21.274][ERROR][RK0][main]: replica 0 waits for coll ps creation barrier
[HCTR][00:42:21.274][ERROR][RK0][main]: replica 0 preparing frequency
[HCTR][00:42:22.127][ERROR][RK0][main]: replica 0 preparing frequency done
[HCTR][00:42:22.160][ERROR][RK0][main]: replica 0 calling init per replica
[HCTR][00:42:22.160][ERROR][RK0][main]: replica 1 calling init per replica
[HCTR][00:42:22.160][ERROR][RK0][tid #140147232323328]: replica 4 calling init per replica
[HCTR][00:42:22.160][ERROR][RK0][tid #140147760801536]: replica 6 calling init per replica
[HCTR][00:42:22.160][ERROR][RK0][tid #140147232323328]: replica 3 calling init per replica
[HCTR][00:42:22.160][ERROR][RK0][main]: replica 7 calling init per replica
[HCTR][00:42:22.160][ERROR][RK0][main]: replica 5 calling init per replica
[HCTR][00:42:22.160][ERROR][RK0][tid #140147425257216]: replica 2 calling init per replica
[HCTR][00:42:22.160][ERROR][RK0][main]: Calling build_v2
[HCTR][00:42:22.160][ERROR][RK0][main]: Calling build_v2
[HCTR][00:42:22.160][ERROR][RK0][tid #140147232323328]: Calling build_v2
[HCTR][00:42:22.160][ERROR][RK0][tid #140147760801536]: Calling build_v2
[HCTR][00:42:22.160][ERROR][RK0][tid #140147232323328]: Calling build_v2
[HCTR][00:42:22.160][ERROR][RK0][main]: Calling build_v2
[HCTR][00:42:22.160][ERROR][RK0][main]: Calling build_v2
[HCTR][00:42:22.160][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][00:42:22.160][ERROR][RK0][tid #140147425257216]: Calling build_v2
[HCTR][00:42:22.160][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][00:42:22.160][ERROR][RK0][tid #140147232323328]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][00:42:22.160][ERROR][RK0][tid #140147760801536]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][00:42:22.160][ERROR][RK0][tid #140147232323328]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][00:42:22.160][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][00:42:22.160][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][00:42:22.160][ERROR][RK0][tid #140147425257216]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[[[[[2022-12-12 00:42:22[2022-12-12 00:42:22[[.2022-12-12 00:42:222022-12-12 00:42:222022-12-12 00:42:22.1607312022-12-12 00:42:222022-12-12 00:42:22...2022-12-12 00:42:22160731: ..160746160752160752.: E160746160758: : : 160755E : : EEE:  /hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.ccEE   E/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc:  /hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc:136] using concurrent impl MPS
/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc :136/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc::/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc136] ::136136:] using concurrent impl MPS136136] ] 136using concurrent impl MPS
] ] using concurrent impl MPSusing concurrent impl MPS] 
using concurrent impl MPSusing concurrent impl MPS

using concurrent impl MPS


[2022-12-12 00:42:22.165228: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:178] v100x8, slow pcie
[2022-12-12 00:42:22.165266: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:196] assigning 8 to cpu
[2022-12-12 00:42:22.165325: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:212] build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8
[2022-12-12 00:42:22.165365: [E2022-12-12 00:42:22 ./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc165367:: 213E]  remote time is 8.68421/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
[:2022-12-12 00:42:22178[.] 2022-12-12 00:42:22165415v100x8, slow pcie.: 
165430E: [ E2022-12-12 00:42:22/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc [.:/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 00:42:22165467[178:.: 2022-12-12 00:42:22] 214165470E.v100x8, slow pcie] :  [165510
cpu time is 97.0588E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 00:42:22: 
[ :.E[2022-12-12 00:42:22/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc196165559 2022-12-12 00:42:22.:] : /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.165597[178assigning 8 to cpuE:165609: 2022-12-12 00:42:22] 
 178: E.v100x8, slow pcie/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] E 165651
:v100x8, slow pcie /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: 178
[[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:E] 2022-12-12 00:42:22[2022-12-12 00:42:22:196 v100x8, slow pcie.2022-12-12 00:42:22.178] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
165757.165760] assigning 8 to cpu:: 165782[: v100x8, slow pcie
178E: 2022-12-12 00:42:22E
]  E. v100x8, slow pcie/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[ 165858/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
[2022-12-12 00:42:22.165948: E:2022-12-12 00:42:22: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:[ 196.E:2122022-12-12 00:42:22/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] 165914 196] .:assigning 8 to cpu: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8165939196
E:assigning 8 to cpu
: ]  196
[Eassigning 8 to cpu[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] 2022-12-12 00:42:22 
2022-12-12 00:42:22:assigning 8 to cpu[./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.196
2022-12-12 00:42:22166080:166073] .: 212[: assigning 8 to cpu166114E] [2022-12-12 00:42:22E
:  build asymm link desc with 8X Tesla V100-SXM2-32GB out of 82022-12-12 00:42:22. E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
.166148/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc :[166161: :/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[2132022-12-12 00:42:22: E212:2022-12-12 00:42:22] .E ] 212.remote time is 8.68421166210 /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccbuild asymm link desc with 8X Tesla V100-SXM2-32GB out of 8] 166224
: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:
build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8: E[[:212
E 2022-12-12 00:42:222022-12-12 00:42:22212]  /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.[.] build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:1663262022-12-12 00:42:22166345build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8
:212: .: 
213] [E166372E] build asymm link desc with 8X Tesla V100-SXM2-32GB out of 82022-12-12 00:42:22[ :  remote time is 8.68421
.2022-12-12 00:42:22/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
166448.:[ :: 166471[2142022-12-12 00:42:22/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc213E: 2022-12-12 00:42:22] .:]  E.cpu time is 97.0588166522213remote time is 8.68421/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc 166536
: ] 
:/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: Eremote time is 8.68421213:[E 
] 2132022-12-12 00:42:22 /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccremote time is 8.68421] [./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:
remote time is 8.684212022-12-12 00:42:22166624:213
.[: 214] 1666722022-12-12 00:42:22E[] remote time is 8.68421: . 2022-12-12 00:42:22cpu time is 97.0588
E166712/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[.
 : :2022-12-12 00:42:22166733/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE214.: : ] 166781E214/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cccpu time is 97.0588:  ] :
E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cccpu time is 97.0588214 :
] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc214cpu time is 97.0588:] 
214cpu time is 97.0588] 
cpu time is 97.0588
[2022-12-12 00:43:39.714700: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:118] solver created. now build & solve
[2022-12-12 00:43:39.754956: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:123] solver built. now solve
[2022-12-12 00:43:39.755025: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:919] num_cached_nodes = 3999999
[2022-12-12 00:43:39.885545: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:127] solver solved
[2022-12-12 00:43:39.885635: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:195] 0 solved master
[2022-12-12 00:43:39.885668: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 0 solved
[2022-12-12 00:43:39.885701: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 0 initing device 0
[2022-12-12 00:43:39.886128: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 00:43:39.887110: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 00:43:39.887963: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 00:43:39.900938: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 3 solved
[2022-12-12 00:43:39.901016: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 3 initing device 3
[2022-12-12 00:43:39.901185: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 1 solved
[2022-12-12 00:43:39.901249: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 1 initing device 1
[2022-12-12 00:43:39.901416: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 00:43:39.901484: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 7 solved
[2022-12-12 00:43:39.901544: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 7 initing device 7
[2022-12-12 00:43:39.901639: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 00:43:39.901942: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 00:43:39.902334: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 2 solved
[2022-12-12 00:43:39.902390: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 2 initing device 2
[2022-12-12 00:43:39.902474: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 5 solved
[2022-12-12 00:43:39.902536: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 5 initing device 5
[2022-12-12 00:43:39.902770: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 00:43:39.902917: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 00:43:39.903784: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 6 solved
[2022-12-12 00:43:39.903838: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] [worker 0 thread 6 initing device 62022-12-12 00:43:39
.903844: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 4 solved
[2022-12-12 00:43:39.903910: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 4 initing device 4
[2022-12-12 00:43:39.904241: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 00:43:39.904299: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 00:43:39.904506: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 00:43:39.905779: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 00:43:39.906528: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 00:43:39.906651: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 00:43:39.907216: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 00:43:39.908465: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 00:43:39.908992: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 00:43:39.909180: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 00:43:39.910317: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 00:43:39.910967: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 00:43:39.911077: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 00:43:39.911712: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 00:43:39.912865: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 00:43:39.913401: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 00:43:39.965554: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 2.00 Bytes
[2022-12-12 00:43:39.965931: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1024.00 Bytes
[2022-12-12 00:43:39.971061: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-12 00:43:39.971148: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 2
[2022-12-12 00:43:39.971194: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 00:43:39.971972: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 15.64 MB
[2022-12-12 00:43:39.972443: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:43:39.973400: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:43:39.973488: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 00:43:39.974165: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 00:43:39.974206: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1.91 GB
[[2022-12-12 00:43:392022-12-12 00:43:39..996263996263: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::19801980] ] eager alloc mem 2.00 Byteseager alloc mem 2.00 Bytes

[[2022-12-12 00:43:392022-12-12 00:43:39..996645996646: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::19801980] ] eager alloc mem 1024.00 Byteseager alloc mem 1024.00 Bytes

[[[[[2022-12-12 00:43:402022-12-12 00:43:402022-12-12 00:43:402022-12-12 00:43:402022-12-12 00:43:40.....   489   489   489   489   512: : : : : EEEEE     /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:::::19801980198019801980] ] ] ] ] eager alloc mem 2.00 Byteseager alloc mem 2.00 Byteseager alloc mem 2.00 Byteseager alloc mem 2.00 Byteseager alloc mem 2.00 Bytes




[[2022-12-12 00:43:402022-12-12 00:43:40..[   919[   9192022-12-12 00:43:40: [2022-12-12 00:43:40: .E2022-12-12 00:43:40.E   935 .   937 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu   943: /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE:: E: 1980E 1980/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu]  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu] :eager alloc mem 1024.00 Bytes/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:eager alloc mem 1024.00 Bytes1980
:1980
] 1980] eager alloc mem 1024.00 Bytes] eager alloc mem 1024.00 Bytes
eager alloc mem 1024.00 Bytes

[2022-12-12 00:43:40.  5956: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-12 00:43:40.  6022: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 2
[2022-12-12 00:43:40.  6033: E [/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc2022-12-12 00:43:40:.638  6062] : eager release cuda mem 1024E
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 00:43:40.  6110: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 2
[2022-12-12 00:43:40.  6152: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 00:43:40.  7200: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 15.64 MB
[2022-12-12 00:43:40.  7429: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-12 00:43:40.  7507: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 2
[2022-12-12 00:43:40.  7550: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 00:43:40.  7564: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-12 00:43:40.  7637[: 2022-12-12 00:43:40E.   7630/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc: :E638 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cceager release cuda mem 2:
638] eager release cuda mem 1024
[2022-12-12 00:43:40.  7704: E [/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc2022-12-12 00:43:40:.638  7715] : eager release cuda mem 400000000E
 [/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc2022-12-12 00:43:40:.638  7727] : eager release cuda mem 2E
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-12 00:43:40.  7789: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000[
2022-12-12 00:43:40.  7811: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc[:2022-12-12 00:43:40638.]   7819eager release cuda mem 2: 
E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-12 00:43:40.  7887: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:[6382022-12-12 00:43:40] .eager release cuda mem 400000000  7902
: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 2
[2022-12-12 00:43:40.  7939: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu[:2022-12-12 00:43:401980.]   7962eager alloc mem 15.64 MB: 
E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 00:43:40.  9645: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 15.64 MB
[2022-12-12 00:43:40. 10192: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 15.64 MB
[2022-12-12 00:43:40. 10692: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 15.64 MB
[2022-12-12 00:43:40. 11217: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 15.64 MB
[2022-12-12 00:43:40. 11389: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:43:40. 11839: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 15.64 MB
[2022-12-12 00:43:40. 12277: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:43:40. 12330: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:43:40. 12413: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 00:43:40. 13015: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:43:40. 13083: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 00:43:40. 13124: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980[] 2022-12-12 00:43:40eager alloc mem 1.91 GB.
 13130: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:43:40. 13196: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] [eager alloc mem 611.00 KB2022-12-12 00:43:40[
.2022-12-12 00:43:40 13228.:  13227E:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc :/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu638:] 1980eager release cuda mem 625663] 
eager alloc mem 611.00 KB
[2022-12-12 00:43:40. 13327: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:43:40. 13377: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 00:43:40. 13965: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[[2022-12-12 00:43:402022-12-12 00:43:40.. 14046 14049: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::[63819802022-12-12 00:43:40] ] .eager release cuda mem 25855eager alloc mem 25.25 KB 14099

: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663[
2022-12-12 00:43:40[.2022-12-12 00:43:40 14161.:  14170E:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu :/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc1980[:] [2022-12-12 00:43:40638eager alloc mem 1.91 GB2022-12-12 00:43:40.] 
. 14227eager release cuda mem 625663 14242: 
: E[E 2022-12-12 00:43:40 /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu./hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc: 14281:1980: 638] E] eager alloc mem 25.25 KB eager release cuda mem 625663
/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc
[:2022-12-12 00:43:40638.]  14340eager release cuda mem 625663: 
E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 00:43:40. 14399: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB[
2022-12-12 00:43:40. 14420: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 00:43:40. 14794: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 00:43:40. 14832: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1.91 GB
[2022-12-12 00:43:40. 14989: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 00:43:40.[ 150212022-12-12 00:43:40: .E 15029 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccE: 638/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu] :eager release cuda mem 258551980
] eager alloc mem 1.91 GB
[2022-12-12 00:43:40. 15072[: 2022-12-12 00:43:40E.  15081/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc[: :2022-12-12 00:43:40E638. ]  15095/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cueager release cuda mem 25855: :
E1980 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cceager alloc mem 1.91 GB:
638] [eager release cuda mem 258552022-12-12 00:43:40
. 15156: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1.91 GB[
2022-12-12 00:43:40. 15181: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1.91 GB
[[[[[[[[2022-12-12 00:43:402022-12-12 00:43:402022-12-12 00:43:402022-12-12 00:43:402022-12-12 00:43:402022-12-12 00:43:402022-12-12 00:43:402022-12-12 00:43:40........393131393131393132393132393131393138393138393144: : : : : : : : EEEEEEEE        /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::::::::19801980198019801980198019801980] ] ] ] ] ] ] ] eager alloc mem 611.00 KBeager alloc mem 611.00 KBeager alloc mem 611.00 KBeager alloc mem 611.00 KBeager alloc mem 611.00 KBeager alloc mem 611.00 KBeager alloc mem 611.00 KBeager alloc mem 611.00 KB







[[[2022-12-12 00:43:402022-12-12 00:43:40[[2022-12-12 00:43:40.[.2022-12-12 00:43:402022-12-12 00:43:40.3942192022-12-12 00:43:40[[394221..394222: .2022-12-12 00:43:402022-12-12 00:43:40: 394228394228: E394240..E: : E : 394248394248 EE /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccE: : /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc: EE:/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc  638::638] :/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc] 638638] eager release cuda mem 625663638::eager release cuda mem 625663] ] eager release cuda mem 625663
] 638638
eager release cuda mem 625663eager release cuda mem 625663
eager release cuda mem 625663] ] 


eager release cuda mem 625663eager release cuda mem 625663

[2022-12-12 00:43:40.394497: E[ 2022-12-12 00:43:40/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu.:[39450919802022-12-12 00:43:40: [[[] [.[E2022-12-12 00:43:402022-12-12 00:43:402022-12-12 00:43:40eager alloc mem 611.00 KB2022-12-12 00:43:403945192022-12-12 00:43:40 ...
.: ./hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu394529394533394534394541E394542:: : : :  : 1980EEEE/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE]     : eager alloc mem 611.00 KB/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu1980/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu
::::] :1980198019801980eager alloc mem 611.00 KB1980] ] ] ] 
] eager alloc mem 611.00 KBeager alloc mem 611.00 KBeager alloc mem 611.00 KBeager alloc mem 611.00 KBeager alloc mem 611.00 KB




[2022-12-12 00:43:40.395346: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:43:40.395421: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:43:40.395481: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:43:40.395537: E [/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc2022-12-12 00:43:40:.638[395552] 2022-12-12 00:43:40[[[[: eager release cuda mem 625663.2022-12-12 00:43:402022-12-12 00:43:402022-12-12 00:43:402022-12-12 00:43:40E
395559.... : 395562395564395562395568/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE: : : : [: EEEE2022-12-12 00:43:401980/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc    .] :/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc395659eager alloc mem 611.00 KB638::::: 
] 638638638638Eeager release cuda mem 625663] ] ] ]  
eager release cuda mem 625663eager release cuda mem 625663eager release cuda mem 625663eager release cuda mem 625663/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu



:1980] eager alloc mem 611.00 KB
[2022-12-12 00:43:40.395821: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB[[[
[2022-12-12 00:43:402022-12-12 00:43:402022-12-12 00:43:402022-12-12 00:43:40....395839395839395841395846: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
: : : EEE   /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:::198019801980] ] ] eager alloc mem 611.00 KBeager alloc mem 611.00 KBeager alloc mem 611.00 KB


[2022-12-12 00:43:40.396171: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:43:40.396240: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:43:40.396465: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:43:40[.2022-12-12 00:43:40396534.: 396535E:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu :/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc1980:] 638eager alloc mem 611.00 KB] 
eager release cuda mem 625663
[2022-12-12 00:43:40.396592: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:43:40.396627: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:43:40.396647: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc[:2022-12-12 00:43:40638.] 396665eager release cuda mem 625663: 
E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[[[2022-12-12 00:43:402022-12-12 00:43:402022-12-12 00:43:40[...2022-12-12 00:43:40396744396746396745.: : : 396758EEE:    E/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc :::/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu638638638:] ] ] 1980eager release cuda mem 625663eager release cuda mem 625663eager release cuda mem 625663] 
eager alloc mem 611.00 KB


[2022-12-12 00:43:40[[.2022-12-12 00:43:402022-12-12 00:43:40396947..: 396951[396950E: 2022-12-12 00:43:40:  E.E/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu 396983 :/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu: /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu1980:E:] 1980 1980eager alloc mem 611.00 KB] /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc] 
eager alloc mem 611.00 KB:eager alloc mem 611.00 KB
638
] eager release cuda mem 625663
[2022-12-12 00:43:40.397186: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:43:40.397307: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[[2022-12-12 00:43:402022-12-12 00:43:40..397376397376: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::1980638] ] eager alloc mem 611.00 KBeager release cuda mem 625663

[2022-12-12 00:43:40.397454: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:[6382022-12-12 00:43:40] .eager release cuda mem 625663397471
: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:43:40.397533: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:43:40.397635: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:43:40.397704: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:43:40.397838: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc[:2022-12-12 00:43:40[638.2022-12-12 00:43:40] 397852.eager release cuda mem 625663: 397859
E:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc :/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc638:] 638eager release cuda mem 625663] [
eager release cuda mem 6256632022-12-12 00:43:40
.[3979322022-12-12 00:43:40: .E397951 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccE: 638/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu] :eager release cuda mem 6256631980[
] 2022-12-12 00:43:40[eager alloc mem 611.00 KB.2022-12-12 00:43:40
397985.: 397995E:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu :[/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu19802022-12-12 00:43:40:] .1980eager alloc mem 611.00 KB398039] 
: eager alloc mem 611.00 KBE
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:43:40.398150: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:43:40.398217: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:[19802022-12-12 00:43:40] .eager alloc mem 611.00 KB398231
: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:43:40.398288: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:[6382022-12-12 00:43:40] .eager release cuda mem 625663398306
: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:43:40.398364: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:43:40.398451: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:43:40.398520: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:43:40.398744: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:43:40.[3988102022-12-12 00:43:40[: .2022-12-12 00:43:40E[398812. 2022-12-12 00:43:40: 398819/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu.E: :398832 E1980: E/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc ]  :/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cceager alloc mem 611.00 KB/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc638:
:] 638638eager release cuda mem 625663] ] 
eager release cuda mem 625663eager release cuda mem 625663

[2022-12-12 00:43:40.398971: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:43:40.399007: [E[2022-12-12 00:43:40 2022-12-12 00:43:40./hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu.[399016:3990182022-12-12 00:43:40: 1980[: .E] 2022-12-12 00:43:40E399042 eager alloc mem 611.00 KB. : /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu
399059/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE:: :[ 1980E19802022-12-12 00:43:40/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu]  ] .:eager alloc mem 611.00 KB/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cceager alloc mem 611.00 KB3991211980
:
: ] 638Eeager alloc mem 611.00 KB]  
eager release cuda mem 625663/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc
:638] eager release cuda mem 625663
[2022-12-12 00:43:40.399261: [E[2022-12-12 00:43:40 2022-12-12 00:43:40./hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu.399268:399272: 1980: E] E eager alloc mem 611.00 KB /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc
/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::6381980] ] eager release cuda mem 625663eager alloc mem 611.00 KB

[2022-12-12 00:43:40.399428: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:43:40.399663: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:43:40.399731: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:43:40.399844: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:43:40.[3999152022-12-12 00:43:40: .[E3999182022-12-12 00:43:40[ : .2022-12-12 00:43:40/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE399927.: : 3999371980/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccE: ] : Eeager alloc mem 611.00 KB638/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc 
] :/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cceager release cuda mem 625663638:
] 638eager release cuda mem 625663] 
eager release cuda mem 625663
[2022-12-12 00:43:40[.2022-12-12 00:43:40[400073.2022-12-12 00:43:40: 400077.E: [400079 E2022-12-12 00:43:40: /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc [.E:/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc2022-12-12 00:43:40400094 638:.: /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc] 638400113E:eager release cuda mem 16399996] :  638[
eager release cuda mem 16399996E/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu] 2022-12-12 00:43:40
 :eager release cuda mem 625663./hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc1980
400177:] : 638[eager alloc mem 611.00 KBE] 2022-12-12 00:43:40
 eager release cuda mem 625663./hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc
400261:: 638E[]  2022-12-12 00:43:40eager release cuda mem 625663/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc.
:400322638: ] Eeager release cuda mem 16399996 [
/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc2022-12-12 00:43:40:.638400366] : eager release cuda mem 16399996E
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 16399996
[2022-12-12 00:43:40.400475: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:43:40.400512: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 16399996
[2022-12-12 00:43:40.400751: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:43:40.400786: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 16399996
[2022-12-12 00:43:40.401008: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:43:40.401044: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 16399996
[2022-12-12 00:43:40.401097: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 3999999 / 100000000 nodes ( 4.00 %~4.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 96000001 / 100000000 nodes ( 96.00 %) | 1.91 GB | 0.49686 secs 
[2022-12-12 00:43:40.401578: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 3999999 / 100000000 nodes ( 4.00 %~4.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 96000001 / 100000000 nodes ( 96.00 %) | 1.91 GB | 0.500173 secs 
[2022-12-12 00:43:40.402235: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 3999999 / 100000000 nodes ( 4.00 %~4.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 96000001 / 100000000 nodes ( 96.00 %) | 1.91 GB | 0.499473 secs 
[2022-12-12 00:43:40.402332: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 3999999 / 100000000 nodes ( 4.00 %~4.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 96000001 / 100000000 nodes ( 96.00 %) | 1.91 GB | 0.500404 secs 
[2022-12-12 00:43:40.402999: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 3999999 / 100000000 nodes ( 4.00 %~4.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 96000001 / 100000000 nodes ( 96.00 %) | 1.91 GB | 0.498705 secs 
[2022-12-12 00:43:40.403424: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 3999999 / 100000000 nodes ( 4.00 %~4.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 96000001 / 100000000 nodes ( 96.00 %) | 1.91 GB | 0.501793 secs 
[2022-12-12 00:43:40.403836: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 3999999 / 100000000 nodes ( 4.00 %~4.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 96000001 / 100000000 nodes ( 96.00 %) | 1.91 GB | 0.500927 secs 
[2022-12-12 00:43:40.404301: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 3999999 / 100000000 nodes ( 4.00 %~4.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 96000001 / 100000000 nodes ( 96.00 %) | 1.91 GB | 0.518179 secs 
[2022-12-12 00:43:40.405119: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 10.23 GB
[2022-12-12 00:43:41.847960: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 10.49 GB
[2022-12-12 00:43:41.848174: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 10.49 GB
[2022-12-12 00:43:41.848466: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 10.49 GB
[2022-12-12 00:43:43.217340: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 10.75 GB
[2022-12-12 00:43:43.217725: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 10.75 GB
[2022-12-12 00:43:43.218580: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 10.75 GB
[2022-12-12 00:43:44.682023: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 10.97 GB
[2022-12-12 00:43:44.682204: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 10.97 GB
[2022-12-12 00:43:44.683310: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 10.97 GB
[2022-12-12 00:43:45.953638: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 11.18 GB
[2022-12-12 00:43:45.953814: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 11.18 GB
[2022-12-12 00:43:45.954197: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 11.18 GB
[2022-12-12 00:43:47.265129: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 11.64 GB
[2022-12-12 00:43:47.265524: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 11.64 GB
[2022-12-12 00:43:47.266205: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 11.64 GB
[2022-12-12 00:43:48.475536: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 11.84 GB
[2022-12-12 00:43:48.477057: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 11.84 GB
[HCTR][00:43:49.265][ERROR][RK0][tid #140147425257216]: replica 2 calling init per replica done, doing barrier
[HCTR][00:43:49.265][ERROR][RK0][tid #140147232323328]: replica 4 calling init per replica done, doing barrier
[HCTR][00:43:49.265][ERROR][RK0][main]: replica 0 calling init per replica done, doing barrier
[HCTR][00:43:49.265][ERROR][RK0][main]: replica 5 calling init per replica done, doing barrier
[HCTR][00:43:49.265][ERROR][RK0][tid #140147760801536]: replica 6 calling init per replica done, doing barrier
[HCTR][00:43:49.265][ERROR][RK0][main]: replica 7 calling init per replica done, doing barrier
[HCTR][00:43:49.265][ERROR][RK0][tid #140147232323328]: replica 3 calling init per replica done, doing barrier
[HCTR][00:43:49.265][ERROR][RK0][main]: replica 1 calling init per replica done, doing barrier
[HCTR][00:43:49.265][ERROR][RK0][tid #140147232323328]: replica 4 calling init per replica done, doing barrier done
[HCTR][00:43:49.265][ERROR][RK0][tid #140147232323328]: replica 3 calling init per replica done, doing barrier done
[HCTR][00:43:49.265][ERROR][RK0][tid #140147760801536]: replica 6 calling init per replica done, doing barrier done
[HCTR][00:43:49.265][ERROR][RK0][tid #140147425257216]: replica 2 calling init per replica done, doing barrier done
[HCTR][00:43:49.265][ERROR][RK0][main]: replica 0 calling init per replica done, doing barrier done
[HCTR][00:43:49.265][ERROR][RK0][main]: replica 5 calling init per replica done, doing barrier done
[HCTR][00:43:49.265][ERROR][RK0][tid #140147760801536]: init per replica done
[HCTR][00:43:49.265][ERROR][RK0][main]: replica 7 calling init per replica done, doing barrier done
[HCTR][00:43:49.265][ERROR][RK0][main]: replica 1 calling init per replica done, doing barrier done
[HCTR][00:43:49.265][ERROR][RK0][tid #140147232323328]: init per replica done
[HCTR][00:43:49.265][ERROR][RK0][tid #140147232323328]: init per replica done
[HCTR][00:43:49.265][ERROR][RK0][tid #140147425257216]: init per replica done
[HCTR][00:43:49.265][ERROR][RK0][main]: init per replica done
[HCTR][00:43:49.265][ERROR][RK0][main]: init per replica done
[HCTR][00:43:49.265][ERROR][RK0][main]: init per replica done
[HCTR][00:43:49.268][ERROR][RK0][main]: init per replica done
[HCTR][00:43:49.303][ERROR][RK0][main]: 2 allocated 3276800 at 0x7f598c238400
[HCTR][00:43:49.303][ERROR][RK0][main]: 2 allocated 6553600 at 0x7f598c558400
[HCTR][00:43:49.303][ERROR][RK0][main]: 2 allocated 3276800 at 0x7f598cb98400
[HCTR][00:43:49.303][ERROR][RK0][main]: 2 allocated 6553600 at 0x7f598ceb8400
[HCTR][00:43:49.304][ERROR][RK0][tid #140147358148352]: 1 allocated 3276800 at 0x7f5acc238400
[HCTR][00:43:49.304][ERROR][RK0][tid #140147358148352]: 1 allocated 6553600 at 0x7f5acc558400
[HCTR][00:43:49.304][ERROR][RK0][tid #140147358148352]: 1 allocated 3276800 at 0x7f5accb98400
[HCTR][00:43:49.304][ERROR][RK0][tid #140147358148352]: 1 allocated 6553600 at 0x7f5acceb8400
[HCTR][00:43:49.304][ERROR][RK0][main]: 5 allocated 3276800 at 0x7f5a8c238400
[HCTR][00:43:49.304][ERROR][RK0][main]: 5 allocated 6553600 at 0x7f5a8c558400
[HCTR][00:43:49.304][ERROR][RK0][main]: 5 allocated 3276800 at 0x7f5a8cb98400
[HCTR][00:43:49.304][ERROR][RK0][main]: 5 allocated 6553600 at 0x7f5a8ceb8400
[HCTR][00:43:49.304][ERROR][RK0][tid #140147232323328]: 4 allocated 3276800 at 0x7f5a58238400
[HCTR][00:43:49.304][ERROR][RK0][tid #140147232323328]: 4 allocated 6553600 at 0x7f5a58558400
[HCTR][00:43:49.304][ERROR][RK0][tid #140147232323328]: 4 allocated 3276800 at 0x7f5a58b98400
[HCTR][00:43:49.304][ERROR][RK0][tid #140147232323328]: 4 allocated 6553600 at 0x7f5a58eb8400
[HCTR][00:43:49.304][ERROR][RK0][main]: 3 allocated 3276800 at 0x7f59e8238400
[HCTR][00:43:49.304][ERROR][RK0][main]: 3 allocated 6553600 at 0x7f59e8558400
[HCTR][00:43:49.304][ERROR][RK0][main]: 3 allocated 3276800 at 0x7f59e8b98400
[HCTR][00:43:49.304][ERROR][RK0][main]: 3 allocated 6553600 at 0x7f59e8eb8400
[HCTR][00:43:49.304][ERROR][RK0][tid #140147358148352]: 7 allocated 3276800 at 0x7f5998238400
[HCTR][00:43:49.304][ERROR][RK0][tid #140147358148352]: 7 allocated 6553600 at 0x7f5998558400
[HCTR][00:43:49.304][ERROR][RK0][tid #140147358148352]: 7 allocated 3276800 at 0x7f5998b98400
[HCTR][00:43:49.304][ERROR][RK0][tid #140147358148352]: 7 allocated 6553600 at 0x7f5998eb8400
[HCTR][00:43:49.304][ERROR][RK0][main]: 6 allocated 3276800 at 0x7f5a20238400
[HCTR][00:43:49.304][ERROR][RK0][main]: 6 allocated 6553600 at 0x7f5a20558400
[HCTR][00:43:49.304][ERROR][RK0][main]: 6 allocated 3276800 at 0x7f5a20b98400
[HCTR][00:43:49.304][ERROR][RK0][main]: 6 allocated 6553600 at 0x7f5a20eb8400
[HCTR][00:43:49.307][ERROR][RK0][main]: 0 allocated 3276800 at 0x7f595c320000
[HCTR][00:43:49.307][ERROR][RK0][main]: 0 allocated 6553600 at 0x7f595c640000
[HCTR][00:43:49.307][ERROR][RK0][main]: 0 allocated 3276800 at 0x7f595cc80000
[HCTR][00:43:49.307][ERROR][RK0][main]: 0 allocated 6553600 at 0x7f595cfa0000
