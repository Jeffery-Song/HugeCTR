2022-12-12 05:48:53.012525: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.018801: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.027011: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.031748: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.043843: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.049828: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.056617: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.066788: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.120086: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.125459: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.128389: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.129345: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.130328: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.131504: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.132718: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.134364: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.135215: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.135567: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.136943: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.137023: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.138636: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.138710: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.140173: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.140564: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.141891: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.142279: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.143619: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.143945: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.145196: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.145713: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.146786: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.148122: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.149519: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.150632: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.151634: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.152674: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.153738: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.154769: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.156019: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.157773: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.158371: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.159731: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.160707: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.161643: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.162720: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.163155: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 05:48:53.163753: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.164768: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.165783: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.171202: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.171407: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.172763: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.172879: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.174264: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.174298: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.176089: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.178319: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.178898: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.180258: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.181058: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.182446: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.183603: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.185185: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.186470: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.187333: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.188931: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.189945: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.191549: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.192712: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.193427: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.194159: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.194550: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.195579: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.196408: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.197175: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.197198: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.197459: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.198648: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.211026: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.212843: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.215028: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.215046: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.216358: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.216386: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.217161: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.218122: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.218359: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.219356: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.219818: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.220648: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.221354: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.232057: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.255151: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.255880: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.256738: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.257565: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.258073: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.258334: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.259061: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.259304: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.260067: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.263165: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.263345: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.263557: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.264058: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.264538: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.265717: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.269052: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.269229: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.269355: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.269644: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.270391: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.270593: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.273088: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.273732: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.273924: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.274650: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.274919: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.277514: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.278096: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.278116: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.278636: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.281492: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.281918: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.282095: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.282555: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.284708: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.285152: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.285286: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.285502: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.287648: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.288046: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.288181: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.288642: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.290505: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.291152: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.291295: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.291562: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.293736: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.294279: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.294490: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.294736: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.296579: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.297296: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.297377: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.297612: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.299911: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.299969: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.300300: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.300345: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.302602: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.302744: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.303270: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.303935: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.305158: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.305316: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.306164: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.306496: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.306723: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.308488: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.309075: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.309856: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.310458: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.310573: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.312044: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.313339: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.313341: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.313567: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.313801: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.316511: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.316644: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.316735: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.316795: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.316934: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.317473: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.320168: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.320294: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.320446: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.320447: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.320585: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.321480: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.324008: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.324142: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.324284: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.324363: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.324474: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.324757: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.325580: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.329167: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.329219: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.329352: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.329388: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.329592: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.330378: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.333133: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 05:48:53.333235: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.333272: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.333408: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.333419: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.334360: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.336858: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.336933: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.337047: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.337531: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.337629: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.337721: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 05:48:53.340187: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.340273: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.340388: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.340984: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.341012: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.341432: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.343856: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.344015: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.344238: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.345051: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.345720: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.345907: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.348021: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.348106: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.348490: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 05:48:53.348880: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.349502: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.349668: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.351398: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 05:48:53.351441: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.351576: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.352171: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.353153: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.354899: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.355058: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.355512: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.356374: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.358326: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.358569: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.359287: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.359564: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.360014: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.363146: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.363516: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.364265: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.364894: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.365234: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.368243: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.368958: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.369267: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.369326: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.373496: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.373925: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.374593: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.378039: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.378410: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.407787: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.410581: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.411625: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.412103: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.415410: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.416326: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.416806: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.420864: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.420872: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.421126: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.427411: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.427480: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.427567: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.433502: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.433556: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.434049: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.439708: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.439744: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.443586: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 05:48:53.451991: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.472273: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.472341: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.474398: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.505677: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.505724: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.508320: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.515955: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.519301: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 05:48:53.520252: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.527430: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.560513: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 05:48:53.569507: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.587601: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.593298: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.595541: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:53.601366: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:54.535534: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:54.536302: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:54.536852: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:54.537588: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 05:48:54.537656: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-12 05:48:54.556151: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:54.556796: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:54.557297: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:54.558114: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:54.558643: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:54.559493: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-12 05:48:54.601956: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 05:48:54.602159: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 05:48:54.660529: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12340
2022-12-12 05:48:54.770426: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:54.771050: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:54.771597: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:54.772067: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 05:48:54.772123: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-12 05:48:54.773337: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:54.773952: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:54.774469: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:54.774934: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 05:48:54.774998: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-12 05:48:54.789743: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:54.790365: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:54.790967: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:54.791623: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:54.791992: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:54.792577: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:54.793165: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:54.794131: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:2/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-12 05:48:54.794407: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:54.794986: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:54.795510: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:54.795544: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:54.796907: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:1/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-12 05:48:54.796942: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:54.797474: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:54.797950: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 05:48:54.797997: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-12 05:48:54.810636: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:54.811275: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:54.811817: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:54.812563: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 05:48:54.812640: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-12 05:48:54.816033: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:54.816653: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:54.817171: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:54.817891: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:54.818417: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:54.818890: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:5/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-12 05:48:54.829346: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:54.830011: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:54.830518: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:54.831097: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:54.831653: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:54.832123: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:3/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-12 05:48:54.855201: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:54.855873: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:54.856701: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:54.857163: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 05:48:54.857221: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-12 05:48:54.859620: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:54.860234: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:54.860557: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 05:48:54.860722: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 05:48:54.860764: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:54.861223: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 05:48:54.861276: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-12 05:48:54.862592: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12345
2022-12-12 05:48:54.871671: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:54.872293: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:54.872823: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:54.873282: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 05:48:54.873336: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-12 05:48:54.874356: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 05:48:54.874552: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 05:48:54.875100: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:54.875734: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:54.876243: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:54.876405: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12343
2022-12-12 05:48:54.876818: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:54.877329: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:54.877805: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:4/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-12 05:48:54.879470: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:54.880114: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:54.880623: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:54.880882: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 05:48:54.881036: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 05:48:54.881291: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:54.881916: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:54.882389: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:6/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-12 05:48:54.882732: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12341
2022-12-12 05:48:54.884409: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 05:48:54.884542: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 05:48:54.886228: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12342
2022-12-12 05:48:54.890535: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:54.891214: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:54.891746: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:54.892322: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:54.892845: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:48:54.893310: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:7/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-12 05:48:54.919951: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 05:48:54.920151: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 05:48:54.921854: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12344
2022-12-12 05:48:54.923836: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 05:48:54.924008: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 05:48:54.925750: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12346
2022-12-12 05:48:54.935440: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 05:48:54.935609: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 05:48:54.937256: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12347
[HCTR][05:48:56.192][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][05:48:56.201][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][05:48:56.202][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][05:48:56.202][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][05:48:56.204][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][05:48:56.204][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][05:48:56.204][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][05:48:56.204][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 1it [00:01,  1.59s/it]warmup run: 1it [00:01,  1.49s/it]warmup run: 1it [00:01,  1.52s/it]warmup run: 99it [00:01, 81.10it/s]warmup run: 1it [00:01,  1.50s/it]warmup run: 1it [00:01,  1.51s/it]warmup run: 97it [00:01, 84.37it/s]warmup run: 91it [00:01, 78.07it/s]warmup run: 1it [00:01,  1.49s/it]warmup run: 199it [00:01, 177.50it/s]warmup run: 1it [00:01,  1.55s/it]warmup run: 100it [00:01, 85.94it/s]warmup run: 96it [00:01, 82.93it/s]warmup run: 1it [00:01,  1.46s/it]warmup run: 195it [00:01, 183.43it/s]warmup run: 183it [00:01, 170.08it/s]warmup run: 98it [00:01, 85.34it/s]warmup run: 300it [00:01, 285.69it/s]warmup run: 100it [00:01, 84.14it/s]warmup run: 200it [00:01, 186.12it/s]warmup run: 192it [00:01, 179.44it/s]warmup run: 103it [00:01, 91.39it/s]warmup run: 293it [00:01, 292.08it/s]warmup run: 274it [00:01, 270.07it/s]warmup run: 199it [00:01, 187.76it/s]warmup run: 401it [00:01, 399.27it/s]warmup run: 201it [00:01, 183.56it/s]warmup run: 301it [00:01, 297.56it/s]warmup run: 288it [00:01, 285.20it/s]warmup run: 204it [00:01, 194.84it/s]warmup run: 393it [00:01, 407.44it/s]warmup run: 366it [00:01, 374.86it/s]warmup run: 299it [00:01, 298.79it/s]warmup run: 502it [00:02, 510.25it/s]warmup run: 302it [00:01, 293.19it/s]warmup run: 402it [00:01, 412.55it/s]warmup run: 386it [00:01, 397.60it/s]warmup run: 306it [00:01, 309.00it/s]warmup run: 492it [00:01, 517.28it/s]warmup run: 458it [00:02, 476.33it/s]warmup run: 400it [00:01, 414.53it/s]warmup run: 605it [00:02, 616.71it/s]warmup run: 403it [00:01, 407.14it/s]warmup run: 502it [00:02, 522.41it/s]warmup run: 482it [00:02, 502.74it/s]warmup run: 408it [00:01, 425.97it/s]warmup run: 592it [00:02, 619.53it/s]warmup run: 550it [00:02, 569.01it/s]warmup run: 500it [00:01, 525.13it/s]warmup run: 707it [00:02, 707.64it/s]warmup run: 504it [00:02, 518.45it/s]warmup run: 601it [00:02, 620.63it/s]warmup run: 582it [00:02, 606.99it/s]warmup run: 511it [00:01, 540.25it/s]warmup run: 693it [00:02, 709.11it/s]warmup run: 642it [00:02, 649.52it/s]warmup run: 602it [00:02, 630.13it/s]warmup run: 809it [00:02, 783.08it/s]warmup run: 606it [00:02, 622.21it/s]warmup run: 704it [00:02, 714.26it/s]warmup run: 681it [00:02, 694.95it/s]warmup run: 614it [00:02, 644.90it/s]warmup run: 794it [00:02, 782.51it/s]warmup run: 733it [00:02, 714.17it/s]warmup run: 704it [00:02, 720.76it/s]warmup run: 910it [00:02, 841.17it/s]warmup run: 706it [00:02, 707.86it/s]warmup run: 806it [00:02, 789.89it/s]warmup run: 780it [00:02, 766.71it/s]warmup run: 716it [00:02, 731.72it/s]warmup run: 894it [00:02, 838.60it/s]warmup run: 824it [00:02, 763.83it/s]warmup run: 805it [00:02, 792.83it/s]warmup run: 1011it [00:02, 886.11it/s]warmup run: 807it [00:02, 781.44it/s]warmup run: 908it [00:02, 848.36it/s]warmup run: 878it [00:02, 821.78it/s]warmup run: 817it [00:02, 800.45it/s]warmup run: 994it [00:02, 882.38it/s]warmup run: 916it [00:02, 804.79it/s]warmup run: 907it [00:02, 850.63it/s]warmup run: 1114it [00:02, 924.40it/s]warmup run: 908it [00:02, 840.11it/s]warmup run: 1008it [00:02, 889.06it/s]warmup run: 976it [00:02, 863.84it/s]warmup run: 918it [00:02, 855.52it/s]warmup run: 1093it [00:02, 910.14it/s]warmup run: 1007it [00:02, 831.10it/s]warmup run: 1009it [00:02, 894.41it/s]warmup run: 1217it [00:02, 952.18it/s]warmup run: 1008it [00:02, 882.18it/s]warmup run: 1108it [00:02, 914.05it/s]warmup run: 1075it [00:02, 897.62it/s]warmup run: 1020it [00:02, 898.46it/s]warmup run: 1192it [00:02, 925.71it/s]warmup run: 1098it [00:02, 849.06it/s]warmup run: 1110it [00:02, 925.96it/s]warmup run: 1319it [00:02, 963.68it/s]warmup run: 1110it [00:02, 919.24it/s]warmup run: 1122it [00:02, 932.28it/s]warmup run: 1208it [00:02, 930.68it/s]warmup run: 1173it [00:02, 913.05it/s]warmup run: 1290it [00:02, 927.01it/s]warmup run: 1188it [00:02, 862.90it/s]warmup run: 1213it [00:02, 953.28it/s]warmup run: 1420it [00:03, 973.50it/s]warmup run: 1213it [00:02, 949.74it/s]warmup run: 1224it [00:02, 956.14it/s]warmup run: 1307it [00:02, 945.60it/s]warmup run: 1274it [00:02, 940.02it/s]warmup run: 1387it [00:02, 929.41it/s]warmup run: 1278it [00:02, 871.03it/s]warmup run: 1314it [00:02, 968.35it/s]warmup run: 1521it [00:03, 982.94it/s]warmup run: 1316it [00:02, 972.68it/s]warmup run: 1408it [00:02, 964.17it/s]warmup run: 1326it [00:02, 969.78it/s]warmup run: 1372it [00:02, 947.20it/s]warmup run: 1369it [00:03, 880.47it/s]warmup run: 1483it [00:03, 932.39it/s]warmup run: 1416it [00:02, 983.35it/s]warmup run: 1622it [00:03, 987.01it/s]warmup run: 1420it [00:02, 990.47it/s]warmup run: 1427it [00:02, 980.70it/s]warmup run: 1508it [00:03, 970.92it/s]warmup run: 1471it [00:03, 958.14it/s]warmup run: 1461it [00:03, 891.34it/s]warmup run: 1579it [00:03, 933.95it/s]warmup run: 1519it [00:02, 994.88it/s]warmup run: 1723it [00:03, 991.11it/s]warmup run: 1522it [00:03, 999.13it/s]warmup run: 1529it [00:02, 990.58it/s]warmup run: 1570it [00:03, 966.27it/s]warmup run: 1608it [00:03, 973.06it/s]warmup run: 1553it [00:03, 898.44it/s]warmup run: 1674it [00:03, 933.01it/s]warmup run: 1621it [00:03, 1001.10it/s]warmup run: 1824it [00:03, 993.82it/s]warmup run: 1626it [00:03, 1009.91it/s]warmup run: 1631it [00:03, 996.53it/s]warmup run: 1669it [00:03, 970.53it/s]warmup run: 1707it [00:03, 975.71it/s]warmup run: 1644it [00:03, 897.74it/s]warmup run: 1769it [00:03, 933.61it/s]warmup run: 1723it [00:03, 1001.60it/s]warmup run: 1925it [00:03, 995.47it/s]warmup run: 1731it [00:03, 1020.20it/s]warmup run: 1732it [00:03, 997.16it/s]warmup run: 1808it [00:03, 984.09it/s]warmup run: 1768it [00:03, 966.02it/s]warmup run: 1735it [00:03, 895.48it/s]warmup run: 1863it [00:03, 928.08it/s]warmup run: 1825it [00:03, 1003.42it/s]warmup run: 2030it [00:03, 1010.83it/s]warmup run: 1836it [00:03, 1026.27it/s]warmup run: 1908it [00:03, 986.23it/s]warmup run: 1833it [00:03, 991.83it/s]warmup run: 1869it [00:03, 978.99it/s]warmup run: 1825it [00:03, 890.71it/s]warmup run: 1960it [00:03, 939.35it/s]warmup run: 1927it [00:03, 998.36it/s] warmup run: 2153it [00:03, 1074.38it/s]warmup run: 1940it [00:03, 1023.17it/s]warmup run: 2010it [00:03, 995.13it/s]warmup run: 1971it [00:03, 990.46it/s]warmup run: 1933it [00:03, 978.77it/s]warmup run: 2072it [00:03, 990.87it/s]warmup run: 1915it [00:03, 837.36it/s]warmup run: 2030it [00:03, 1006.35it/s]warmup run: 2276it [00:03, 1119.79it/s]warmup run: 2050it [00:03, 1043.53it/s]warmup run: 2130it [00:03, 1054.28it/s]warmup run: 2086it [00:03, 1037.63it/s]warmup run: 2039it [00:03, 1001.88it/s]warmup run: 2195it [00:03, 1061.74it/s]warmup run: 2017it [00:03, 888.15it/s]warmup run: 2149it [00:03, 1058.61it/s]warmup run: 2399it [00:03, 1150.68it/s]warmup run: 2171it [00:03, 1091.35it/s]warmup run: 2250it [00:03, 1096.96it/s]warmup run: 2208it [00:03, 1090.16it/s]warmup run: 2161it [00:03, 1064.67it/s]warmup run: 2319it [00:03, 1112.34it/s]warmup run: 2136it [00:03, 972.90it/s]warmup run: 2268it [00:03, 1096.70it/s]warmup run: 2522it [00:04, 1173.61it/s]warmup run: 2292it [00:03, 1124.44it/s]warmup run: 2371it [00:03, 1128.15it/s]warmup run: 2330it [00:03, 1126.42it/s]warmup run: 2283it [00:03, 1108.02it/s]warmup run: 2442it [00:03, 1146.78it/s]warmup run: 2255it [00:03, 1035.17it/s]warmup run: 2387it [00:03, 1123.50it/s]warmup run: 2646it [00:04, 1190.87it/s]warmup run: 2412it [00:03, 1146.62it/s]warmup run: 2492it [00:03, 1151.22it/s]warmup run: 2452it [00:03, 1152.86it/s]warmup run: 2404it [00:03, 1137.64it/s]warmup run: 2565it [00:04, 1171.00it/s]warmup run: 2374it [00:04, 1080.61it/s]warmup run: 2507it [00:03, 1143.53it/s]warmup run: 2768it [00:04, 1199.29it/s]warmup run: 2533it [00:03, 1162.70it/s]warmup run: 2613it [00:04, 1167.34it/s]warmup run: 2574it [00:04, 1171.72it/s]warmup run: 2525it [00:03, 1158.86it/s]warmup run: 2688it [00:04, 1187.68it/s]warmup run: 2493it [00:04, 1111.15it/s]warmup run: 2627it [00:04, 1158.25it/s]warmup run: 2891it [00:04, 1207.67it/s]warmup run: 2654it [00:04, 1174.81it/s]warmup run: 2734it [00:04, 1178.24it/s]warmup run: 2696it [00:04, 1185.27it/s]warmup run: 2646it [00:03, 1173.15it/s]warmup run: 2809it [00:04, 1194.25it/s]warmup run: 2612it [00:04, 1132.75it/s]warmup run: 2747it [00:04, 1168.65it/s]warmup run: 3000it [00:04, 680.08it/s] warmup run: 2773it [00:04, 1176.64it/s]warmup run: 2853it [00:04, 1180.46it/s]warmup run: 2815it [00:04, 1184.60it/s]warmup run: 2766it [00:04, 1179.11it/s]warmup run: 2932it [00:04, 1203.37it/s]warmup run: 2730it [00:04, 1145.68it/s]warmup run: 2864it [00:04, 1167.52it/s]warmup run: 3000it [00:04, 683.86it/s] warmup run: 2893it [00:04, 1183.01it/s]warmup run: 2974it [00:04, 1187.32it/s]warmup run: 2934it [00:04, 1183.38it/s]warmup run: 2887it [00:04, 1187.80it/s]warmup run: 3000it [00:04, 687.63it/s] warmup run: 2847it [00:04, 1152.88it/s]warmup run: 3000it [00:04, 683.54it/s] warmup run: 2985it [00:04, 1177.45it/s]warmup run: 3000it [00:04, 692.94it/s] warmup run: 3000it [00:04, 686.03it/s] warmup run: 3000it [00:04, 700.44it/s] warmup run: 2968it [00:04, 1167.38it/s]warmup run: 3000it [00:04, 653.06it/s] 

warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]



warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]

warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   5%|▌         | 164/3000 [00:00<00:01, 1639.12it/s]warmup should be done:   6%|▌         | 166/3000 [00:00<00:01, 1657.46it/s]warmup should be done:   5%|▌         | 161/3000 [00:00<00:01, 1606.85it/s]warmup should be done:   5%|▌         | 164/3000 [00:00<00:01, 1635.38it/s]warmup should be done:   6%|▌         | 166/3000 [00:00<00:01, 1652.05it/s]warmup should be done:   5%|▌         | 163/3000 [00:00<00:01, 1624.65it/s]warmup should be done:   5%|▌         | 162/3000 [00:00<00:01, 1614.78it/s]warmup should be done:   5%|▌         | 163/3000 [00:00<00:01, 1621.06it/s]warmup should be done:  11%|█         | 331/3000 [00:00<00:01, 1657.19it/s]warmup should be done:  11%|█         | 333/3000 [00:00<00:01, 1663.72it/s]warmup should be done:  11%|█         | 332/3000 [00:00<00:01, 1655.71it/s]warmup should be done:  11%|█         | 330/3000 [00:00<00:01, 1648.02it/s]warmup should be done:  11%|█         | 325/3000 [00:00<00:01, 1623.27it/s]warmup should be done:  11%|█         | 327/3000 [00:00<00:01, 1630.22it/s]warmup should be done:  11%|█         | 328/3000 [00:00<00:01, 1637.02it/s]warmup should be done:  11%|█         | 325/3000 [00:00<00:01, 1620.43it/s]warmup should be done:  17%|█▋        | 498/3000 [00:00<00:01, 1662.84it/s]warmup should be done:  16%|█▋        | 495/3000 [00:00<00:01, 1645.42it/s]warmup should be done:  16%|█▋        | 488/3000 [00:00<00:01, 1622.49it/s]warmup should be done:  17%|█▋        | 498/3000 [00:00<00:01, 1651.93it/s]warmup should be done:  17%|█▋        | 500/3000 [00:00<00:01, 1659.35it/s]warmup should be done:  16%|█▋        | 491/3000 [00:00<00:01, 1627.86it/s]warmup should be done:  16%|█▋        | 488/3000 [00:00<00:01, 1617.79it/s]warmup should be done:  16%|█▋        | 492/3000 [00:00<00:01, 1629.51it/s]warmup should be done:  22%|██▏       | 666/3000 [00:00<00:01, 1666.29it/s]warmup should be done:  22%|██▏       | 660/3000 [00:00<00:01, 1646.15it/s]warmup should be done:  22%|██▏       | 666/3000 [00:00<00:01, 1656.62it/s]warmup should be done:  22%|██▏       | 664/3000 [00:00<00:01, 1650.96it/s]warmup should be done:  22%|██▏       | 651/3000 [00:00<00:01, 1618.96it/s]warmup should be done:  22%|██▏       | 655/3000 [00:00<00:01, 1629.37it/s]warmup should be done:  22%|██▏       | 655/3000 [00:00<00:01, 1628.83it/s]warmup should be done:  22%|██▏       | 650/3000 [00:00<00:01, 1613.61it/s]warmup should be done:  28%|██▊       | 833/3000 [00:00<00:01, 1663.29it/s]warmup should be done:  28%|██▊       | 825/3000 [00:00<00:01, 1642.31it/s]warmup should be done:  27%|██▋       | 818/3000 [00:00<00:01, 1626.89it/s]warmup should be done:  27%|██▋       | 819/3000 [00:00<00:01, 1629.64it/s]warmup should be done:  28%|██▊       | 830/3000 [00:00<00:01, 1645.41it/s]warmup should be done:  27%|██▋       | 812/3000 [00:00<00:01, 1608.93it/s]warmup should be done:  28%|██▊       | 832/3000 [00:00<00:01, 1645.39it/s]warmup should be done:  27%|██▋       | 813/3000 [00:00<00:01, 1596.40it/s]warmup should be done:  33%|███▎      | 982/3000 [00:00<00:01, 1629.36it/s]warmup should be done:  33%|███▎      | 1000/3000 [00:00<00:01, 1655.86it/s]warmup should be done:  33%|███▎      | 990/3000 [00:00<00:01, 1636.76it/s]warmup should be done:  33%|███▎      | 981/3000 [00:00<00:01, 1621.13it/s]warmup should be done:  33%|███▎      | 996/3000 [00:00<00:01, 1647.42it/s]warmup should be done:  32%|███▏      | 973/3000 [00:00<00:01, 1608.49it/s]warmup should be done:  33%|███▎      | 997/3000 [00:00<00:01, 1640.84it/s]warmup should be done:  32%|███▏      | 973/3000 [00:00<00:01, 1581.82it/s]warmup should be done:  38%|███▊      | 1145/3000 [00:00<00:01, 1626.46it/s]warmup should be done:  38%|███▊      | 1135/3000 [00:00<00:01, 1611.85it/s]warmup should be done:  39%|███▊      | 1161/3000 [00:00<00:01, 1645.87it/s]warmup should be done:  38%|███▊      | 1144/3000 [00:00<00:01, 1618.62it/s]warmup should be done:  39%|███▉      | 1166/3000 [00:00<00:01, 1647.13it/s]warmup should be done:  38%|███▊      | 1154/3000 [00:00<00:01, 1629.24it/s]warmup should be done:  39%|███▉      | 1163/3000 [00:00<00:01, 1644.00it/s]warmup should be done:  38%|███▊      | 1132/3000 [00:00<00:01, 1574.80it/s]warmup should be done:  44%|████▎     | 1308/3000 [00:00<00:01, 1623.65it/s]warmup should be done:  43%|████▎     | 1298/3000 [00:00<00:01, 1615.85it/s]warmup should be done:  44%|████▎     | 1307/3000 [00:00<00:01, 1619.37it/s]warmup should be done:  44%|████▍     | 1317/3000 [00:00<00:01, 1626.39it/s]warmup should be done:  44%|████▍     | 1331/3000 [00:00<00:01, 1644.16it/s]warmup should be done:  44%|████▍     | 1326/3000 [00:00<00:01, 1638.02it/s]warmup should be done:  44%|████▍     | 1328/3000 [00:00<00:01, 1641.73it/s]warmup should be done:  43%|████▎     | 1291/3000 [00:00<00:01, 1576.81it/s]warmup should be done:  49%|████▊     | 1460/3000 [00:00<00:00, 1615.80it/s]warmup should be done:  49%|████▉     | 1472/3000 [00:00<00:00, 1625.59it/s]warmup should be done:  49%|████▉     | 1469/3000 [00:00<00:00, 1616.72it/s]warmup should be done:  49%|████▉     | 1480/3000 [00:00<00:00, 1624.55it/s]warmup should be done:  50%|████▉     | 1496/3000 [00:00<00:00, 1642.25it/s]warmup should be done:  50%|████▉     | 1495/3000 [00:00<00:00, 1647.53it/s]warmup should be done:  50%|████▉     | 1490/3000 [00:00<00:00, 1634.07it/s]warmup should be done:  48%|████▊     | 1451/3000 [00:00<00:00, 1581.33it/s]warmup should be done:  55%|█████▍    | 1635/3000 [00:01<00:00, 1624.81it/s]warmup should be done:  54%|█████▍    | 1623/3000 [00:01<00:00, 1617.42it/s]warmup should be done:  54%|█████▍    | 1632/3000 [00:01<00:00, 1617.93it/s]warmup should be done:  55%|█████▍    | 1643/3000 [00:01<00:00, 1624.52it/s]warmup should be done:  55%|█████▌    | 1661/3000 [00:01<00:00, 1650.46it/s]warmup should be done:  55%|█████▌    | 1654/3000 [00:01<00:00, 1632.96it/s]warmup should be done:  55%|█████▌    | 1661/3000 [00:01<00:00, 1638.93it/s]warmup should be done:  54%|█████▎    | 1612/3000 [00:01<00:00, 1587.72it/s]warmup should be done:  60%|█████▉    | 1798/3000 [00:01<00:00, 1625.84it/s]warmup should be done:  60%|█████▉    | 1786/3000 [00:01<00:00, 1619.13it/s]warmup should be done:  60%|██████    | 1806/3000 [00:01<00:00, 1625.96it/s]warmup should be done:  60%|█████▉    | 1794/3000 [00:01<00:00, 1618.18it/s]warmup should be done:  61%|██████    | 1825/3000 [00:01<00:00, 1636.81it/s]warmup should be done:  61%|██████    | 1818/3000 [00:01<00:00, 1632.22it/s]warmup should be done:  61%|██████    | 1827/3000 [00:01<00:00, 1648.17it/s]warmup should be done:  59%|█████▉    | 1773/3000 [00:01<00:00, 1592.36it/s]warmup should be done:  65%|██████▍   | 1949/3000 [00:01<00:00, 1621.91it/s]warmup should be done:  65%|██████▌   | 1961/3000 [00:01<00:00, 1625.00it/s]warmup should be done:  66%|██████▌   | 1969/3000 [00:01<00:00, 1626.18it/s]warmup should be done:  65%|██████▌   | 1957/3000 [00:01<00:00, 1619.84it/s]warmup should be done:  66%|██████▋   | 1993/3000 [00:01<00:00, 1650.71it/s]warmup should be done:  66%|██████▋   | 1989/3000 [00:01<00:00, 1636.10it/s]warmup should be done:  66%|██████▌   | 1982/3000 [00:01<00:00, 1631.83it/s]warmup should be done:  64%|██████▍   | 1934/3000 [00:01<00:00, 1596.25it/s]warmup should be done:  71%|███████   | 2124/3000 [00:01<00:00, 1624.67it/s]warmup should be done:  70%|███████   | 2115/3000 [00:01<00:00, 1630.94it/s]warmup should be done:  71%|███████   | 2132/3000 [00:01<00:00, 1626.39it/s]warmup should be done:  71%|███████   | 2120/3000 [00:01<00:00, 1621.98it/s]warmup should be done:  72%|███████▏  | 2159/3000 [00:01<00:00, 1653.20it/s]warmup should be done:  72%|███████▏  | 2153/3000 [00:01<00:00, 1637.15it/s]warmup should be done:  72%|███████▏  | 2146/3000 [00:01<00:00, 1631.20it/s]warmup should be done:  70%|██████▉   | 2095/3000 [00:01<00:00, 1598.28it/s]warmup should be done:  76%|███████▌  | 2287/3000 [00:01<00:00, 1626.17it/s]warmup should be done:  76%|███████▌  | 2282/3000 [00:01<00:00, 1640.00it/s]warmup should be done:  76%|███████▋  | 2295/3000 [00:01<00:00, 1625.35it/s]warmup should be done:  76%|███████▌  | 2283/3000 [00:01<00:00, 1622.01it/s]warmup should be done:  77%|███████▋  | 2319/3000 [00:01<00:00, 1643.75it/s]warmup should be done:  77%|███████▋  | 2310/3000 [00:01<00:00, 1628.79it/s]warmup should be done:  78%|███████▊  | 2325/3000 [00:01<00:00, 1631.89it/s]warmup should be done:  75%|███████▌  | 2255/3000 [00:01<00:00, 1596.94it/s]warmup should be done:  82%|████████▏ | 2450/3000 [00:01<00:00, 1623.32it/s]warmup should be done:  82%|████████▏ | 2446/3000 [00:01<00:00, 1620.80it/s]warmup should be done:  82%|████████▏ | 2458/3000 [00:01<00:00, 1621.73it/s]warmup should be done:  83%|████████▎ | 2484/3000 [00:01<00:00, 1644.67it/s]warmup should be done:  82%|████████▏ | 2446/3000 [00:01<00:00, 1627.05it/s]warmup should be done:  82%|████████▏ | 2474/3000 [00:01<00:00, 1630.54it/s]warmup should be done:  83%|████████▎ | 2490/3000 [00:01<00:00, 1637.15it/s]warmup should be done:  80%|████████  | 2415/3000 [00:01<00:00, 1595.89it/s]warmup should be done:  87%|████████▋ | 2613/3000 [00:01<00:00, 1621.82it/s]warmup should be done:  87%|████████▋ | 2609/3000 [00:01<00:00, 1623.02it/s]warmup should be done:  87%|████████▋ | 2621/3000 [00:01<00:00, 1622.75it/s]warmup should be done:  87%|████████▋ | 2613/3000 [00:01<00:00, 1639.35it/s]warmup should be done:  88%|████████▊ | 2649/3000 [00:01<00:00, 1640.15it/s]warmup should be done:  88%|████████▊ | 2640/3000 [00:01<00:00, 1636.66it/s]warmup should be done:  88%|████████▊ | 2655/3000 [00:01<00:00, 1638.07it/s]warmup should be done:  86%|████████▌ | 2575/3000 [00:01<00:00, 1596.85it/s]warmup should be done:  93%|█████████▎| 2776/3000 [00:01<00:00, 1619.83it/s]warmup should be done:  92%|█████████▏| 2772/3000 [00:01<00:00, 1624.68it/s]warmup should be done:  93%|█████████▎| 2784/3000 [00:01<00:00, 1624.09it/s]warmup should be done:  93%|█████████▎| 2780/3000 [00:01<00:00, 1647.32it/s]warmup should be done:  94%|█████████▍| 2815/3000 [00:01<00:00, 1644.35it/s]warmup should be done:  94%|█████████▎| 2806/3000 [00:01<00:00, 1643.05it/s]warmup should be done:  94%|█████████▍| 2821/3000 [00:01<00:00, 1644.00it/s]warmup should be done:  91%|█████████ | 2735/3000 [00:01<00:00, 1597.24it/s]warmup should be done:  98%|█████████▊| 2937/3000 [00:01<00:00, 1631.18it/s]warmup should be done:  98%|█████████▊| 2940/3000 [00:01<00:00, 1624.26it/s]warmup should be done:  98%|█████████▊| 2950/3000 [00:01<00:00, 1632.33it/s]warmup should be done:  98%|█████████▊| 2948/3000 [00:01<00:00, 1655.60it/s]warmup should be done:  99%|█████████▉| 2982/3000 [00:01<00:00, 1651.31it/s]warmup should be done:  99%|█████████▉| 2973/3000 [00:01<00:00, 1651.03it/s]warmup should be done:  97%|█████████▋| 2897/3000 [00:01<00:00, 1601.26it/s]warmup should be done: 100%|█████████▉| 2986/3000 [00:01<00:00, 1625.52it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1646.88it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1642.27it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1641.24it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1630.04it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1628.93it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1625.74it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1623.50it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1595.98it/s]




warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]

warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   5%|▌         | 160/3000 [00:00<00:01, 1598.97it/s]warmup should be done:   6%|▌         | 170/3000 [00:00<00:01, 1697.22it/s]warmup should be done:   5%|▍         | 138/3000 [00:00<00:02, 1377.55it/s]warmup should be done:   6%|▌         | 171/3000 [00:00<00:01, 1702.56it/s]warmup should be done:   6%|▌         | 171/3000 [00:00<00:01, 1702.35it/s]warmup should be done:   6%|▌         | 165/3000 [00:00<00:01, 1641.88it/s]warmup should be done:   6%|▌         | 168/3000 [00:00<00:01, 1670.61it/s]warmup should be done:   6%|▌         | 168/3000 [00:00<00:01, 1671.09it/s]warmup should be done:  11%|█         | 321/3000 [00:00<00:01, 1602.40it/s]warmup should be done:  11%|█▏        | 341/3000 [00:00<00:01, 1701.77it/s]warmup should be done:  11%|█▏        | 342/3000 [00:00<00:01, 1702.76it/s]warmup should be done:  11%|█         | 336/3000 [00:00<00:01, 1673.90it/s]warmup should be done:  11%|█▏        | 342/3000 [00:00<00:01, 1702.90it/s]warmup should be done:  11%|█         | 330/3000 [00:00<00:01, 1639.33it/s]warmup should be done:  11%|█         | 336/3000 [00:00<00:01, 1668.52it/s]warmup should be done:   9%|▉         | 276/3000 [00:00<00:02, 1217.14it/s]warmup should be done:  17%|█▋        | 512/3000 [00:00<00:01, 1705.23it/s]warmup should be done:  16%|█▋        | 492/3000 [00:00<00:01, 1649.92it/s]warmup should be done:  17%|█▋        | 504/3000 [00:00<00:01, 1676.49it/s]warmup should be done:  17%|█▋        | 503/3000 [00:00<00:01, 1667.93it/s]warmup should be done:  17%|█▋        | 513/3000 [00:00<00:01, 1700.55it/s]warmup should be done:  16%|█▋        | 494/3000 [00:00<00:01, 1626.04it/s]warmup should be done:  17%|█▋        | 513/3000 [00:00<00:01, 1679.99it/s]warmup should be done:  15%|█▍        | 445/3000 [00:00<00:01, 1415.37it/s]warmup should be done:  22%|██▏       | 672/3000 [00:00<00:01, 1677.01it/s]warmup should be done:  22%|██▏       | 663/3000 [00:00<00:01, 1670.52it/s]warmup should be done:  23%|██▎       | 683/3000 [00:00<00:01, 1700.71it/s]warmup should be done:  23%|██▎       | 684/3000 [00:00<00:01, 1702.42it/s]warmup should be done:  22%|██▏       | 671/3000 [00:00<00:01, 1670.06it/s]warmup should be done:  22%|██▏       | 659/3000 [00:00<00:01, 1633.24it/s]warmup should be done:  23%|██▎       | 682/3000 [00:00<00:01, 1680.52it/s]warmup should be done:  20%|██        | 612/3000 [00:00<00:01, 1509.62it/s]warmup should be done:  28%|██▊       | 834/3000 [00:00<00:01, 1684.66it/s]warmup should be done:  28%|██▊       | 840/3000 [00:00<00:01, 1675.77it/s]warmup should be done:  28%|██▊       | 839/3000 [00:00<00:01, 1673.31it/s]warmup should be done:  29%|██▊       | 856/3000 [00:00<00:01, 1705.59it/s]warmup should be done:  28%|██▊       | 827/3000 [00:00<00:01, 1648.44it/s]warmup should be done:  28%|██▊       | 854/3000 [00:00<00:01, 1687.08it/s]warmup should be done:  28%|██▊       | 851/3000 [00:00<00:01, 1680.96it/s]warmup should be done:  26%|██▌       | 776/3000 [00:00<00:01, 1552.34it/s]warmup should be done:  34%|███▎      | 1005/3000 [00:00<00:01, 1692.36it/s]warmup should be done:  34%|███▎      | 1008/3000 [00:00<00:01, 1676.14it/s]warmup should be done:  34%|███▎      | 1007/3000 [00:00<00:01, 1673.14it/s]warmup should be done:  34%|███▍      | 1027/3000 [00:00<00:01, 1706.58it/s]warmup should be done:  33%|███▎      | 995/3000 [00:00<00:01, 1656.35it/s]warmup should be done:  34%|███▍      | 1023/3000 [00:00<00:01, 1682.39it/s]warmup should be done:  34%|███▍      | 1020/3000 [00:00<00:01, 1683.93it/s]warmup should be done:  31%|███▏      | 942/3000 [00:00<00:01, 1586.11it/s]warmup should be done:  39%|███▉      | 1176/3000 [00:00<00:01, 1676.49it/s]warmup should be done:  39%|███▉      | 1176/3000 [00:00<00:01, 1695.01it/s]warmup should be done:  40%|███▉      | 1198/3000 [00:00<00:01, 1704.79it/s]warmup should be done:  39%|███▉      | 1175/3000 [00:00<00:01, 1672.01it/s]warmup should be done:  39%|███▉      | 1163/3000 [00:00<00:01, 1663.38it/s]warmup should be done:  40%|███▉      | 1190/3000 [00:00<00:01, 1686.37it/s]warmup should be done:  40%|███▉      | 1192/3000 [00:00<00:01, 1678.19it/s]warmup should be done:  37%|███▋      | 1107/3000 [00:00<00:01, 1606.13it/s]warmup should be done:  45%|████▍     | 1344/3000 [00:00<00:00, 1677.20it/s]warmup should be done:  45%|████▍     | 1348/3000 [00:00<00:00, 1700.00it/s]warmup should be done:  46%|████▌     | 1370/3000 [00:00<00:00, 1708.70it/s]warmup should be done:  45%|████▍     | 1344/3000 [00:00<00:00, 1675.11it/s]warmup should be done:  44%|████▍     | 1331/3000 [00:00<00:01, 1666.70it/s]warmup should be done:  45%|████▌     | 1361/3000 [00:00<00:00, 1693.74it/s]warmup should be done:  45%|████▌     | 1360/3000 [00:00<00:00, 1678.17it/s]warmup should be done:  42%|████▏     | 1270/3000 [00:00<00:01, 1613.44it/s]warmup should be done:  50%|█████     | 1512/3000 [00:00<00:00, 1675.13it/s]warmup should be done:  51%|█████     | 1519/3000 [00:00<00:00, 1701.57it/s]warmup should be done:  50%|█████     | 1512/3000 [00:00<00:00, 1675.63it/s]warmup should be done:  51%|█████▏    | 1541/3000 [00:00<00:00, 1705.56it/s]warmup should be done:  50%|█████     | 1500/3000 [00:00<00:00, 1672.25it/s]warmup should be done:  51%|█████     | 1528/3000 [00:00<00:00, 1675.74it/s]warmup should be done:  51%|█████     | 1531/3000 [00:00<00:00, 1682.84it/s]warmup should be done:  48%|████▊     | 1436/3000 [00:00<00:00, 1627.23it/s]warmup should be done:  56%|█████▌    | 1680/3000 [00:01<00:00, 1676.45it/s]warmup should be done:  56%|█████▋    | 1691/3000 [00:01<00:00, 1705.01it/s]warmup should be done:  56%|█████▌    | 1680/3000 [00:01<00:00, 1675.12it/s]warmup should be done:  57%|█████▋    | 1712/3000 [00:01<00:00, 1693.26it/s]warmup should be done:  57%|█████▋    | 1696/3000 [00:01<00:00, 1673.66it/s]warmup should be done:  57%|█████▋    | 1700/3000 [00:01<00:00, 1684.78it/s]warmup should be done:  56%|█████▌    | 1668/3000 [00:01<00:00, 1656.36it/s]warmup should be done:  53%|█████▎    | 1604/3000 [00:01<00:00, 1642.98it/s]warmup should be done:  62%|██████▏   | 1848/3000 [00:01<00:00, 1677.39it/s]warmup should be done:  62%|██████▏   | 1863/3000 [00:01<00:00, 1708.68it/s]warmup should be done:  62%|██████▏   | 1849/3000 [00:01<00:00, 1676.73it/s]warmup should be done:  63%|██████▎   | 1882/3000 [00:01<00:00, 1690.16it/s]warmup should be done:  62%|██████▏   | 1864/3000 [00:01<00:00, 1674.40it/s]warmup should be done:  62%|██████▏   | 1870/3000 [00:01<00:00, 1689.00it/s]warmup should be done:  61%|██████▏   | 1840/3000 [00:01<00:00, 1673.82it/s]warmup should be done:  59%|█████▉    | 1772/3000 [00:01<00:00, 1651.81it/s]warmup should be done:  67%|██████▋   | 2016/3000 [00:01<00:00, 1677.33it/s]warmup should be done:  68%|██████▊   | 2035/3000 [00:01<00:00, 1710.48it/s]warmup should be done:  67%|██████▋   | 2017/3000 [00:01<00:00, 1676.50it/s]warmup should be done:  68%|██████▊   | 2052/3000 [00:01<00:00, 1688.68it/s]warmup should be done:  68%|██████▊   | 2032/3000 [00:01<00:00, 1673.91it/s]warmup should be done:  68%|██████▊   | 2040/3000 [00:01<00:00, 1690.97it/s]warmup should be done:  67%|██████▋   | 2011/3000 [00:01<00:00, 1684.72it/s]warmup should be done:  65%|██████▍   | 1940/3000 [00:01<00:00, 1658.54it/s]warmup should be done:  74%|███████▎  | 2207/3000 [00:01<00:00, 1705.69it/s]warmup should be done:  73%|███████▎  | 2185/3000 [00:01<00:00, 1674.40it/s]warmup should be done:  73%|███████▎  | 2184/3000 [00:01<00:00, 1660.74it/s]warmup should be done:  74%|███████▍  | 2221/3000 [00:01<00:00, 1685.63it/s]warmup should be done:  73%|███████▎  | 2182/3000 [00:01<00:00, 1692.11it/s]warmup should be done:  74%|███████▎  | 2210/3000 [00:01<00:00, 1689.88it/s]warmup should be done:  73%|███████▎  | 2200/3000 [00:01<00:00, 1670.93it/s]warmup should be done:  70%|███████   | 2110/3000 [00:01<00:00, 1668.80it/s]warmup should be done:  78%|███████▊  | 2353/3000 [00:01<00:00, 1675.14it/s]warmup should be done:  78%|███████▊  | 2352/3000 [00:01<00:00, 1665.99it/s]warmup should be done:  79%|███████▉  | 2378/3000 [00:01<00:00, 1699.19it/s]warmup should be done:  78%|███████▊  | 2354/3000 [00:01<00:00, 1699.56it/s]warmup should be done:  80%|███████▉  | 2390/3000 [00:01<00:00, 1683.91it/s]warmup should be done:  79%|███████▉  | 2380/3000 [00:01<00:00, 1690.66it/s]warmup should be done:  79%|███████▉  | 2368/3000 [00:01<00:00, 1670.73it/s]warmup should be done:  76%|███████▌  | 2281/3000 [00:01<00:00, 1679.35it/s]warmup should be done:  84%|████████▍ | 2521/3000 [00:01<00:00, 1675.26it/s]warmup should be done:  84%|████████▍ | 2521/3000 [00:01<00:00, 1670.23it/s]warmup should be done:  85%|████████▍ | 2548/3000 [00:01<00:00, 1695.35it/s]warmup should be done:  84%|████████▍ | 2527/3000 [00:01<00:00, 1706.04it/s]warmup should be done:  85%|████████▌ | 2560/3000 [00:01<00:00, 1685.62it/s]warmup should be done:  85%|████████▍ | 2536/3000 [00:01<00:00, 1672.19it/s]warmup should be done:  85%|████████▌ | 2551/3000 [00:01<00:00, 1693.36it/s]warmup should be done:  82%|████████▏ | 2451/3000 [00:01<00:00, 1684.01it/s]warmup should be done:  90%|████████▉ | 2689/3000 [00:01<00:00, 1674.22it/s]warmup should be done:  90%|████████▉ | 2689/3000 [00:01<00:00, 1671.71it/s]warmup should be done:  91%|█████████ | 2718/3000 [00:01<00:00, 1693.07it/s]warmup should be done:  90%|████████▉ | 2699/3000 [00:01<00:00, 1709.20it/s]warmup should be done:  90%|█████████ | 2704/3000 [00:01<00:00, 1673.25it/s]warmup should be done:  91%|█████████ | 2721/3000 [00:01<00:00, 1694.56it/s]warmup should be done:  91%|█████████ | 2729/3000 [00:01<00:00, 1684.61it/s]warmup should be done:  87%|████████▋ | 2621/3000 [00:01<00:00, 1688.29it/s]warmup should be done:  95%|█████████▌| 2857/3000 [00:01<00:00, 1672.76it/s]warmup should be done:  95%|█████████▌| 2857/3000 [00:01<00:00, 1668.72it/s]warmup should be done:  96%|█████████▋| 2888/3000 [00:01<00:00, 1694.26it/s]warmup should be done:  96%|█████████▌| 2870/3000 [00:01<00:00, 1709.16it/s]warmup should be done:  96%|█████████▋| 2891/3000 [00:01<00:00, 1693.88it/s]warmup should be done:  97%|█████████▋| 2898/3000 [00:01<00:00, 1683.06it/s]warmup should be done:  96%|█████████▌| 2872/3000 [00:01<00:00, 1670.53it/s]warmup should be done:  93%|█████████▎| 2791/3000 [00:01<00:00, 1691.01it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1692.86it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1690.69it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1689.72it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1679.88it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1677.60it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1673.70it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1671.89it/s]warmup should be done:  99%|█████████▊| 2961/3000 [00:01<00:00, 1691.19it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1620.59it/s]
WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f8a94d510d0>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f8a94d442b0>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f8a95857a30>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f8a94d42130>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f8a94d51190>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f8a95858d30>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f8a95855e80>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f8a94d421f0>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
2022-12-12 05:50:25.343334: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f85ce158370 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 05:50:25.343399: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 05:50:25.343802: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f85c302df00 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 05:50:25.343859: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 05:50:25.351911: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 05:50:25.353236: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 05:50:25.380361: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f85c7028ad0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 05:50:25.380428: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 05:50:25.389504: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 05:50:26.243000: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f85c302ced0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 05:50:26.243076: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 05:50:26.255792: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 05:50:26.264104: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f85c3029670 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 05:50:26.264165: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 05:50:26.272104: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 05:50:26.331438: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f85c3030b40 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 05:50:26.331507: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 05:50:26.339385: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 05:50:26.345901: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f85ca795ba0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 05:50:26.345960: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 05:50:26.355956: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 05:50:26.363891: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f85c70310b0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 05:50:26.363962: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 05:50:26.373837: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 05:50:32.641219: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 05:50:32.684275: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 05:50:32.719852: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 05:50:32.932970: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 05:50:33.081385: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 05:50:33.182119: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 05:50:33.398332: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 05:50:33.402222: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
[HCTR][05:51:23.700][ERROR][RK0][tid #140213451986688]: replica 5 reaches 1000, calling init pre replica
[HCTR][05:51:23.700][ERROR][RK0][tid #140213451986688]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][05:51:23.710][ERROR][RK0][tid #140213451986688]: coll ps creation done
[HCTR][05:51:23.710][ERROR][RK0][tid #140213451986688]: replica 5 waits for coll ps creation barrier
[HCTR][05:51:23.767][ERROR][RK0][main]: replica 7 reaches 1000, calling init pre replica
[HCTR][05:51:23.767][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][05:51:23.773][ERROR][RK0][main]: coll ps creation done
[HCTR][05:51:23.773][ERROR][RK0][main]: replica 7 waits for coll ps creation barrier
[HCTR][05:51:23.892][ERROR][RK0][main]: replica 4 reaches 1000, calling init pre replica
[HCTR][05:51:23.892][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][05:51:23.897][ERROR][RK0][main]: coll ps creation done
[HCTR][05:51:23.897][ERROR][RK0][main]: replica 4 waits for coll ps creation barrier
[HCTR][05:51:23.974][ERROR][RK0][main]: replica 1 reaches 1000, calling init pre replica
[HCTR][05:51:23.975][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][05:51:23.980][ERROR][RK0][main]: coll ps creation done
[HCTR][05:51:23.980][ERROR][RK0][main]: replica 1 waits for coll ps creation barrier
[HCTR][05:51:23.981][ERROR][RK0][main]: replica 3 reaches 1000, calling init pre replica
[HCTR][05:51:23.981][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][05:51:23.986][ERROR][RK0][main]: coll ps creation done
[HCTR][05:51:23.986][ERROR][RK0][main]: replica 3 waits for coll ps creation barrier
[HCTR][05:51:23.989][ERROR][RK0][tid #140212764145408]: replica 0 reaches 1000, calling init pre replica
[HCTR][05:51:23.989][ERROR][RK0][tid #140212764145408]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][05:51:23.994][ERROR][RK0][tid #140212764145408]: coll ps creation done
[HCTR][05:51:23.994][ERROR][RK0][tid #140212764145408]: replica 0 waits for coll ps creation barrier
[HCTR][05:51:24.061][ERROR][RK0][tid #140213317768960]: replica 2 reaches 1000, calling init pre replica
[HCTR][05:51:24.061][ERROR][RK0][tid #140213317768960]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][05:51:24.068][ERROR][RK0][tid #140213317768960]: coll ps creation done
[HCTR][05:51:24.068][ERROR][RK0][tid #140213317768960]: replica 2 waits for coll ps creation barrier
[HCTR][05:51:24.127][ERROR][RK0][main]: replica 6 reaches 1000, calling init pre replica
[HCTR][05:51:24.127][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][05:51:24.132][ERROR][RK0][main]: coll ps creation done
[HCTR][05:51:24.132][ERROR][RK0][main]: replica 6 waits for coll ps creation barrier
[HCTR][05:51:24.132][ERROR][RK0][tid #140212764145408]: replica 0 preparing frequency
[HCTR][05:51:25.003][ERROR][RK0][tid #140212764145408]: replica 0 preparing frequency done
[HCTR][05:51:25.045][ERROR][RK0][tid #140212764145408]: replica 0 calling init per replica
[HCTR][05:51:25.045][ERROR][RK0][tid #140213317768960]: replica 2 calling init per replica
[HCTR][05:51:25.045][ERROR][RK0][main]: replica 4 calling init per replica
[HCTR][05:51:25.045][ERROR][RK0][main]: replica 1 calling init per replica
[HCTR][05:51:25.045][ERROR][RK0][main]: replica 7 calling init per replica
[HCTR][05:51:25.045][ERROR][RK0][main]: replica 3 calling init per replica
[HCTR][05:51:25.045][ERROR][RK0][main]: replica 6 calling init per replica
[HCTR][05:51:25.045][ERROR][RK0][tid #140213451986688]: replica 5 calling init per replica
[HCTR][05:51:25.045][ERROR][RK0][tid #140212764145408]: Calling build_v2
[HCTR][05:51:25.045][ERROR][RK0][tid #140213317768960]: Calling build_v2
[HCTR][05:51:25.045][ERROR][RK0][main]: Calling build_v2
[HCTR][05:51:25.045][ERROR][RK0][main]: Calling build_v2
[HCTR][05:51:25.045][ERROR][RK0][main]: Calling build_v2
[HCTR][05:51:25.045][ERROR][RK0][tid #140213451986688]: Calling build_v2
[HCTR][05:51:25.045][ERROR][RK0][main]: Calling build_v2
[HCTR][05:51:25.045][ERROR][RK0][main]: Calling build_v2
[HCTR][05:51:25.045][ERROR][RK0][tid #140212764145408]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][05:51:25.045][ERROR][RK0][tid #140213317768960]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][05:51:25.045][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][05:51:25.045][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][05:51:25.045][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][05:51:25.045][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][05:51:25.045][ERROR][RK0][tid #140213451986688]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][05:51:25.045][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[[[[[[2022-12-12 05:51:25[[2022-12-12 05:51:252022-12-12 05:51:25.2022-12-12 05:51:252022-12-12 05:51:252022-12-12 05:51:25.2022-12-12 05:51:252022-12-12 05:51:25. 45794... 45794.. 45793:  45805 45805 45805:  45804 45804: E: : : E: : E EEE EE /hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc   /hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc  /hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc:/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc:/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc:136:::136::136] 136136136] 136136] using concurrent impl MPS] ] ] using concurrent impl MPS] ] using concurrent impl MPS
using concurrent impl MPSusing concurrent impl MPSusing concurrent impl MPS
using concurrent impl MPSusing concurrent impl MPS





[2022-12-12 05:51:25. 50114: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:178] v100x8, slow pcie
[2022-12-12 05:51:25. 50153: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[:2022-12-12 05:51:25196.]  50161assigning 8 to cpu: 
E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:178] v100x8, slow pcie
[2022-12-12 05:51:25[.2022-12-12 05:51:25 50209.:  50206E:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc :/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[196:2022-12-12 05:51:25] 178.assigning 8 to cpu] [ 50239
v100x8, slow pcie2022-12-12 05:51:25: 
.E 50250 : [/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE2022-12-12 05:51:25: .212/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc 50281] :: build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8[178[E
2022-12-12 05:51:25] 2022-12-12 05:51:25 .[v100x8, slow pcie./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[ 503012022-12-12 05:51:25
 50308:2022-12-12 05:51:25: .: 196[[.E 50345E] 2022-12-12 05:51:252022-12-12 05:51:25 50348 :  assigning 8 to cpu..: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
 50394 50387E: 2022-12-12 05:51:25:: :  178/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.212[EE/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] : 50437] 2022-12-12 05:51:25  :v100x8, slow pcie213: build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc178
] E
 50508::] remote time is 8.68421 [: 178196[v100x8, slow pcie
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 05:51:25E] ] 2022-12-12 05:51:25
:.[ v100x8, slow pcieassigning 8 to cpu.178 50607[2022-12-12 05:51:25/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc

 50624] : 2022-12-12 05:51:25.:: [v100x8, slow pcieE. 50667212E2022-12-12 05:51:25
  50689: ]  .[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: [Ebuild asymm link desc with 8X Tesla V100-SXM2-32GB out of 8/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc 507302022-12-12 05:51:25:E2022-12-12 05:51:25 
:: .196 ./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc213E 50770[] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc 50777:]  : 2022-12-12 05:51:25assigning 8 to cpu:: 214remote time is 8.68421/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE.
196E] 
:  50844]  cpu time is 97.0588196/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[: assigning 8 to cpu/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
] :2022-12-12 05:51:25E[
:assigning 8 to cpu212. 2022-12-12 05:51:25196
]  50943/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.] build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8: : 50970assigning 8 to cpu
E213[: 
 [] 2022-12-12 05:51:25E[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 05:51:25remote time is 8.68421. 2022-12-12 05:51:25:.
 51043/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.[214 51057: :[ 510642022-12-12 05:51:25] : E2122022-12-12 05:51:25: .cpu time is 97.0588E ] .E 51114
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccbuild asymm link desc with 8X Tesla V100-SXM2-32GB out of 8 51143 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:
: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE:212E[: 212]  2022-12-12 05:51:25213/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.] :build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8
: 51268remote time is 8.68421212[
214: 
] 2022-12-12 05:51:25] E[build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8.[cpu time is 97.0588 2022-12-12 05:51:25
 513442022-12-12 05:51:25
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.: .:[ 51362E 513692132022-12-12 05:51:25:  : ] .E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccEremote time is 8.68421 51409 : 
: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc213[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE:] 2022-12-12 05:51:25: 213remote time is 8.68421.214/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] 
 51485] :remote time is 8.68421: cpu time is 97.0588[213
E
2022-12-12 05:51:25]  [.remote time is 8.68421/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 05:51:25 51545
:.: 214 51572E[] :  2022-12-12 05:51:25cpu time is 97.0588E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.
 : 51599/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc214: :] E214cpu time is 97.0588 ] 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cccpu time is 97.0588:
214] cpu time is 97.0588
[2022-12-12 05:52:44.177843: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:118] solver created. now build & solve
[2022-12-12 05:52:44.217798: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 381.47 MB
[2022-12-12 05:52:44.217908: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 381.47 MB
[2022-12-12 05:52:44.219078: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:74] mapping nid to rank...
[2022-12-12 05:52:44.298494: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:91] counting slots...
[2022-12-12 05:52:44.693511: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:105] Final num slot is 49
[2022-12-12 05:52:44.693609: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:109] counting blocks...
[2022-12-12 05:52:52. 81636: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:118] Final num block is 1023
[2022-12-12 05:52:52. 81733: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:123] counting freq and density...
[2022-12-12 05:52:53.804418: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:154] averaging freq and density...
[2022-12-12 05:52:53.804515: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:155] 1023
[2022-12-12 05:52:53.807281: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:123] solver built. now solve
[2022-12-12 05:52:53.807346: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:354] constructing optimal solver, device=8, stream=1
1023 blocks, 8 devices
[2022-12-12 05:52:54.162492: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:527] Add Var...
[2022-12-12 05:52:54.190576: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:545] Capacity...
[2022-12-12 05:52:54.192070: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:548] Connect CPU...
[2022-12-12 05:52:54.212578: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:550] Connect Access To Storage...
[2022-12-12 05:52:54.733144: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:554] Time...
[2022-12-12 05:56:16. 41566: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:569] Coll Cache init block placement array
[2022-12-12 05:56:16. 50884: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:645] Coll Cache init block placement array done
[2022-12-12 05:56:16.147313: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:647] Coll Cache model reset done
[2022-12-12 05:56:16.197173: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:127] solver solved
[2022-12-12 05:56:16.197279: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:195] 0 solved master
[2022-12-12 05:56:16.197313: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 0 solved
[2022-12-12 05:56:16.197355: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 0 initing device 0
[2022-12-12 05:56:16.197952: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-12 05:56:16.198005: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 05:56:16.198940: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 05:56:16.199615: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 05:56:16.212702: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 5 solved
[2022-12-12 05:56:16.212777: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 5 initing device 5
[[2022-12-12 05:56:162022-12-12 05:56:16..213039213044: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc::202202] ] [2 solved4 solved2022-12-12 05:56:16

.213090[: [2022-12-12 05:56:16[E2022-12-12 05:56:16.2022-12-12 05:56:16 .213150./hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc213153: 213140:: E: [202E E2022-12-12 05:56:16]  /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc ./hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc6 solved:/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc213219:
205:: 205] 202[E] worker 0 thread 2 initing device 2] 2022-12-12 05:56:16 worker 0 thread 4 initing device 4
1 solved./hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu

213297:: 1815E] [ Building Coll Cache with ... num gpu device is 82022-12-12 05:56:16/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc
.:213362205: ] Eworker 0 thread 6 initing device 6 [
/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc2022-12-12 05:56:16:.205213425] : worker 0 thread 1 initing device 1E
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 05:56:16.213718: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-12 05:56:16.213745: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815[] 2022-12-12 05:56:16Building Coll Cache with ... num gpu device is 8.
213766: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 05:56:16.213799: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 05:56:16.213865: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-12 05:56:16.213921: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 05:56:16.213990: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[[2022-12-12 05:56:162022-12-12 05:56:16..214052214066: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::2021980] ] 3 solvedeager alloc mem 381.47 MB

[2022-12-12 05:56:16.214136: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 3 initing device 3
[2022-12-12 05:56:16.214147: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 7 solved
[2022-12-12 05:56:16.214199: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 7 initing device 7
[2022-12-12 05:56:16.214546: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-12 05:56:16.214590: E[ 2022-12-12 05:56:16/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu.:2145981980: ] Eeager alloc mem 381.47 MB 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-12 05:56:16.214670: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 05:56:16.218052: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 05:56:16.218200: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 05:56:16.218404: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 05:56:16.218479: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 05:56:16.218576: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 05:56:16.219042: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 05:56:16.219540: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 05:56:16.222448: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 05:56:16.222549: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 05:56:16.222659: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 05:56:16.222725: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 05:56:16.222787: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 05:56:16.222825: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 05:56:16.223316: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 05:56:16.277673: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1023.00 Bytes
[2022-12-12 05:56:16.282831: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1023
[2022-12-12 05:56:16.282922: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 05:56:16.283725: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 76.68 MB
[2022-12-12 05:56:16.284369: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 05:56:16.285459: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 05:56:16.285505: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 76.14 MB
[2022-12-12 05:56:16.303451: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1023.00 Bytes
[[2022-12-12 05:56:162022-12-12 05:56:16..303517303522: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::19801980] ] eager alloc mem 1023.00 Byteseager alloc mem 1023.00 Bytes

[[2022-12-12 05:56:162022-12-12 05:56:16..303947303947: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::19801980] ] eager alloc mem 1023.00 Byteseager alloc mem 1023.00 Bytes

[2022-12-12 05:56:16.305280: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1023.00 Bytes
[2022-12-12 05:56:16.305452: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1023.00 Bytes
[2022-12-12 05:56:16.308639: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1023
[2022-12-12 05:56:16.308715: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 05:56:16.309645: E [/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc2022-12-12 05:56:16:.638309678] : eager release cuda mem 1023E
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 76.68 MB
[2022-12-12 05:56:16.309765: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 05:56:16.309819: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1023
[2022-12-12 05:56:16[.2022-12-12 05:56:16309893.: 309913E:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc :/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc638:] 638eager release cuda mem 1023] 
eager release cuda mem 400000000
[[2022-12-12 05:56:162022-12-12 05:56:16..309979309995: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::638638] ] eager release cuda mem 1023eager release cuda mem 400000000

[2022-12-12 05:56:16.310081: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 05:56:16.310578: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1023
[2022-12-12 05:56:16.310656: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 05:56:16.310732: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1023
[2022-12-12 05:56:16[.2022-12-12 05:56:16310799.: 310809E:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu :/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc1980:] 638eager alloc mem 76.68 MB] 
eager release cuda mem 400000000
[2022-12-12 05:56:16.311210: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 05:56:16.311979: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 76.68 MB
[2022-12-12 05:56:16.312258: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 05:56:16.312303: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 76.19 MB
[2022-12-12 05:56:16.312487: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 76.68 MB
[2022-12-12 05:56:16.313115: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 76.68 MB
[2022-12-12 05:56:16.318317: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 76.68 MB
[2022-12-12 05:56:16.319403: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 76.68 MB
[2022-12-12 05:56:16.328412: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 05:56:16.328933: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 05:56:16.328969: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 05:56:16.329073: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 05:56:16.329179: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 05:56:16.329218: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 05:56:16.329448: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 05:56:16.329501: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 76.22 MB
[2022-12-12 05:56:16.329959: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 05:56:16.329995[: 2022-12-12 05:56:16E. 330007/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc: :W638 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cceager release cuda mem 625663:
43] WORKER[0] alloc host memory 76.25 MB
[2022-12-12 05:56:16.330059: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 75.96 MB
[2022-12-12 05:56:16.330101: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[[2022-12-12 05:56:162022-12-12 05:56:16..330147330144: : WE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::43638] ] WORKER[0] alloc host memory 75.91 MBeager release cuda mem 625663

[2022-12-12 05:56:16.330217: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 76.28 MB
[2022-12-12 05:56:16.330261: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 05:56:16.330307: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 76.04 MB
[2022-12-12 05:56:16.336223: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 05:56:16.336845: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 05:56:16.336897: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 9.52 GB
[2022-12-12 05:56:16.361198: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 05:56:16.361799: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 05:56:16.361840: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 9.53 GB
[2022-12-12 05:56:16.381232: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 05:56:16.381356: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 05:56:16.381456: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 05:56:16.381698: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 05:56:16.381881: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 05:56:16.381925: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 9.53 GB
[2022-12-12 05:56:16.381972: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 05:56:16.382015: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 9.51 GB
[2022-12-12 05:56:16.382065: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 05:56:16.382108: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 9.54 GB
[2022-12-12 05:56:16.382318: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 05:56:16.382361: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 9.54 GB
[2022-12-12 05:56:16.382724: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 05:56:16.383004: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 05:56:16.383333: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 05:56:16.383378: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 9.50 GB
[2022-12-12 05:56:16.383625: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 05:56:16.383670: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 9.49 GB
[[[[[[[[2022-12-12 05:56:192022-12-12 05:56:192022-12-12 05:56:192022-12-12 05:56:192022-12-12 05:56:192022-12-12 05:56:192022-12-12 05:56:192022-12-12 05:56:19........506891506892506891506891506893506892506897506891: : : : : : : : EEEEEEEE        /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::::::::19261926192619261926192619261926] ] ] ] ] ] ] ] Device 6 init p2p of link 0Device 7 init p2p of link 4Device 2 init p2p of link 1Device 3 init p2p of link 2Device 0 init p2p of link 3Device 4 init p2p of link 5Device 1 init p2p of link 7Device 5 init p2p of link 6







[2022-12-12 05:56:19.507472: [[[E2022-12-12 05:56:19[2022-12-12 05:56:19[[2022-12-12 05:56:19[ .2022-12-12 05:56:19.2022-12-12 05:56:192022-12-12 05:56:19.2022-12-12 05:56:19/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu507479.507480..507479.:: 507482: 507483507484: 5074831980E: E: : E: ]  E EE Eeager alloc mem 611.00 KB/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu 
:/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu1980:] ::1980:] 1980eager alloc mem 611.00 KB19801980] 1980eager alloc mem 611.00 KB] 
] ] eager alloc mem 611.00 KB] 
eager alloc mem 611.00 KBeager alloc mem 611.00 KBeager alloc mem 611.00 KB
eager alloc mem 611.00 KB



[2022-12-12 05:56:19.508550: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 05:56:19.508657: E[ 2022-12-12 05:56:19/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc.:[508669638[[2022-12-12 05:56:19: ] 2022-12-12 05:56:19[[2022-12-12 05:56:19.Eeager release cuda mem 625663.2022-12-12 05:56:192022-12-12 05:56:19.508673 
508697..508685: /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc: 508704508706: E:E: : E 638 EE /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc] /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:eager release cuda mem 625663:/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638
638::638] ] 638638] eager release cuda mem 625663eager release cuda mem 625663] ] eager release cuda mem 625663

eager release cuda mem 625663eager release cuda mem 625663


[2022-12-12 05:56:19.526570: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 6 init p2p of link 5
[2022-12-12 05:56:19.526732: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 05:56:19.527096: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 2
[2022-12-12 05:56:19.527272: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 05:56:19.527647: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663[
2022-12-12 05:56:19.527661: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 0 init p2p of link 6
[2022-12-12 05:56:19.527825: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 05:56:19.527938: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 7 init p2p of link 1
[2022-12-12 05:56:19.528098: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 05:56:19.528191: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 05:56:19.528472: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 5 init p2p of link 4
[2022-12-12 05:56:19.528623: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 05:56:19.528673: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 05:56:19.528847: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 2 init p2p of link 3
[2022-12-12 05:56:19.528940: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 05:56:19.529014: E [/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu2022-12-12 05:56:19:.1980529014] : eager alloc mem 611.00 KBE
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 4 init p2p of link 7
[2022-12-12 05:56:19.529202: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 05:56:19.529240: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 3 init p2p of link 0
[2022-12-12 05:56:19.529402: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 05:56:19.529505: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 05:56:19.529938: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 05:56:19.530082: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 05:56:19.530292: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 05:56:19.544971: E[ 2022-12-12 05:56:19/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu.:5449881926: ] EDevice 0 init p2p of link 1 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 7 init p2p of link 6
[2022-12-12 05:56:19.[5451172022-12-12 05:56:19: .E545122 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE: 1980/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu] :eager alloc mem 611.00 KB1980
] eager alloc mem 611.00 KB
[2022-12-12 05:56:19.545453: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 6 init p2p of link 4
[2022-12-12 05:56:19.545569: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 05:56:19.546050: E[ 2022-12-12 05:56:19/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc.:546062638: ] Eeager release cuda mem 625663 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 05:56:19.546146: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 3
[2022-12-12 05:56:19.546265: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 05:56:19.546441: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 05:56:19.546694: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 5 init p2p of link 7
[2022-12-12 05:56:19.546812: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 05:56:19.547097: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 2 init p2p of link 0
[2022-12-12 05:56:19.547168: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 05:56:19.547227: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 05:56:19.547456: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 4 init p2p of link 2
[2022-12-12 05:56:19.547577: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 05:56:19.547680: [E2022-12-12 05:56:19 ./hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu547708:: 1926E]  Device 3 init p2p of link 5/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc
:638] eager release cuda mem 625663
[2022-12-12 05:56:19.547843: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 05:56:19.548110: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 05:56:19.548418: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 05:56:19.548683: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 05:56:19.569964: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 3 init p2p of link 1
[2022-12-12 05:56:19.570085: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 05:56:19.570928: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 4 init p2p of link 6
[2022-12-12 05:56:19.570978: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 05:56:19.571044: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 05:56:19.571225: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 0 init p2p of link 2
[2022-12-12 05:56:19.571352: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 05:56:19.571821: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 6 init p2p of link 7
[2022-12-12 05:56:19.[5719342022-12-12 05:56:19: .E571937 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE: 1980/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc] :eager alloc mem 611.00 KB638
] eager release cuda mem 625663
[2022-12-12 05:56:19.572248: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 05:56:19.572347: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 0
[2022-12-12 05:56:19.572468: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] [eager alloc mem 611.00 KB2022-12-12 05:56:19
.572478: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 7 init p2p of link 5
[2022-12-12 05:56:19.572605: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 05:56:19.572695: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 2 init p2p of link 4
[2022-12-12 05:56:19.572810: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 05:56:19.572870: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 05:56:19.573347: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 05:56:19.573496: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 05:56:19.573563: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 5 init p2p of link 3
[2022-12-12 05:56:19.573659: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] [eager release cuda mem 6256632022-12-12 05:56:19
.573683: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 05:56:19.574538: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 05:56:19.593526: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 80400000
[2022-12-12 05:56:19.593985: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 19997139 / 100000000 nodes ( 20.00 %~20.00 %) | remote 54692829 / 100000000 nodes ( 54.69 %) | cpu 25310032 / 100000000 nodes ( 25.31 %) | 9.54 GB | 3.3794 secs 
[2022-12-12 05:56:19.595367: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 80400000
[2022-12-12 05:56:19.595547: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 80400000
[2022-12-12 05:56:19.596043: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 19934506 / 100000000 nodes ( 19.93 %~20.00 %) | remote 54755462 / 100000000 nodes ( 54.76 %) | cpu 25310032 / 100000000 nodes ( 25.31 %) | 9.51 GB | 3.38229 secs 
[2022-12-12 05:56:19.596349: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 19911549 / 100000000 nodes ( 19.91 %~20.00 %) | remote 54778419 / 100000000 nodes ( 54.78 %) | cpu 25310032 / 100000000 nodes ( 25.31 %) | 9.50 GB | 3.38294 secs 
[2022-12-12 05:56:19.596439: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 80400000
[2022-12-12 05:56:19.596674: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 80400000
[2022-12-12 05:56:19.597338: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 80400000
[2022-12-12 05:56:19.597474: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 80400000
[2022-12-12 05:56:19.597812: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 19900081 / 100000000 nodes ( 19.90 %~20.00 %) | remote 54789887 / 100000000 nodes ( 54.79 %) | cpu 25310032 / 100000000 nodes ( 25.31 %) | 9.49 GB | 3.38402 secs 
[2022-12-12 05:56:19[.2022-12-12 05:56:19598049.: 598058E:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc :/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu638:] 1955eager release cuda mem 80400000] 
Asymm Coll cache (policy: coll_cache_asymm_link) | local 19988464 / 100000000 nodes ( 19.99 %~20.00 %) | remote 54701504 / 100000000 nodes ( 54.70 %) | cpu 25310032 / 100000000 nodes ( 25.31 %) | 9.54 GB | 3.38401 secs 
[2022-12-12 05:56:19.599832: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 19972785 / 100000000 nodes ( 19.97 %~20.00 %) | remote 54717183 / 100000000 nodes ( 54.72 %) | cpu 25310032 / 100000000 nodes ( 25.31 %) | 9.53 GB | 3.38593 secs 
[2022-12-12 05:56:19.600140: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 19960738 / 100000000 nodes ( 19.96 %~20.00 %) | remote 54729230 / 100000000 nodes ( 54.73 %) | cpu 25310032 / 100000000 nodes ( 25.31 %) | 9.52 GB | 3.40215 secs 
[2022-12-12 05:56:19.601770: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 19980878 / 100000000 nodes ( 19.98 %~20.00 %) | remote 54709090 / 100000000 nodes ( 54.71 %) | cpu 25310032 / 100000000 nodes ( 25.31 %) | 9.53 GB | 3.38711 secs 
[2022-12-12 05:56:19.601865: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 17.84 GB
[2022-12-12 05:56:20.972456: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 18.11 GB
[2022-12-12 05:56:20.994832: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 18.11 GB
[2022-12-12 05:56:21. 17689: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 18.11 GB
[2022-12-12 05:56:22.283353: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 18.37 GB
[2022-12-12 05:56:22.283789: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 18.37 GB
[2022-12-12 05:56:22.286655: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 18.37 GB
[2022-12-12 05:56:23.395359: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 18.58 GB
[2022-12-12 05:56:23.395531: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 18.58 GB
[2022-12-12 05:56:23.395860: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 18.58 GB
[2022-12-12 05:56:24.565140: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 18.80 GB
[2022-12-12 05:56:24.566285: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 18.80 GB
[2022-12-12 05:56:24.567042: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 18.80 GB
[2022-12-12 05:56:25.649245: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 19.26 GB
[2022-12-12 05:56:25.650015: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 19.26 GB
[2022-12-12 05:56:25.650919: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 19.26 GB
[2022-12-12 05:56:27. 87730: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 19.45 GB
[2022-12-12 05:56:27. 88713: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 19.45 GB
[HCTR][05:56:28.317][ERROR][RK0][main]: replica 1 calling init per replica done, doing barrier
[HCTR][05:56:28.317][ERROR][RK0][tid #140212764145408]: replica 0 calling init per replica done, doing barrier
[HCTR][05:56:28.317][ERROR][RK0][main]: replica 3 calling init per replica done, doing barrier
[HCTR][05:56:28.317][ERROR][RK0][main]: replica 4 calling init per replica done, doing barrier
[HCTR][05:56:28.317][ERROR][RK0][tid #140213451986688]: replica 5 calling init per replica done, doing barrier
[HCTR][05:56:28.317][ERROR][RK0][main]: replica 7 calling init per replica done, doing barrier
[HCTR][05:56:28.317][ERROR][RK0][main]: replica 6 calling init per replica done, doing barrier
[HCTR][05:56:28.317][ERROR][RK0][tid #140213317768960]: replica 2 calling init per replica done, doing barrier
[HCTR][05:56:28.317][ERROR][RK0][main]: replica 6 calling init per replica done, doing barrier done
[HCTR][05:56:28.317][ERROR][RK0][main]: replica 3 calling init per replica done, doing barrier done
[HCTR][05:56:28.317][ERROR][RK0][tid #140213451986688]: replica 5 calling init per replica done, doing barrier done
[HCTR][05:56:28.317][ERROR][RK0][main]: replica 1 calling init per replica done, doing barrier done
[HCTR][05:56:28.317][ERROR][RK0][tid #140213317768960]: replica 2 calling init per replica done, doing barrier done
[HCTR][05:56:28.317][ERROR][RK0][main]: replica 7 calling init per replica done, doing barrier done
[HCTR][05:56:28.317][ERROR][RK0][tid #140213451986688]: init per replica done
[HCTR][05:56:28.317][ERROR][RK0][main]: replica 4 calling init per replica done, doing barrier done
[HCTR][05:56:28.317][ERROR][RK0][tid #140212764145408]: replica 0 calling init per replica done, doing barrier done
[HCTR][05:56:28.317][ERROR][RK0][main]: init per replica done
[HCTR][05:56:28.317][ERROR][RK0][main]: init per replica done
[HCTR][05:56:28.317][ERROR][RK0][main]: init per replica done
[HCTR][05:56:28.317][ERROR][RK0][tid #140213317768960]: init per replica done
[HCTR][05:56:28.317][ERROR][RK0][main]: init per replica done
[HCTR][05:56:28.317][ERROR][RK0][main]: init per replica done
[HCTR][05:56:28.320][ERROR][RK0][tid #140212764145408]: init per replica done
[HCTR][05:56:28.355][ERROR][RK0][tid #140213317768960]: 2 allocated 3276800 at 0x7f682e238400
[HCTR][05:56:28.355][ERROR][RK0][tid #140213317768960]: 2 allocated 6553600 at 0x7f682e558400
[HCTR][05:56:28.355][ERROR][RK0][tid #140213317768960]: 2 allocated 3276800 at 0x7f682eb98400
[HCTR][05:56:28.355][ERROR][RK0][tid #140213317768960]: 2 allocated 6553600 at 0x7f682eeb8400
[HCTR][05:56:28.356][ERROR][RK0][main]: 1 allocated 3276800 at 0x7f676c238400
[HCTR][05:56:28.356][ERROR][RK0][main]: 1 allocated 6553600 at 0x7f676c558400
[HCTR][05:56:28.356][ERROR][RK0][main]: 1 allocated 3276800 at 0x7f676cb98400
[HCTR][05:56:28.356][ERROR][RK0][main]: 1 allocated 6553600 at 0x7f676ceb8400
[HCTR][05:56:28.356][ERROR][RK0][tid #140213183551232]: 7 allocated 3276800 at 0x7f6830238400
[HCTR][05:56:28.356][ERROR][RK0][tid #140213183551232]: 7 allocated 6553600 at 0x7f6830558400
[HCTR][05:56:28.356][ERROR][RK0][tid #140213183551232]: 7 allocated 3276800 at 0x7f6830b98400
[HCTR][05:56:28.356][ERROR][RK0][tid #140213183551232]: 7 allocated 6553600 at 0x7f6830eb8400
[HCTR][05:56:28.356][ERROR][RK0][main]: 4 allocated 3276800 at 0x7f682c238400
[HCTR][05:56:28.356][ERROR][RK0][main]: 4 allocated 6553600 at 0x7f682c558400
[HCTR][05:56:28.356][ERROR][RK0][main]: 4 allocated 3276800 at 0x7f682cb98400
[HCTR][05:56:28.356][ERROR][RK0][main]: 4 allocated 6553600 at 0x7f682ceb8400
[HCTR][05:56:28.356][ERROR][RK0][tid #140213326161664]: 6 allocated 3276800 at 0x7f6748238400
[HCTR][05:56:28.356][ERROR][RK0][tid #140213326161664]: 6 allocated 6553600 at 0x7f6748558400
[HCTR][05:56:28.356][ERROR][RK0][tid #140213326161664]: 6 allocated 3276800 at 0x7f6748b98400
[HCTR][05:56:28.356][ERROR][RK0][tid #140213326161664]: 6 allocated 6553600 at 0x7f6748eb8400
[HCTR][05:56:28.356][ERROR][RK0][main]: 3 allocated 3276800 at 0x7f675c238400
[HCTR][05:56:28.356][ERROR][RK0][main]: 3 allocated 6553600 at 0x7f675c558400
[HCTR][05:56:28.356][ERROR][RK0][main]: 3 allocated 3276800 at 0x7f675cb98400
[HCTR][05:56:28.356][ERROR][RK0][main]: 3 allocated 6553600 at 0x7f675ceb8400
[HCTR][05:56:28.356][ERROR][RK0][tid #140213451986688]: 5 allocated 3276800 at 0x7f6758238400
[HCTR][05:56:28.356][ERROR][RK0][tid #140213451986688]: 5 allocated 6553600 at 0x7f6758558400
[HCTR][05:56:28.356][ERROR][RK0][tid #140213451986688]: 5 allocated 3276800 at 0x7f6758b98400
[HCTR][05:56:28.356][ERROR][RK0][tid #140213451986688]: 5 allocated 6553600 at 0x7f6758eb8400
[HCTR][05:56:28.359][ERROR][RK0][tid #140212764145408]: 0 allocated 3276800 at 0x7f6784320000
[HCTR][05:56:28.359][ERROR][RK0][tid #140212764145408]: 0 allocated 6553600 at 0x7f6784640000
[HCTR][05:56:28.359][ERROR][RK0][tid #140212764145408]: 0 allocated 3276800 at 0x7f6784c80000
[HCTR][05:56:28.359][ERROR][RK0][tid #140212764145408]: 0 allocated 6553600 at 0x7f6784fa0000
